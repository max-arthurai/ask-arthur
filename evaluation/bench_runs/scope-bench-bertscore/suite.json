{"id": "4feac22d-31c1-42a0-b252-1adca2000ebf", "name": "scope-bench-QA-with-golden", "scoring_method": {"name": "bertscore", "type": "built_in", "config": {"precision_weight": 0.1, "model_type": "microsoft/deberta-v3-base"}, "output_type": "continuous", "categories": null}, "test_cases": [{"id": "dfdd97cc-9df7-4d98-a306-0f48214d7d27", "input": "What enrichments are available in Arthur?", "reference_output": "The enrichments available in Arthur are:\\n\\n- anomaly detection\\n- hotspots\\n- explainability\\n- bias mitigation"}, {"id": "51d72f8a-c329-469e-a00d-7ab12ec66087", "input": "What model types do not allow explainability?", "reference_output": "The model types that do not allow for explainability are Object detection & token sequence (LLM) models."}, {"id": "5aba4ddc-2962-412b-9dd0-3f14954786a2", "input": "What enrichments are available for object detection models?", "reference_output": "The enrichments available for object detection models are:\\n\\n- anomaly detection"}, {"id": "c5142157-54c1-4f56-a2e5-21202a2e20fb", "input": "What enrichments are available for tabular binary classification models?", "reference_output": "The enrichments available for object detection models are:\\n\\n- anomaly detection\\n- hotspots\\n- explainability\\n- bias mitigation"}, {"id": "7fa1360f-9632-44d7-8868-6d5b45d1f1d1", "input": "Can I restore a deleted Arthur model? ", "reference_output": "If you delete an Arthur model you cannot restore it. You can re-register a new model with the same schema, but the history of your model\u2019s inferences are deleted and cannot be restored."}, {"id": "a3688edd-c062-4d5d-8542-c526075e8785", "input": "What are the two alert levels available in Arthur? ", "reference_output": "The two levels of alerts available in Arthur are Warning and Critical."}, {"id": "0c2c9c1a-7769-4c81-b695-7388ddcbbf4e", "input": "What are the additional requirements for on-prem deployment specific to the airgapped mode?", "reference_output": "The requirements for an airgapped on-prem deployment, in addition to the online on-prem deployment requirements, are:\\n\\n- An existing private container registry\\n- Existing private Python registries (PyPI, Anaconda) when the model explanation feature is enabled\\n- Access to your private container and Python registries"}, {"id": "c265c857-9311-49d2-9b89-24603aaab946", "input": "What artifacts do I need to enable explainability for my model? ", "reference_output": "To enable explainability for your Arthur model you will need:\\n\\n- A python script that wraps your models predict function\\n- your serialized model file\\n- A requirements.txt with the dependencies to support the above"}, {"id": "9d51a3ac-a7cb-4b13-92f6-a137c59d409a", "input": "Is KMS required for Arthur's backup and restore capability?", "reference_output": "No, but Arthur highly recommends that your EBS volumes are encrypted with KMS."}, {"id": "b32f4653-bad2-4180-8afc-b04bc0b53323", "input": "What Bench scorers can I use for evaluating LLMs at writing summaries?", "reference_output": "The Bench scorers that can evaluate LLMs at writing summaries are:\\n\\n- Summary Quality (summary_quality)\\n- BERT Score (bert_score)"}, {"id": "5e742e10-1dcd-43b0-9d5a-b0227a8bfc3d", "input": "What Bench scorers can I use for evaluating LLMs at answering questions?", "reference_output": "The Bench scorers that can evaluate LLMs at answering questions are:\\n\\n- Exact Match (exact_match)\\n- QA Correctness (qa_correctness)\\n- BERT Score (bert_score)"}, {"id": "0ba7dcb4-e439-418f-b09d-24a120339ddc", "input": "What Bench scorers can I use for evaluating LLMs at coding?", "reference_output": "The Bench scorers that can evaluate LLMs at coding are:\\n\\n- Python Unit Testing (python_unit_testing)"}, {"id": "05adad2b-eecc-422e-8877-1868229f1565", "input": "How do I view my Bench test results?", "reference_output": "To view your Bench test results in the local UI, run the command `bench` from the same local directory as your bench file directory"}, {"id": "e0af19dd-7279-4fbe-8a91-e87d36c31a69", "input": "What is the difference between Bench test suites and test runs?", "reference_output": "A test suite stores the input & reference output data for your testing use case along with a scorer. A test run evaluates the candidate outputs provided in the run and assigns a score to each test case."}], "created_at": "2024-03-26T15:31:36.338972", "updated_at": "2024-03-26T15:31:36.338972", "description": null, "last_run_time": "2024-03-26T15:31:59.983438", "num_runs": 32, "page": null, "page_size": null, "total_pages": null, "total_count": null}