{"id": "1d88cbdf-1a2a-4dea-8343-ffa5f2dc3a9c", "name": "claude-3-haiku-20240307+sentence-transformers_all-MiniLM-L12-v2@7", "test_suite_id": "5d0d8642-7bbf-4d7d-a744-cfdee7647a3a", "test_cases": [{"id": "9302872f-61dc-4092-add4-0b928fc05679", "output": "Based on the provided context, the following enrichments are available in Arthur:\n\n- Anomaly Detection\n- Bias Mitigation\n- Explainability\n- Hot Spots\n- Token Likelihood\n\nThe context mentions that these enrichments can be enabled when registering a model with the Arthur API.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "1a3cfeeb-c6a0-41de-9444-c609ec78bc71", "output": "Based on the context provided, the document does not mention any specific model types that do not allow explainability. The focus of the document is on model schemas within the MLOps ecosystem, and it does not discuss the explainability of different model types. The context does not contain information to answer the question \"What model types do not allow explainability?\".", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "2b285103-b671-43c0-8485-3253fadcc23e", "output": "Based on the provided context, the Arthur platform supports the following enrichments for object detection models:\n\n1. Anomaly Detection: Arthur can detect anomalies in the object detection outputs, such as unexpected objects or changes in the detected objects over time.\n2. Bias Mitigation: Arthur can help identify and mitigate biases in the object detection model, ensuring fair and unbiased performance.\n3. Explainability: Arthur provides explainability features to help understand the model's decision-making process for object detection tasks.\n\nThe context does not provide any example Python code, as the question is asking about the available enrichments rather than specific implementation details.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "640965fd-e43b-4794-9719-de37084d9221", "output": "Based on the context provided, the following enrichments are available for tabular multiclass classification models in Arthur:\n\n- Anomaly Detection\n- Hot Spots \n- Explainability\n- Bias Mitigation\n\nThe context mentions that these enrichments can be enabled for multiclass classification model types in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "16e9a74b-faca-45ac-aea5-5fbf28ea0c85", "output": "Based on the provided context, the Arthur Scope FAQ does not mention anything about alert levels. The FAQ covers topics such as using Arthur without the Python SDK, model access requirements, handling proprietary data, handling missing ground truth labels, troubleshooting SDK errors, authentication, and model types. There is no information about the available alert levels in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "45550df3-ab06-4bde-b929-4e84c7796b80", "output": "Based on the provided context, the key additional requirements for on-prem deployment in airgapped mode are:\n\n1. The Kubernetes cluster must have access to a local container registry, as the cluster will not have access to the internet to pull container images.\n2. The Kubernetes cluster must have access to a local PyPI/Conda registry to ensure optimal utilization of platform features.\n3. All required artifacts, including the KOTS CLI installer and plugins, must be downloaded and made available locally, as the cluster will not have internet access.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "c8c0a65e-7374-4abe-9d68-458a7a0d48c0", "output": "To enable explainability for your model in Arthur, you will need to provide access to your model's `predict()` function. Specifically, you need to provide the following assets required for explainability:\n\n1. The `predict()` function of your model, which takes the model inputs and returns the model outputs.\n2. Any additional dependencies or libraries required to run your model's `predict()` function.\n\nBy providing these assets, Arthur can access your model and generate explanations for the model's predictions.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "15379cb5-bba6-43d1-8b2f-80b2a00d0227", "output": "Based on the provided context, there is no mention of KMS (Key Management Service) being required for Arthur's backup and restore capability. The document focuses on the steps to restore various components of the Arthur platform, such as the RDS Postgres database, ClickHouse data, messaging infrastructure, and enrichments, without any references to KMS.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "242d60dd-516d-4724-b515-fc5acd86f83c", "output": "Unfortunately, the provided context does not contain any information about benchmarking scorers for evaluating LLMs at writing summaries. The context is focused on explaining token likelihood, which is a metric for understanding the confidence of an LLM's predictions, but it does not mention any specific benchmarking tools or metrics for evaluating LLM performance on summarization tasks. Without additional context about LLM evaluation for summarization, I cannot provide a meaningful answer to the question.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "bde29bd5-7fc5-4b40-811f-f03f6a8eeaca", "output": "Based on the context provided, the Arthur team mentions that the procedure they followed to manually evaluate the LLM responses in their dataset will be included as a scoring workflow in Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models. Therefore, the Bench scorer you can use for evaluating LLMs at answering questions is the one that will be provided as part of the Arthur Bench tool.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "7c3bdff6-414e-49b6-ac52-ad6a7d168f13", "output": "The Bench documentation does not explicitly mention scorers for evaluating LLMs at coding tasks. However, the Bench package provides a few scorers that could be useful for this purpose:\n\n1. **ExactMatch**: This scorer can be used to check if the candidate output exactly matches the reference output, including case sensitivity. This could be useful for evaluating the correctness of code outputs.\n\n2. **Custom Scorers**: The Bench documentation mentions that you can create custom scorers to provide even more flexibility. You could potentially create a custom scorer that evaluates the correctness, style, and efficiency of the generated code.\n\nTo use these scorers for evaluating LLMs at coding tasks, you would need to create a test suite with the appropriate input text (coding prompts) and reference outputs (expected code solutions). You can then run the test suite and analyze the scores to assess the LLM's performance on the coding tasks.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "8d479d1b-5ed7-4926-8d58-5282a8248eee", "output": "To view your Bench test results, you have two options:\n\n1. Local setup:\n   - Ensure the `BENCH_FILE_DIR` environment variable is set to the location of your test run data.\n   - Run the `bench` command from the command line to launch the local Bench UI.\n   - The local UI will provide a visual interface to view and explore your test results.\n\n2. SaaS (Coming Soon):\n   - Set the environment variables `ARTHUR_BENCH_AUTOLOG`, `ARTHUR_API_URL`, and `ARTHUR_API_KEY` to connect to the Arthur platform.\n   - Your test results will be automatically logged to the Arthur platform, which will host the Bench server and provide a web-based UI to view and analyze your test data.\n\nThe documentation provides examples for both the local and SaaS setup options to help you get started with viewing your Bench test results.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "a88c1ebb-490c-4669-91c3-ca56bc83861e", "output": "The key difference between Bench test suites and test runs is:\n\n1. Test Suites: A test suite is the overall collection of test cases that you want to evaluate your AI model or application against. It defines the input data, reference outputs, and scoring metrics to be used.\n\n2. Test Runs: A test run is the execution of a test suite against a specific candidate model or application. When you run a test suite, you provide the candidate outputs that you want to evaluate, and the test run will score those outputs against the reference data defined in the test suite.\n\nIn summary, the test suite defines the overall test framework, while the test run executes that framework against a specific candidate to generate performance metrics and results. You can run multiple test runs against the same test suite to compare different models or configurations.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}], "updated_at": "2024-03-27T11:33:10.044656", "created_at": "2024-03-27T11:33:10.044656", "page": null, "page_size": null, "total_pages": null, "total_count": null}