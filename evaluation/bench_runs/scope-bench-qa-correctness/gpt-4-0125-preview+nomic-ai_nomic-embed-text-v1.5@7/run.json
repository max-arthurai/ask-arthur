{"id": "3193035e-2d26-4432-81ce-fdf68c49a7ab", "name": "gpt-4-0125-preview+nomic-ai_nomic-embed-text-v1.5@7", "test_suite_id": "5d0d8642-7bbf-4d7d-a744-cfdee7647a3a", "test_cases": [{"id": "9302872f-61dc-4092-add4-0b928fc05679", "output": "The enrichments available in Arthur are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "1a3cfeeb-c6a0-41de-9444-c609ec78bc71", "output": "The document does not provide information on model types that do not allow explainability.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "2b285103-b671-43c0-8485-3253fadcc23e", "output": "The available enrichments for object detection models are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "640965fd-e43b-4794-9719-de37084d9221", "output": "The available enrichments for tabular binary classification models are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "16e9a74b-faca-45ac-aea5-5fbf28ea0c85", "output": "The document does not mention specific alert levels available in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "45550df3-ab06-4bde-b929-4e84c7796b80", "output": "For an airgap Kubernetes cluster install, the additional requirements include downloading all necessary files and artifacts beforehand since there will be no internet access, and ensuring the Kubernetes cluster has connectivity to a private container and PyPI/Conda registry for optimal platform utilization.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "c8c0a65e-7374-4abe-9d68-458a7a0d48c0", "output": "To enable explainability, you need to provide access to your model's `predict` function.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "15379cb5-bba6-43d1-8b2f-80b2a00d0227", "output": "No, KMS is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "242d60dd-516d-4724-b515-fc5acd86f83c", "output": "The document does not mention any specific Bench scorers for evaluating LLMs at writing summaries.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "bde29bd5-7fc5-4b40-811f-f03f6a8eeaca", "output": "The document does not specify the names or types of Bench scorers available for evaluating LLMs at answering questions.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "7c3bdff6-414e-49b6-ac52-ad6a7d168f13", "output": "The document does not specify which Bench scorers can be used for evaluating LLMs at coding.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "8d479d1b-5ed7-4926-8d58-5282a8248eee", "output": "To view your Bench test results locally, set the `BENCH_FILE_DIR` environment variable to point to the location of your new test runs and run `bench` from the command line.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "a88c1ebb-490c-4669-91c3-ca56bc83861e", "output": "A Bench test suite is created to define the data and evaluation criteria for testing, while a test run is the execution of a test suite against a set of candidate outputs to evaluate performance.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}], "updated_at": "2024-03-27T11:32:26.553013", "created_at": "2024-03-27T11:32:26.553013", "page": null, "page_size": null, "total_pages": null, "total_count": null}