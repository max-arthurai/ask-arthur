{"id": "d46dd95e-c62f-48b1-ba48-bf0724d2910a", "name": "claude-3-opus-20240229+nomic-ai_nomic-embed-text-v1.5@5", "test_suite_id": "1c594684-5fd1-4bba-b209-d6c321bc01ac", "test_cases": [{"id": "6631fb8f-c16e-4b51-9bdb-e46dc35dbe1f", "output": "Based on the FAQ, the main enrichments available in Arthur are:\n\n1. Anomaly Detection\n2. Bias Mitigation \n3. Explainability\n\nTo enable explainability specifically, you need to provide Arthur access to your model's predict function.", "score": 0.9238613843917847, "input": null, "reference_output": null, "score_result": {"score": 0.9238613843917847, "category": null}}, {"id": "12245a16-8cc3-4cad-a0df-8d65b2ccdbb4", "output": "Based on the given context, there is no information about which specific model types do not allow explainability. The article discusses model schemas in general and uses a tabular classification model as an example, but does not mention anything about explainability limitations of different model types.", "score": 0.8223905563354492, "input": null, "reference_output": null, "score_result": {"score": 0.8223905563354492, "category": null}}, {"id": "907e435b-f3d1-4201-bdf3-a514a8975b2d", "output": "Based on the provided context, there is no information about which specific enrichments are available for object detection models in Arthur. The context is a general FAQ about using the Arthur platform, but does not go into details about supported enrichments for particular model types.", "score": 0.7912339568138123, "input": null, "reference_output": null, "score_result": {"score": 0.7912339568138123, "category": null}}, {"id": "22d619b4-c43b-4d17-9dd5-1c3d9e81b44a", "output": "Based on the context provided, the following enrichments are available for tabular multiclass classification models in Arthur:\n\n- Anomaly Detection\n- Hot Spots\n- Explainability\n\nThe context does not mention which enrichments are available specifically for tabular binary classification models.", "score": 0.7457533478736877, "input": null, "reference_output": null, "score_result": {"score": 0.7457533478736877, "category": null}}, {"id": "c30d8faf-8e01-4e50-b93c-e633c9da7fd1", "output": "Based on the provided context, there is no information about restoring deleted Arthur models. The FAQ covers topics like using Arthur without the Python SDK, data privacy, handling missing ground truth labels, SDK errors, and choosing drift metrics, but it does not mention anything about restoring deleted models.", "score": 0.8405285477638245, "input": null, "reference_output": null, "score_result": {"score": 0.8405285477638245, "category": null}}, {"id": "e9bc0fdf-b920-48eb-bce7-9d10b050a664", "output": "Based on the provided context, there is no information about the specific alert levels available in Arthur. The FAQ does not cover details about the alerting system or alert levels.", "score": 0.8541466593742371, "input": null, "reference_output": null, "score_result": {"score": 0.8541466593742371, "category": null}}, {"id": "4040ca43-f1db-4111-9df4-124ef94eea08", "output": "Based on the provided context, there are no specific additional requirements mentioned for on-prem deployment in airgapped mode. The context only states that the Arthur platform can be installed without Internet access, once all the required files are downloaded and available locally.", "score": 0.7696245312690735, "input": null, "reference_output": null, "score_result": {"score": 0.7696245312690735, "category": null}}, {"id": "498e6ba4-c53d-405d-88bd-88b27aa1106f", "output": "To enable explainability for your model in Arthur, you need to provide access to the model's predict function. Arthur generally does not need a copy of your actual model, but it does require access to the predict function in order to generate explanations for the model's predictions.", "score": 0.878014326095581, "input": null, "reference_output": null, "score_result": {"score": 0.878014326095581, "category": null}}, {"id": "3c216b5e-f6ea-41c6-933a-a20d4bf59e08", "output": "No, KMS (Key Management Service) is not mentioned as a requirement for Arthur's backup and restore capability based on the provided context. The document outlines the steps to restore various components like RDS Postgres, ClickHouse data, messaging infrastructure, and enrichments, but does not indicate that KMS is needed.", "score": 0.7299269437789917, "input": null, "reference_output": null, "score_result": {"score": 0.7299269437789917, "category": null}}, {"id": "9e8c3e89-6a0c-4ae3-85d6-7a141c9bdcfe", "output": "I apologize, but the given context does not contain information about Bench scorers for evaluating LLM summary writing. The context discusses token likelihood for generative text models, but does not mention any specific evaluation metrics or scorers for summaries.", "score": 0.8075678944587708, "input": null, "reference_output": null, "score_result": {"score": 0.8075678944587708, "category": null}}, {"id": "1925c2f0-86e8-4e5c-8c5c-572775db7beb", "output": "Based on the context provided, Arthur Bench will include a scoring workflow for manually evaluating LLM responses to questions. The workflow used in this hallucination experiment, where LLM responses were categorized as either correct, avoiding answering, or containing a hallucination, will be included as a scoring workflow in Arthur Bench for comparing LLMs, prompts, and hyperparameters.", "score": 0.787600576877594, "input": null, "reference_output": null, "score_result": {"score": 0.787600576877594, "category": null}}, {"id": "93e6a397-63d3-4678-bb58-42e192ed38c5", "output": "Based on the provided context, there is no information about specific Bench scorers that can be used for evaluating LLMs at coding tasks. The context discusses configuring the exact_match scorer to be case-insensitive, but does not mention any code evaluation scorers.", "score": 0.7573161125183105, "input": null, "reference_output": null, "score_result": {"score": 0.7573161125183105, "category": null}}, {"id": "67293353-3c84-4967-810d-ee179a1ae002", "output": "Based on the provided context, there are two ways to view your Bench test results:\n\n1. Local: Set the BENCH_FILE_DIR environment variable to point to the location of your test runs, then run `bench` from the command line to launch the local UI in your browser.\n\n2. SaaS (Coming Soon): Set the ARTHUR_BENCH_AUTOLOG, ARTHUR_API_URL, and ARTHUR_API_KEY environment variables to log results to the Arthur platform.", "score": 0.8475780487060547, "input": null, "reference_output": null, "score_result": {"score": 0.8475780487060547, "category": null}}, {"id": "bb8aa841-074e-4b8f-aa62-ea33d91280bd", "output": "Based on the context provided, the key differences between test suites and test runs in Arthur Bench are:\n\n- A test suite defines the inputs, reference outputs, and scoring method to use for evaluating an AI model\n- A test run executes a test suite by providing candidate outputs from a model and comparing them to the reference outputs using the specified scoring method\n\nIn summary, a test suite specifies what to test, while a test run actually performs the testing using the suite's configuration.", "score": 0.761450469493866, "input": null, "reference_output": null, "score_result": {"score": 0.761450469493866, "category": null}}], "updated_at": "2024-03-26T16:07:46.943096", "created_at": "2024-03-26T16:07:46.943096", "page": null, "page_size": null, "total_pages": null, "total_count": null}