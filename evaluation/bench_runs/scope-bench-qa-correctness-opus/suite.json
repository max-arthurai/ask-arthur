{"id": "4a810683-5bd4-44aa-b42a-784620f41c0d", "name": "scope-bench-qa-correctness-opus", "scoring_method": {"name": "qa_correctness", "type": "built_in", "config": {}, "output_type": "categorical", "categories": [{"name": "incorrect", "description": "model output is incorrect given the context"}, {"name": "correct", "description": "model output is correct given the context"}, {"name": "invalid", "description": "grader returned an invalid response"}]}, "test_cases": [{"id": "f879070d-a94a-44c2-9f67-ff60b802b749", "input": "What enrichments are available in Arthur?", "reference_output": null}, {"id": "6fc74721-9cba-4844-8f15-f4469422d124", "input": "What model types do not allow explainability?", "reference_output": null}, {"id": "b63703c2-2fad-47af-9aa3-00b595aeff22", "input": "What enrichments are available for object detection models?", "reference_output": null}, {"id": "f680f682-5b0f-4b7a-8e48-bccb6fd7ee51", "input": "What enrichments are available for tabular binary classification models?", "reference_output": null}, {"id": "d9f912c8-a449-4745-beec-26e562416d08", "input": "What are the two alert levels available in Arthur? ", "reference_output": null}, {"id": "a7abaee1-a0e3-4d8c-bf88-83b546521882", "input": "What are the additional requirements for on-prem deployment specific to the airgapped mode?", "reference_output": null}, {"id": "e64b4d2b-3e58-4e39-9b23-a639f42f4cd2", "input": "What artifacts do I need to enable explainability for my model? ", "reference_output": null}, {"id": "b1c20735-345d-4998-9198-1b64b34239e8", "input": "Is KMS required for Arthur's backup and restore capability?", "reference_output": null}, {"id": "64c42b24-cb61-4870-9dfe-679466bf1344", "input": "What Bench scorers can I use for evaluating LLMs at writing summaries?", "reference_output": null}, {"id": "22cd57d3-a9a9-4c66-bc14-2e82b0c50449", "input": "What Bench scorers can I use for evaluating LLMs at answering questions?", "reference_output": null}, {"id": "ffdb3450-ebb2-49bc-ab2c-c367cc89c4a0", "input": "What Bench scorers can I use for evaluating LLMs at coding?", "reference_output": null}, {"id": "a72ef6a5-a67c-47c9-8c66-eb4482504f53", "input": "How do I view my Bench test results?", "reference_output": null}, {"id": "0af305b2-29b4-476f-a3af-84ee88452b96", "input": "What is the difference between Bench test suites and test runs?", "reference_output": null}], "created_at": "2024-03-27T11:40:38.193865", "updated_at": "2024-03-27T11:40:38.193865", "description": null, "last_run_time": "2024-03-27T11:46:38.093257", "num_runs": 32, "page": null, "page_size": null, "total_pages": null, "total_count": null}