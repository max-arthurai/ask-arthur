Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/add_scorer_config.html
 Add Scorer Configurations - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Add Scorer Configurations#
Many scorers included in the Bench package have optional parameters that provide flexibility for users to match scorers with their use case. Please visit our SDK documentation to view the optional configurations avaiable for each scorer.
As an example, in the quickstart, we showed how to use the exact_match scorer. By default, the exact_match scorer is case sensitive. This means, the scorer returns the value of 1.0 only when the candidate output matches the content and the capitalization of the reference output.
If we want to ignore capitalization differences, we can add a configuration to the exact_match scorer.
Creating the test suite#
Instantiate a test suite with a name, scorer, input text, and reference outputs. For our use case, instead of invoking the scorer using the string representation (which corresponds to the default config), we will explicitly call the scorer and add optional configurations.
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import ExactMatch
suite = TestSuite(
name='bench_quickstart',
scoring_method=ExactMatch(case_sensitive=False),
input_text_list=["What year was FDR elected?", "What is the opposite of down?"],
reference_output_list=["1932", "up"]
)
Running the test#
To create a test run, we need to specify the candidate responses.
run = suite.run('quickstart_run', candidate_output_list=["1932", "Up"])
print(run.test_cases)
>>> [TestCaseOutput(output='1932', score=1.0), TestCaseOutput(output='Up', score=1.0)]
We have now logged the results for both test cases as 1.0 even though the capitalization doesn’t match the reference. This is non-default behavior for which we needed to configure the scorer while creating the test suite.
Additional resources#
We also support creating custom scorers that provide even more flexibility. Please view the guide here to learn how custom scorers can be created.
Next
Custom Scoring
Previous
Compare Generation Settings
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Add Scorer Configurations
Creating the test suite
Running the test
Additional resources
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/code_evaluation.html
 Code Evaluation - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Code Evaluation#
Basic Usage#
Code evaluation refers to the process of checking whether LLM-written code passes unit tests
To use a code evaluation scoring method, instantiate the scorer with the unit tests you want to attach to the suite, and proceed with test suite creation / test case running as usual.
Here we show the basic usage for the PythonUnitTesting code evaluation scorer. See the Data Requirements and Example Walkthrough sections below for more details on preparing unit tests and candidate solutions.
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import PythonUnitTesting
# create scorer from unit_tests: List[str]
python_scorer = PythonUnitTesting(unit_tests=unit_tests)
# create test suite
# we explain how to prepare the data for python_unit_test_df below
python_suite = TestSuite(
"python_testsuite",
python_scorer,
reference_data=python_unit_test_df
)
Data Requirements#
Unit tests#
Unit tests must be compatible with the code_eval evaluator metric from HuggingFace, which is what the PythonUnitTesting scorer uses under the hood.
Format
Each unit test is expected to invoke the candidate function by name and assert its output
The general format of the unit test expected by bench is as follows (the name check is not required)
def check(candidate):
assert candidate(test_input_0) = test_output_0
assert candidate(test_input_1) = test_output_1
assert candidate(test_input_2) = test_output_2
# ...
check(candidate_function_name)
For example, here is the unit test for the greatest_common_divisor task from the HumanEval dataset:
def check(candidate):
assert candidate(3, 7) == 1
assert candidate(10, 15) == 5
assert candidate(49, 14) == 7
assert candidate(144, 60) == 12
check(greatest_common_divisor)
Provide unit tests as strings
Unit tests can be passed to the PythonUnitTesting scorer as a list of strings, which is likely the simpler option if you are loading tests from a benchmark dataset (e.g. HumanEval as we do in the example below):
# create scorer from unit_test: List[str]
python_scorer = PythonUnitTesting(unit_tests=unit_tests)
Provide unit tests as directory
Alternatively you can load unit tests from a directory to evaluate your candidate solutions. Given a directory of unit test scripts:
unit_test_dir_name:
- unit_test_0.py
- unit_test_1.py
...
The PythonUnitTesting scorer can be created just from that directory name:
python_scorer = PythonUnitTesting(unit_test_dir=unit_test_dir_name)
Solutions#
Candidate solutions will only be evaluated to be correct if they contain:
a function to call (in the HumanEval dataset, this is called the entry_point)
any necessary imports
This is correct:
import math
def greatest_common_divisor(a: int, b: int) -> int:
return math.gcd(a, b)
This will be scored as incorrect due to missing the math import
def greatest_common_divisor(a: int, b: int) -> int:
return math.gcd(a, b)
This will be scored as incorrect due to missing a function entrypoint for the unit test to invoke:
import math
return math.gcd(a, b)
Input prompts & reference outputs#
Input prompts and reference outputs (AKA canonical / golden solutions) have no requirements in Bench. These components are only for your own analysis, and are not used by the scorers under the hood in code evaluation.
Example Walkthrough#
Here is some example code that you can use to generate and compare python coding solutions using OpenAI’s GPT-3.5 and Anthropic’s Claude-2 on the HumanEval dataset from HuggingFace
Environment setup#
First we set environment variables for OPENAI_API_KEY and ANTHROPIC_API_KEY before running our generation code
pip install openai anthropic
export "OPENAI_API_KEY"="sk-..."
export "ANTHROPIC_API_KEY"="sk-ant-..."
Data preparation#
Our dataset is the HumanEval dataset from HuggingFace loaded into a pandas DataFrame
from datasets import load_dataset
import pandas as pd
humaneval_code_dataset = load_dataset("openai_humaneval")
humaneval_df = pd.DataFrame(humaneval_code_dataset["test"])
humaneval_df_sample = humaneval_df.sample(20, random_state=278487)
Prepare unit tests#
We prepare the unit tests to invoke each candidate function using the test and entry_point fields of the HumanEval dataset:
unit_tests = [
f'\n{humaneval_df_sample.test.values[i]}\ncheck({humaneval_df_sample.entry_point.values[i]})'
for i in range(len(humaneval_df_sample))
]
Generate solutions#
from langchain.chat_models import ChatOpenAI, ChatAnthropic
gpt35 = ChatOpenAI()
claude = ChatAnthropic()
prompt_template = """
You are a bot that gives answers to coding tasks only. If the task is a coding task, give an expert python solution.
If the task is unrelated, give the response "I don't know."
ALWAYS mark the beginning and end of your solution with ```python markdown markers.
Without these markers, the code cannot be extracted. Therefore the markers are required.
===
<text>
===
Solution:
"""
# used to extract the portion of an LLM response which is python code
extract_python = lambda x : x.replace('python\n', '').replace('```', '').replace(' def', 'def')
def get_solutions(model):
filled_prompt_templates = [
prompt_template.replace("<text>", humaneval_df_sample.prompt.values[i])
for i in range(len(humaneval_df_sample))
]
return [extract_python(model.predict(x)) for x in filled_prompt_templates]
gpt35_solutions = get_solutions(gpt35)
claude_solutions = get_solutions(claude)
Create and run test suite
Now that you have generated solutions for each model, we can create a test suite and a run for each LLM
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import PythonUnitTesting
python_scorer = PythonUnitTesting(unit_tests=unit_tests)
python_suite = TestSuite(
"humaneval_testsuite",
python_scorer,
input_text_list=list(humaneval_df_sample.prompt.values),
reference_output_list=list(humaneval_df_sample.canonical_solution.values),
)
python_suite.run("gpt-3.5-turbo", candidate_output_list=gpt35_solutions)
python_suite.run("claude-2", candidate_output_list=claude_solutions)
Best practices#
Prompt templating for code extraction#
Evaluation becomes more straightforward if you can easily extract the part of an LLM response which is its actual code solution. The simplest way to do that seems to be including an instruction in your prompt or system message that specifies to place code in between “```python markers” in markdown, as we did in the example above.
Function signature#
Performance tends to improve on coding when your task description contains an explicit function signature that you want the solution to adhere to, as well as including example input/output behavior in its docstring.
As an example, here is the input prompt for the greatest_common_divisor coding task from HumanEval:
def greatest_common_divisor(a: int, b: int) -> int:
""" Return a greatest common divisor of two integers a and b
>>> greatest_common_divisor(3, 5)
1
>>> greatest_common_divisor(25, 15)
5
"""
Note that the HumanEval dataset prompts all contain docstrings like this one
Next
Python API Reference
Previous
Custom Scoring
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Code Evaluation
Basic Usage
Data Requirements
Unit tests
Solutions
Input prompts & reference outputs
Example Walkthrough
Environment setup
Data preparation
Prepare unit tests
Generate solutions
Best practices
Prompt templating for code extraction
Function signature
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/compare_generation_settings.html
 Compare Generation Settings - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Compare Generation Settings#
In this guide we compare LLM-generated answers to questions using different temperature settings. Higher temperature improves creativity and diversity of answers, but increases the likelihood that responses veer into nonsense.
We use a custom scorer that compares each LLM temperature setting based on how many typos each response contains
Environment setup#
In this guide, we use the OpenAI API and use the pyspellchecker package for a custom scorer
pip install openai pyspellchecker
export OPENAI_API_KEY="sk-..."
Data preparation#
We write out some basic questions which we will use to test how much temperature impacts the responses
inputs = ["What planet are we on?", "What time is it?", "What day is it?", "What is love?"]
LLM response generation#
We use different temperature settings to generate three different lists of responses:
from langchain.chat_models import ChatOpenAI
chatgpt_zero_temp = ChatOpenAI(temperature=0.0, max_tokens=100)
chatgpt_low_temp = ChatOpenAI(temperature=0.5, max_tokens=100)
chatgpt_med_temp = ChatOpenAI(temperature=1.2, max_tokens=100)
chatgpt_high_temp = ChatOpenAI(temperature=1.9, max_tokens=100)
baseline_responses = [chatgpt_zero_temp.predict(x) for x in inputs]
low_temp_responses = [chatgpt_low_temp.predict(x) for x in inputs]
med_temp_responses = [chatgpt_med_temp.predict(x) for x in inputs]
high_temp_responses = [chatgpt_high_temp.predict(x) for x in inputs]
Create test suite#
For this test suite, we want to measure how corrupted the responses get as we increase the generation temperature.
Let’s define a quick custom scorer that uses the pyspellchecker package to scan for typos in the response, and we will then see how much the typo score changes between the low, medium, and high temperature model generations.
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import Scorer
from spellchecker import SpellChecker
import string
from typing import List, Optional
class CustomSpellingScore(Scorer):
"""
Custom scoring which scores each LLM response with the formula 1 / (2 ^ number of typos)
This gives a typo-free response a score of 1, and each additional typo further decreases the score
"""
def __init__(self):
self.spell_checker = SpellChecker()
@staticmethod
def name() -> str:
return "spell_checker"
@staticmethod
def requires_reference() -> bool:
return False
def run_batch(self, candidate_batch: List[str], reference_batch: Optional[List[str]] = None,
input_text_batch: Optional[List[str]] = None, context_batch: Optional[List[str]] = None) -> List[float]:
res = []
for s in candidate_batch:
# remove punctuation
s = ''.join(ch for ch in s if ch not in string.punctuation)
# get number of typos in s
num_typos = len(self.spell_checker.unknown(s.split()))
# custom score is 1/(2^num_typos)
res.append(1.0 / (2**num_typos))
return res
my_suite = TestSuite(
"test-spelling",
CustomSpellingScore(),
input_text_list=inputs,
reference_output_list=baseline_responses
)
Run test suite#
my_suite.run("low_temp_responses", candidate_output_list=low_temp_responses)
my_suite.run("med_temp_responses", candidate_output_list=med_temp_responses)
my_suite.run("high_temp_responses", candidate_output_list=high_temp_responses)
View results#
Run bench from your command line to visualize the run results comparing the different temperature settings.
Next
Add Scorer Configurations
Previous
Compare Prompts
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Compare Generation Settings
Environment setup
Data preparation
LLM response generation
Create test suite
Run test suite
View results
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/compare_llm_providers.html
 Compare LLM Providers - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Compare LLM Providers#
In this guide we compare LLMs answers at summarizing text.
Environment setup#
In this guide, we use the OpenAI API and the Cohere API
pip install openai cohere
export OPENAI_API_KEY="sk-..."
export COHERE_API_KEY="..."
Data preparation#
We load a publically available congressional bill summarization dataset from HuggingFace.
import pandas as pd
from datasets import load_dataset
billsum = load_dataset("billsum", split="ca_test")
billsum_df = pd.DataFrame(billsum).sample(10, random_state=278487)
LLM response generation#
We use OpenAI and Cohere to generate summaries of these bills:
from langchain.llms import OpenAI, Cohere
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
gpt3 = OpenAI(temperature=0.0, max_tokens=100)
command = Cohere(temperature=0.0, max_tokens=100)
prompt_template = PromptTemplate(
input_variables=["text"],
template="""
You are an expert summarizer of text. A good summary
captures the most important information in the text and doesnt focus too much on small details.
Text: {text}
Summary:
"""
)
gpt3_chain = LLMChain(llm=gpt3, prompt=prompt_template)
command_chain = LLMChain(llm=command, prompt=prompt_template)
# generate summaries with truncated text
gpt3_summaries = [gpt3_chain.run(bill[:3000]) for bill in billsum_df.text]
command_summaries = [command_chain.run(bill[:3000]) for bill in billsum_df.text]
Create test suite#
For this test suite, we want to compare gpt-3 against command. We will use the SummaryQuality scoring metric to A/B test each set of candidate responses against the reference summaries from the dataset
from arthur_bench.run.testsuite import TestSuite
my_suite = TestSuite(
"congressional_bills",
"summary_quality",
input_text_list=list(billsum_df.text),
reference_output_list=list(billsum_df.summary)
)
Run test suite#
my_suite.run("gpt3_summaries", candidate_output_list=gpt3_summaries)
my_suite.run("command_summaries", candidate_output_list=command_summaries)
View results#
Run bench from your command line to visualize the run results comparing the different temperature settings.
Next
Compare Prompts
Previous
Creating test suites
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Compare LLM Providers
Environment setup
Data preparation
LLM response generation
Create test suite
Run test suite
View results
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/compare_prompts.html
 Compare Prompts - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Compare Prompts#
In this guide we compare prompts
Environment setup#
In this guide, we use the OpenAI API
pip install openai
export OPENAI_API_KEY="sk-..."
Data preparation#
We load a publically available congressional bill summarization dataset from HuggingFace.
We also prepare an example bill with its summary to include in a prompt as an example response.
import pandas as pd
from datasets import load_dataset
billsum = load_dataset("billsum")
billsum_df = pd.DataFrame(billsum["ca_test"]).sample(10, random_state=278487)
example_bill = billsum["test"][6]["text"]
example_bill_summary = billsum["test"][6]["summary"]
LLM response generation#
We use two different prompt templates to generate responses
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
gpt35 = ChatOpenAI(temperature=0.0, max_tokens=100)
prompt0_template= PromptTemplate(
input_variables=["text"],
template="""
Text: {text}
Summary:
"""
)
prompt1_template = PromptTemplate(
input_variables=["text", "example_bill", "example_bill_summary"],
template="""
You are an expert summarizer of legal text. A good summary
captures the most important information in the text and doesnt focus too much on small details.
Make sure to use your expert legal knowledge in summarizing.
===
Text: {example_bill}
Summary: {example_bill_summary}
===
Text: {text}
Summary:
"""
)
prompt0_chain = LLMChain(llm=gpt35, prompt=prompt0_template)
prompt1_chain = LLMChain(llm=gpt35, prompt=prompt1_template)
# generate summaries with truncated text
prompt0_summaries = [prompt0_chain.run(bill[:3000]) for bill in billsum_df.text]
prompt1_summaries = [
prompt1_chain({"text" : bill[:3000], "example_bill" : example_bill, "example_bill_summary" : example_bill_summary})["text"]
for bill in billsum_df.text
]
Create test suite#
For this test suite, we will use BERTScore to measure how much the candidate summaries approach the reference summaries by upgrading our prompt with task-specific detail and an example.
from arthur_bench.run.testsuite import TestSuite
my_suite = TestSuite(
"congressional_bills_to_reference",
"bertscore",
input_text_list=list(billsum_df.text),
reference_output_list=list(billsum_df.summary)
)
Run test suite#
my_suite.run("prompt0_summaries", candidate_output_list=prompt0_summaries)
my_suite.run("prompt1_summaries", candidate_output_list=prompt1_summaries)
View results#
Run bench from your command line to visualize the run results comparing the different temperature settings.
Next
Compare Generation Settings
Previous
Compare LLM Providers
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Compare Prompts
Environment setup
Data preparation
LLM response generation
Create test suite
Run test suite
View results
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/concepts.html
 Concepts - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Concepts#
Data#
Testing LLMs involves preparing the following data for your use case:
Inputs to the LLM. Depending on the task at hand, these inputs are likely formatted to follow a prompt template.
Reference Outputs: these are your baseline outputs, which are optional in Arthur Bench but recommended to get a comprehensive understanding of your model’s performance relative to its expected outputs. These reference outputs would likely be either ground truth responses to the inputs, or could be outputs from a baseline LLM that you are evaluating against.
Candidate Outputs: these are the outputs from your candidate LLM that you are scoring.
Context: contextual information used to produce the candidate output, e.g. for retrieval-augmented Question & Answering tasks.
As an example, consider the task of Question & Answering about specific documents:
Input: “What war was referred to in the Gettysburg Address?”
Reference Output: American Civil War
Candidate Output: The war referenced in the Gettysburg Address is the American Civil War
Context: (Wikipedia) “The Gettysburg Address is a speech that U.S. President Abraham Lincoln delivered during the American Civil War at the dedication of the Soldiers’ National Cemetery, now known as Gettysburg National Cemetery, in Gettysburg, Pennsylvania on the afternoon of November 19, 1863, four and a half months after the Union armies defeated Confederate forces in the Battle of Gettysburg, the Civil War’s deadliest battle.”
Testing#
Test Suites#
A Test Suite stores the input & reference output data for your testing use case along with a scorer.
For example, for a summarization use case, your test suite could be created with:
the documents to summarize
baseline summaries as reference outputs to evaluate against
the SummaryQuality scorer
Test suites allow you to save and reuse your evaluation datasets over time with a consistent scorer to help you understand what drives changes in performance.
To view how to create test suites from various data formats, view our creating test suites guide
Test runs#
When a test suite is run, its scorer evaluates the candidate outputs provided in the run and assigns a score to each test case.
To run your test suite on candidate data, pass the data to the run() function of your test suite, along with any additional metadata you want to be logged for that run. To view the metadata you can save with your test runs, see the SDK docs
To view how to create test runs from various data formats, visit our test suites guide
Next
Creating test suites
Previous
Guides
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Concepts
Data
Testing
Test Suites
Test runs
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/contributing.html
 Contributing - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Contributing#
We welcome contributions and feedback from the community!
Creating a custom scorer#
All scorers should inherit from the Scorer base class and provide a custom implementation of the run_batch method.
A scorer can leverage any combination of input texts, context texts, and reference texts to score candidate generations. All computed scores must be float values where a higher value indicates a better score. If you have a scorer that does not fit these constraints, please get in touch with the Arthur team.
Steps for adding a custom scorer:
Install bench from source, in development mode:
pip install -e .
Add your Scorer implementation in a new file in arthur_bench/scoring. For scorers that require prompt templating, we use the LangChain library.
Register your scorer by adding it to the scorer enum in arthur_bench/models/models.py
At this point, you should be able to create test suites with your new scorer and test your implementation locally.
Contributing your scorer:
Fork the bench repository and create a pull request from your fork. This Github guide provides more in depth instructions.
Your scorer docstring should use Sphinx format for compatibility with documentation.
Provide unit tests for the scorer in a separate file in the test directory.
Next
Usage Data Collection
Previous
arthur_bench.utils
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Contributing
Creating a custom scorer
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/creating_test_suites.html
 Creating test suites - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Creating test suites#
What data should I use?#
It is best to use data that is as close to your production use case as possible. We recommend sampling some historic data and manually validating a set of 25+ cases.
Public datasets on HuggingFace like the Dolly dataset and the HumanEval dataset can be a great starting place to benchmark on your use case before you have data that is closer to your actual production setting.
When no baseline examples or labels easily exist for the inputs you want to evaluate LLM performance on, you can use an existing LLM to generate a baseline for the task and then iterate from there.
Ways to create a test suite#
The TestSuite class is the main touch point for creating and running tests in Arthur Bench. No matter how you prepare your data for a test suite, you use the common interface provided by importing the TestSuite class:
from arthur_bench.run.testsuite import TestSuite
You can provide data for your TestSuite via the following options, each of which we give examples of in the sections below:
List[str]
pd.DataFrame
CSV file
HuggingFace Dataset
To see the exact specifications for the TestSuite class, visit our SDK docs.
List[str] -> TestSuite#
You can create and run a test suite by passing lists of strings directly as the test suite data:
suite = TestSuite(
"bench_quickstart",
"exact_match",
input_text_list=["What year was FDR elected?", "What is the opposite of down?"],
reference_output_list=["1932", "up"]
)
suite.run('quickstart_run', candidate_output_list=["1932", "up is the opposite of down"])
This path also allows you to pass LLM responses directly into a test suite as a set of baseline reference outputs and/or as a run of candidate outputs. For example, you can use gpt-4 outputs as a baseline for a test suite, and then run gpt-3.5-turbo as a candidate to see how it compares.
Creating and running the test suite directly from LLM-generated strings:
from langchain.chat_models import ChatOpenAI
gpt35 = ChatOpenAI()
gpt4 = ChatOpenAI(model_name="gpt-4")
inputs = ["What year was FDR elected?", "What is the opposite of down?"]
baseline_outputs = [gpt4.predict(x) for x in inputs]
candidate_outputs = [gpt35.predict(x) for x in inputs]
suite = TestSuite(
"bench_llm_quickstart",
"exact_match",
input_text_list=inputs,
reference_output_list=baseline_outputs
)
suite.run('quickstart_llm_run', candidate_output_list=candidate_outputs)
DataFrame -> TestSuite#
If you have your test suite and/or model response data in a pandas DataFrame you can create test suites and runs directly from those dataframes
Here is an example test suite built from a dataframe with the default reference data and candidate data column names that TestSuite expects (you can also use other column names as we show below)
Creating and running the default DataFrame test suite:
import pandas as pd
df = pd.DataFrame({
"input": ["What year was FDR elected?", "What is the opposite of down?"],
"reference_output": ["1932", "up"],
"candidate_output": ["1932", "up is the opposite of down"]
})
test_suite = TestSuite(
"suite_from_df",
"exact_match",
reference_data=df
)
test_suite.run("candidate_from_df", candidate_data=df)
Alternatively you can create and run test suites from dataframes with custom column names.
Creating and running the custom DataFrame test suite:
import pandas as pd
df = pd.DataFrame({
"my_input": ["What year was FDR elected?", "What is the opposite of down?"],
"baseline_output": ["1932", "up"],
"gpt35_output": ["1932", "up is the opposite of down"]
})
test_suite = TestSuite(
"suite_from_df_custom",
"exact_match",
reference_data=df,
input_column="my_input",
reference_column="baseline_output"
)
test_suite.run(
"candidate_from_df_custom",
candidate_data=df,
candidate_column="gpt35_output"
)
.csv -> TestSuite#
If your test suite and/or model response data already exists in CSV files you can create test suites and runs directly from those files
Here is an example test suite CSV with the default reference data and candidate data column names that TestSuite expects (you can also use other column names as we show below)
test_suite_data_default_columns.csv
input, reference_output, candidate_output
What year was FDR elected?, 1932, 1932
What is the opposite of down?, up, up is the opposite of down
Creating and running the default csv test suite:
test_suite = TestSuite(
"suite_from_csv",
"exact_match",
reference_data_path="/path/to/test_suite_data_default_columns.csv"
)
test_suite.run(
"candidate_from_csv",
candidate_data_path="/path/to/test_suite_data_default_columns.csv"
)
Alternatively you can create and run test suites from .csv files with custom column names:
test_suite_data_custom_columns.csv
my_input, baseline_output, gpt35_output
What year was FDR elected?, 1932, 1932
What is the opposite of down?, up, up is the opposite of down
Creating and running the custom csv test suite:
test_suite = TestSuite(
"suite_from_csv_custom",
"exact_match",
reference_data_path="/path/to/test_suite_data_custom_columns.csv",
input_column="my_input",
reference_column="baseline_output"
)
test_suite.run(
"candidate_from_csv_custom",
candidate_data_path="/path/to/test_suite_data_custom_columns.csv",
candidate_column="gpt35_output"
)
HuggingFace dataset -> DataFrame -> TestSuite#
Here we create a small question-answering test suite from the Dolly dataset downloaded from HuggingFace. We set up the test suite to use BERTScore to measure similarity between candidate answers and reference answers
Creating and running the dolly test suite:
# get dolly dataset from huggingface into a pandas dataframe
import pandas as pd
from datasets import load_dataset
dolly = load_dataset("databricks/databricks-dolly-15k")
dolly_df = pd.DataFrame(dolly["train"])
# make test suite from a question-answering subset of the data
dolly_df_sample = dolly_df[dolly_df["category"]=="open_qa"].sample(25, random_state=278487)
dolly_suite = TestSuite(
"suite_from_huggingface_dolly",
"bertscore",
reference_data=dolly_df_sample,
input_column="instruction",
reference_column="response"
)
# run test suite on gpt-3.5-turbo generated answers to the questions
from langchain.chat_models import ChatOpenAI
gpt35 = ChatOpenAI()
dolly_suite.run(
"gpt-3.5",
candidate_output_list=[gpt35.predict(x) for x in dolly_df_sample.instruction])
Next
Compare LLM Providers
Previous
Concepts
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Creating test suites
What data should I use?
Ways to create a test suite
List[str] -> TestSuite
DataFrame -> TestSuite
.csv -> TestSuite
HuggingFace dataset -> DataFrame -> TestSuite
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/custom_scoring.html
 Custom Scoring - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Custom Scoring#
In this guide, we will walk through the process of evaluating LLM performance using a custom scorer. We will
Define a custom scorer
Create a test suite with that scorer
Run the test suite and view scores
Define a custom scorer#
To create a custom scorer that satisfies the Scorer interface (defined in the section below), implement the scoring logic in the run_batch method. Additionally, provide your scorer a name in the name() method.
This example custom scorer is called TrigramRepetition, which scores responses with a 0.0 if they contain repeated trigrams above a thresholded number of times. For our scorer, we override the requires_reference() method to return False instead of True, since this custom scorer evaluates the candidate outputs without the need for a reference.
Make sure nltk is installed as a package to your environment, which our custom scorer uses.
pip install nltk
Make sure nltk is installed as a package to your environment, which our custom scorer uses.
pip install nltk
from arthur_bench.scoring import Scorer
import nltk
# make sure corpus is downloaded
nltk.download('punkt')
from nltk import trigrams
from typing import List, Optional
class TrigramRepetition(Scorer):
def __init__(self, threshold: int = 3):
self.threshold = threshold
@staticmethod
def name() -> str:
return "trigram_repetition"
@staticmethod
def requires_reference() -> bool:
return False
def run_batch(self, candidate_batch: List[str], reference_batch: Optional[List[str]] = None,
input_text_batch: Optional[List[str]] = None, context_batch: Optional[List[str]] = None) -> List[float]:
repeat_scores = []
for text in candidate_batch:
tokens = [t.lower() for t in nltk.word_tokenize(text)]
all_trigrams = trigrams(tokens)
counts = {}
for tri in all_trigrams:
if tri in counts:
counts[tri] += 1
else:
counts[tri] = 1
max_repeat = max(counts.values())
repeat_scores.append(float(max_repeat < self.threshold))
return repeat_scores
Using a custom scorer in a test suite#
We pass in our custom scorer as the scoring_method parameter to the test suite:
from arthur_bench.run.testsuite import TestSuite
repetition_test = TestSuite(
'custom_trigram_suite',
scoring_method=TrigramRepetition(),
input_text_list=[
"Talk to me but don't repeat yourself too much",
"Talk to me but don't repeat yourself too much"
]
)
Run the test suite#
Now that we’ve loaded in our custom scorer, our test suite can be run as usual on any candidate generations.
run = repetition_test.run(
'test_run',
candidate_output_list=[
'a great response with no repetition!',
'a bad response that repeats response that repeats response that repeats response that repeats'
]
)
print(run.scores)
>>> [1.0, 0.0]
Scorer Validation#
Test suites expect scorer configurations to remain consistent from run to run, so that each runs scores can be compared and reliably tracked throughout time. Let’s see what happens if we attempt to use this suite at a later time, but edit the underlying parameters.
scorer = TrigramRepetition(threshold=7)
repetition_test = TestSuite('custom_trigram_suite', scoring_method=scorer)
We see the following warning:
scoring method configuration has changed from test suite creation.
By default, bench will save the json serializable attributes of your scorer as the configuration. If you need more advanced serialization for validation or re-initialization, implement the to_dict() and from_dict() methods on your custom class. You can find the full scorer spec here.
Scorer interface#
All scorers in bench implement the scorer interface. Let’s take a look at that interface:
class Scorer(ABC):
"""
Base class for all scorers.
"""
@staticmethod
@abstractmethod
def name() -> str:
"""
Get the name of this Scorer
:return: the Scorer name
"""
raise NotImplementedError
@staticmethod
def requires_reference() -> bool:
return True
@abstractmethod
def run_batch(self, candidate_batch: List[str], reference_batch: Optional[List[str]] = None,
input_text_batch: Optional[List[str]] = None, context_batch: Optional[List[str]] = None) -> List[float]:
"""
Score a batch of candidate generations.
:param candidate_batch: candidate generations to score
:param reference_batch: reference strings representing target outputs
:param input_text_batch: optional corresponding inputs
:param context_batch: optional corresponding contexts, if needed by scorer
"""
raise NotImplementedError
To create a custom scorer, you need to implement the name and run_batch methods, and optionally override the requires_reference method if your scorer doesn’t require reference or target data.
Contributing#
If you think you’ve got a useful scorer, please consider contributing!
Next
Code Evaluation
Previous
Add Scorer Configurations
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Custom Scoring
Define a custom scorer
Using a custom scorer in a test suite
Run the test suite
Scorer Validation
Scorer interface
Contributing
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/quickstart.html
 Quickstart - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Quickstart#
Make sure you have completed installation from the setup guide before moving on to this quickstart.
Environment setup#
The environment variable BENCH_FILE_DIR points to the local directory where your test data is saved and visualized by Arthur Bench.
If you are running this quickstart right after completing the setup guide, then take a moment to reset BENCH_FILE_DIR to its default value, "./bench_runs". This will direct the bench UI to point to your new quickstart test suite instead of the examples from the setup.
export BENCH_FILE_DIR="./bench_runs"
Creating your first test suite#
Instantiate a test suite with a name, data, and scorer.
This example creates a test suite from lists of strings directly with the exact_match scorer.
from arthur_bench.run.testsuite import TestSuite
suite = TestSuite(
'bench_quickstart',
'exact_match',
input_text_list=["What year was FDR elected?", "What is the opposite of down?"],
reference_output_list=["1932", "up"]
)
You can create test suites from a pandas DataFrame or from a path to a local CSV file. See the test suite creation guide to view all the ways you can create test suites.
You can view all scorers available out of the box with bench here on our scoring page, as well as customize your own.
Running your first test suite#
To create a Test Run, you only need to specify the candidate responses. See the test suite creation guide to view all the ways you can run test suites.
run = suite.run('quickstart_run', candidate_output_list=["1932", "up is the opposite of down"])
print(run)
>>> [TestCaseOutput(output='1932', score=1.0), TestCaseOutput(output='up is the opposite of down', score=0.0)]
You should now have logged test case results with scores of 1.0 and 0.0, respectively.
View results in local UI#
Now run bench from the command line to launch the local UI and explore the test results.
bench
Next steps#
Now that you have set up and ran your first test suite, check out the rest of the scorers available in Arthur Bench out of the box.
To learn more about the basic concepts around data and testing in Arthur Bench, visit our basic concepts guide.
Next
Scoring
Previous
Setup
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Quickstart
Environment setup
Creating your first test suite
Running your first test suite
View results in local UI
Next steps
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/scoring.html
 Scoring - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Scoring#
A Scorer is the criteria used to quantitatively evaluate LLM outputs. When you test LLMs with Arthur Bench, you attach a Scorer to each test suite you create - this defines how performance will be measured consistently across that test suite.
For a walkthrough on how to extend the Scorer class to create your own scorer specialized to your data and/or use-case to use with Arthur Bench, check out the custom scoring guide
If you would like to contribute scorers to the open source Arthur Bench repo, check out our contributing guide
Here is a list of all the scorers available by default in Arthur Bench (listed alphabetically):
Scorer
Tasks
Type
Requirements
BERT Score (bertscore)
any
Embedding-Based
Reference Output, Candidate Output
Exact Match (exact_match)
any
Lexicon-Based
Reference Output, Candidate Output
Hallucination (hallucination)
any
Prompt-Based
Candidate Output, Context
Hedging Language (hedging_language)
any
Embedding-Based
Candidate Output
Python Unit Testing (python_unit_testing)
Python Generation
Code Evaluator
Candidate Output, Unit Tests (see the code eval guide)
QA Correctness (qa_correctness)
Question-Answering
Prompt-Based
Input, Candidate Output, Context
Readability (readability)
any
Lexicon-Based
Candidate Output
Specificity (specificity)
any
Lexicon-Based
Candidate Output
Summary Quality (summary_quality)
Summarization
Prompt-Based
Input, Reference Output, Candidate Output
Word Count Match (word_count_match)
any
Lexicon-Based
Reference Output, Candidate Output
For better understandability we have broken down the Scorers based on the type of procedure each Scorer uses.
Prompt-Based Scorers#
qa_correctness#
The QA correctness scorer evaluates the correctness of an answer, given a question and context. This scorer does not require a reference output, but does require context. Each row of the Test Run will receive a binary 0, indicating an incorrect output, or 1, indicating a correct output.
summary_quality#
The Summary Quality scorer evaluates a summary against its source text and a reference summary for comparison. It evaluates summaries on dimensions including relevance and syntax. Each row of the test run will receive a binary 0, indicating that the reference output was scored higher than the candidate output, or 1, indicating that the candidate output was scored higher than the reference output.
hallucination#
The Hallucination scorer takes a response and a context (e.g. in a RAG setting where context is used to ground an LLM’s responses) and identifies when information in the response is not substantiated by the context . The scorer breaks down the response into a list of claims and checks the claims against the context for support. This binary score is 1 if all claims are supported, and 0 otherwise.
Embedding-Based Scorers#
bertscore#
BERTScore is a quantitative metric to compare the similarity of two pieces of text. Using bertscore will score each row of the test run as the bert score between the reference output and the candidate output.
hedging_language#
The Hedging Language scorer evaluates whether a candidate response is similar to generic hedging language used by an LLM (“As an AI language model, I don’t have personal opinions, emotions, or beliefs”). Each row of the Test Run will receive a score between 0.0 and 1.0 indicating the extent to which hedging language is detected in the response (using BERTScore similarity to the target hedging phrase). A score above 0.5 typically suggests the model output contains hedging language.
Lexicon-Based Scorers#
exact_match#
The Exact Match scorer evaluates whether the candidate output exactly matches the reference output. This is case sensitive. Each row of the Test Run will receive a binary 0, indicating a non-match, or 1, indicating an exact match.
readability#
The Readability scorer evaluates the reading ease of the candidate output according to the Flesch Reading Ease Score. The higher the score, the easier the candidate output is to read: scores of 90-100 correlate to a 5th grade reading level, while scores less than 10 are classified as being “extremely difficult to read, and best understood by university graduates.”
specificity#
The Specificity scorer outputs a score of 0 to 1, where smaller values correspond to candidate outputs with more vague language while higher values correspond to candidate outputs with more precise language. Specificity is calculated through 3 heuristic approaches: identifying the presence of predefined words that indicate vagueness, determing how rare the words used are according to word frequencies calculated by popular NLP corpora, and detecting the use of proper nouns and numbers.
word_count_match#
For scenarios where there is a preferred output length, word_count_match calculates a corresponding score on the scale of 0 to 1. Specifically, this scorers calculates how similar the number of words in the candidate output is to the number of words in the reference output, where a score of 1.0 indicates that there are the same number of words in the candidate output as in the reference output. Scores less than 1.0 are calculated as ((len_reference-delta)/len_reference) where delta is the absolute difference in word lengths between the candidate and reference outputs. All negative computed values are truncated to 0.
Code Evaluators#
python_unit_testing#
The Python Unit Testing scorer evaluates candidate solutions to coding tasks against unit tests. This scorer wraps the code_eval evaluator interface from HuggingFace. It is important to note that this function requires that solution code uses standard python libraries only.
Next
Guides
Previous
Quickstart
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Scoring
Prompt-Based Scorers
qa_correctness
summary_quality
hallucination
Embedding-Based Scorers
bertscore
hedging_language
Lexicon-Based Scorers
exact_match
readability
specificity
word_count_match
Code Evaluators
python_unit_testing
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html
 arthur_bench.client.auth - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.auth#
Submodules#
arthur_bench.client.auth.helpers.get_arthur_internal_user_org(api_http_host: str, auth_token: str, verify_ssl: bool = True) → str  None#
Get the current organization for the provided Arthur auth token belonging to anArthur internal user
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
auth_token – auth token to pass to the API
verify_ssl – if True, verify that the SSL certificate is valid and not
self-signed
Returns:
the organization ID associated with the provided access key, None if no
such organization exists
Permissions:
N/A
arthur_bench.client.auth.helpers.get_auth_info(api_http_host: str, auth_token: str, verify_ssl: bool = True) → AuthenticationInfo#
Get the AuthInfo struct associated with the provided access key
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
:param auth_token: Token to fetch authentication info for
:param verify_ssl: Boolean for whether requests should verify that the SSL
certificate is valid and not self-signed
:return: the AuthInfo associated with the provided access key
:permissions: N/A
arthur_bench.client.auth.helpers.get_current_org(api_http_host: str, auth_token: str, verify_ssl: bool = True) → str  None#
Get the current organization for the provided access key
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
:param auth_token: API Key to pass to the API
:param verify_ssl: Boolean for whether requests should verify that the SSL
certificate is valid and not self-signed
Returns:
the organization ID associated with the provided access key, None if no
such organization exists
arthur_bench.client.auth.helpers.user_login(api_http_host: str, login: str, password: str, verify_ssl: bool = True) → str#
Static convenience function to get a new auth token for the provided username andpassword
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
login – the username or password to use to log in
password – password for the user
verify_ssl – Boolean for whether requests should verify that the SSL
certificate is valid and not self-signed
Returns:
an access_key
class arthur_bench.client.auth.refresh.AuthRefresher(url: str, login: str, password: str, verify_ssl: bool)#
Bases: object
ALGORITHMS = ['HS256']#
AUTH_KEY = 'Authorization'#
MINS_BEFORE_EXPIRY_TO_REFRESH = 5#
refresh() → Tuple[Dict[str, str], timedelta]#
Authorization header update function for an HTTPClient
Fetches a new session token and returns the new token, and how long to wait
before refreshing it (by calling this method again)
:return: Headers to update (Authorization), and time to wait before refreshing
again
Next
arthur_bench.client.http
Previous
arthur_bench.client
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.auth
Submodules
get_arthur_internal_user_org()
get_auth_info()
get_current_org()
user_login()
AuthRefresher
AuthRefresher.ALGORITHMS
AuthRefresher.AUTH_KEY
AuthRefresher.MINS_BEFORE_EXPIRY_TO_REFRESH
AuthRefresher.refresh()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html
 arthur_bench.client - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client#
Subpackages#
arthur_bench.client.auth
Submodules
get_arthur_internal_user_org()
get_auth_info()
get_current_org()
user_login()
AuthRefresher
AuthRefresher.ALGORITHMS
AuthRefresher.AUTH_KEY
AuthRefresher.MINS_BEFORE_EXPIRY_TO_REFRESH
AuthRefresher.refresh()
arthur_bench.client.http
Submodules
construct_url()
HTTPClient
HTTPClient.delete()
HTTPClient.get()
HTTPClient.patch()
HTTPClient.post()
HTTPClient.put()
HTTPClient.send()
HTTPClient.set_path_prefix()
validate_multistatus_response_and_get_failures()
validate_response_status()
arthur_bench.client.local
Submodules
LocalBenchClient
LocalBenchClient.create_new_test_run()
LocalBenchClient.create_test_suite()
LocalBenchClient.delete_test_run()
LocalBenchClient.delete_test_suite()
LocalBenchClient.get_runs_for_test_suite()
LocalBenchClient.get_summary_statistics()
LocalBenchClient.get_test_run()
LocalBenchClient.get_test_suite()
LocalBenchClient.get_test_suite_by_name()
LocalBenchClient.get_test_suites()
PageInfo
PageInfo.end
PageInfo.page
PageInfo.page_size
PageInfo.sorted_pages
PageInfo.start
PageInfo.total_count
PageInfo.total_pages
arthur_bench.client.rest
Subpackages
arthur_bench.client.rest.admin
Submodules
arthur_bench.client.rest.bench
Submodules
Submodules
ArthurClient
Submodules#
class arthur_bench.client.bench_client.BenchClient#
Bases: ABC
Base class for saving and loading bench data
check_run_exists(suite_id: str, run_name: str) → bool#
Check if run with given name if it exists for suite with id suite_id
Parameters:
client – BenchClient object for fetching test suite data
suite_id – the id of the test suite to check run names
run_name – the test run name to check for
Returns:
True if run with name is found, False otherwise
Raises:
ArthurInternalError – if using a client that does not support pagination
abstract create_new_test_run(test_suite_id: str, json_body: CreateRunRequest) → CreateRunResponse#
Create a new run for a test suite.
Parameters:
test_suite_id – the uuid of the test suite to log a run for
json_body – run request containing run_metadata and scored model
generations
abstract create_test_suite(json_body: TestSuiteRequest) → PaginatedTestSuite#
Create a new test suite.
Parameters:
json_body – test suite request object consisting of test suite metadata
and test cases
abstract delete_test_run(test_suite_id: str, test_run_id: str)#
Delete a test run from a suite.
abstract delete_test_suite(test_suite_id: str)#
Delete a test suite. All associated runs will also be deleted
abstract get_runs_for_test_suite(test_suite_id: str, sort: CommonSortEnum  TestRunSortEnum = CommonSortEnum.CREATED_AT_ASC, page: int = 1, page_size: int = 5) → PaginatedRuns#
Get runs for a given test suite.
Parameters:
test_suite_id – the uuid of the test suite
sort – optional sort key. possible values are ‘name’, ‘avg_score’, and ‘
created_at’.
use ‘-’ prefix for descending sort. defaults to ‘created_at’
page – the page to fetch
page_size – page size to fetch
get_suite_if_exists(name: str) → PaginatedTestSuite  None#
Get a full test suite with name if it exists.
Parameters:
client – BenchClient object for fetching test suite data
Returns:
complete test suite with all test cases joined,
or None if no suite with name exists
Raises:
ArthurInternalError – if using a client that does not support pagination
abstract get_summary_statistics(test_suite_id: str, run_ids: list[str]  None = None, page: int = 1, page_size: int = 5) → TestSuiteSummary#
Fetch aggregate statistics of a test suite. Returns averages and score
distributions for runs in test suite.
Parameters:
test_suite_id – uuid of the test suite
run_id – optional run id. run will be included in response regardless of
page information if provided
page – the page to fetch
page_size – page size to fetch
abstract get_test_run(test_suite_id: str, test_run_id: str, page: int = 1, page_size: int = 5, sort: TestCaseSortEnum  None = None) → PaginatedRun#
Get a test run by id.
Parameters:
test_suite_id – uuid of the test suite
test_run_id – uuid of the test run
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
sort – sort key to sort the retrieved results
abstract get_test_suite(test_suite_id: str, page: int = 1, page_size: int = 5) → PaginatedTestSuite#
Get a test suite by id.
Parameters:
test_suite_id – the uuid of the test suite to fetch
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
abstract get_test_suites(name: str  None = None, sort: CommonSortEnum  TestSuiteSortEnum = TestSuiteSortEnum.LAST_RUNTIME_ASC, scoring_method: List[str]  None = None, page: int = 1, page_size: int = 5) → PaginatedTestSuites#
Get metadata for all test suites.
Parameters:
name – filter test suites by name if provided
sort – optional sort key. possible values are ‘name’, ‘last_run_time’,
‘created_at’, use ‘-’ prefix for descending sort.
defaults to ‘last_run_time’
method (scoring) – optional filter on scoring method name,
multiple names may be provided
page – the page to fetch
page_size – page size to fetch
Next
arthur_bench.client.auth
Previous
Python API Reference
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client
Subpackages
Submodules
BenchClient
BenchClient.check_run_exists()
BenchClient.create_new_test_run()
BenchClient.create_test_suite()
BenchClient.delete_test_run()
BenchClient.delete_test_suite()
BenchClient.get_runs_for_test_suite()
BenchClient.get_suite_if_exists()
BenchClient.get_summary_statistics()
BenchClient.get_test_run()
BenchClient.get_test_suite()
BenchClient.get_test_suites()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html
 arthur_bench.client.http - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.http#
Submodules#
arthur_bench.client.http.helper.construct_url(*parts: str, validate=True, default_https=True) → str#
Construct a URL from various parts
Useful for joining pieces which may or may not have leading and/or trailing
slashes. e.g. construct_url(”https://arthur.ai/”, “/api/v3”, “/users”) will yield
the same valid url as construct_url(”https://arthur.ai”, “api/v3/”, “users/”):
“https://arthur.ai/api/v3/users”.
Parameters:
validate – if True, validate that the URL is valid
default_https – if True, allow urls without a scheme and use https by default
parts – strings from which to construct the url
Returns:
a fully joined url, with NO trailing slash
class arthur_bench.client.http.requests.HTTPClient(base_url: str, path_prefix: str  None = None, default_headers: Dict[str, str]  None = None, verify_ssl: bool = True, timeout_sec: float = 300.0, allow_insecure: bool = True, header_refresh_func: Callable[[], Tuple[Dict[str, str], timedelta]]  None = None)#
Bases: object
A requests-based HTTP Client intended for interacting with JSON-based APIs.
Supports response validation, retries, connection reuse, and multipart requests.
delete(endpoint: str, headers: Dict[str, str]  None = None, return_raw_response: bool = False, params: Dict  bytes  None = None, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP DELETE request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
params – query parameters to add to the request
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the
status code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
get(endpoint: str, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP GET request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, allow any 2XX
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
Raises:
ArthurUserError – failed due to user error
ArthurInternalError – failed due to an internal error
patch(endpoint: str, json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP POST request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
json – data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize.
if files is also supplied, this should be a map from name to content,
to be sent along with the files as a multipart request
files – a map from file names to file-like objects, to be sent as
multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the
status code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
post(endpoint: str, json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP POST request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
json – data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize. if files is also supplied, this should be a
map from name to content, to be sent along with the files as a multipart
request
files – a map from file names to file-like objects, to be sent as
multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
put(endpoint: str, json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP PUT request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
json – data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize. if files is also supplied, this should be a
map from name to content, to be sent along with the files as a
multipart request
files – a map from file names to file-like objects,
to be sent as multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
send(endpoint: str, method: str = 'GET', json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP request
Parameters:
endpoint – the specific endpoint to append to the client URL
method – the HTTP method to use
headers – headers to use for this request in addition to the client
d
efault headers
:param json: data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize. if
files is also supplied, this should be a map from name to content,
to be sent along with the files as a multipart request
Parameters:
files – a map from file names to file-like objects, to be sent as
multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, allow any 2XX
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
Raises:
ArthurUserError – failed due to user error
ArthurInternalError – failed due to an internal error
set_path_prefix(path_prefix: str) → None#
Update the client’s path prefix
This update the path prefix which is prepended to ‘endpoint’ paths.
arthur_bench.client.http.validation.validate_multistatus_response_and_get_failures(response: Response, raise_on_failures: bool = False) → Tuple[List[dict], List[dict]]#
Validate a 207 MultiStatus response and return the failures it contains.
Parameters:
response – requests.Response object to validate, with the following body format:
{
"counts": {
"success": 0,
"failure": 0,
"total": 0
},
"results": [
{
"message": "success",
"status": 200
}
]
}
raise_on_failures – if True, raise an exception if the response contains any
failures
:return: a tuple of two lists: user-caused failures and internal failures
:raises ArthurInternalValueError: If the response does not have 207 status code, or
is incorrectly formatted,
or ‘counts’ and ‘results’ do not agree
Raises:
ResponseClientError – if raise_on_failures and the response contains only
client errors
:raises ResponseServerError: if raise_on_failures and the response contains server
errors
arthur_bench.client.http.validation.validate_response_status(response_or_code: Response  int, expected_status_code: int  None = None, allow_redirects: bool  None = False) → None#
Validate the status code of a requests.Response object or (int) status code.
:param response_or_code: the requests.Response object or status code to validate
:param expected_status_code: the expected status code to check for. If None, all
codes <300 will be valid, and 3XX codes will be subject to allow_redirects
:param allow_redirects: if True will not raise an exception for 3XX status codes
:return: None
:raises InternalValueError: if expected_status_code is not None and does not match
the response code
:raises ResponseServerError: if the response has a 5XX status code
:raises ResponseClientError: if the response has a 4XX status code
:raises ResponseRedirectError: if the response has a 3XX status code
Next
arthur_bench.client.local
Previous
arthur_bench.client.auth
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.http
Submodules
construct_url()
HTTPClient
HTTPClient.delete()
HTTPClient.get()
HTTPClient.patch()
HTTPClient.post()
HTTPClient.put()
HTTPClient.send()
HTTPClient.set_path_prefix()
validate_multistatus_response_and_get_failures()
validate_response_status()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html
 arthur_bench.client.local - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.local#
Submodules#
class arthur_bench.client.local.client.LocalBenchClient(root_dir: str  Path  None = None)#
Bases: BenchClient
Client for managing local file system test suites and runs
create_new_test_run(test_suite_id: str, json_body: CreateRunRequest) → CreateRunResponse#
Create a new run for a test suite.
Parameters:
test_suite_id – the uuid of the test suite to log a run for
json_body – run request containing run_metadata and scored model
generations
create_test_suite(json_body: TestSuiteRequest) → PaginatedTestSuite#
Create a new test suite.
Parameters:
json_body – test suite request object consisting of test suite metadata
and test cases
delete_test_run(test_suite_id: str, test_run_id: str)#
Delete a test run from a suite.
delete_test_suite(test_suite_id: str)#
Delete a test suite. All associated runs will also be deleted
get_runs_for_test_suite(test_suite_id: str, sort: CommonSortEnum  TestRunSortEnum = CommonSortEnum.CREATED_AT_ASC, page: int = 1, page_size: int = 5) → PaginatedRuns#
Get runs for a given test suite.
Parameters:
test_suite_id – the uuid of the test suite
sort – optional sort key. possible values are ‘name’, ‘avg_score’, and ‘
created_at’.
use ‘-’ prefix for descending sort. defaults to ‘created_at’
page – the page to fetch
page_size – page size to fetch
get_summary_statistics(test_suite_id: str, run_ids: list[str]  None = None, page: int = 1, page_size: int = 5) → TestSuiteSummary#
Fetch aggregate statistics of a test suite. Returns averages and score
distributions for runs in test suite.
Parameters:
test_suite_id – uuid of the test suite
run_id – optional run id. run will be included in response regardless of
page information if provided
page – the page to fetch
page_size – page size to fetch
get_test_run(test_suite_id: str, test_run_id: str, page: int = 1, page_size: int = 5, sort: TestCaseSortEnum  None = None) → PaginatedRun#
Get a test run by id.
Parameters:
test_suite_id – uuid of the test suite
test_run_id – uuid of the test run
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
sort – sort key to sort the retrieved results
get_test_suite(test_suite_id: str, page: int = 1, page_size: int = 5) → PaginatedTestSuite#
Get a test suite by id.
Parameters:
test_suite_id – the uuid of the test suite to fetch
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
get_test_suite_by_name(test_suite_name: str) → PaginatedTestSuite#
Additional getter to maintain backwards compatibility with non-identified
local files
get_test_suites(name: str  None = None, sort: CommonSortEnum  TestSuiteSortEnum = TestSuiteSortEnum.LAST_RUNTIME_ASC, scoring_method: List[str]  None = None, page: int = 1, page_size: int = 5) → PaginatedTestSuites#
Get metadata for all test suites.
Parameters:
name – filter test suites by name if provided
sort – optional sort key. possible values are ‘name’, ‘last_run_time’,
‘created_at’, use ‘-’ prefix for descending sort.
defaults to ‘last_run_time’
method (scoring) – optional filter on scoring method name,
multiple names may be provided
page – the page to fetch
page_size – page size to fetch
class arthur_bench.client.local.client.PageInfo(sorted_pages: List, start: int, end: int, page: int, page_size: int, total_pages: int, total_count: int)#
Bases: object
end: int#
page: int#
page_size: int#
sorted_pages: List#
start: int#
total_count: int#
total_pages: int#
Next
arthur_bench.client.rest
Previous
arthur_bench.client.http
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.local
Submodules
LocalBenchClient
LocalBenchClient.create_new_test_run()
LocalBenchClient.create_test_suite()
LocalBenchClient.delete_test_run()
LocalBenchClient.delete_test_suite()
LocalBenchClient.get_runs_for_test_suite()
LocalBenchClient.get_summary_statistics()
LocalBenchClient.get_test_run()
LocalBenchClient.get_test_suite()
LocalBenchClient.get_test_suite_by_name()
LocalBenchClient.get_test_suites()
PageInfo
PageInfo.end
PageInfo.page
PageInfo.page_size
PageInfo.sorted_pages
PageInfo.start
PageInfo.total_count
PageInfo.total_pages
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html
 arthur_bench.client.rest.admin - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.rest.admin#
Submodules#
class arthur_bench.client.rest.admin.client.ArthurAdminClient(http_client: HTTPClient)#
Bases: object
A Python client to interact with the Arthur Admin API
authenticate() → AuthenticationInfo#
Returns authentication info for the calling, token-bearing user
get_current_user() → UserResponse#
Returns the currently authenticated user
login(json_body: LoginRequest) → Tuple[User, RequestsCookieJar]#
If the login attempt is successful, the user will be returned in the response
body and an HttpOnly, set-cookie “Authorization” header will be returned
that contains a JWT to be used in subsequent requests to the API in either
the “Authorization” or cookie header
Parameters:
json_body –
class arthur_bench.client.rest.admin.models.AuthenticationInfo(*, organization_ids: List[str], issuer: str, external_user_id: str  None = None, internal_user_id: str  None = None, service_account_id: str  None = None, username: str, first_name: str, last_name: str, email: str, roles: List[str])#
Bases: BaseModel
email: str#
The email of the Arthur authenticated user or the email of the external IDP user ifthe IDP is configured with that claim. For Arthur service accounts, this will be
empty.
external_user_id: str  None#
An identifier for an external-IdP token bearer. Populated if this user’s token came
from an IDP and the IDP configuration specified an oidc.CustomClaimNames that mapped
UserID to a claim.
first_name: str#
The first name of the Arthur authenticated user, or the first name claim if the
external IDP is configured with one.
internal_user_id: str  None#
An identifier for an Arthur-internal user. Populated for Arthur-authenticated users
with user tokens.
issuer: str#
The identifier of the IDP managing this user.
last_name: str#
The last name of the Arthur authenticated user, or the last name claim if the
external IDP is configured with one.
organization_ids: List[str]#
A list of organization IDs.
roles: List[str]#
The list of roles that this user has. For Arthur tokens, there will always be onerole in this array; however, there can be more than one for external providers.
service_account_id: str  None#
An identifier for an Arthur service account. Populated for Arthur-authenticated
service account tokens.
username: str#
Either the Arthur username or the username specified by an external IDP. This will
be set to arthur.ServiceAccountName for service account tokens.
class arthur_bench.client.rest.admin.models.LoginRequest(*, login: str, password: str)#
Bases: BaseModel
login: str#
either an email or a username
password: str#
class arthur_bench.client.rest.admin.models.User(*, id: str, first_name: str  None = None, last_name: str  None = None, email: str, username: str  None = None, roles: List[str]  None = None, alert_notifications_enabled: bool  None = None, show_intro_sequence: bool  None = None, help_mode_enabled: bool  None = None, created_at: datetime  None = None)#
Bases: BaseModel
alert_notifications_enabled: bool  None#
Whether or not the user will receive email notifications when alerts are triggered,
defaults to ‘false’
created_at: datetime  None#
UTC timestamp of when the user was created
email: str#
The user’s email
first_name: str  None#
The user’s first name
help_mode_enabled: bool  None#
Used by the Arthur dashboard to determine whether or not to show dashboard tooltips
id: str#
the unique id of the user
last_name: str  None#
The user’s last name
roles: List[str]  None#
The user’s roles in the current organization.
show_intro_sequence: bool  None#
Used by the Arthur dashboard to determine whether or not to show the user an intro
sequence upon login
username: str  None#
The username the user can use to login
class arthur_bench.client.rest.admin.models.UserContext(*, name: str  None = None, id: str  None = None)#
Bases: BaseModel
id: str  None#
UUID of the context.
name: str  None#
Name of the context.
class arthur_bench.client.rest.admin.models.UserResponse(*, id: str  None = None, organization_id: str, organization_name: str  None = None, first_name: str  None = None, last_name: str  None = None, email: str  None = None, username: str  None = None, roles: List[str]  None = None, alert_notifications_enabled: bool  None = None, show_intro_sequence: bool  None = None, help_mode_enabled: bool  None = None, plan: str  None = None, created_at: datetime  None = None, contexts: List[UserContext]  None = None)#
Bases: BaseModel
Represents an application user, if the client is using a service token then onlyorganization_id and roles will be populated in the object
alert_notifications_enabled: bool  None#
Whether or not the user will receive email notifications when alerts are triggered,defaults to ‘false’
contexts: List[UserContext]  None#
Contexts that the user has permissions in.
created_at: datetime  None#
UTC timestamp of when the user was created
email: str  None#
The user’s email
first_name: str  None#
The user’s first name
help_mode_enabled: bool  None#
Used by the Arthur dashboard to determine whether or not to show dashboard tooltips
id: str  None#
the unique id of the user
last_name: str  None#
The user’s last name
organization_id: str#
The ID of the users current context
organization_name: str  None#
The name of the users current context
plan: str  None#
string representation of what plan the org of the returned user is associated with(ie. self-service or paidSaas)
roles: List[str]  None#
The user’s roles
show_intro_sequence: bool  None#
used by the Arthur dashboard to determine whether the user should be shown the introsequence upon login
username: str  None#
The username the user can use to login
Next
arthur_bench.client.rest.bench
Previous
arthur_bench.client.rest
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.rest.admin
Submodules
ArthurAdminClient
ArthurAdminClient.authenticate()
ArthurAdminClient.get_current_user()
ArthurAdminClient.login()
AuthenticationInfo
AuthenticationInfo.email
AuthenticationInfo.external_user_id
AuthenticationInfo.first_name
AuthenticationInfo.internal_user_id
AuthenticationInfo.issuer
AuthenticationInfo.last_name
AuthenticationInfo.organization_ids
AuthenticationInfo.roles
AuthenticationInfo.service_account_id
AuthenticationInfo.username
LoginRequest
LoginRequest.login
LoginRequest.password
User
User.alert_notifications_enabled
User.created_at
User.email
User.first_name
User.help_mode_enabled
User.id
User.last_name
User.roles
User.show_intro_sequence
User.username
UserContext
UserContext.id
UserContext.name
UserResponse
UserResponse.alert_notifications_enabled
UserResponse.contexts
UserResponse.created_at
UserResponse.email
UserResponse.first_name
UserResponse.help_mode_enabled
UserResponse.id
UserResponse.last_name
UserResponse.organization_id
UserResponse.organization_name
UserResponse.plan
UserResponse.roles
UserResponse.show_intro_sequence
UserResponse.username
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html
 arthur_bench.client.rest.bench - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.rest.bench#
Submodules#
class arthur_bench.client.rest.bench.client.ArthurBenchClient(http_client: HTTPClient)#
Bases: BenchClient
A Python client to interact with the Arthur Bench API
create_new_test_run(test_suite_id: str, json_body: CreateRunRequest) → CreateRunResponse#
Creates a new test run with model version / associated metadata
Parameters:
test_suite_id –
json_body –
create_test_suite(json_body: TestSuiteRequest) → PaginatedTestSuite#
Creates a new test suite from reference data using specified scoring_method for
scoring
Parameters:
json_body –
delete_test_run(test_suite_id: str, test_run_id: str)#
Deletes a test run
Is idempotent.
Parameters:
test_suite_id –
test_run_id –
delete_test_suite(test_suite_id: str)#
Deletes test suite
Is idempotent.
Parameters:
test_suite_id –
get_runs_for_test_suite(test_suite_id: str, sort: CommonSortEnum  TestRunSortEnum = CommonSortEnum.CREATED_AT_ASC, page: int = 1, page_size: int = 5) → PaginatedRuns#
Get runs for a particular test suite (identified by test_suite_id)
Parameters:
test_suite_id –
sort –
get_summary_statistics(test_suite_id: str, run_ids: list[str]  None = None, page: int = 1, page_size: int = 5) → TestSuiteSummary#
Get paginated summary statistics of a test suite
Defaults to page size of 5.
Parameters:
test_suite_id –
run_id –
page –
page_size –
get_test_run(test_suite_id: str, test_run_id: str, page: int = 1, page_size: int = 5, sort: TestCaseSortEnum  None = TestCaseSortEnum.SCORE_ASC) → PaginatedRun#
Get a test run with input, output, and reference data
Parameters:
test_suite_id –
test_run_id –
page –
page_size –
sort – sort key to sort the retrieved results
get_test_suite(test_suite_id: str, page: int = 1, page_size: int = 5) → PaginatedTestSuite#
Get reference data for an existing test suite
Parameters:
test_suite_id –
get_test_suites(name: str  None = None, sort: CommonSortEnum  TestSuiteSortEnum = TestSuiteSortEnum.LAST_RUNTIME_ASC, scoring_method: List[str]  None = None, page: int = 1, page_size: int = 5) → PaginatedTestSuites#
Gets test suites
Sort by latest run by default.
If name query parameter is provided, filter on test suite name.
param name:
param sort:
param scoring_method:
score_hallucination(json_body: HallucinationScoreRequest) → HallucinationScoreResponse#
Next
arthur_bench.exceptions
Previous
arthur_bench.client.rest.admin
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.rest.bench
Submodules
ArthurBenchClient
ArthurBenchClient.create_new_test_run()
ArthurBenchClient.create_test_suite()
ArthurBenchClient.delete_test_run()
ArthurBenchClient.delete_test_suite()
ArthurBenchClient.get_runs_for_test_suite()
ArthurBenchClient.get_summary_statistics()
ArthurBenchClient.get_test_run()
ArthurBenchClient.get_test_suite()
ArthurBenchClient.get_test_suites()
ArthurBenchClient.score_hallucination()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html
 arthur_bench.client.rest - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.rest#
Subpackages#
arthur_bench.client.rest.admin
Submodules
ArthurAdminClient
ArthurAdminClient.authenticate()
ArthurAdminClient.get_current_user()
ArthurAdminClient.login()
AuthenticationInfo
AuthenticationInfo.email
AuthenticationInfo.external_user_id
AuthenticationInfo.first_name
AuthenticationInfo.internal_user_id
AuthenticationInfo.issuer
AuthenticationInfo.last_name
AuthenticationInfo.organization_ids
AuthenticationInfo.roles
AuthenticationInfo.service_account_id
AuthenticationInfo.username
LoginRequest
LoginRequest.login
LoginRequest.password
User
User.alert_notifications_enabled
User.created_at
User.email
User.first_name
User.help_mode_enabled
User.id
User.last_name
User.roles
User.show_intro_sequence
User.username
UserContext
UserContext.id
UserContext.name
UserResponse
UserResponse.alert_notifications_enabled
UserResponse.contexts
UserResponse.created_at
UserResponse.email
UserResponse.first_name
UserResponse.help_mode_enabled
UserResponse.id
UserResponse.last_name
UserResponse.organization_id
UserResponse.organization_name
UserResponse.plan
UserResponse.roles
UserResponse.show_intro_sequence
UserResponse.username
arthur_bench.client.rest.bench
Submodules
ArthurBenchClient
ArthurBenchClient.create_new_test_run()
ArthurBenchClient.create_test_suite()
ArthurBenchClient.delete_test_run()
ArthurBenchClient.delete_test_suite()
ArthurBenchClient.get_runs_for_test_suite()
ArthurBenchClient.get_summary_statistics()
ArthurBenchClient.get_test_run()
ArthurBenchClient.get_test_suite()
ArthurBenchClient.get_test_suites()
ArthurBenchClient.score_hallucination()
Submodules#
class arthur_bench.client.rest.client.ArthurClient(url: str  None = None, login: str  None = None, password: str  None = None, api_key: str  None = None, organization_id: str  None = None, verify_ssl: bool  None = None, allow_insecure: bool = False, offline: bool = False)#
Bases: object
Next
arthur_bench.client.rest.admin
Previous
arthur_bench.client.local
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.rest
Subpackages
Submodules
ArthurClient
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html
 arthur_bench.exceptions - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.exceptions#
Submodules#
exception arthur_bench.exceptions.exceptions.ArthurError#
Bases: Exception
Base Error for Arthur SDK. This class should not be used directly, Arthur exceptions
should inherit from either ArthurUserError or ArthurInternalError.
exception arthur_bench.exceptions.exceptions.ArthurInternalError#
Bases: ArthurError
Exception raised when user input is correct but an error occurs. Can be used
directly but children are preferred.
exception arthur_bench.exceptions.exceptions.ArthurUserError#
Bases: ArthurError
Exception raised due to incorrect user input to the Arthur SDK. Can be used directly
but children are preferred.
exception arthur_bench.exceptions.exceptions.ExpectedParameterNotFoundError#
Bases: ArthurInternalError
Exception raised when a field or property should be available from Arthur but is
unexpectedly missing.
exception arthur_bench.exceptions.exceptions.ForbiddenError#
Bases: ResponseClientError
Exception raised when a 403 Forbidden response is received from the API.
exception arthur_bench.exceptions.exceptions.InternalTypeError#
Bases: ArthurInternalError, TypeError
Exception raised when a value is unexpected.
exception arthur_bench.exceptions.exceptions.InternalValueError#
Bases: ArthurInternalError, ValueError
Exception raised when a value is unexpected.
exception arthur_bench.exceptions.exceptions.MethodNotApplicableError#
Bases: ArthurUserError
Exception raised when the method called is not valid for the resource.
exception arthur_bench.exceptions.exceptions.MissingParameterError#
Bases: ArthurUserError
Exception raised when parameters supplied to the Arthur SDK are missing.
exception arthur_bench.exceptions.exceptions.NotFoundError#
Bases: ResponseClientError
Exception raised when a 404 Not Found response is received from the API.
exception arthur_bench.exceptions.exceptions.PaymentRequiredError#
Bases: ResponseClientError
Exception raised when a 402 response is received from the API due to a user trying
to access features not available in their plan.
exception arthur_bench.exceptions.exceptions.ResponseClientError#
Bases: ArthurUserError
Exception raised when a 4XX response is received from the API.
exception arthur_bench.exceptions.exceptions.ResponseRedirectError#
Bases: ArthurInternalError
Exception raised when a 3XX response is unexpectedly received from the API.
exception arthur_bench.exceptions.exceptions.ResponseServerError#
Bases: ArthurInternalError
Exception raised when a 5XX response is received from the API.
exception arthur_bench.exceptions.exceptions.UnauthorizedError#
Bases: ResponseClientError
Exception raised when a 401 Unauthorized response is received from the API.
exception arthur_bench.exceptions.exceptions.UserTypeError#
Bases: ArthurUserError, TypeError
Exception raised when a user supplies an argument of the incorrect type to the
Arthur SDK.
exception arthur_bench.exceptions.exceptions.UserValueError#
Bases: ArthurUserError, ValueError
Exception raised when a user supplies an invalid value to the Arthur SDK.
arthur_bench.exceptions.exceptions.arthur_excepted(message=None)#
Decorator to wrap user-facing Arthur functions with exception handling that
describes to the user whether the error is their fault or is our fault and should be
reported.
:param message: an optional message to prefix the error with, should describe the
failure e.g. “failed to send
inferences” or “an error occurred while creating the model.”
:return: the decorator function
Next
arthur_bench.models
Previous
arthur_bench.client.rest.bench
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.exceptions
Submodules
ArthurError
ArthurInternalError
ArthurUserError
ExpectedParameterNotFoundError
ForbiddenError
InternalTypeError
InternalValueError
MethodNotApplicableError
MissingParameterError
NotFoundError
PaymentRequiredError
ResponseClientError
ResponseRedirectError
ResponseServerError
UnauthorizedError
UserTypeError
UserValueError
arthur_excepted()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html
 arthur_bench.models - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.models#
Submodules#
class arthur_bench.models.models.CategoricalHistogramItem(*, count: int, category: Category)#
Bases: BaseModel
category: Category#
count: int#
class arthur_bench.models.models.Category(*, name: str, description: str  None = None)#
Bases: BaseModel
description: str  None#
name: str#
class arthur_bench.models.models.CommonSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
CREATED_AT_ASC = 'created_at'#
CREATED_AT_DESC = '-created_at'#
NAME_ASC = 'name'#
NAME_DESC = '-name'#
class arthur_bench.models.models.CreateRunRequest(*, name: str, test_case_outputs: List[TestCaseOutput], description: str  None = None, model_name: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None, model_version: str  None = None)#
Bases: BaseModel
class Config#
Bases: object
allow_population_by_field_name = True#
classmethod consistent_categories(v)#
description: str  None#
Optional description of the run
foundation_model: str  None#
Optional foundation model name identifiying the pretrained model used to generate
outputs
model_name: str  None#
Optional model name identifying the model used to generate outputs
model_version: str  None#
Optional model version identifying the version of the model used to generate outputs
name: str#
Name identifier of the run
prompt_template: str  None#
Optional prompt template name identifying the global prompt used to generate outputs
test_cases: List[TestCaseOutput]#
List of outputs and scores for all cases in the test suite
class arthur_bench.models.models.CreateRunResponse(*, id: UUID)#
Bases: BaseModel
id: UUID#
class arthur_bench.models.models.HistogramItem(*, count: int, low: float, high: float)#
Bases: BaseModel
Boundaries and count for a single bucket of a run histogram
count: int#
high: float#
low: float#
class arthur_bench.models.models.PaginatedRun(*, id: UUID, name: str, test_suite_id: UUID, test_case_runs: List[RunResult], updated_at: datetime, created_at: datetime, page: int  None = None, page_size: int  None = None, total_pages: int  None = None, total_count: int  None = None)#
Bases: BaseModel
Paginated list of prompts, reference outputs, model outputs, and scores for a
particular run.
class Config#
Bases: object
allow_population_by_field_name = True#
created_at: datetime#
id: UUID#
name: str#
page: int  None#
page_size: int  None#
test_cases: List[RunResult]#
test_suite_id: UUID#
total_count: int  None#
total_pages: int  None#
updated_at: datetime#
class arthur_bench.models.models.PaginatedRuns(*, test_runs: List[TestRunMetadata], page: int, page_size: int, total_pages: int, total_count: int)#
Bases: BaseModel
Paginated list of runs for a test suite.
page: int#
page_size: int#
test_runs: List[TestRunMetadata]#
total_count: int#
total_pages: int#
class arthur_bench.models.models.PaginatedTestSuite(*, id: UUID, name: str, scoring_method: ScoringMethod, test_cases: List[TestCaseResponse], created_at: datetime, updated_at: datetime, description: str  None = None, last_run_time: datetime  None = None, num_runs: int = 0, page: int  None = None, page_size: int  None = None, total_pages: int  None = None, total_count: int  None = None)#
Bases: BaseModel
Test suite and optional page information
created_at: datetime#
description: str  None#
id: UUID#
last_run_time: datetime  None#
name: str#
num_runs: int#
page: int  None#
page_size: int  None#
scoring_method: ScoringMethod#
test_cases: List[TestCaseResponse]#
total_count: int  None#
total_pages: int  None#
updated_at: datetime#
class arthur_bench.models.models.PaginatedTestSuites(*, test_suites: List[TestSuiteMetadata], page: int, page_size: int, total_pages: int, total_count: int)#
Bases: BaseModel
Paginated list of test suites.
page: int#
page_size: int#
test_suites: List[TestSuiteMetadata]#
total_count: int#
total_pages: int#
class arthur_bench.models.models.RunResult(*, id: UUID, output: str, score: float, input: str  None = None, reference_output: str  None = None, score_result: ScoreResult)#
Bases: BaseModel
id: UUID#
input: str  None#
output: str#
reference_output: str  None#
score: float#
score_result: ScoreResult#
classmethod score_result_backwards_compatible(values)#
class arthur_bench.models.models.ScoreResult(*, score: float  None = None, category: Category  None = None)#
Bases: BaseModel
category: Category  None#
classmethod contains_score(values)#
score: float  None#
class arthur_bench.models.models.ScorerOutputType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
Indicates the output type of the scorer
Categorical = 'categorical'#
Continuous = 'continuous'#
class arthur_bench.models.models.ScoringMethod(*, name: str, type: ScoringMethodType, config: dict = {}, output_type: ScorerOutputType = ScorerOutputType.Continuous, categories: List[Category]  None = None)#
Bases: BaseModel
Scoring method configuration
categories: List[Category]  None#
Valid categories returned by the scorer. Only valid if categories is True.
config: dict#
Configuration as used by the scorer to_dict and from_dict methods
name: str#
Name of the scorer
output_type: ScorerOutputType#
Whether the scoring method returns categorical scores
classmethod scoring_method_categorical_defined(values)#
type: ScoringMethodType#
Whether the scoring method was bench default or custom implementation
class arthur_bench.models.models.ScoringMethodType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
Indicates whether the scoring method was provided by the package or a custom
implementation
BuiltIn = 'built_in'#
Custom = 'custom'#
class arthur_bench.models.models.SummaryItem(*, id: UUID, name: str, avg_score: float, histogram: List[HistogramItem  CategoricalHistogramItem])#
Bases: BaseModel
Aggregate statistics for a single run: average score and score distribution
avg_score: float#
classmethod either_continuous_or_categorical(v)#
Validate that the items in the histogram list are all
containing low/high floats or are all containing a category
histogram: List[HistogramItem  CategoricalHistogramItem]#
id: UUID#
name: str#
class arthur_bench.models.models.TestCaseOutput(*, id: UUID, output: str, score: float  None = None, score_result: ScoreResult)#
Bases: BaseModel
A generated output, score pair
id: UUID#
Optional unique identifier for this test case of the suite and run
output: str#
Generated output for test case
score: float  None#
Score assigned to output. This field is decprecated, used score_result instead
score_result: ScoreResult#
Score information about output. Contains float score and / or category description
classmethod score_result_backwards_compatible(values)#
class arthur_bench.models.models.TestCaseRequest(*, input: str, reference_output: str  None = None)#
Bases: BaseModel
An input, reference output pair.
input: str#
Input to the test case. Does not include the prompt template.
reference_output: str  None#
Reference or “Golden” output for the given input.
class arthur_bench.models.models.TestCaseResponse(*, id: UUID, input: str, reference_output: str  None = None)#
Bases: BaseModel
id: UUID#
input: str#
Input to the test case. Does not include the prompt template.
reference_output: str  None#
Reference or “Golden” output for the given input.
class arthur_bench.models.models.TestCaseSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
SCORE_ASC = 'score'#
SCORE_DESC = '-score'#
class arthur_bench.models.models.TestRunMetadata(*, id: UUID, name: str, created_at: datetime, updated_at: datetime, avg_score: float  None = None, model_version: str  None = None, prompt_template: str  None = None)#
Bases: BaseModel
avg_score: float  None#
created_at: datetime#
id: UUID#
model_version: str  None#
name: str#
prompt_template: str  None#
updated_at: datetime#
class arthur_bench.models.models.TestRunSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
AVG_SCORE_ASC = 'avg_score'#
AVG_SCORE_DESC = '-avg_score'#
class arthur_bench.models.models.TestSuiteMetadata(*, id: UUID, name: str, scoring_method: ScoringMethod, last_run_time: datetime  None = None, description: str  None = None, created_at: datetime  None = None, updated_at: datetime  None = None)#
Bases: BaseModel
created_at: datetime  None#
description: str  None#
id: UUID#
last_run_time: datetime  None#
name: str#
scoring_method: ScoringMethod#
updated_at: datetime  None#
class arthur_bench.models.models.TestSuiteRequest(*, name: str, description: str  None = None, scoring_method: ScoringMethod, test_cases: ConstrainedListValue[TestCaseRequest])#
Bases: BaseModel
Test case data and metadata for the test suite.
description: str  None#
Optional description of the test suite
name: str#
Name of the test suite
classmethod null_reference_outputs_all_or_none(v)#
Validate that all or none of test case reference outputs are null
scoring_method: ScoringMethod#
Scoring configuration to use as criteria for the test suite
classmethod scoring_method_backwards_compatible(v)#
test_cases: List[TestCaseRequest]#
List of input texts and optional reference outputs to consistently score
model generations against
class arthur_bench.models.models.TestSuiteSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
LAST_RUNTIME_ASC = 'last_run_time'#
LAST_RUNTIME_DESC = '-last_run_time'#
class arthur_bench.models.models.TestSuiteSummary(*, summary: List[SummaryItem], page: int, page_size: int, total_pages: int, total_count: int, num_test_cases: int, categorical: bool = False)#
Bases: BaseModel
Aggregate descriptions of runs of a test suite.
Provides averages and score distributions
categorical: bool#
num_test_cases: int#
page: int#
page_size: int#
summary: List[SummaryItem]#
total_count: int#
total_pages: int#
class arthur_bench.models.scoring.HallucinationScoreRequest(*, response: str, context: str)#
Bases: BaseModel
Request for hallucination classification
context: str#
Context with which to determine if the model generated response is supported
response: str#
Model generated response
class arthur_bench.models.scoring.HallucinationScoreResponse(*, hallucination: bool, reason: str)#
Bases: BaseModel
Hallucination classification
hallucination: bool#
True if hallucination, false otherwise
reason: str#
Justification for the hallucination classification
Next
arthur_bench.run
Previous
arthur_bench.exceptions
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.models
Submodules
CategoricalHistogramItem
CategoricalHistogramItem.category
CategoricalHistogramItem.count
Category
Category.description
Category.name
CommonSortEnum
CommonSortEnum.CREATED_AT_ASC
CommonSortEnum.CREATED_AT_DESC
CommonSortEnum.NAME_ASC
CommonSortEnum.NAME_DESC
CreateRunRequest
CreateRunRequest.Config
CreateRunRequest.Config.allow_population_by_field_name
CreateRunRequest.consistent_categories()
CreateRunRequest.description
CreateRunRequest.foundation_model
CreateRunRequest.model_name
CreateRunRequest.model_version
CreateRunRequest.name
CreateRunRequest.prompt_template
CreateRunRequest.test_cases
CreateRunResponse
CreateRunResponse.id
HistogramItem
HistogramItem.count
HistogramItem.high
HistogramItem.low
PaginatedRun
PaginatedRun.Config
PaginatedRun.Config.allow_population_by_field_name
PaginatedRun.created_at
PaginatedRun.id
PaginatedRun.name
PaginatedRun.page
PaginatedRun.page_size
PaginatedRun.test_cases
PaginatedRun.test_suite_id
PaginatedRun.total_count
PaginatedRun.total_pages
PaginatedRun.updated_at
PaginatedRuns
PaginatedRuns.page
PaginatedRuns.page_size
PaginatedRuns.test_runs
PaginatedRuns.total_count
PaginatedRuns.total_pages
PaginatedTestSuite
PaginatedTestSuite.created_at
PaginatedTestSuite.description
PaginatedTestSuite.id
PaginatedTestSuite.last_run_time
PaginatedTestSuite.name
PaginatedTestSuite.num_runs
PaginatedTestSuite.page
PaginatedTestSuite.page_size
PaginatedTestSuite.scoring_method
PaginatedTestSuite.test_cases
PaginatedTestSuite.total_count
PaginatedTestSuite.total_pages
PaginatedTestSuite.updated_at
PaginatedTestSuites
PaginatedTestSuites.page
PaginatedTestSuites.page_size
PaginatedTestSuites.test_suites
PaginatedTestSuites.total_count
PaginatedTestSuites.total_pages
RunResult
RunResult.id
RunResult.input
RunResult.output
RunResult.reference_output
RunResult.score
RunResult.score_result
RunResult.score_result_backwards_compatible()
ScoreResult
ScoreResult.category
ScoreResult.contains_score()
ScoreResult.score
ScorerOutputType
ScorerOutputType.Categorical
ScorerOutputType.Continuous
ScoringMethod
ScoringMethod.categories
ScoringMethod.config
ScoringMethod.name
ScoringMethod.output_type
ScoringMethod.scoring_method_categorical_defined()
ScoringMethod.type
ScoringMethodType
ScoringMethodType.BuiltIn
ScoringMethodType.Custom
SummaryItem
SummaryItem.avg_score
SummaryItem.either_continuous_or_categorical()
SummaryItem.histogram
SummaryItem.id
SummaryItem.name
TestCaseOutput
TestCaseOutput.id
TestCaseOutput.output
TestCaseOutput.score
TestCaseOutput.score_result
TestCaseOutput.score_result_backwards_compatible()
TestCaseRequest
TestCaseRequest.input
TestCaseRequest.reference_output
TestCaseResponse
TestCaseResponse.id
TestCaseResponse.input
TestCaseResponse.reference_output
TestCaseSortEnum
TestCaseSortEnum.SCORE_ASC
TestCaseSortEnum.SCORE_DESC
TestRunMetadata
TestRunMetadata.avg_score
TestRunMetadata.created_at
TestRunMetadata.id
TestRunMetadata.model_version
TestRunMetadata.name
TestRunMetadata.prompt_template
TestRunMetadata.updated_at
TestRunSortEnum
TestRunSortEnum.AVG_SCORE_ASC
TestRunSortEnum.AVG_SCORE_DESC
TestSuiteMetadata
TestSuiteMetadata.created_at
TestSuiteMetadata.description
TestSuiteMetadata.id
TestSuiteMetadata.last_run_time
TestSuiteMetadata.name
TestSuiteMetadata.scoring_method
TestSuiteMetadata.updated_at
TestSuiteRequest
TestSuiteRequest.description
TestSuiteRequest.name
TestSuiteRequest.null_reference_outputs_all_or_none()
TestSuiteRequest.scoring_method
TestSuiteRequest.scoring_method_backwards_compatible()
TestSuiteRequest.test_cases
TestSuiteSortEnum
TestSuiteSortEnum.LAST_RUNTIME_ASC
TestSuiteSortEnum.LAST_RUNTIME_DESC
TestSuiteSummary
TestSuiteSummary.categorical
TestSuiteSummary.num_test_cases
TestSuiteSummary.page
TestSuiteSummary.page_size
TestSuiteSummary.summary
TestSuiteSummary.total_count
TestSuiteSummary.total_pages
HallucinationScoreRequest
HallucinationScoreRequest.context
HallucinationScoreRequest.response
HallucinationScoreResponse
HallucinationScoreResponse.hallucination
HallucinationScoreResponse.reason
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html
 arthur_bench.run - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.run#
Submodules#
class arthur_bench.run.testrun.TestRun(*, name: str, test_case_outputs: List[TestCaseOutput], description: str  None = None, model_name: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None, model_version: str  None = None, test_suite_id: UUID, client: BenchClient, id: UUID  None = None)#
Bases: CreateRunRequest
class Config#
Bases: object
arbitrary_types_allowed = True#
property categories: List[str  None]#
client: BenchClient#
classmethod from_flattened(run_name: str, ids: List[UUID], candidate_output_list: List[str], scores: List[float]  List[ScoreResult], client: BenchClient, test_suite_id: UUID, model_name: str  None = None, model_version: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None)#
id: UUID  None#
property output: List[str]#
save() → UUID#
Save a test run.
property scores: List[float  None]#
test_suite_id: UUID#
class arthur_bench.run.testsuite.TestSuite(name: str, scoring_method: str  Scorer, description: str  None = None, reference_data: DataFrame  None = None, reference_data_path: str  None = None, input_column: str = 'input', reference_column: str = 'reference_output', input_text_list: List[str]  None = None, reference_output_list: List[str]  None = None, client: BenchClient  None = None)#
Bases: object
Reusable pipeline for running a test suite built from reference_data and evaluated
using scoring_method
Parameters:
name – name of the test suite
scoring_method – scoring method or scorer instance to use to evaluate the
results of a test run, as a string/enum or class instance
description – short description of the task tested by this suite
reference_data – dataframe of prompts and reference outputs
reference_data_path – filepath to csv of prompts and reference outputs,
required if not specifying reference_data
input_column – the column of reference_data containing prompts, defaults to
‘prompt’
reference_column – the column of reference_data containing reference outputs,
defaults to ‘reference’
input_text_list – list of strings of input texts that can be provided instead
of dataframe columns
reference_output_list – list of strings of reference outputs that can be
provided instead of dataframe columns
async arun(run_name: str, candidate_data: DataFrame  None = None, candidate_data_path: str  None = None, candidate_column: str = 'candidate_output', candidate_output_list: List[str]  None = None, context_column: str  None = None, context_list: List[str]  None = None, save: bool = True, batch_size: int = 5, model_name: str  None = None, model_version: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None) → TestRun#
property description: str  None#
property input_texts: List[str]#
property name: str#
property reference_outputs: List[str  None]#
run(run_name: str, candidate_data: DataFrame  None = None, candidate_data_path: str  None = None, candidate_column: str = 'candidate_output', candidate_output_list: List[str]  None = None, context_column: str  None = None, context_list: List[str]  None = None, save: bool = True, batch_size: int = 1, model_name: str  None = None, model_version: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None) → TestRun#
Score a test run on candidate outputs.
Parameters:
run_name – name for the test run
candidate_data – dataframe of candidate responses to test prompts
candidate_data_path – filepath to csv containing candidate responses to
test prompts
candidate_column – the column of candidate data containing candidate
responses, defaults to ‘candidate_output’
candidate_output_list – list of strings of candidate outputs that can be
provided instead of dataframe
context_column – the column of reference_data containing supporting
context for answering Question & Answering tasks
context_list – list of strings containing supporting context for answering
question and answering tasks
save – whether to save the run results to file
batch_size – the batch_size to use when computing scores
model_name – model name for model used to generate outputs
model_version – model version of model used to generate outputs
foundation_model – foundation model name used to generate outputs
prompt_template – prompt template name used to generate outputs
Returns:
TestRun object containing scored outputs
save()#
Save a test suite to local file system.
property scoring_method: str#
property test_cases: List[TestCaseResponse]#
Next
arthur_bench.scoring
Previous
arthur_bench.models
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.run
Submodules
TestRun
TestRun.Config
TestRun.Config.arbitrary_types_allowed
TestRun.categories
TestRun.client
TestRun.from_flattened()
TestRun.id
TestRun.output
TestRun.save()
TestRun.scores
TestRun.test_suite_id
TestSuite
TestSuite.arun()
TestSuite.description
TestSuite.input_texts
TestSuite.name
TestSuite.reference_outputs
TestSuite.run()
TestSuite.save()
TestSuite.scoring_method
TestSuite.test_cases
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html
 arthur_bench.scoring - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.scoring#
class arthur_bench.scoring.ScoringMethodName(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
BERTScore = 'bertscore'#
ExactMatch = 'exact_match'#
Hallucination = 'hallucination'#
HedgingLanguage = 'hedging_language'#
PythonUnitTesting = 'python_unit_testing'#
QACorrectness = 'qa_correctness'#
Readability = 'readability'#
Specificity = 'specificity'#
SummaryQuality = 'summary_quality'#
WordCountMatch = 'word_count_match'#
arthur_bench.scoring.scorer_from_string(method: str) → type[arthur_bench.scoring.scorer.Scorer]#
Submodules#
class arthur_bench.scoring.bertscore.BERTScore(model_type='microsoft/deberta-v3-base', precision_weight=0.1)#
Bases: Scorer
Tailored bert score implementation.
https://arxiv.org/abs/1904.09675
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.exact_match.ExactMatch(case_sensitive=True)#
Bases: Scorer
Returns 1 if candidate matches reference, 0 if candidate does not match reference.
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
class arthur_bench.scoring.hallucination.Hallucination#
Bases: Scorer
Score each output against a context using Arthur’s hosted hallucination checker
A score of 1.0 means the hallucination checker estimates the output is supported by
the context
A score of 0.0 means the hallucination checker found information in the output
unsupported by the context
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.hedging_language.HedgingLanguage(model_type: str = 'microsoft/deberta-v3-base', hedging_language: str = "As an AI language model, I don't have personal opinions, emotions, or beliefs.")#
Bases: Scorer
Given an input question and model output, determine if the output contains hedging
language such as “As an AI language model, I don’t have personal opinions, emotions,
or beliefs”. The values returned are a similarity score (BERTScore), with higher
values corresponding to higher likelihood of hedging language being present in the
model output.
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.python_unit_testing.PythonUnitTesting(unit_test_dir: str  None = None, unit_tests: List[str]  None = None)#
Bases: Scorer
Wrapping the HuggingFace code_eval metric
Scores each candidate_output as a function against a pre-prepared unit test
Note: considers any code with non-standard python libraries (e.g. numpy) to have an
error
https://huggingface.co/spaces/evaluate-metric/code_eval
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 1) → List[ScoreResult]#
Score a set of test cases. This method doesn’t need to be implemented in most
cases, but can be overriden to add additional functionality such as
task-specific logging.
Parameters:
candidate_outputs – candidate generations to score
reference_outputs – reference strings representing target outputs
inputs – input strings being tested
contexts – optional corresponding contexts, if needed by scorer
batch_size – size of batches
Returns:
scoring results for this run. Float scores are deprecated,
use ScoreResult instead
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.qa_quality.QAQualityCorrectness(llm: BaseChatModel  None = None)#
Bases: Scorer
Given an input question, context string, and model generation, determine if the
generation produced a correct answer.
async arun_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Reference batch is not used for this scoring method, QA correctness requires an
input_text_batch and context_batch
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Reference batch is not used for this scoring method, QA correctness requires an
input_text_batch and context_batch
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
static validate_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → Tuple[List[str], List[str]]#
class arthur_bench.scoring.readability.Readability#
Bases: Scorer
Flesch Reading Ease Score: the higher the score, the easier to read.
Scores of 100-90 correlate to a 5th grade reading level, while scores <10 are
classified as “Extremely difficult to read, and best understood by university
graduates.”
https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
class arthur_bench.scoring.scorer.Scorer#
Bases: ABC
Base class for all scorers. Compute a float score for a given model generation.
async arun(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 5) → List[float]  List[ScoreResult]#
Async version of run method.
async arun_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Async version of run_batch method.
static categories() → List[Category]  None#
All possible values returned by the scorer if output type is categorical.
classmethod from_dict(config: dict)#
Load a scorer from a json configuration file.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
abstract static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 1) → List[float]  List[ScoreResult]#
Score a set of test cases. This method doesn’t need to be implemented in most
cases, but can be overriden to add additional functionality such as
task-specific logging.
Parameters:
candidate_outputs – candidate generations to score
reference_outputs – reference strings representing target outputs
inputs – input strings being tested
contexts – optional corresponding contexts, if needed by scorer
batch_size – size of batches
Returns:
scoring results for this run. Float scores are deprecated,
use ScoreResult instead
abstract run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
to_metadata() → ScoringMethod#
classmethod type() → ScoringMethodType#
Supplies whether a scorer is built-in or custom.
This method is implemented by checking whether the Scorer class is part of the
arthur_bench.scoring module.
:return: the type (built-in or custom)
class arthur_bench.scoring.specificity.Specificity#
Bases: Scorer
Returns a score from 0.0 to 1.0 indicating how specific the candidate output
language is. Higher scores indicate that the language is more specific,
Lower scores indicate more vague language.
Specificity is computed through detecting words that indicate vagueness (predefined)
determing how rare the words used are according to word frequencies calculated by
popular nlp corpora, and detecting use of proper nouns and numbers.
get_mean_word_freq(candidate_output: str) → float#
Returns mean word frequency of candidate output. Higher values indicate that
moree common words on average are used in the candidate output.
Considers only words with frequency <0.001, truncating probability of words with
higher frequencies to 0.001.
get_num_vague_words(candidate_output: str) → int#
Returns number of words in candidate_output which are is a list of pre-defined
vague words.
get_pn_and_num(candidate_output: str) → int#
Returns total number of Proper Nouns and Numbers in candidate output.
Determined heuristically via NNP and CD nltk tags.
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
class arthur_bench.scoring.summary_quality.SummaryQuality(llm: BaseChatModel  None = None, context_window: int = 4096, tokenizer: Encoding  None = None)#
Bases: Scorer
Comprehensive measure of summarization quality compared to a reference summary.
async arun(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 5) → List[float]  List[ScoreResult]#
Async version of run method.
async arun_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Summary quality requires input_text_batch. Asynchronous implementation
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 1) → List[ScoreResult]  List[float]#
Score a set of test cases. This method doesn’t need to be implemented in most
cases, but can be overriden to add additional functionality such as
task-specific logging.
Parameters:
candidate_outputs – candidate generations to score
reference_outputs – reference strings representing target outputs
inputs – input strings being tested
contexts – optional corresponding contexts, if needed by scorer
batch_size – size of batches
Returns:
scoring results for this run. Float scores are deprecated,
use ScoreResult instead
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Summary quality requires input_text_batch.
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
static validate_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → Tuple[List[str], List[str]]#
arthur_bench.scoring.summary_quality.truncate_input_text(input_text, ref_output, cand_output, context_window: int = 4096, tokenizer: ~tiktoken.core.Encoding = <Encoding 'cl100k_base'>) → Tuple[str, bool]#
Truncates the input_text to fit in LLM evaluator context
Truncate the input text so that the filled-in COMPARE prompt
which contains {input text + summary A + summary B} fits in the evaluator context
window
Returns the tuple (text, whether text was truncated)
class arthur_bench.scoring.utils.suppress_warnings(logger_name: str)#
Bases: object
A context-manager class to temporarily set the logging level for a logger to ERROR
before returning it to its previous state.
class arthur_bench.scoring.word_count_match.WordCountMatch#
Bases: Scorer
Calculates how similar the number of words in the candidate output is to the the
number of words in the reference output. Scores span from 0 to 1.
A score of 1.0 indicates that there are the same number of words in the candidate
output as in the reference output. Scores less than 1.0 are calculated as
((len_reference-delta)/len_reference) where delta is the absolute difference in
word lengths between the candidate and reference outputs.
All negative computed values are truncated to 0.
Utilizes lexicon count, removing punctuations: https://pypi.org/project/textstat/
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
Next
arthur_bench.server
Previous
arthur_bench.run
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.scoring
ScoringMethodName
ScoringMethodName.BERTScore
ScoringMethodName.ExactMatch
ScoringMethodName.Hallucination
ScoringMethodName.HedgingLanguage
ScoringMethodName.PythonUnitTesting
ScoringMethodName.QACorrectness
ScoringMethodName.Readability
ScoringMethodName.Specificity
ScoringMethodName.SummaryQuality
ScoringMethodName.WordCountMatch
scorer_from_string()
Submodules
BERTScore
BERTScore.name()
BERTScore.run_batch()
BERTScore.to_dict()
ExactMatch
ExactMatch.categories()
ExactMatch.is_categorical()
ExactMatch.name()
ExactMatch.run_batch()
Hallucination
Hallucination.categories()
Hallucination.is_categorical()
Hallucination.name()
Hallucination.requires_reference()
Hallucination.run_batch()
Hallucination.to_dict()
HedgingLanguage
HedgingLanguage.name()
HedgingLanguage.requires_reference()
HedgingLanguage.run_batch()
HedgingLanguage.to_dict()
PythonUnitTesting
PythonUnitTesting.categories()
PythonUnitTesting.is_categorical()
PythonUnitTesting.name()
PythonUnitTesting.requires_reference()
PythonUnitTesting.run()
PythonUnitTesting.run_batch()
PythonUnitTesting.to_dict()
QAQualityCorrectness
QAQualityCorrectness.arun_batch()
QAQualityCorrectness.categories()
QAQualityCorrectness.is_categorical()
QAQualityCorrectness.name()
QAQualityCorrectness.requires_reference()
QAQualityCorrectness.run_batch()
QAQualityCorrectness.to_dict()
QAQualityCorrectness.validate_batch()
Readability
Readability.name()
Readability.requires_reference()
Readability.run_batch()
Scorer
Scorer.arun()
Scorer.arun_batch()
Scorer.categories()
Scorer.from_dict()
Scorer.is_categorical()
Scorer.name()
Scorer.requires_reference()
Scorer.run()
Scorer.run_batch()
Scorer.to_dict()
Scorer.to_metadata()
Scorer.type()
Specificity
Specificity.get_mean_word_freq()
Specificity.get_num_vague_words()
Specificity.get_pn_and_num()
Specificity.name()
Specificity.requires_reference()
Specificity.run_batch()
SummaryQuality
SummaryQuality.arun()
SummaryQuality.arun_batch()
SummaryQuality.categories()
SummaryQuality.is_categorical()
SummaryQuality.name()
SummaryQuality.run()
SummaryQuality.run_batch()
SummaryQuality.to_dict()
SummaryQuality.validate_batch()
truncate_input_text()
suppress_warnings
WordCountMatch
WordCountMatch.name()
WordCountMatch.run_batch()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html
 arthur_bench.server - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.server#
Submodules#
Next
arthur_bench.telemetry
Previous
arthur_bench.scoring
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.server
Submodules
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html
 arthur_bench.telemetry - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.telemetry#
Submodules#
Next
arthur_bench.utils
Previous
arthur_bench.server
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.telemetry
Submodules
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html
 arthur_bench.utils - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.utils#
Submodules#
arthur_bench.utils.loaders.get_file_extension(filepath: str  PathLike) → str#
arthur_bench.utils.loaders.load_suite_from_csv(filepath: str  PathLike, input_column: str, reference_column: str  None = None) → List[TestCaseRequest]#
Load test case data from csv file.
Parameters:
filepath – string or pathlike object pointing to csv file
input_column – column in file containing inputs
reference_column – column in file containing reference outputs
arthur_bench.utils.loaders.load_suite_from_dataframe(data: DataFrame, input_column: str, reference_column: str  None = None) → List[TestCaseRequest]#
Load test case data from a pandas dataframe.
Parameters:
data – dataframe where each row is a test case consisting of a column for i
input and a column for reference
input_column – column in dataframe containing inputs
reference_column – column in dataframe containing reference outputs
arthur_bench.utils.loaders.load_suite_from_json(filepath: str  PathLike) → TestSuiteRequest#
Load a full test suite from a json file.
Parameters:
filepath – string or pathlike object pointing to json file containing test
suite data
arthur_bench.utils.loaders.load_suite_from_list(inputs: List[str], reference_outputs: List[str]  None) → List[TestCaseRequest]#
Load test case data from lists of strings.
Parameters:
inputs – list of string inputs for each test case
reference_outputs – list of string reference outputs for each input
Next
Contributing
Previous
arthur_bench.telemetry
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.utils
Submodules
get_file_extension()
load_suite_from_csv()
load_suite_from_dataframe()
load_suite_from_json()
load_suite_from_list()
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/setup.html
 Setup - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Setup#
Package installation#
Install Bench to your python environment with optional dependencies for serving results locally (recommended):
pip install 'arthur-bench[server]'
Alternatively, install Bench to your python environment with minimal dependencies:
pip install arthur-bench
Choosing Local vs SaaS:#
Bench has two options for tracking datasets and results:
Local only (default): save data and run server on the same machine that is running the bench package
Arthur SaaS Platform (Coming soon!): Use the package client to log data and results to the Arthur platform. Arthur manages data storage and persistence and hosts the bench server.
Local#
Bench spins up a local UI (like tensorboard) to provide a visual interface for your test data.
View Examples#
Running these commands will view launch the bench UI locally to view the example test suites from the Arthur Bench GitHub repo.
# clone the bench repo
git clone https://github.com/arthur-ai/bench.git
# set the BENCH_FILE_DIR environment variable to point to the example test suite data in the repo
export BENCH_FILE_DIR="./bench/examples/bench_runs/"
# launch the bench UI
bench
You will see a url for a local server that you can copy and paste into your browser to navigate the UI.
Viewing examples in the bench UI will look something like this:
When you want to view the local UI for your own test suites going forward, make sure your BENCH_FILE_DIR environment variable is configured to point to the location of your new test runs, and run bench from the command line.
SaaS (Coming Soon!)#
Bench can be used automatically in conjunction with your team’s existing Arthur platform account. To connect to the Arthur Platform from Bench, you will need an Arthur Bench account and API key.
To log results to the platform, you just need to set the remote url and api key environment variables before creating and running suites. For example,
import os
os.environ['ARTHUR_BENCH_AUTOLOG'] = 'true'
os.environ['ARTHUR_API_URL'] = 'https://app.arthur.ai'
os.environ['ARTHUR_API_KEY'] = 'FILL ME IN'
Next
Quickstart
Previous
Home
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Setup
Package installation
Choosing Local vs SaaS:
Local
View Examples
SaaS (Coming Soon!)
=====================
Content type: arthur_bench_docs
Source: https://bench.readthedocs.io/en/latest/telemetry.html
 Usage Data Collection - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Usage Data Collection#
By default, Arthur is collecting anonymous usage data.
Data being collected#
We track usage data in order to best understand what features users like and use.
Specifically, we collect:
Scoring methods used
Number of runs per test suite
Number of test cases for a test run
An example event looks like (user-id is a random identifier, not tied to any personal data).
{
"event_properties": {
"num_test_suites_load": 3,
"test_suites_all": ["summary_quality", "bertscore", "qa_correctness"]
},
"event_type": "test_suites_load",
"user_id": "fdc73011-2c71-41f3-b174-0d338e2f3f53"
}
Opting-out#
To opt-out, run
bench --disable_push_usage_data
If you want to opt back in, run
bench --enable_push_usage_data
You can also opt-out, by setting the environment variable BENCH_TELEMETRY_DISABLED=1. To opt-out and instead log events that would have been pushed, set BENCH_TELEMETRY_DISABLED=log.
Previous
Contributing
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Usage Data Collection
Data being collected
Opting-out
=====================
