Content type: arthur_blog
Source: https://www.arthur.ai/gap
 GAP
The Generative Assessment ProjectA research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open source models.We'll periodically update the page with our newest, insightful findings on the rapidly-evolving LLM landscapeLLM-Guided Evaluation ExperimentIn this experiment, we looked into LLM sensitivity by testing well-known LLMs as both candidates and evaluators.October 5, 2023Read MoreHedging Answers ExperimentIn this experiment, we test how often commonly-used models respond with hedging answers.August 17, 2023Read MoreHallucination ExperimentWe sought out to explore, both quantitatively and qualitatively, how some of today’s top LLMs compare when responding to challenging questions. August 17, 2023Read MoreThe Most Robust Way to Evaluate LLMsBench is our solution to help teams evaluate the different LLM options out there in a quick, easy and consistent way.Learn More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy