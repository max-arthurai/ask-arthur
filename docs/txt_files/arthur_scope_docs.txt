Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs
 Welcome to Arthur Scope!
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Welcome to Arthur Scope!As your team's data science operations center, Arthur helps enterprise teams monitor, measure and optimize AI performance at scale.Suggest EditsLooking to operationalize your machine learning systems in production? Arthur's observability platform helps enterprise teams monitor, measure, and improve machine learning at scale.
What Happens When AI Meets the Real World?
As the AI performance company, Arthur helps data scientists, product owners, and business leaders accelerate model operations at scale. Our platform monitors, measures, and improves machine learning models for better results across three core areas: accuracy, explainability, and fairness.
Video Overview
Got 4 minutes? Check out a video overview of our product:
Get Started Today
Jump into the Quickstart Guide or start learning about the Arthur Platform.Updated 3 months ago What’s NextQuickstartTable of Contents
What Happens When AI Meets the Real World?
Video Overview
Get Started Today
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/text-binary-classification-1
 Binary Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Binary ClassificationBinary Classification for Text Models in ArthurSuggest EditsBinary classification models predict a binary outcome (ie. one of two potential classes). In Arthur, these models fall into the category of classification and are represented by the Multiclass model type.
Some common examples of Text binary classification are:
Is this email spam or not?
Was this review written by a human or a robot?
Frequently, these models output not only a yes/no answer but also a probability for each (i.e. prob_yes and prob_no). These probabilities are then categorized into yes/no based on a threshold. In these cases, during onboarding teams will supply their classification threshold and continuously track the class probabilities (i.e. prob_yes, prob_no).
Formatted Data in Arthur
Text binary classification models require three things to be specified in their schema: the text input, predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (text input)Probability of Prediction AProbability of Prediction BGround TruthNon-Input Attribute (numeric or categorical)Ubi amor, ibi dolor.95.05AMaleLupus in fabula.86.14BFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their binary classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 3 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below are common examples of the required runnable predict function (that outputs two values, the probability of each potential class).
prediction to ground truth mappingExample Prediction FunctionExample Prediction Function with Transformations## Option 1:
Single Prediction Column, Single Ground Truth Column
# Map PredictedValue Column to its corresponding GroundTruth value.
# This tells Arthur that the `pred_proba_credit_default` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map_1 = {'pred_proba_credit_default' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_1,
)
## Option 2:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
pred_to_ground_truth_map_2 = {'pred_0' : 0,
'pred_1' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_2,
positive_predicted_attr = 'pred_1'
)
## Option 3:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
pred_to_ground_truth_map_3 = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# Building the Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=pred_to_ground_truth_map_3,
positive_predicted_attr = 'pred_1'
)
# example_entrypoint.py
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(x)
# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Available Metrics
When onboarding Text classification models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) when teams onboard a binary classification model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case text) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the only attribute required to onboard a model is the text attribute. So, it is only possible to monitor non-input attributes for fairness in text models.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXX (Non-Input Attributes)Updated 3 months ago What’s NextLearn more about the model onboarding process or jump right into an NLP onboarding quickstartModel OnboardingNLP OnboardingTable of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/image-binary-classification-3
 Binary Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Binary ClassificationBinary Classification for Image Models in Arthur ScopeSuggest EditsBinary classification models predict a binary outcome (i.e., one of two potential classes). In Arthur Scope, these models fall into the classification category and are represented by the Multiclass model type.
Some common examples of Image binary classification are:
Does this CT scan contain a tumor?
Is a roof in a satellite image showing signs of damage?
Frequently, these models output both a yes/no answer and a probability for each (i.e., prob_yes and prob_no). These probabilities are then categorized into yes/no based on a threshold. In these cases, during onboarding, teams will supply their classification threshold and continuously track the class probabilities (i.e., prob_yes, prob_no).
Formatted Data in Arthur
Image binary classification models require three things to be specified in their schema: the image input, the predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (image input)Probability of Prediction AProbability of Prediction BGround TruthNon-Input Attribute (numeric or categorical)image_1.jpg.95.05AMaleimage_2.jpg.86.14BFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their binary classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 3 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below are common examples of the required runnable predict function (that outputs two values, the probability of each potential class).
prediction to ground truth mappingExample Prediction FunctionExample Prediction Function with Transformations## Option 1:
Single Prediction Column, Single Ground Truth Column
# Map PredictedValue Column to its corresponding GroundTruth value.
# This tells Arthur that the `pred_proba_credit_default` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map_1 = {'pred_proba_credit_default' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_1,
)
## Option 2:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
pred_to_ground_truth_map_2 = {'pred_0' : 0,
'pred_1' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_2,
positive_predicted_attr = 'pred_1'
)
## Option 3:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
pred_to_ground_truth_map_3 = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# Building the Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=pred_to_ground_truth_map_3,
positive_predicted_attr = 'pred_1'
)
# example_entrypoint.py
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(x)
# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Available Metrics
When onboarding tabular classification models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
When teams onboard a binary classification model, the following metrics are automatically available in the UI (out-of-the-box). Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the only attribute required to onboard a model is the text attribute. So, monitoring non-input attributes for fairness in image models is only possible.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXX (Non-Input Attributes)Updated 3 months ago What’s NextModel OnboardingTable of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/spark-ml
 Spark ML
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Spark MLSuggest EditsThis guide provides an example of integrating with the ArthurAI platform to monitor a SparkML model. We'll use an example dataset to train a SparkML model from scratch, but you could also use an existing Spark Pipeline.
Pythonfrom pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
import pyspark.sql.functions as f
from pyspark.ml.classification import LogisticRegression
import pandas as pd
import numpy as np
from arthurai import ArthurAI
from arthurai.client.apiv3 import InputType, OutputType, Stage
Train and Save SparkML Model
First, we'll instantiate a Spark session and load in a sample dataset. In this example, we'll use a dataset derived from the famous Boston Housing dataset to build a simple model.
Pythonspark = SparkSession.builder.appName('app').getOrCreate()
data = spark.read.csv('./data/boston_housing.csv', header=True, inferSchema=True)
train, test = data.randomSplit([0.7, 0.3])
We'll use a LASSO classification model to try to predict the is_expensive column from all the others. This column encodes whether or not a property value was above or below the local average.
As preprocessing, we'll use the VectorAssembler class to pull together the input columns into a single numeric feature vector.
Pythonfeature_columns = data.columns[:-1] # here we omit the final column
assembler = VectorAssembler(inputCols=feature_columns,outputCol="features")
lasso_classifier = LogisticRegression(featuresCol="features", labelCol="is_expensive", maxIter=10, regParam=0.3, elasticNetParam=1.0)
Using a Pipeline, we'll combine our preprocessing steps and our ML model, and we'll fit to the training data and save. If you have an existing Spark Pipeline, you can load from disk.
Pythonpipeline = Pipeline(stages=[assembler, lasso_classifier])
fitted_pipeline = pipeline.fit(train)
fitted_pipeline.write().overwrite().save('./data/models/boston_housing_spark_model_pipeline')
Onboard to Arthur
Pythonarthur = ArthurAI(url='https://app.arthur.ai', login="<YOUR_USERNAME_OR_EMAIL>", password="<YOUR_PASSWORD>")
To onboard our model with Arthur, we'll register the schema of the data coming into and out of the model. For simplicity, you can use a Pandas Dataframe for this step. We will take a sample of the SparkDF to the driver, and use this to register the model to Arthur.
Pythonsample_df = train.take(5000).toPandas()
sample_Y = sample_df.loc[['is_expensive']]
sample_X = sample_df.drop('is_expensive', axis=1)
Python# instantiate basic model
arthur_model = arthur.model({
"partner_model_id": "Boston Housing",
"input_type": InputType.Tabular,
"output_type": OutputType.Multiclass,
"is_batch": True})
# use pandas DataFrames to register data schema
arthur_model.from_dataframe(sample_X, Stage.ModelPipelineInput)
arthur_model.add_binary_classifier_output_attributes(
positive_predicted_attr='expensive',
pred_to_ground_truth_map={
'prediction_expensive': 'ground_truth_expensive',
'prediction_cheap': 'ground_truth_cheap'
},
threshold=0.75
)
The from_dataframe() method will inspect your dataset and infer the input schema, datatypes, and sample statistics. You can review the model structure and see if any fixes are needed.
Pythonarthur_model.review()
Python# chas and rad were inferred as categorical, lets change those to be continuous
arthur_model.get_attribute('chas', Stage.ModelPipelineInput).set(categorical=False)
arthur_model.get_attribute('rad', Stage.ModelPipelineInput).set(categorical=False)
arthur_model.review()
Monitoring for bias
For any attributes that you want to monitor for bias, you set the monitor_for_bias boolean. In fact, these don't have to be model inputs, they can also be of stage NonInputData.
Pythonsensitive_attributes = ["Gender", "Race", "Income_Bracket"]
for attribute_name in sensitive_attributes:
arthur_model.get_attribute(attribute_name, Stage.ModelPipelineInput).monitor_for_bias = True
Save
Now you're ready to save your model and finish onboarding.
Pythonarthur_model.save()
Set reference data
You can set a baseline dataset in order to speed up the calculation of data drift and inference anomaly scoring.
This reference set is typically the training set the model was fitted to, or a subsample. You can use either a pandas
DataFrame or a directory of parquet files. The reference data can include model input features, ground truth features
or model predictions on training sets. However, it is recommended that only model input features are provided.
Pythonarthur_model.set_reference_data(directory_path="./data/august_training_data/")
(sparkml_explainability)=
Enabling Explainability for SparkML
To enable explainability, you'll supply a python file that implements a predict() function for a single observation
(a numpy array). This predict function can contain anything you need, including loading a serialized model,
preprocessing/transformations, and making a final prediction. The returned result should be a numpy array. You'll also
supply a requirements file for all the dependencies for running an inference through your model.
For more details around enabling explainability, see the {doc}/user-guide/walkthroughs/explainability guide.
Below we provide a Spark specific example.
The first step is to save your SparkML model and pipeline so it can be imported for use in the predict() function
Pythonfitted_pipeline.write().overwrite().save('./data/models/boston_housing_spark_model_pipeline')
Next is to create your predict() function.
Python# entrypoint.py
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
# To start the spark session on the model server specify the master url as local.
# By default this will run spark using 1 thread, to increase threads you can specify
# local[x] where x is the number of threads. When allocating more compute and memory to the spark
# session be sure to increase the amount allocated to the model server when calling ArthurModel.enable_explainability()
# in the sdk (by default 1 cpu and 1gb of memory is allocated to the model server).
spark = SparkSession.builder.master('local').appName('app').getOrCreate()
loaded_pipeline = PipelineModel.load("./data/models/boston_housing_spark_model_pipeline")
def predict(input_data):
col_names = ['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','b','lstat']
input_df = pd.DataFrame(input_data, columns=col_names)
spark_df = spark.createDataFrame(input_df)
predictions = loaded_pipeline.transform(spark_df)
return np.array([float(x.prediction) for x in predictions.select('prediction').collect()])
You are then ready to {ref}enable explainability <enabling_explainability>
Pythonarthur_model.enable_explainability(
df=sample_df,
project_directory='.',
user_predict_function_import_path='entrypoint',
requirements_file='requirements.txt')
Send Batch of Inferences
Once your model has been onboarded, it is ready to receive inferences and model telemetry.
There are some standard inputs needed to identify inferences and batches.
First, each inference needs a unique identifier so that it can later be joined with ground truth. Include a column named partner_inference_id and ensure these IDs are unique across batches. For example, if you run predictions across your customer base on a daily-batch cadence, then a unique identfier could be composed of your customer_id plus the date.
Second, each inference needs to be associated with a batch_id, but this id will be shared among one or more inferences.
Finally, each inference needs an inference_timestamp and these don't have to be unique.
Additionally, the predictions/scores from your model should match the column names in the registered schema. If we take a look above at arthur_model.review() we'll recall that columns we created correspond to the clasiffier's output probabilities over the classes ("prediction_cheap" and "prediction_expensive") and the corresponding ground truth over the possible classes in one-hot form ("ground_truth_cheap" and "ground_truth_expensive").
We will process a batch of datapoints through the Pipeline and save the inputs (and predictions) to parquet. We will do the same for the ground truths.
Pythonloaded_pipeline = PipelineModel.load("./data/models/boston_housing_spark_model_pipeline")
inferencesDF = loaded_pipeline.transform(test).withColumnRenamed("probability", "prediction_expensive")
uuidUdf= udf(lambda : str(uuid.uuid4()), StringType())
inferencesDF = inferencesDF.withColumn('partner_inference_id', uuidUdf())
# add required columns
inferencesDF["inference_timestamp"] = datetime.utcnow()
inferencesDF["batch_id"] = "inferences_batch_001"
inference_df["partner_inference_id"] = ...
# write inferences
inferencesDF.write.parquet("./data/inference_files/inferences.parquet")
# write ground truths
ground_truth_DF = test.select(["ground_truth_cheap", "ground_truth_expensive"])
ground_truth_DF["partner_inference_id"] = ...
ground_truth_DF["ground_truth_timestamp"] = datetime.utcnow()
ground_truth_DF["batch_id"] = "gt_batch_001"
ground_truth_batch.write.parquet("./data/ground_truth_files/ground_truth.parquet")
With our model's inputs and outputs save as parquet, we upload a batch by pointing to the directory containing one or more parquet files. The directory will be traversed and all parquet files will be joined into the corresponding batch.
Note, the model inputs and predictions will be uploaded separately from the ground truth.
Pythonarthur_model.send_bulk_inferences(directory_path='./data/inference_files/')
You can separately upload ground truths for each inference. Every row in the ground truth file(s) should have an external_id column that matches any IDs you create for the inferences.
Pythonarthur_model.send_bulk_ground_truths(directory_path='./data/ground_truth_files/')
Updated 3 months ago Table of Contents
Train and Save SparkML Model
Onboard to Arthur
Set reference data
Enabling Explainability for SparkML
Send Batch of Inferences
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/appendix
 Appendix
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by AppendixSuggest EditsRunning the Velero CLI
Velero provides a Command-Line Interface (CLI) for taking backups and performing restores. The CLI can be installed locally, or it can be invoked by kubectl exec on the Velero Backup Controller pod.
Local Installation
Refer to the Velero Documentation for installing Velero on your platform.
Velero uses your KUBECONFIG file to connect to the cluster.
$ velero --help
Velero is a tool for managing disaster recovery, specifically for Kubernetes
cluster resources. It provides a simple, configurable, and operationally robust
way to back up your application state and associated data.
If you're familiar with kubectl, Velero supports a similar model, allowing you to
execute commands such as 'velero get backup' and 'velero create schedule'. The same
operations can also be performed as 'velero backup get' and 'velero schedule create'.
Usage:
velero [command]
Available Commands:
backup
Work with backups
backup-location
Work with backup storage locations
bug
Report a Velero bug
client
Velero client related commands
completion
Generate completion script
create
Create velero resources
debug
Generate debug bundle
delete
Delete velero resources
describe
Describe velero resources
get
Get velero resources
help
Help about any command
install
Install Velero
plugin
Work with plugins
restic
Work with restic
restore
Work with restores
schedule
Work with schedules
snapshot-location Work with snapshot locations
uninstall
Uninstall Velero
version
Print the velero version and associated image
Flags:
--add_dir_header
If true, adds the file directory to the header
--alsologtostderr
log to standard error as well as files
--colorized optionalBool
Show colored output in TTY. Overrides 'colorized' value from $HOME/.config/velero/config.json if present. Enabled by default
--features stringArray
Comma-separated list of features to enable for this Velero process. Combines with values from $HOME/.config/velero/config.json if present
-h, --help
help for velero
--kubeconfig string
Path to the kubeconfig file to use to talk to the Kubernetes apiserver. If unset, try the environment variable KUBECONFIG, as well as in-cluster configuration
--kubecontext string
The context to use to talk to the Kubernetes apiserver. If unset defaults to whatever your current-context is (kubectl config current-context)
--log_backtrace_at traceLocation
when logging hits line file:N, emit a stack trace (default :0)
--log_dir string
If non-empty, write log files in this directory
--log_file string
If non-empty, use this log file
--log_file_max_size uint
Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
--logtostderr
log to standard error instead of files (default true)
-n, --namespace string
The namespace in which Velero should operate (default "velero")
--skip_headers
If true, avoid header prefixes in the log messages
--skip_log_headers
If true, avoid headers when opening log files
--stderrthreshold severity
logs at or above this threshold go to stderr (default 2)
-v, --v Level
number for the log level verbosity
--vmodule moduleSpec
comma-separated list of pattern=N settings for file-filtered logging
Use "velero [command] --help" for more information about a command.
Executing on the Velero Backup Controller Pod
If it is not possible to install the Velero CLI on the local workstation, you can still run Velero commands directly on the Velero pod as follows:
Shell$ velero_namespace="Put your Velero namespace here"
$ kubectl exec velero-699dc869d4-r24bh -n $velero_namespace -c velero -- /velero help
Velero is a tool for managing disaster recovery, specifically for Kubernetes
cluster resources. It provides a simple, configurable, and operationally robust
way to back up your application state and associated data.
<<<output-truncated-for-brevity>>>
Backups using Velero
Creating a Backup
To take a backup of Arthur, you would invoke the CLI as follows.
Shell$ arthur_namespace="Put your Arthur namespace here"
$ velero_namespace="Put your Velero namespace here"
$ velero backup create <some-unique-name> \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--storage-location=docs-demo-backup-location-velero
Listing all Backups
You can list all backups using the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero backup get -n $velero_namespace
Describing a Backup
You can get an overview of the backup using the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero backup describe <insert-backup-name> -n $velero_namespace
Debugging a Backup
For debugging a backup, you can access the backup's logs using the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero backup logs <insert-backup-name> -n $velero_namespace  head
Restores using Velero
Similar to Backup, Restore happens using the Velero CLI. A restore takes a Backup object and then executes the restore procedure.
Attempting a Restore
You can execute a restore with the following Velero CLI command:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero restore create \
--from-backup <insert-backup-name> \
--namespace $velero_namespace \
--restore-volumes=true
Listing all Restore attempts
Just like with the Backup, Velero will create a Restore Velero Resource, which you can inspect with the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero restore get -n $velero_namespace
Describing a Restore attempt
You can get an overview of the restore attempt using the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero restore describe <insert-restore-name> -n $velero_namespace
Debugging a Restore attempt
For debugging a restore attempt, you can access the logs using the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero restore logs <insert-restore-name> -n $velero_namespace  head
Running Backups on a Schedule
There are two ways the Arthur platform can be backed up on a schedule:
Scripting the entire backup process {ref}(see example) <scripted_solution>, and executing it on a fixed schedule from a job runner (Jenkins, Gitlab-CI etc.)
Leveraging native schedulers to back up the individual components of the platform:
Clickhouse is backed up at midnight (by default) using Kubernetes CronJobs out-of-the-box.
Use Velero Schedules to create Velero Backups:
Messaging infrastructure
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
$ storage_location="Put your storage location here"
$ arthur_namespace="Put your Arthur namespace here"
$ velero_namespace="Put your Velero namespace here"
$ velero schedule create messaging-infra-backup-nightly \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='app in (cp-zookeeper,cp-kafka)' \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,services,endpoints,configmaps,poddisruptionbudgets
--storage-location=$storage_location \
--schedule "0 0 * * *" \
--ttl 720h0m0s
Enrichments (infrastructure and workflows)
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
$ storage_location="Put your storage location here"
$ arthur_namespace="Put your Arthur namespace here"
$ velero_namespace="Put your Velero namespace here"
$ velero schedule create enrichments-workflows-backup-nightly \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--include-resources=workflows \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--schedule "0 0 * * *" \
--ttl 720h0m0s
$ velero schedule create qa-enrichments-infra-backup-nightly \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='component in (kafka-mover-init-connector, model_server)' \
--include-resources=deployments,services \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--schedule "0 0 * * *" \
--ttl 720h0m0s
RDS databases can be automatically backed up on a schedule, not at a specific point in time but within a 30-minute window. And during this window, the database is snapshotted at a random time. Due to this limitation from AWS, ensure there are no operations (like model CRUD, etc.) on the Arthur platform during the backup window.
shell
$ aws rds modify-db-instance \
--db-instance-identifier RDS_DB_NAME \
--backup-retention-period 14 \
--preferred-backup-window 23:45-00:15 \
--profile AWS_PROFILE_NAME \
--region AWS_REGION \
--apply-immediately
Sample Backup Script (manual)
The following script can be used to run all the backup steps together:
Shell#!/bin/bash
set -euo pipefail
IFS=$'\n\t'
# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location="Put your storage location here"
arthur_namespace="Put your Arthur namespace here"
velero_namespace="Put your Velero namespace here"
backup_date=$(date +%Y-%m-%d-%H-%M-%S);
name=arthur-backup-$backup_date
echo "Creating a new backup with name $name"
echo "Taking a backup of CH data"
kubectl create job $name-clickhouse-backup \
--namespace=$arthur_namespace \
--from=cronjob/clickhouse-backup-cronjob
ch_backup_jobname=$(kubectl get jobs -o name -n "$arthur_namespace"  grep "$name-clickhouse-backup")
kubectl wait $ch_backup_jobname \
--namespace=$arthur_namespace \
--for=condition=complete \
--timeout=30m
echo "Taking a backup of the enrichments infrastructure"
velero backup create $name-enrichments \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='component in (kafka-mover-init-connector, model_server)' \
--include-resources=deployments,services \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
echo "Taking a backup of workflows"
velero backup create $name-workflows \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--include-resources=workflows \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
echo "Taking a backup of Kafka/Kafka-ZK StatefulSets, their EBS Volumes, and related components"
velero backup create $name-messaging \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='app in (cp-zookeeper,cp-kafka)' \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,services,endpoints,configmaps,poddisruptionbudgets \
--storage-location=$storage_location \
--wait
echo "Taking a backup of the RDS database"
aws rds create-db-cluster-snapshot \
--db-cluster-snapshot-identifier $name-snapshot \
--db-cluster-identifier RDS_DB_NAME \
--profile AWS_PROFILE_NAME \
--region AWS_REGION
Updated 3 months ago Table of Contents
Running the Velero CLI
Local Installation
Executing on the Velero Backup Controller Pod
Backups using Velero
Creating a Backup
Listing all Backups
Describing a Backup
Debugging a Backup
Restores using Velero
Attempting a Restore
Listing all Restore attempts
Describing a Restore attempt
Debugging a Restore attempt
Running Backups on a Schedule
Sample Backup Script (manual)
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/on-prem-deployment
 Installation Overview
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Installation OverviewSuggest EditsArthur Scope can be installed to existing Kubernetes clusters (K8s) or virtual machines (VM). For both Kubernetes and virtual machine deployments, Arthur supports two types of installation methods, online and airgapped. The online method allows fast and continuous differential upgrades. The airgapped install supports environments where the cluster has no outbound internet connectivity.Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/installing-kubernetes
 Installing Kubernetes
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Installing KubernetesSuggest EditsThis section covers the steps required for installing Arthur on a Kubernetes cluster. There are separate steps required for online and airgapped installations. An additional set of instructions are also available for installing Arthur that’s scoped within a K8s namespace.Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/backing-up-the-arthur-platform
 Backing Up the Arthur Platform
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Backing Up the Arthur PlatformSuggest EditsOnce all the Pre-Requisites have been met, the various Arthur platform components can be backed up. The process to manually backup individual components is detailed below, which may also be {ref}scripted<scripted_solution>.
Backing up Clickhouse Data
By default, the Arthur Platform ships with a Kubernetes CronJob, which backs up Clickhouse daily at midnight.
To manually back up ClickHouse data, you can run the following commands:
Shellarthur_namespace="Put your Arthur namespace here"
$ kubectl get cronjobs -n $arthur_namespace  grep -i clickhouse
NAME
SCHEDULE
SUSPEND
ACTIVE
LAST SCHEDULE
AGE
clickhouse-backup-cronjob
0 0 * * *
False
0
14h
2d18h
$ kubectl create job clickhouse-backup --from=cronjob/clickhouse-backup-cronjob -n $arthur_namespace
job.batch/clickhouse-backup created
$ kubectl get jobs -n $arthur_namespace
NAME
COMPLETIONS
DURATION
AGE
clickhouse-backup-cronjob-27735840
1/1
8m35s
14m
Backing Up Enrichments
The Arthur Platform uses Velero to take a backup of the Enrichments Infrastructure and the Enrichments workflows.
The Enrichments infrastructure and Enrichment Workflows are orchestrated as separate backups and will require running 2 separate commands.
Backing Up Enrichments Infrastructure
To manually back up the Enrichments infrastructure, run the following commands:
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location="Put your storage location here"
arthur_namespace="Put your Arthur namespace here"
velero_namespace="Put your Velero namespace here"
$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
arthur_namespace=<insert-arthur-namespace-here>
velero_namespace=<insert-velero-namespace-here>
$ velero backup create $name-enrichments \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='component in (kafka-mover-init-connector, model_server)' \
--include-resources=deployments,services \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
Backing Up Enrichments Workflows
To manually back up the Enrichments Workflows, run the following commands:
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location="Put your storage location here"
arthur_namespace="Put your Arthur namespace here"
velero_namespace="Put your Velero namespace here"
$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
velero backup create $name-workflows \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--include-resources=workflows \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
Backing Up Messaging Infrastructure
The Arthur Platform uses Velero to take a backup of the Kafka (and ZooKeeper) Deployment State and EBS Volumes.
To manually back up Kafka, run the following commands:
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location="Put your storage location here"
arthur_namespace="Put your Arthur namespace here"
velero_namespace="Put your Velero namespace here"
$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
$ velero backup create $name-messaging \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='app in (cp-zookeeper,cp-kafka)' \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,services,endpoints,configmaps,poddisruptionbudgets \
--storage-location=$storage_location \
--wait
Backing Up RDS Postgres
RDS database backups are called Snapshots. To manually create a snapshot of an RDS database, execute the below script:
Shell$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
$ aws rds create-db-cluster-snapshot \
--db-cluster-snapshot-identifier $name-snapshot \
--db-cluster-identifier RDS_DB_NAME \
--profile AWS_PROFILE_NAME \
--region AWS_REGION
📘RDS CompatibilityThe command is only compatible for a multi-region RDS database cluster. If you are using a single-region RDS database, the command to use is aws rds create-db-snapshot.
For more information, please refer to the AWS Documentation:
Multi-region RDS cluster and AWS CLI command
Single-region RDS instance and AWS CLI command
Updated 3 months ago Table of Contents
Backing up Clickhouse Data
Backing Up Enrichments
Backing Up Enrichments Infrastructure
Backing Up Enrichments Workflows
Backing Up Messaging Infrastructure
Backing Up RDS Postgres
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/generative-llm
 Token Sequence (LLM)
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Token Sequence (LLM)Suggest EditsGenerative text model outputs are computer-generated text that mimics human language patterns and structures based on patterns learned from a large dataset. In Arthur, these models are listed under the Token Sequence model type.
Some common examples of Generative Text models are:
Headline summarization
Question and Answering (chatbots)
Formatted Data in Arthur
Depending on how you built your generative model and what you are looking to track, there are different types of data that you can track in the platform.
Attribute (User Text Input)Output (Text Output)Output Likelihood (Token Likelihood) (Optional)Non Input Attribute (numeric or categorical)Dulce est desipere in locoActa, non verba[{"Acta": 0.7, ",": 0.3, "non": 0.8, "verba": 0.34}]Political TopicSi vis amari amaCastigat ridendo mores[{"Cast": 0.56, "igat": 0.4, "ridendo": 0.24, "mores": 0.67}]Entertainment Topic
Predict Function and Mapping
Teams need to specify the relationship between the prediction and ground truth columns to help set up their Arthur model's environment to calculate default performance metrics. Here is an example of what that might look like:
Registering Predict / GT mapping## Registering columns
arthur_model.build_token_sequence_model(
input_column="user_input",
output_text_column="output_text"
## optional for model types with token likelihood
output_likelihood_column="token_likelihoods"
)
Available Metrics
When onboarding Token Sequence models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) when teams onboard a Token Sequence model. Find out more about how to use these metrics in the
Performance Metrics section. For metric definitions, check out the Glossary.
MetricMetric TypeAverage Token LikelihoodPerformanceLikelihood StabilityPerformanceAverage Sequence LengthPerformanceInference CountIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Some things of note: For unstructured data types (like text and image), feature drift is calculated for non-input attributes. Additionally, generative text models create text input and output that can be tracked with multivariate drift.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftMultivariate Drift for Prompts (Text Input)Multivariate DriftMultivariate Drift for Predictions (Text Output)Multivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install
 Airgap Kubernetes Cluster (K8s) Install
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Airgap Kubernetes Cluster (K8s) InstallSuggest EditsEnsure your K8s cluster is ready for Arthur platform installation by following Installing Arthur Pre-requisites guide.
Preparing Container Registries
Prepare your private container image registry for Arthur artifacts by creating the following list of repositories:
Admin Console:
arthurai/dex
arthurai/kotsadm
arthurai/kotsadm-migrations
arthurai/local-volume-fileserver
arthurai/local-volume-provider
arthurai/minio
arthurai/postgres
Application:
arthurai/alert-service
arthurai/alpine
arthurai/api-service
arthurai/argocli
arthurai/argoexec
arthurai/aws-cli
arthurai/beta-client
arthurai/busybox
arthurai/clickhouse-operator
arthurai/clickhouse-server
arthurai/client
arthurai/cp-kafka
arthurai/cp-kafka-connect
arthurai/cp-schema-registry
arthurai/cp-zookeeper
arthurai/custom-hpa
arthurai/dataset-service
arthurai/ingestion-service
arthurai/kafka-connect-monitor
arthurai/kafka-exporter
arthurai/kafka-prometheus-jmx-exporter
arthurai/kubectl
arthurai/mc
arthurai/metric-service
arthurai/metrics-exporter
arthurai/minio
arthurai/model-server
arthurai/model-server-controller
arthurai/postgresql
arthurai/python-jobs
arthurai/python-spark-jobs
arthurai/pytorch-jobs
arthurai/query-service
arthurai/redis
arthurai/scala-spark-jobs
arthurai/schema-service
arthurai/workflow-controller
arthurai/zookeeper-exporter
As an example, here's how you can create a new arthurai/alert-service repository on AWS ECR.
Shellexport AWS_REGION=<your_region>
aws ecr create-repository --repository-name=arthurai/alert-service
Download Installation Files
Go to the download portal using the URL and the password provided by Arthur.
Select the "Bring my own cluster" option
Click the “Download license” button to download your license in the YAML file.
Download the "KOTS Airgap Bundle" and the "arthur Airgap Bundle".
Setup for Installation
Make sure you're in the correct kubectl environment context before running the installer.
Shellkubectl config current-context
Install the KOTS kubectl extension on your local machine:
Shellcurl https://kots.io/install  bash
If the Linux workstation you're running kubectl from is also in the airgap environment; download the "KOTS CLI" from the download portal and install it like below:
Shelltar zxf kots_linux_amd64.tar.gz
# move it to a location that's on your path
sudo mv kots /usr/local/bin/kubectl-kots
📘The "KOTS CLI" and "KOTS Airgap Bundle" must be installed at the same time and therefore will be on the same version.
If your workstation is a Mac, you can download the latest Kots CLI Darwin binary version from https://kots.io/.
Start Installation
Push the Admin Console images to your private registry:
Shellkubectl kots admin-console push-images ./kotsadm.tar.gz [Your private registry host]/arthurai \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
As an option, you can also pre-upload the application images to your private registry before running the installer:
Shellkubectl kots admin-console push-images ./arthur-x.x.x.airgap [Your private registry host]/arthurai \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
Install the Admin Console (see here for {doc}Namespace-Scoped Installs <k8s_install_namespace_scoped>):
Shellkubectl kots install arthur \
--no-port-forward \
--namespace arthur \
--shared-password [Provide an Admin Console password] \
--kotsadm-namespace arthurai \
--kotsadm-registry [Your private container image repository] \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
Create a port forwarding tunnel to Admin Console. Go to http://localhost:8800 to access the Admin Console:
Shellkubectl kots admin-console --namespace arthur
Follow the instructions on the Admin Console to complete your installation by providing the private registry details and arthur-x.x.x.airgap bundle.
📘The upload process can take couple of hours so ensure your laptop does not go to sleep.You may follow the instructions Airgap Kubernetes Cluster (K8s) Install with CLI to install the Admin Console and Arthur app programmatically using the CLI only
Configure Arthur.
Verify Installation
Monitor the Admin Console dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with thekubectl CLI:
Shellkubectl get deployment,statefulset,pod -n arthur
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
Customize Installation
Configure graphs on Admin Console by clicking on the Configure Prometheus Address button and providing your Prometheus endpoint (e.g.,http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090).Updated 3 months ago Table of Contents
Preparing Container Registries
Download Installation Files
Setup for Installation
Start Installation
Verify Installation
Customize Installation
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/text
 Text
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by TextSuggest EditsText input models are a type of machine learning model that operates on text data, such as natural language text found in documents, emails, social media posts, and other forms of written communication. These models are designed to learn the patterns and relationships between words and phrases in order to perform tasks such as sentiment analysis, language translation, and text classification. Text input models can be built using a variety of techniques, including neural networks, decision trees, and support vector machines.
Embeddings in Arthur
Text models in Arthur take in Text input, i.e. raw text of the documents or social media posts teams are using to predict. While there are a few enrichments (namely Anomaly Detection) that use model embeddings, Arthur computes these embeddings internally.
Currently, Arthur does not take in embeddings or vector inputs.
Tokenization in Arthur
Text inputs are tokenized in Arthur for both anomaly detection and explainability. To create these tokens, Arthur has a few different text delimiters.
Text Delimiters
Here are the different delimiters available in Arthur.
NameDefined in ArthurDescriptionCOMMA","Splits on a single comma.COMMA_PLUS",+"Splits on one or more commas.NOT_WORD"\W+"Splits on any character that is not a word.PIPE""Splits on a single pipe.PIPE_PLUS"+Splits on one or more pipes.WHITESPACE"\s+"Splits on whitespace.Updated 3 months ago Table of Contents
Embeddings in Arthur
Tokenization in Arthur
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/reference
 Creating a Connection to Arthur
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…JUMP TOIntroductionCreating a Connection to ArthurArthur API ReferenceArthur SDK ClientArthur SDK DocumentationtionTest Explainability LocallyPowered by JUMP TOIntroductionCreating a Connection to ArthurArthur API ReferenceArthur SDK ClientArthur SDK DocumentationtionTest Explainability LocallyPowered by Creating a Connection to ArthurThis page will help you get started with Arthur API.There are a few ways to create a connection to Arthur.
Creating an API Key
To access the Arthur platform outside of Arthur, you specify your access rights. One way to do that is through an API key. To get an API key from Arthur, users need to go into the Settings of their Organizational dashboard. From there, they need to click on the API Keys Tab and click the Create API Key button.
📘API Key == Header in API ExamplesWhen using the API example structure in this documentation, you should place your API key in the header section of the call you are trying to build.
Connecting to a Specific Arthur Model within the SDK
After connecting to your Arthur instance, whether you are using the API or Arthur Python SDK, it is common for teams to want to connect to a specific Arthur model. There are two ways to identify a specific model within Arthur, shown below:
Partner Model ID: The Partner Model ID is an ID given to the model by the customer team/user that onboarded the model. This can be found in the UI under the Details section of your model.
Arthur Model ID: The Arthur Model ID is a unique ID given to the model by the Arthur platform. This can be found in your Arthur instance in the URL of your model.
Connecting to the Python SDK
The Arthur Python SDK, discussed in more detail here, is one of the most common ways teams interact with the Arthur platform in a notebook environment.
Connect to Platform
When connecting to the Arthur platform with the SDK, there are two
API KeyUsername Passwordimport arthurai as ArthurAI
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>",
password = "<YOUR_PASSWORD>")
import arthurai as ArthurAI
arthur = ArthurAI(url="YOUR_ORGANIZAIONS_ARTHUR_URL",
login="username", password = "password")
Specifying Organization ID
If you log in with your Username and Password and are a member of multiple organizations within Arthur, you must specify which organization you want to connect to within the notebook. This can be done by adding the organization_id to your connection.
Pythonimport arthurai as ArthurAI
connection = ArthurAI(url="YOUR_ORGANIZAIONS_ARTHUR_URL",
login="username", password = "password", organization_id = "org_id")
You should contact your system administrator if you are unsure what your organization ID is. You can also see a list of all possible organization IDs you have access to in a notebook by running this Python script:
Pythonconnection = ArthurAI(url="YOUR_ORGANIZAIONS_ARTHUR_URL",
login="username", password = "password", verify_ssl=False)
import requests
headers = {
'Content-Type': 'application/json',
'Authorization': connection.client.session.headers['Authorization']
}
session = requests.Session()
response = session.get("https://<YOUR_ORGANIZATIONS_URL>/api/v3/organizations?page_size=1000000", headers = headers,verify=False)
print(response.json())
Connect to Specific Arthur Model
Either of these can be used to connect to Arthur with this code:
Python## Using Partner Model ID
arthur_model = connection.get_model(place_partner_id, id_type="partner_model_id")
## Using Arthur Model ID
arthur_model = connection.get_model(place_model_id, id_type="id")
Connecting to the API
To connect to the API, users have the same permissions as above.
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges
 Kubernetes Cluster (K8s) Install with Namespace Scope Privileges
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Kubernetes Cluster (K8s) Install with Namespace Scope PrivilegesSuggest EditsIf you would like to install the Arthur platform with namespace scoped privileges, there are certain components that will fail since they will need cluster-level access. These cluster-level components are CRDs (Custom Resource Definitions) which are required for the proper functioning and operation of the Arthur platform. However, these CRDs only need to be installed once with cluster-admin privileges, and elevated access is not required for normal usage of the platform.
📘Both the Admin Console and Arthur application can be installed at the cluster-level or namespace-scope independent of each other.
CRDs leveraged by Arthur Platform
The Arthur platform makes use of the following two CRDs:
Argo Workflows : Kubernetes-native Open-source workflow manager
ClickHouse Operator : Column-oriented OLAP datastore
Installing Admin Console within a Namespace
By default, the Admin Console is installed at the Cluster level, available in all namespaces. If you would like to install the Admin Console only within a specific namespace, you can use the following flag to the kots command:
Shellkubectl kots install arthur \
--use-minimal-rbac
--skip-rbac-check
Since the Admin Console will not have access to the Cluster, certain Preflight checks will fail. The Cluster Admin is responsible for ensuring sufficient resources are provisioned with the correct version of K8s.
Installing Cluster-level CRDs for Arthur from Nexus
Since the Arthur platform requires CRDs for normal operation, these will need to be installed by the Cluster Admin before installing Arthur itself, in no particular order. The instructions below show how to download the CRD charts from our publicly hosted repository.
Argo Workflows:
Shellhelm repo add arthurai-released https://repository.arthur.ai/repository/charts --username <nexus-username> --password <nexus-password>
helm install argo-workflows-crd arthurai-released/argo-workflows-crd --version 0.19.1-arthur-1
ClickHouse Operator:
Shellhelm repo add arthurai-released https://repository.arthur.ai/repository/charts --username <nexus-username> --password <nexus-password>
helm install clickhouse-operator-crd arthurai-released/clickhouse-operator-crd --version 0.19.2-arthur-1
🚧Please reach out to our Sales Team if you do not have credentials to our Nexus repository.
Installing Cluster-level CRDs for Arthur from Airgap Bundle
If you are in an air-gapped environment with no access to the public internet, the CRD charts are also available in the Airgap bundle provided to you. The instructions below show how to extract the charts from the Airgap bundle.
Shelltar -xvf arthur-<version>.airgap
cd arthur-<version>
tar -xvf app.tar.gz
cd app
Argo Workflows:
Shellhelm install argo-workflows-crd argo-workflows-crd-0.14.0-arthur-2.tgz
ClickHouse Operator:
Shellhelm install clickhouse-operator-crd arthurai-released/clickhouse-operator-crd-0.18.4-arthur-2.tgz
You can verify the CRDs have been installed successfully by executing the following command:
Shellkubectl get crd  grep -iE 'argoclickhouse'
Now that we have the prereqs installed with elevated access, we can now switch over to namespace-scoped access to complete the installation either using the Admin Console using the Admin Console or using the CLI.Updated 3 months ago Table of Contents
CRDs leveraged by Arthur Platform
Installing Admin Console within a Namespace
Installing Cluster-level CRDs for Arthur from Nexus
Installing Cluster-level CRDs for Arthur from Airgap Bundle
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/externalizing-the-relational-database
 Externalizing the Relational Database
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Externalizing the Relational DatabaseSuggest EditsIf desired, you can bring your own Postgres instance to use as your Arthur's relational database. Follow the steps on this page to prepare your Postgres instance.
First, deploy your Postgres instance in your desired environment with appropriate ingress firewall configuration.
Create databases for the Arthur platform.
CREATE DATABASE arthurai
CREATE DATABASE alert_service;
CREATE DATABASE dataset_service;
CREATE DATABASE metric_service;
-- for stand alone instance
CREATE USER arthurai WITH PASSWORD 'SuperSecret';
-- for RDS instance
CREATE ROLE arthurai WITH PASSWORD 'SuperSecret' LOGIN;
REVOKE ALL PRIVILEGES ON DATABASE postgres FROM arthurai;
GRANT ALL PRIVILEGES ON DATABASE arthurai TO arthurai;
GRANT ALL PRIVILEGES ON DATABASE alert_service TO arthurai;
GRANT ALL PRIVILEGES ON DATABASE dataset_service TO arthurai;
GRANT ALL PRIVILEGES ON DATABASE metric_service TO arthurai;
If you have been using the embedded database and you wish to switch to using an external Postgres, backup the embedded
database and restore it to the new external Postgres with pg_dump and pg_restore.
Connecting to the database using SSL/TLS
If your postgres instance supports SSL/TLS connections, and you want to connect to your external database
with an encrypted connection, you simply need to set Database SSL Mode in the initial configuration. By default, this
is set to disable. However, you can enable an encrypted connection using the value require.
{note}An externally managed Postgres instance is strongly recommended for production-grade installs.
Updated 3 months ago Table of Contents
Connecting to the database using SSL/TLS
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/image-regression-2
 Regression
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by RegressionImage Regression Models within ArthurSuggest EditsRegression models predict a numeric outcome. In Arthur, these models are listed under the regression model type.
Some common examples of Image regression are:
Predict user age based on a photograph
Predict house price from the image of the home
Formatted Data in Arthur
Image regression models require two columns: image input and numeric output. When onboarding a reference dataset (and setting a model schema), you need to specify a target column for each inference's ground truth. Many teams also choose to onboard metadata for the model (i.e., any information you want to track about your inferences) as non-input attributes.
Attribute (Image Input)Prediction (numeric)Ground Truth (numeric)Non-Input Attribute (numeric or categorical)image_1.jpg45.3462.42High School Educationimage_2.jpg55.153.2Graduate Degree
Predict Function and Mapping
These are some examples of common values teams need to onboard for their regression models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics.
Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Single Column Ground Truth
output_mapping = {
'prediction_column':'gt_column'}
# Build Arthur Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
## Example prediction function for binary classification
def predict(x):
return model.predict(x)
Available Metrics
When onboarding regression models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a regression model. Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeRoot Mean Squared ErrorPerformanceMean Absolute ErrorPerformanceR SquaredPerformanceInference CountIngestionAverage PredictionIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/generative-text
 Generative Text
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Generative TextSuggest EditsThis page discusses the basics of setting up a generative text model and onboarding it to Arthur Scope to monitor generative performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>")
Preparing Data for Arthur
Arthur Scope does not need your model object itself to monitor performance - only predictions are required
All you need to monitor your model with Arthur is to upload the predictions your model makes. Here's how to format
predictions for common generative text model schemas.
Use the Arthur data type TOKENS for tokenized input and output texts. Arthur expects a list of strings as below for
tokenized data.
Python[
{
"input_text": "this is the raw input to my model",
"input_tokens": ["this", "is", "the", "raw", "input", "to", "my", "model"],
"output_text": "this is model generated text",
"output_tokens": ["this", "is", "model", "generated", "text"]
}
]
Use the Arthur data type TOKEN_LIKELIHOODS for generated outputs of tokens and their likelihoods. Arthur expects this data type to be formatted as an array of maps from token strings to float likelihoods. Each array index should correspond to one token in the generated sequence. If supplying both TOKENS and TOKEN_LIKELIHOODS for predicted values, the two arrays must be equal in length.
Python[
{
"input_text": "this is the raw input to my model",
"input_tokens": ["this", "is", "the", "raw", "input", "to", "my", "model"],
"output_text": "this is model generated text",
"output_tokens": ["this", "is", "model", "generated", "text"],
"output_probs": [
{"this": 0.4, "the": 0.5, "a": 0.1},
{"is": 0.8, "could": 0.1, "may": 0.1},
{"model": 0.33, "human": 0.33, "robot": 0.33},
{"generated": 0.9, "written": 0.03, "dreamt": 0.07},
{"text": 0.7, "rant": 0.2, "story": 0.1}
]
}
]
Arthur supports maps of up to 5 token - float key pairs.
The Arthur SDK provides helper functions for mapping OpenAI response objects or log tensor arrays to Arthur format.
See the SDK reference for more guidance on usage.
Registering a Generative Text Model
Each generative text model is created with a name and with output_type = OutputType.TokenSequence. We also need to specify an input type, which in this case will be InputType.NLP for a text to text model. Here, we register a token sequence model with NLP input specifying a text_delimiter of NOT_WORD:
Pythonarthur_nlp_model = arthur.model(name="NLPQuickstart",
input_type=InputType.NLP,
model_type=OutputType.TokenSequence,
text_delimiter=TextDelimiter.NOT_WORD)
Arthur uses the text delimiter to tokenize model input texts and generated texts and track derived insights like sequence length. You can also register your own pre-tokenized values with Arthur for more complex tokenizers. If the registered model uses a custom tokenizer, this is the recommended process outlined in the below section on building a generative text model.
Below, we show different ways of building a generative text model that depends on which attributes you want to monitor for your model.
Building a Generative Text Model
To build a generative text model in the Arthur SDK, use the build_token_sequence_model method on the Arthur Model.
Here we add one attribute for the input text and one attribute for the model output or generated text.
Both of these attributes will have the UNSTRUCTURED_TEXT value type in the ArthurModel after calling this method - this means that this data is saved as a string in each inference.
You should build your model this way if you will only monitor its input and output text and not monitor any of its token processing or likelihood scores.
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text')
Registering Pre-tokenized Text
Optionally, token sequence models also support adding token information. In the below example, the tokenized input text is specified in theinput_token_column and the final tokens selected for the generated output are specified in theoutput_token_column.
This method builds a model with four attributes to monitor for your generative text model.
While the text attributes will still have the UNSTRUCTURED_TEXT value type, the token attributes will have the TOKENS value type means that these attributes are represented as a list of tokens for each inference.
You should build your model this way if you are going to monitor the inferences in their tokenized form as well as in their text form - this may help distinguish performance behaviors due to the base model from performance behaviors due to the tokenization.
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text',
input_token_column='input_tokens',
output_token_column='output_tokens')
Registering Tokens With Likelihoods
You can attach likelihoods to the generated tokens by specifying the output_likelihood_column:
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text',
input_token_column='input_tokens',
output_token_column='output_tokens',
output_likelihood_column='output_probs')
It is not required to specify both a output_token_column and an output_likelihood_column-
if only the output_likelihood_column is specified, greedy decoding will be assumed.
Registering a Ground Truth Sequence
Lastly, adding a ground truth sequence to the model is optional. Ground truth has the same tokenization support as model input and output texts.
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text',
ground_truth_text_column='ground_truth_text')
Adding Inference Metadata
We now have a model schema with input, predicted value, and ground truth data defined. Additionally, we can add non-input data attributes to track other information associated with each inference but not necessarily part of the model pipeline.
For generative text models, tracking production signals as performance feedback is often of interest. Here,
we add one continuous attribute and one boolean attribute to measure the success of our model for a use case.
Pythonarthur_nlp_model.add_attribute(name='edit_duration', value_type=ValueType.Float, stage=Stage.NonInputData)
arthur_nlp_model.add_attribute(name='accepted_by_user', value_type=ValueType.Boolean, stage=Stage.NonInputData)
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save()you can call arthur_model.review() the model schema to check that it is correct.
For a TokenSequence model with NLP input, the model schema should look similar to this:
Python
name
stage
value_type
categorical
is_unique
0
text_attr
PIPELINE_INPUT
UNSTRUCTURED_TEXT
False
True
1
pred_value
PREDICTED_VALUE
UNSTRUCTURED_TEXT
False
False
...
2
pred_tokens
PREDICTED_VALUE
TOKEN_LIKELIHOODS
False
False
3
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct use thearthur_model.review(), you are finished registering your model and its attributes, ready to complete onboarding your model.
To finish onboarding your TokenSequence model, the following steps apply, which is the same for NLP models as it is for models of any InputType and OutputType:
finishing_onboarding.md
Sending Inferences
Since we've already formatted the data, we can use the send_inferences method of the SDK to upload the inferences to Arthur. This functionality is also available directly through the API.
Pythonarthur_nlp_model.send_inferences([
{
"input_text": "this is the raw input to my model",
"input_tokens": ["this", "is", "the", "raw", "input", "to", "my", "model"],
"output_text": "this is model generated text",
"output_tokens": ["this", "is", "model", "generated", "text"],
"output_probs": [
{"this": 0.4, "the": 0.5, "a": 0.1},
{"is": 0.8, "could": 0.1, "may": 0.1},
{"model": 0.33, "human": 0.33, "robot": 0.33},
{"generated": 0.9, "written": 0.03, "dreamt": 0.07},
{"text": 0.7, "rant": 0.2, "story": 0.1}
]
}
])
Arthur supports maps of up to 5 token - float key pairs.
The Arthur SDK provides a helper function to map tensor arrays into an Arthur format.
See the SDK reference for more guidance on usage
Enrichments
For an overview of configuring enrichments for NLP models, see the {doc}/user-guide/walkthroughs/enrichments guide.
Explainability is not currently supported for TokenSequence models, but anomaly detection will be enabled by default.Updated 3 months ago Table of Contents
Getting Started
Preparing Data for Arthur
Registering a Generative Text Model
Building a Generative Text Model
Adding Inference Metadata
Reviewing the Model Schema
Finishing Onboarding
Sending Inferences
Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/cv-onboarding
 CV Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by CV OnboardingSuggest EditsThis page shows the basics of setting up computer vision (CV) models and onboarding Arthur Scope to monitor vision-specific performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>")
Registering a CV Model
Each computer vision model is created with input_type = InputType.Image and with specified width and height
dimensions for the processed images. Here, we register a classification model on 1024x1024 images:
Pythonarthur_cv_model = arthur.model(name="ImageQuickstart",
input_type=InputType.Image,
model_type=OutputType.Multiclass,
pixel_height=1024,
pixel_width=1024)
You can send images to the Arthur platform with any dimensions, and we'll keep the original you send as wellas a resized copy in the model dimensions. If you enable explainability for your model, the resized versions will be
passed to it to generate explanations.
The different OutputType values currently supported for computer vision models are classification, regression, and object detection.
Formatting Data
Computer vision models require the same structure as Tabular and NLP models. However, the attribute value for Image attributes should be a valid path to the image file for that inference.
Here is an example of a valid reference_data data frame to build an ArthurModel with:
Python
image_attr
pred_value
ground_truth
non_input_1
0
'img_path/img_0.png'
0.1
0
0.2
1
'img_path/img_1.png'
0.05
0
-0.3
2
'img_path/img_2.png'
0.02
1
0.7
...
3
'img_path/img_3.png'
0.8
1
1.2
4
'img_path/img_4.png'
0.4
0
-0.5
...
Non-Input Attributes
Any non-pixel features to be tracked in images for performance comparison or bias detection should be added as
non-input attributes. For example, metadata about people's identities captured in images for a CV model should be included as non-input attributes.
Reviewing the Model Schema
Before you call arthur_model.save()you can call arthur_model.review() the model schema to check that your data is parsed correctly.
For an image model, the model schema should look like this:
Python
name
stage
value_type
categorical
is_unique
0
image_attr
PIPELINE_INPUT
IMAGE
False
True
1
pred_value
PREDICTED_VALUE
FLOAT
False
False
...
2
ground_truth
GROUND_TRUTH
INTEGER
True
False
3
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Object Detection
Formatting Bounding Boxes
If using an Object Detection model, bounding boxes should be formatted as lists in the form:
[class_id, confidence, top_left_x, top_left_y, width, height]
The first two components of the bounding box list represent the classification being made within the bounding box. Theclass_id represents the ID of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction (0.0 for completely unconfident and 1.0 for completely confident).
The next four components of the bounding box list represent the location of the bounding box within the image: the
top_left_x and top_left_y represent the X and Y pixel coordinates of the top-left corner of the bounding box. These pixel coordinates are calculated from the origin, which is in the top left corner of the image. This means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. The width represents the number of pixels the bounding box covers from left to right and the height represents the number of pixels the bounding box covers from top to bottom.
So using the following model schema as an example:
Python	name
stage
value_type
0	image_attr
PIPELINE_INPUT
IMAGE
1	label
GROUND_TRUTH
BOUNDING_BOX
2	objects_detected
PREDICTED_VALUE
BOUNDING_BOX
a valid dataset would look like
Python#
image_attr
objects_detected
ground_truth
non_input_1
0,
'img_path/img_0.png',
[[0, 0.98, 12, 20, 50, 25],
[0, 1, 14, 22, 48, 29],
0.2
[1, 0.47, 92, 140, 80, 36]]
1,
'img_path/img_1.png',
[[1, 0.22, 4, 5, 14, 32]]
[1, 1, 25, 43, 49, 25]
-0.3
#...
#
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using thearthur_model.review(), you are finished locally configuring your model and its attributes - so you are ready to complete onboarding your model.
To finish onboarding your CV model, the following steps apply, which is the same for CV models as it is for models
of any InputType and OutputType:
finishing_onboarding.md
Enrichments
For an overview of configuring enrichments for image models, see the Enabling Enrichments section.
For a step-by-step walkthrough of setting up the explainability Enrichment for image models, see the Assets Required For Explainability section.Updated 3 months ago Table of Contents
Getting Started
Registering a CV Model
Formatting Data
Non-Input Attributes
Reviewing the Model Schema
Object Detection
Formatting Bounding Boxes
Finishing Onboarding
Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/organizations-and-users
 Organizations and Users
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Organizations and UsersSuggest EditsBy default, a new organization, "My Organization" is created by the installer for convenience. You can also create new organizations using the API with the superadmin user.
Log in with superadmin credentials
The first thing you will need is a superadmin authorization token. To obtain this, you will need to make a POST request to your organization's /login endpoint with the password set in the Admin Console Config page.
TextJSONPOST /login
{
"login": "superadmin",
"password": "<superadmin-password>"
}
The response will look like this:
JSON{
"id": "ed1dcb56-352a-4130-8f52-1fd1225196b1",
"first_name": "Super",
"last_name": "Admin",
"email": "[email protected]",
"username": "superadmin",
"roles": null,
"active": true,
"show_intro_sequence": true,
"help_mode_enabled": false,
"created_at": "2021-08-09T19:57:44.92047Z"
}
The response will also include a set-cookie HTTP header with an authorization token.` Copy the authorization token value and use it in subsequent requests as your auth token.
set-cookie: Authorization=<authorization-token>; Path=/; Expires=Mon, 30 Aug 2021 16:51:07 GMT; Secure;
cURL example
Bashcurl --location --request POST 'https://<your-domain>/api/v3/login' --header 'Content-Type: application/json' --data-raw '{ "login": "superadmin", "password": "<superadmin-password>" }' -v
Create a New Organization
To create a new organization, you will need to make a POST request to /organizations with the body specifying the name. Ensure you are using a super admin authentication token to make this request.
TextJSONPOST /organizations
{
"name": "my-new-organization"
}
The response will look like this:
JSON{
"id": "38faff8b-4edf-44c5-b103-aeca4ea71110",
"name": "my-new-organization",
"plan": "enterprise",
"created_at": "2021-08-18T19:51:22.291504554Z"
}
Remember to save the id; you will need this to add users to your organization.
cURL Example
Bashcurl --location --request POST '<your-domain>/api/v3/organizations' --header 'Content-Type: application/json' --header 'Authorization: <your-superadmin-access-control-token>' --data-raw '{ "name": "my-new-organization" }' -v
Create The First User in an Organization
To create a new user in the new organization, you will need to make a POST request to
/users?organization_id=<your_organization_id> using a super admin authentication token. You can set the role of the new user to Administrator,
Model Owner, or User. Refer to the Platform Access Control for the description of the roles.
TextJSONPOST /users?organization_id=<your_organization_id>
{
"username": "newuser",
"email": "[email protected]",
"password": "G00dP@$$w0rd!",
"first_name": "New",
"last_name": "User",
"roles": [
"Administrator"
],
"alert_notifications_enabled": true
}
The response will look like this.
JSON{
"id": "b6554927-9ac4-4531-bf76-fe640b8223b7",
"first_name": "New",
"last_name": "User",
"email": "[email protected]",
"username": "newuser",
"roles": null,
"active": true,
"show_intro_sequence": true,
"help_mode_enabled": true,
"created_at": "2021-08-18T20:20:18.535137592Z"
}
You can now log in to the dashboard as this user.
cURL Example
This action can be performed as either the super administrator or an organization administrator.
If you'd like to use an organization administrator, repeat the Login API call performed earlier with the credentials for that user and save the returned authorization token.
Bashcurl --location --request POST 'https://<your-domain>/api/v3/users?organization_id=<your-organization-id>' --header 'Content-Type: application/json' --header 'Authorization: <your-superadmin-token>' --data-raw '{ "username": "<username>", "email": "<email-address>", "password": "<password>", "first_name": "<first-name>", "last_name": "<last-name>", "roles": [ "Administrator" ], "alert_notifications_enabled": true }'
Adding Additional Users
Although you can continue to create users through the API, it is generally easier to create an Administrator user and then invite additional users from the UI. To add additional users this way, login to Arthur AI with an Administrator user on a web browser and follow these steps:
In the top right corner, you will see a series of icons. Click on the Organization icon that looks like a tree with three nodes.
You will see a dropdown menu. Click on Manage Members
Under the heading, Invite Members, you can type in the email address of the person you wish to invite. That person will receive email instructions for creating a user in the organization.
Once the new user follows the emailed instructions, they can log in with their newly created username and password. You will then be able to view that new user on this Manage Members page.
As an Administrator, you can continue to use this page to manage users and roles.
Adding Existing Users To Existing Organizations via API:
To add an existing user to an existing organization, create a PATCH request to /organizations/<org_id>/users. Supplying in the body a JSON object defining the role (Administrator, Model Owner, or User) you want to add the user with. Any attributes other than roles that are supplied in the body will affect the user across all organizations that the user is a part of.
TextJSONPATCH /organizations/<org_id>/users
[
{
"user_id": "b6554927-9ac4-4531-bf76-fe640b8223b7",
"role": "Model Owner"
} ,
{
"user_id": "b6554927-9ac4-4531-bf76-fe640b8223b7",
"role": "Model Owner"
}
]
The response will look like this.
JSON{
"updated_user_count": 10
}
Updated 3 months ago Table of Contents
Log in with superadmin credentials
cURL example
Create a New Organization
cURL Example
Create The First User in an Organization
cURL Example
Adding Additional Users
Adding Existing Users To Existing Organizations via API:
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/explainability
 Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ExplainabilityUnderstand why your model is making decisionsSuggest EditsExplainability is one of the core pillars of the Arthur Scope platform. Teams utilize explainability to build trust with valuable insights into how their ML models make decisions. They use it to explore patterns, investigate problems, and ensure compliance with explanations for their model outputs. Finally, they can use it to debug by understanding not only why predictions are being made but also evaluating how changes to model inputs change predictive outputs.
Levels of Explainability in Arthur UI
Explainability can be evaluated at multiple levels within the Arthur platform. These levels include:
Global Explainability (Tabular Models)
Global explainability in machine learning models refers to the ability to understand and explain a model's overall behavior and decision-making process across its entire dataset or population. Teams often use global explainability as a gut check to ensure that the features they consider the most important are the ones driving decisions.
Teams can also drive value from global explanations through interactions with other metrics within Arthur Scope. In the Arthur UI, for example, global explainability can be visualized in conjunction with a data drift of choice at the bottom of the Overview page.
In this figure, we can see that Pay_0 is the most important feature.
Using explainability in conjunction with other metrics can help teams understand and debug models and drive actions such as model retraining or new feature/architecture exploration.
📘Global Explanations are an aggregate of local explanationsWithin Arthur, global explanations are created by aggregating the absolute value of local explanations. The absolute value is taken because, as you will see in local explainability, features can positively or negatively impact whether or not a prediction is made. By taking the average absolute value of all local explanations, global explanations are a measure of feature impact (not necessarily positive or negative).
Local Explainability
Local explainability in machine learning refers to the ability to explain the reasoning behind a model's prediction for a particular instance or input. Teams can access local explainability for inferences within an Arthur Models Inference Tab.
Select Positive Predicted Attribute: Teams need to select a positive predicted attribute when running local explanations because it focuses the explanation on a specific prediction of interest. I.E., we are generating explanations for what causes that prediction to occur.
For example, in binary classification, this helps us read the explanation to understand whether the feature is driving the prediction for that class in a positive way or a negative way by driving the prediction for the other class instead. This is the same for multiclass models; however, unlike binary, for negative importance scores, we cannot attribute this negative importance to the other class. This is because there are more than two classes. It could be driving predictions to any other predicted class instead.
Tabular Inferences
We can visualize local importance scores for each model input feature for tabular inferences.
Here we can see
What If Capabilities
"What If" local explainability functionality refers to the ability to interactively explore and understand the impact of changing input features on a model's prediction for a particular instance or input. To utilize What-Ifs for a specific inference, a user needs to toggle on the "What-If" functionality in the Arthur UI for that inference. Then they can change any feature and evaluate how it changes the model's predictions or relative importance scores.
This example shows how different inputs for this inference affected the local importance scores for the inference and the predicted probabilities.
Text Inferences
For text models, teams can visualize the words that drive predictions. Note: Explainability is only available for classification and regression text models. We do not currently have explainability functionality for generative text models. Instead, teams can look into Token Likelihood for generative models.
In this example, we can see that the model mispredicted consult_history_and_phy instead of urology. We can use explainability to examine the top tokens that drove this misprediction between the two classes.
Is Bag of Words: Bag of words is a text representation technique in natural language processing that converts a document into a collection of its words. When "Is Bag Of Words" is turned on, explainability scores are calculated per word - not considering where duplicate words are placed in the text. However, when it is turned off, each duplicate word is treated as a unique token. So, placement in the text is taken into consideration.
Tokenization: Features for text input models are often called tokens. As described further in the Text input section, Arthur default to create word-based tokens based on whitespace. If whitespace does not make sense in your use case, make sure to set up a different tokenization when enabling explainability.
Image Inferences
For image classification/regression models, teams can visualize the segments of the image that drive different predictions.
Querying More Levels of Explainability with the Python SDK
Beyond the explainability functionality provided within the UI, many teams choose to pull custom reports or charts with our Arthur Query Service. Common examples can be found here:
Querying Explainability
Available Post-Hoc Explainers in Arthur
Arthur supports open-source LIME and SHAP for explainability within the platform.
ExplainerAvailable For Tabular ModelsAvailable For Text ModelsAvailable For Image ModelsLIMEXXXSHAPX
Please reference our Enabling Enrichments section in the Model Onboarding section for recommendations regarding which explainer to use.
Available Arthur Schemas
Explainability is available for all model types except Object Detection and Generative Text.Updated 3 months ago What’s NextLearn more about enabling explainability in general in the enabling enrichments section of Model Onboarding. However, if you are having specific troubles check out our pages on troubleshooting and debugging explainability enablement.Enabling EnrichmentsTroubleshooting ExplainabilityTable of Contents
Levels of Explainability in Arthur UI
Global Explainability (Tabular Models)
Local Explainability
Querying More Levels of Explainability with the Python SDK
Available Post-Hoc Explainers in Arthur
Available Arthur Schemas
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles
 Arthur Permissions by Standard Roles
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur Permissions by Standard RolesSuggest Edits
Model ReaderAlert ManagerUserModel WriterFree Tier UserModel OwnerOrg ManagerAdministratorOrg CreatorSuper AdminuserreadXXXXXXmodelreadXXXXXXtagreadXXXXXXalert_rulereadXXXXXXalertreadXXXXXXalertresolveXXXXXXalert_notification_configreadXXXXXXalert_notification_configwriteXXXXXXalert_notification_configdeleteXXXXXXinsightreadXXXXXXinsightresolveXXXXXXalert_summary_configreadXXXXXXalert_summary_configwriteXXXXXXalert_summary_configdeleteXXXXXXalert_summary_subscriberreadXXXXXXalert_summary_subscriberwriteXXXXXXalert_summary_subscriberdeleteXXXXXXreference_datareadXXXXXraw_datareadXXXXXqueryexecuteXXXXXmetric_queryreadXXXXXXenrichment_configreadXXXXXXorganization_usagereadXXXXXmodel_pinned_columns_globalreadXXXXXXmodelwriteXXXXXmodeldeleteXXXXXtagwriteXXXXXtagdeleteXXXXXalert_rulewriteXXXXXalert_ruledeleteXXXXXmetric_querywriteXXXXXmetric_querydeleteXXXXXreference_datawriteXXXXraw_datawriteXXXXenrichment_configwriteXXXXorganization_metricsreadXXXXservice_accountreadXXXXservice_accountwriteXXXXservice_accountdeleteXXXXground_truthwriteXXXXinvite_userwriteXXXXuserwriteXXXuserdeleteXXXauthentication_configreadXXXauthentication_configwriteXXXalertnotifyXXalert_summarynotifyXXinvite_userwriteXXmodel_pinned_columns_globalwriteXXmodel_pinned_columns_globaldeleteXXorganization_globalreadXXorganization_globalwriteXXorganizationreadXXorganizationdeleteXXcustom_rolesreadXXcustom_roleswriteXXcustom_rolesdeleteXXsystem_configwriteXmodel_statuswriteXenrichment_statuswriteXUpdated about 2 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/configuring-for-high-availability
 Configuring for High Availability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Configuring for High AvailabilitySuggest EditsIntroduction
The Arthur Platform is built to run in a High Availability configuration, ensuring that the application can function
in the event of a Data Center outage, hardware outages, or other similar infrastructure issues.
In order to take advantage of this, there are a few requirements in how your infrastructure is setup:
Installing across 3 Availability Zones
Specifying the correct Instance Types
Configuring the cluster for Auto-scaling
Notes about this document
{note}Note that this document is written using AWS terminology, as this is one of the environments/infrastructure that Arthur uses
internally for our environments. However, these setup steps should work across various cloud providers using similar features.
{note}Note that this document is written with the pre-requisite that you are installing Arthur in a High Availability configuration.
At the minimum, this means that there should be 3 instances across which Arthur is deployed.
Installing across 3 Availability Zones
In order to ensure continuous operation during an Availability Zone (AZ) outage, Arthur must be installed on a cluster
that has 3 Availability Zones. This ensures that in the event of one AZ outage that the rest of the components can still
operate.
To do this in AWS, create 3 separate Auto-Scaling Groups (ASGs) - one for each AZ. You can configure which AZ an ASG provisions
instances into when you create the ASG.
When Arthur deploys, the stateful services (eg: databases, messaging queues, etc.) will be balanced across the 3 AZs automatically using
kubernetes pod anti-affinity rules (pods will not schedule onto nodes where there already exists another pod that is of the same component).
Specifying the correct Instance Types
Generally speaking, the best way to ensure you have deployed the correct Instance Types is to monitor resource utilization across the cluster
to determine when your services are hitting resource limits.
When initially configuring a cluster for Arthur, we recommend 3 nodes, where each node has at least 16 vCPUs and 64G of RAM (eg: an m5a.4xlarge instance type).
This is a good starting point for a general-purpose cluster that will grow with your production usage.
Configuring the cluster for Auto-scaling
Arthur's stateless components horizontally auto-scale, but in order to take the maximum advantage of this, you will need to configure and install
an additional component that performs node autoscaling (eg: adding more instances).
AWS specifies how to setup cluster autoscaling in their documentation.
Generally speaking, it involves setting up an IAM role and granting permissions to autoscale the cluster, and then installing a third-party component
to perform the autoscaling (eg: cluster-autoscalerUpdated 3 months ago Table of Contents
Introduction
Notes about this document
Installing across 3 Availability Zones
Specifying the correct Instance Types
Configuring the cluster for Auto-scaling
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/what-are-enrichments
 Enrichments
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by EnrichmentsEnrich your monitoring process with state-of-the-art techniquesSuggest EditsEnrichments are additional services that the Arthur platform provides for state-of-the-art proactive model monitoring.
Enrichments in Arthur
Anomaly Detection: monitor and alert on incoming changes to your data distribution (compared to the reference dataset) based on complex interactions between features
Hotspots: automatically illuminate segments of underperformance within incoming inferences
Explainability: understand why your model is making decisions, by computing the importance of individual features from your data on your model's outcomes
Bias Mitigation: methods for model post-processing that improve the fairness of outcomes without re-deploying your model
Once activated, these enrichments are automatically computed on Arthur's backend, with results viewable in the online UI dashboard and queryable from Arthur's API.
Available Enrichments By Different Model Types
Due to the specialized nature of enrichments, they are only available for certain model types.
Model TypeAnomaly DetectionBias MitigationExplainabilityHot SpotsTabular ClassificationXXXXTabular RegressionXXText ClassificationXXText RegressionXXText Sequence Generation (LLM)X (on inputs)CV ClassificationXXCV RegressionXXCV Object DetectionX
Viewing Enabled Enrichments in the UI
You are also able to view the enrichments enabled for your specific model within the Arthur UI by clicking on the details sections of the model's overview page.
Enrichment Workflows
As enrichments are add-ons meant to enrich standard model monitoring, they run on their own workflows within Arthur.Updated 3 months ago Table of Contents
Enrichments in Arthur
Available Enrichments By Different Model Types
Viewing Enabled Enrichments in the UI
Enrichment Workflows
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/drift-and-anomaly
 Data Drift Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Data Drift MetricsTrack the stability of your model by comparing real-world data to a reference datasetSuggest EditsData drift is one of the top causes of model performance decay over time. Data drift measures how much the input data stream to the model changes over time. Tracking data drift over time can help teams identify when models are no longer performing as expected and take proactive steps to maintain or improve their performance. Teams use data drift to illuminate and debug issues like:
Upstream data issues, such as a third-party data provider changing their tagging of missing data fromnull to -1
Data quality issues, such as a faulty sensor tagging a feature 10x lower
Natural demographic shifts, such as an interesting segment of new users from a younger age group using your platform
Sudden changes in relationships, such as the covid-19 pandemic, immediately shift relationships between features and predictions
Selecting Samples For Comparison
Data drift metrics are essentially metrics built to compare two samples of the same data distribution. It calculates how much that distribution drifts from one sample to another. This section talks about choosing those two samples of comparison.
Using a Reference Dataset
The reference dataset is a representative sample of the input features your model ingests. It is used to compute baseline model analytics. By capturing the data distribution you expect your model to receive, Arthur can detect, surface, and diagnose data drift before it impacts results.
More information about reference datasets can be found in the documentation
Examples of how reference data is structured for different model types can be found in the Model Input / Output Types section.
Best practices for selecting a reference dataset can be found in theCreating An Arthur Model Object section
Comparing Points in Time
While the Arthur UI is designed to compare production data against a reference dataset, Arthur can compare any two distribution samples. One of the most popular ways teams use this is by comparing two samples of production data to one another (i.e., how has data from one week, one month, or one quarter ago drifted compared to now). An example of this query can be seen in Querying Data Drift resource.
Type of Data Drift Metrics
We have established that data drift metrics are a comparison of distributions. Now, we can look at the distributions that ML teams often compare.
Feature Drift
Feature drift (also known as covariate drift) refers to the changes in the distribution of input variables to a machine learning model.
Metrics in Arthur for Feature Drift
Arthur allows teams to decide between the most common metrics for feature drift for easy comparison within the UI.
Metrics AvailablePSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest
Prediction Drift
Prediction Drift tracks the discrepancy of your ML model outputs over time.
📘Prediction drift as a proxy for concept driftTeams may be familiar with the different types of distributional drifts that can occur within ML systems, the two most popular being covariate and concept drift. Covariate, known in Arthur as feature drift , refers to the distributions of features going into the model changing. Concept drift refers to a changing relationship between the inputs (features) and outputs (predictions) of the model. While concept drift is best tracked with model accuracy metrics, in situtations where there is not ground truth prediction drift is a common proxy for tracking concept drift. See more in this blog post: https://neptune.ai/blog/concept-drift-best-practices
Metrics in Arthur for Prediction Drift
Since prediction drift is another univariate drift technique, it has all the same options as feature drift for available univariate drift metrics.
Metrics AvailablePSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest
Multivariate Drift (Anomaly Detection)
The previous statistical drift metrics listed for both feature and prediction drift are univariate metrics of data drift. This means they only track one attribute at a time, which is incredibly useful for diagnosing specific issues within a feature. However, sometimes teams want to explore the changing relationships between features. This is the purpose of multivariate drift.
Metrics in Arthur for Multivariate Drift
Currently, in the Arthur platform, Arthur provides one multivariate drift metric based on the average of our model-based anomaly score technique.
Metrics AvailableMultivariate Drift
Using Drift to Drive Action
In practice, data drift is best used as a technique to instigate action within teams. To drive this action, teams have to use different features within Arthur.
Investigating Features in Tabular Models
While data drift is commonly used as a proxy for performance for models that do not receive ground truth soon after the time of prediction (if ever), it is also.
This gif shows an example of how data drift can be used in conjunction with feature importance to track down the root cause of model underperformance.
In the Arthur UI, there are two charts below distributional drift to enable quick evaluation:
Feature Importance x Drift: Using drift in conjunction with feature importance allows teams to understand how impactful drifted features are in modeling predictions
Attribute Distribution: Evaluate the numerical or categorical distributions of the attribute that have drifted to understand the cause of univariate drift.
Anomaly Detection + Multivariate Drift
While univariate drift can be tracked for NonInputData attributes, the most common data distributional drift tracked for Text and Image models is multivariate drift (or anomaly detection). For a more detailed look at how teams use our anomaly detection enrichment to drive value, please refer to the Anomaly Detection page.Updated about 2 months ago Table of Contents
Selecting Samples For Comparison
Using a Reference Dataset
Comparing Points in Time
Type of Data Drift Metrics
Feature Drift
Prediction Drift
Multivariate Drift (Anomaly Detection)
Using Drift to Drive Action
Investigating Features in Tabular Models
Anomaly Detection + Multivariate Drift
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/kubernetes-preparation
 Installing Arthur Pre-requisites
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Installing Arthur Pre-requisitesSuggest EditsThis is a guide to help you prepare your existing Kubernetes cluster for installing the Arthur platform.
The examples use Helm 3.
Make sure you're in the correct kubectl environment context before running the installer.
Install Ingress
Nginx
Nginx is an enterprise-grade cloud-agnostic open source ingress controller that can be used to access the Arthur application.
Shellhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm upgrade --install -n ingress-system \
--create-namespace \
ingress-nginx \
ingress-nginx/ingress-nginx
[Optional] To monitor nginx using Prometheus and add an AWS managed SSL certificate, create a values.yaml file with following contents -
Shellcontroller:
metrics:
enabled: true
serviceMonitor:
enabled: true
additionalLabels:
release: "kube-prometheus-stack"
service:
annotations:
service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"
service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <ACM certificate ARN>
service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https
service.beta.kubernetes.io/aws-load-balancer-type: <nlb> # optional annotation that creates a Network Load Balancer. Defaults to elb (Classic Load Lalancer)
service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01
service.beta.kubernetes.io/aws-load-balancer-internal: true # optional annotation that creates a non-internet-facing loadbalancer. Defaults to false
targetPorts:
http: "tohttps"
allowSnippetAnnotations: "true"
config:
http-snippet: 
server {
listen 2443;
return 308 https://$host$request_uri;
}
use-forwarded-headers: "true"
ingressClassResource:
name: nginx
enabled: true
default: false
controllerValue: "k8s.io/internal-ingress-nginx" # default: k8s.io/ingress-nginx
containerPort:
http: 8080
tohttps: 2443
https: 80
Upgrade or install the helm chart with the values.yaml you created.
Shellhelm upgrade --install -n ingress-system \
--create-namespace \
ingress-nginx \
ingress-nginx/ingress-nginx \
-f values.yaml
If you need to install Nginx in the same namespace as Arthur (not recommended) and want to use our network-policy to restrict ingress to the Arthur application, use the below command to add labels to the pods and services.
The network-policy allows traffic between pods and services that have these labels.
Shellhelm upgrade --install -n arthur --set controller.podLabels.network-app=arthurai,controller.service.labels.network-app=arthurai,defaultBackend.podLabels.network-app=arthurai,.service.labels.network-app=arthurai \
ingress-nginx \
ingress-nginx/ingress-nginx
Look up the hostname for the Ingress and configure it in your DNS (e.g. arthur.mydomain.com).
Shellkubectl get svc -n ingress-system ingress-nginx-controller -ojsonpath='{.status.loadBalancer.ingress[*].hostname}'
Install Prometheus
Installing the Chart
Shellhelm repo add \
prometheus-community \
https://prometheus-community.github.io/helm-charts
helm repo update
Shellhelm upgrade --install -n monitoring \
--create-namespace \
kube-prometheus-stack \
prometheus-community/kube-prometheus-stack \
-f /path/to/values.yaml # see below for contents of this file
helm upgrade --install -n monitoring \
--create-namespace \
prometheus-adapter \
prometheus-community/prometheus-adapter
Note: The values.yaml is not incremental. Helm uses a single values.yaml file, so all configuration must be present in the same values.yaml file. If you are doing this step by step, you must re-apply the prior changes to values.yaml file.
Setting up retention for Grafana and Prometheus
By default, Prometheus and Grafana will use local pod storage to store metrics/dashboards. These metrics/dashboards will be lost if the pod restarts for any reason. To avoid this and keep the metrics/dashboards for a longer period of time, add the following to your values.yaml to use a persistent volume store:
prometheus:
prometheusSpec:
serviceMonitorSelectorNilUsesHelmValues: false
retention: 30d # metrics rolled over every 30 days
retentionSize: 49GiB # size of metrics retained before they are rolled over
storageSpec:
volumeClaimTemplate:
spec:
storageClassName: gp2
accessModes: ["ReadWriteOnce"]
resources:
requests:
storage: 50Gi # size of disk for metrics
grafana:
persistence:
type: pvc
enabled: true
storageClassName: gp2
accessModes:
- ReadWriteOnce
size: 1Gi # size of disk for dashboards
Run the following command to apply the updated configurations (replace the path to the values.yaml file):
Shellhelm upgrade --install -n monitoring \
kube-prometheus-stack \
prometheus-community/kube-prometheus-stack \
-f /path/to/values.yaml
Setting up ingress for Prometheus, AlertManager and Grafana
Grafana and Prometheus are useful to have exposed on an ingress route so that cluster administrators can access real-time telemetry and observe the behavior of the Arthur Platform.
Please note that Grafana comes with a default username and password which should be changed immediately once installed. We also highly recommend installing Prometheus and Grafana within a VPC where the domains will not be exposed to the public internet.
The steps to enable ingress are:
Copy the values.yaml file below with the ingress configuration
Make the following edits to the yaml file that describe your environment:
The ingressClassName
If you installed using the nginx chart above, this should be nginx.
If you are using a custom nginx ingressClass, this will be the name of that ingress class
If you unsure what your ingressClass is called, run kubectl get ingressclass
The URL hostnames that you want to expose for these services
Note - these URL hostnames will need to be published DNS entries
Run the following command to deploy (replace the path to the values.yaml file)
helm upgrade --install -n monitoring \
kube-prometheus-stack \
prometheus-community/kube-prometheus-stack \
-f /path/to/values.yaml
To confirm this is working, navigate to the URL hostname defined in the values.yaml and you should be taken
to the front page for either Grafana or Prometheus.
Change the default password for Grafana.
Here is the values.yaml file that configures ingress for prometheus/grafana:
prometheus:
ingress:
enabled: true
ingressClassName: nginx
# Confirm this is correct or replace me
hosts:
- prometheus.mydomain.com
# Replace me
alertmanager:
ingress:
enabled: true
ingressClassName: nginx
hosts:
- alertmanager.mydomain.com
# Replace me
grafana:
ingress:
enabled: true
ingressClassName: nginx
# Confirm this is correct or replace me
hosts:
- grafana.mydomain.com
# Replace me
Verifying the install
Verify that Prometheus CRDs are installed:
Shellkubectl api-resources  grep monitoring
Verify that Prometheus is up and running:
Shellkubectl --namespace monitoring get pods -l "release=kube-prometheus-stack"
If everything is installed correctly, the following command should not return "ServiceUnavailable":
Shellkubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Monitoring and Alerting for the Arthur Platform
When you are ready to setup monitoring and Alerting, please reach out to your Arthur support representative and we can share additional details on this.
Prometheus alerts can be configured to trigger when certain rules that are setup to track Prometheus metrics violate for a period of time, which is customizable. For more information see the Prometheus alerting documentation These rules can then be configured to send via a notification channel (eg: email, Slack, etc) so that someone can be notified. This is the job of the Alert Manager.
Install Metrics Server
Example:
Shellhelm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm upgrade --install -n monitoring \
--create-namespace \
metrics-server \
bitnami/metrics-server \
--set apiService.create=true \
--set --extraArgs.kubelet-preferred-address-types=InternalIP
Verify that you can retrieve metric snapshots.
Shellkubectl top node
Configure the cluster-autoscaler
In a production environment, it is vital to ensure that there are enough resources (memory and cpu) available for pods to get scheduled on the Kubernetes cluster. Please follow the instructions for your cloud provider to install the cluster-autoscaler on your cluster.
Verify that the cluster-autoscaler is successfully installed.
Shellkubectl get deployments -n kube-system  grep -i cluster-autoscaler
Cloud Provider-specific Configuration
If installing on an existing Amazon AWS EKS, follow the additional steps Deploying on Amazon AWS EKSUpdated 14 days ago Table of Contents
Install Ingress
Nginx
Install Prometheus
Installing the Chart
Setting up retention for Grafana and Prometheus
Setting up ingress for Prometheus, AlertManager and Grafana
Verifying the install
Monitoring and Alerting for the Arthur Platform
Install Metrics Server
Configure the cluster-autoscaler
Cloud Provider-specific Configuration
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/text-multi-class-classification-1
 Multiclass Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Multiclass ClassificationSuggest EditsMulticlass classification models predict one class from more than two potential classes. In Arthur, these models fall into the category of classification and are represented by the Multiclass model type.
Some common examples of Text multiclass classification are:
Is the sentiment of this tweet neutral, positive, or negative?
What category does this document fall into?
Similar to binary classification, these models frequently output not only the predicted class but also a probability for each class predicted. The highest probability class is then the predicted output. In these cases, a threshold does not need to be provided to Arthur and it will automatically track the highest probability class as the predicted output.
Formatted Data in Arthur
Text multiclass classification models require three things to be specified in their schema: text input, predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (Text Input)Probability of Prediction AProbability of Prediction BProbability of Prediction CGround TruthNon-Input Attribute (numeric or categorical)Alea iacta est.90.05.05AMaleCastigat ridendo mores..46.14.40BFemalePlenus venter non studet libenter..16.17.71CFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their multi-class classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 2 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Option 1:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
output_mapping_1 = {
'pred_class_one_column':'one',
'pred_class_two_column':'two',
'pred_class_three_column':'three'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=output_mapping_1
)
## Option 2:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
output_mapping_2 = {
'pred_class_one_column':'gt_class_one_column',
'pred_class_two_column':'gt_class_two_column',
'pred_class_three_column':'gt_class_three_column'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping_2
)
## Example prediction function for binary classification
def predict(x):
return model.predict_proba(x)
Available Metrics
When onboarding tabular classification models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a multiclass classification model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the only attribute required to onboard a model is the image attribute. So, it is only possible to monitor non-input attributes for fairness in image models.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/sagemaker-data-capture
 SageMaker
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by SageMakerUsing SageMaker Data CaptureSuggest EditsModels deployed with AWS SageMaker can be configured to automatically push their real-time inferences to Arthur Scope by utilizing SageMaker Data Capture. This guide walks through setting up that integration and utilizing a Lambda function to send Data Capture to log files to be ingested by the Arthur platform.
Prerequisites
The model for which inferences are being ingested has already been onboarded onto Arthur.
The SageMaker model schema matches that of its Arthur model counterpart.
SageMaker Configuration
AWS SageMaker offers two features that enable this Arthur integration: Real-time endpoints & Data Capture. Endpoints are APIs that expose a trained model. Users can use the API to retrieve predictions from the hosted model in the endpoint. Data Capture is a feature that logs the inputs and outputs of each prediction from the hosted model endpoints.
To enable Data Capture in a way that accurately logs all input and output data needed for the Arthur integration, a configuration must be passed in when deploying an endpoint (see below).
Configuring Data Capture through the SageMaker SDK
An extended description of the following configuration can be found in the "SageMaker Python SDK" tab of the SageMaker Data Capture documentation.
Pythonfrom sagemaker.model import Model
from sagemaker.model_monitor import DataCaptureConfig
s3_capture_upload_path = f"s3://{bucket-name}/{model-specific-path}/datacapture"
model = Model( ... )
data_capture_config = DataCaptureConfig(
enable_capture=True,
sampling_percentage=100,
destination_s3_uri=s3_capture_upload_path,
capture_options=['REQUEST','RESPONSE'],
)
model.deploy(
data_capture_config=data_capture_config,
...
)
This integration requires that DataCaptureConfig be set such that:
capture_options includes both REQUEST and RESPONSE to record model inputs and outputs for each inference
sampling_percentage is set to 100to comprehensively ingest all new inferences
enable_capture is set to True
Configuring Data Capture through the SageMaker API
Users can also call the CreateEndpoint API to create a real-time endpoint via the API. To ensure that this endpoint is deployed with Data Capture enabled, it must receive an EndpointConfigName that matches an EndpointConfig created using the CreateEndpointConfig API with the following specifications:
{
...,
"DataCaptureConfig": {
"CaptureContentTypeHeader": {
"CsvContentTypes": [ "string" ],
"JsonContentTypes": [ "string" ]
},
"CaptureOptions": [
{
"CaptureMode": "Input"
},
{
"CaptureMode": "Output"
}
],
"DestinationS3Uri": "string",
"EnableCapture": true,
"InitialSamplingPercentage": 100,
"KmsKeyId": "string"
},
"EndpointConfigName": "string",
...
}
This integration requires that DataCaptureConfig be set such that:
CaptureContentTypeHeader be specified to an Arthur-supported content type (see section below). If no CsvContentTypes or JsonContentTypes are specified, SageMaker will by default base64 encode when capturing the data. This content type is currently not supported by the Arthur platform.
CaptureOptions be set to both the Input and Output Capture Modes.
EnableCapture be set to true.
InitialSamplingPercentage be set to 100.
Supported Data Formats
AWS SageMaker algorithms can accept and produce numerous MIME types for the HTTP payloads used in retrieving predictions from endpoint-hosted models. The MIME type utilized in an endpoint invocation also corresponds to the format of the Data Captured inference.
The Arthur platform supports the following MIME types/data formats for those types:
MIME Type: text/csv
37,Self-emp-not-inc,227253,Preschool,1,Married-civ-spouse,Sales,Husband,White,Male,0,0,30,Mexico\n24,Private,211129,Bachelors,13,Never-married,Exec-managerial,Other-relative,White,Female,0,0,60,United-States\n
Each inference is represented as an ordered row of comma-separate values, where each value represents a feature in the inference
These features must be specified in the same order as their counterparts in the corresponding Arthur Model
If multiple inferences are included in a single call to invoke_endpoint, each inference is separated by \n
MIME Type: application/json
Arthur currently supports two unique JSON formats, described with examples below.
Option 1: Column-Ordered List of Feature-Values
JSON{
"instances": [
{
"features": [1.5, 16, "testStringA", false]
},
{
"features": [2.0, 12, "testStringB", true]
}
]
}
Each inference is represented as a new object inside a JSON array
The upper-level key mapping to this inference array is named one of the following: instances, predictions
Each object within this JSON array is a key mapping to an ordered array of features
The second level key mapping to this feature array is named one of the following: features, probabilities
Option 2: Feature-Name Keys to Values Map
JSON{
"predictions": [
{
"closest_cluster": 5,
"distance_to_cluster": 36.5
},
{
"closest_cluster": 2,
"distance_to_cluster": 90.3
}
]
}
Each inference is represented as an object inside a JSON array
The upper-level key mapping to this inference array is named one of the following: instances, predictions
Each object within this JSON array has keys representing feature names mapping to their corresponding feature values.
The names of these features cannot be any one of the following: instances, predictions, features, probabilities
Specifying Partner Inference ID on Arthur-Ingested Data Capture Inferences
The Arthur platform enforces that each uploaded inference has a Partner Inference ID, which is a unique identifier used as the matching mechanism for joining ground truth data. Arthur's SageMaker integration populates the Arthur Inference ID from two possible sources in SageMaker. The default is to use SageMaker's EventID, which is a random ID auto-generated by SageMaker for each request. SageMaker's EventID is captured in the eventMetadata/eventId field of the data capture output files. As another option, SageMaker allows Invoke-Endpoint API callees to specify an InferenceId (or inference-id) to a call when using the API, SDK function, or CLI to invoke an endpoint. When InferenceId is specified, SageMaker appends an eventMetadata/inferenceId field to the Data Capture event. Both approaches generate a single eventId or inferenceId for each call to Invoke-Endpoint. If an InferenceId is specified, Arthur will use it as the Arthur Partner Inference ID. Otherwise, it will default to the SageMaker EventId.
One tricky part about SageMaker's Invoke-Endpoint API it allows requesting multiple inferences in a single Invoke-Endpoint API call. In this case, the SageMaker EventId or callee-specified InferenceId would be shared by all inferences in the call and would not be unique. When this occurs, the Arthur integration will append an index number to either the EventId or InferenceId based on the inference order in the call to Invoke-Endpoint.
When ingesting Data Capture inferences from SageMaker, the following table describes the partner inference ID any given inference is assigned on the Arthur platform.
SageMaker Invoke Call
Without Inference ID provided in Invoke Endpoint
With Inference ID provided in Invoke Endpoint
Single Inference in Invoke Endpoint
EventId
InferenceId
Multiple Inferences in Invoke Endpoint
EventId_{index_within_invoke_endpoint_call}
InferenceId_{index_within_invoke_endpoint_call}
InferenceId and EventId refer to the Inference ID and Event ID, respectively, provided when calling Invoke Endpoint either through the API or boto3 SDK.
index_within_invoke_endpoint_call refers to the index of the specific inference within a group of multiple/mini-batch inferences sent through the Invoke-Endpoint call.
For example, for an Invoke-Endpoint call including CSV data 1,2,3,4\n5,6,7,8\n and Inference ID abcdefg-12345, the inference containing the features 1,2,3,4 would have a partner inference ID of abcdefg-12345_0 on the Arthur platform and the inference containing the features 5,6,7,8 would have a partner inference ID of abcdefg-12345_1.
(s3_batch_ingestion)=
AWS Lambda Setup
This section provides an example of a single-Lambda-per-Arthur-model setup. The following code is meant to serve as an example and can be implemented in a variety of ways that fit your organization's tech stack.
This build sets up an S3 object creation Lambda trigger to run the function whenever SageMaker writes a file to the bucket. This sample code will then pull the file and upload it to Arthur. In its current form, the code assumes that all S3 object notifications will be for the single model for the configured ARTHUR_MODEL_ID. See the following sections for the configurations required to use the lambda.
Lambda Function Configurations
To create the Lambda function, go to the AWS Lambda Console, click "Create function" and select "Use a Blueprint", then search for and select s3-get-object-python.
Then ensure you select or create an Execution Role with access to your Data Capture upload bucket(s).
Finally, ensure the following configurations are set and create the function:
Timeout: 15 min 0 sec (must be set after Lambda function creation in the "Configuration" / "General configuration" tab)
Environment Variables
ARTHUR_HOST: The host URL for your Arthur deployment (or https://app.arthur.ai/ for SaaS customers)
ARTHUR_MODEL_ID: The ID assigned to the Arthur model to accept the new inferences
ARTHUR_ACCESS_TOKEN: An access token for the Arthur platform
This can be replaced by the retrieval of this token at Lambda runtime
The token must, at the very least, provide raw_data write access
Lambda Function Trigger
Source: S3
Bucket: Your Data Capture configured S3 bucket
Event: All object create events
Prefix: Path to your model's specific Data Capture output directory
Suffix: .jsonl
AWS S3 Trigger Overlap Error
AWS prevents multiple S3 triggers from applying to the same subset(s) of files. Therefore, be careful in specifying your Prefix / Suffix and the files they indicate. For example, in the following setup of S3 triggers, AWS would raise errors because of overlap with .jsonl files in /modelA/datacapture (triggers A + C) as well as overlap with .tar.gz files in /modelA (triggers B + C):
Trigger A: (Prefix: s3://bucket/modelA/datacapture) (Suffix: .jsonl)
Trigger B: (Prefix: s3://bucket/modelA) (Suffix: .tar.gz)
Trigger C: (Prefix: s3://bucket/modelA) (Suffix: Unspecified)
In the above cases, AWS will still successfully create the Lambda but will then raise the following error at the top of their UI:
Your Lambda function "lambda-function-name" was successfully created, but an error occurred when creating the trigger: Configuration is ambiguously defined. Cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type.
Lambda Code
Pythonimport urllib.parse
import boto3
import os
import requests
s3 = boto3.client('s3')
ARTHUR_MODEL_ID = os.environ["ARTHUR_MODEL_ID"]
# 12345678-1234-1234-1234-1234567890ab
ARTHUR_HOST = os.environ["ARTHUR_HOST"]
# https://app.arthur.ai/
if ARTHUR_HOST[-1] != '/':
ARTHUR_HOST += '/'
# Ensure trailing slash exists
# TODO BY USER
# FILL IN CODE TO RETRIEVE AN ARTHUR API KEY
ARTHUR_ACCESS_TOKEN = os.environ["ARTHUR_ACCESS_TOKEN"]
ARTHUR_ENDPOINT = f"api/v3/models/{ARTHUR_MODEL_ID}/inferences/integrations/sagemaker_data_capture"
def lambda_handler(event, context):
# Get the object from the event
bucket = event['Records'][0]['s3']['bucket']['name']
key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
try:
s3_object = s3.get_object(Bucket=bucket, Key=key)
datacapture_body = s3_object.get('Body')
request_url = urllib.parse.urljoin(ARTHUR_HOST, ARTHUR_ENDPOINT)
print(f"Request: POST {request_url}")
response = requests.post(
request_url,
files={'inference_data': ('smdatacapture.jsonl', datacapture_body, s3_object['ContentType'])},
headers={'Authorization': ARTHUR_ACCESS_TOKEN}
)
print(f"Response: {response.content}")
except Exception as e:
print(e)
print('Error getting object {} from bucket {}. '
'Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
raise e
Summary
With your SageMaker endpoint deployed (with Data Capture configured) and a Lambda function ready for S3 updates, you can send requests to your SageMaker endpoint to generate predictions. The predictions will be logged as files in S3 by Data Capture, and the lambda function will upload the inferences to Arthur, where you can see them in the dashboard.Updated 3 months ago Table of Contents
Prerequisites
SageMaker Configuration
Configuring Data Capture through the SageMaker SDK
Configuring Data Capture through the SageMaker API
Supported Data Formats
Specifying Partner Inference ID on Arthur-Ingested Data Capture Inferences
AWS Lambda Setup
Lambda Function Configurations
Lambda Function Trigger
Lambda Code
Summary
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like
 What does ongoing maintenance look like?
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by What does ongoing maintenance look like?Suggest EditsUpdated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/composing-functions
 Composing Advanced Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Composing Advanced FunctionsSuggest EditsTo see functions you can use with this syntax, checkout the Aggregation Functions and the Transformation Functions guides.
Any function with a parameter with the type signature [string or nested] is able to accept the following as a value:
a string constant that represents a property of the model
an object of the form:
JSON{
"alias_ref": "string",
"nested_function": {"...":
"..."}
}
Only one of alias_ref or nested_function may be present at a time. See the following explanations of each.
Alias References
The alias_ref field allows specifying another selected column's alias to use as input to the function. This example uses an alias_ref to pull in another column to the add function:
JSON{
"select": [
{
"function": "abs",
"alias": "absLoan",
"parameters": {
"property": "loan"
}
},
{
"function": "add",
"alias": "plus2",
"parameters": {
"left": 2,
"right": {
"alias_ref": "absLoan"
}
}
}
]
}
This request returns:
JSON{
"query_result": [
{
"absLoan": 55.45,
"plus2": 57.45
}
]
}
Nested Functions
The nested_function field allows specifying another function definition to use as input. Here's an example of how to calculate absolute error for a regression model. In this example, we pass the nested subtract function as input to the abs function via the nested_function object for the property parameter of abs:
JSON{
"select": [
{
"function": "abs",
"alias": "abs_error",
"parameters": {
"property": {
"nested_function": {
"function": "subtract",
"alias": "error",
"parameters": {
"left": "Predicted_FICO_Score",
"right": {
"alias_ref": "ground_truth"
}
}
}
}
}
},
{
"property": "Predicted_FICO_Score"
},
{
"property": "Consumer_Credit_Score",
"alias": "ground_truth"
}
]
}
This query returns:
JSON{
"query_result": [
{
"Consumer_Credit_Score": 660,
"Predicted_FICO_Score": 688.10004,
"abs_error": 28.100040000000035
},
{
"Consumer_Credit_Score": 663,
"Predicted_FICO_Score": 681,
"abs_error": 18
},
"..."
]
}
📘If you use the same function multiple times in a query, you need to give each one a distinct "alias". Otherwise, the names will conflict.Updated 3 months ago Table of Contents
Alias References
Nested Functions
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/hot-spots
 Hot Spots
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Hot SpotsAutomatically illuminate underperforming segments in your dataSuggest EditsWhen a system has high-dimensional data, finding the right data input regions, such as troubleshooting, becomes a difficult problem. Hotspots automate identifying regions associated with poor ML performance to significantly reduce the time and error of finding such regions. Arthur Scope utilizes a proprietary tree-based algorithm to search out areas of underperformance and explain them through human-understandable language. Find out more about the algorithm here:
Hot Spots in Arthur Scope
Hot Spots are under the Insights tab in your Arthur Model dashboard.
Time Intervals: To highlight when underperformance occurred, Hot Spots are calculated for specific segments of time. For batch models, this is every batch. For streaming models, this is every 7 days.
Performance Threshold: In the UI, Hot Spots we currently only have accuracy Hot Spots available to detect performance under 40%.
Subgroups: The segments of data that have been identified as underperforming in each Hot Spot
Clicking on a Hot Spot
Subgroup Performance: The accuracy rate for this subgroup of data
Inference Count: How many inferences were included in this subgroup
Subgroup Rules: The rules that define the inferences included in this subgroup. Rules are set up to incorporate any metadata sent to Arthur, not just model features (i.e., can include any non-input attributes provided to Arthur). To ensure actionable Hot Spots, we allow a maximum of 7 rules.
View Inferences Button: Click on this button to go to the inferences deep dive page automatically filtered to match the rules within this subgroup for continued exploration
Status Button: Change the status from New to Acknowledged to alert other team members that you have evaluated this hot spot.
Available Arthur Schemas
Currently only available for Tabular binary and multiclass classification models within Arthur.
Understanding the Algorithm
To learn more about the algorithm used for Hot Spots. Please refer to the Arthur Algorithms documentation section.Updated 3 months ago What’s NextLearn more about enabling enrichments for your model in the Model Onboarding section. Otherwise, click on Explainability to learn about another type of enrichment.Enabling EnrichmentsExplainabilityTable of Contents
Hot Spots in Arthur Scope
Clicking on a Hot Spot
Available Arthur Schemas
Understanding the Algorithm
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/sending-inferences-1
 Sending Inferences
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Sending InferencesSuggest EditsNow that you have registered your model successfully, you can connect your production pipeline to Arthur.
Creating Arthur Connection
To be able to send inference data to the platform, you will need to create a connection to not only your Arthur platform but also the model the inferences are being tracked for. Information about creating your API key and connecting to the Arthur platform/model objects can be found in the UI Guide.
Formatting Inference Data
The first thing you need to do to send inferences to Arthur is to format the data into a structure Arthur will understand. This will follow a similar structure to the formatting to onboard your reference dataset. However, there are some differences in added attributes to point out.
The following attributes are formatted the exact same as your reference dataset and are required for all inferences sent.
Model Attributes: All features the model uses to create predictions
Model Predictions: Model predictions for each row of data
The next two parameters are required for inference datasets. However, these are not explicitly required for teams onboarding inferences with the Arthur Python SDK.
Inference Timestamp: Typically refers to the time of model prediction
If the inference timestamp is not specified, the SDK will auto-populate this field with the time the inferences were logged into the Arthur platform
Partner Inference ID: A way to match specific inferences in Arthur against your other systems and update your inferences with ground truth labels as they become available in the future. The most appropriate choice for a partner inference ID depends on your specific circumstances, but common strategies include using existing IDs and joining metadata with non-unique IDs.
If you already have existing IDs that are unique to each inference and easily attached to future ground truth labels, you can simply use those (casting to strings if needed).
Another common approach is constructing a partner inference ID from multiple pieces of metadata. For example, if your model makes predictions about your customers at least once daily, you might construct your partner inference IDs as {customer_id}-{date}. This would be easy to reconstruct when sending ground truth labels much later: simply look up the labels for all the customers passed to the model on a given day and append that date to their ID.
If you don’t supply partner inference IDs, the SDK will generate them for you and return them to your send_inferences() call. These can be kept for future reference or discarded if you’ve already sent ground truth values or don’t plan to in the future.
🚧Match Partner Inference ID with Internal Distinct IDsMany ML models in production do not receive ground truth at the time of prediction. The technique that Arthur uses to onboard ground truth later utilizes your partner inference ID. Teams that want to take advantage of metrics, unique Arthur enrichments, and popular reporting workflows that require ground truth will have to have a linkable connection between their data base and ours (through Partner Inference ID).
Finally, the remaining information can be onboarded to Arthur but does not need to be:
Non-Input Attributes: Specifying values for non-input attributes is not required at the time of inference, i.e., you can send inferences with Null non-input attributes
❗️Sent Inference Data is ImmutableYou may not update non-input attribute data (or any inference data for that matter) after sending it to the Arthur platform. The only value that can be updated in Arthur is ground truth, which we will see in the next section.
Send Inferences to Arthur
Inferences are commonly sent to Arthur using our Python SDK but can also be sent with our API or 3rd party integrations.
Python SDK (Quick Integration) : The most common way to log inferences is with the Python SDK. This can be done by adding our send_inferences()to your model's prediction function. Teams must connect to Arthur within their prediction script, run predictions, and send results to Arthur. However, this option would have you add latency to the speed with which your model generates inferences. For more efficient approaches, see options 2 and 3.
Python SDK (Streaming Uploads): If you write your model's inputs and outputs to a data stream, you can add a listener to that stream to log those inferences with Arthur. For example, if you have a Kafka topic, you might add a new arthur consumer group to listen to new events and pass them to the send_inferences() method. If your inputs and predictions live in different topics or you want to add non-input data from another topic, you might use Kafka Streams to join the various topics before sending them to Arthur.
Python SDK (Inference Upload Jobs): Another option is to read data from the rest and send it into the Arthur platform. Depending on their architecture, some teams choose a job or event-driven approach. They often have jobs that look up inferences since the last run, run a script that formats and writes the data into parquet files, and then use the Python SDK function send_bulk_inferences() to send the parquet files to the Arthur platform.
JSON Payload Function: For model deployments that do not have a Python script to run, teams often choose to send inferences to our API through JSON payload.
3rd Party Integration: Arthur has several integrations with third-party services, frameworks, and platforms. Check out our Integrations page to explore more potential integrations.
Python SDKJSON PayloadInference Upload Jobs####################################################
# New code to fetch the ArthurModel
# connect to Arthur
import os
from arthurai import ArthurAI
arthur = ArthurAI(
url="https://app.arthur.ai",
access_key=os.environ["ARTHUR_API_KEY"])
# retrieve the arthur model
arthur_model = arthur.get_model(os.environ["ARTHUR_PARTNER_MODEL_ID"], id_type='partner_model_id')
####################################################
# your original model prediction function
# which can be on its own as a python script
# or wrapped by an API like a Flask app
def predict():
# get data to apply model to
inference_data = ...
# generate inferences
# in this example, the predictions are classification probabilities
predictions = model.predict_proba(...)
####################################################
#### NEW PART OF YOUR MODEL'S PREDICTION SCRIPT
# SEND NEW INFERENCES TO ARTHUR
arthur_model.send_inferences(
inference_data,
predictions=predictions)
####################################################
return predictions
Updated 3 months ago Table of Contents
Creating Arthur Connection
Formatting Inference Data
Send Inferences to Arthur
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs
 Platform Readiness for Existing Cluster Installs
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Platform Readiness for Existing Cluster InstallsSuggest EditsThe Arthur platform can be installed on an on-prem or cloud-based pre-provisioned Kubernetes cluster, so all the data and controls adhere to existing corporate practices.
Arthur Cluster Installs FAQs
SysAdmin
What kind of privileges/hardware does the SysAdmin installing the Arthur platform need?
The SysAdmin will need a workstation with the following requirements:
running Linux or MacOS. The KOTS CLI installer does not support Windows.
sudo/root access. To install the KOTS CLI plugin for kubectl.
connection to the Kubernetes cluster, using kubectl, and privileges to deploy K8s Objects at either Cluster or Namespace scope (at least).
(recommended) access to the Internet. For downloading the installer, plugins and fetching updates.
How can I download the artifacts required for installing the Arthur platform?
All artifacts required for installing the Arthur platform are available on a customer-specific password-protected portal, which your sales team can give you access to. It is
recommended that the portal is accessible from within your corporate network, since the artifacts are around mutiple GBs in size.
Does my kubernetes cluster need access to the internet?
The Arthur platform can be installed without Internet access, once all the required files are downloaded and available locally. However, we recommend access to the Internet from the Kubernetes cluster for an efficient install and upgrade experience. Please inform your sales team about any network restrictions and optionally, if its possible to {ref}whitelist specific URLs <requirements_for_online_installation>.
Cloud Providers
Which Kubernetes distributions that Arthur supports out-of-the-box?
Arthur is architected to run on any distribution of Kubernetes, however certain commercial distributions are untested. The Arthur application is validated/tested on:
Amazon AWS EKS
Which cloud providers has Arthur been tested on?
The Arthur platform has been tested on the following cloud providers:
Amazon AWS
What container runtimes does Arthur support?
Containers in the Arthur Platform run on the following container runtimes:
docker (slated to be deprecated in Kubernetes 1.24)
containerd
Kubernetes Server
What version(s) of Kubernetes Server does Arthur support?
Please refer to the requirements documentation.
Can the Arthur platform be scoped to a dedicated namespace?
The Arthur platform can be deployed and scoped to a specific namespace, though there are some cluster-level CustomResourceDefinitions that need to be pre-installed. See details here.
What are the minimum resource requirements for operating the Arthur Platform?
Optimal performance of the Arthur platform is ensured on a 6 node cluster (though test clusters can be provisioned with 3 nodes) with each node having 16 CPUs, 32GB Memory (RAM) and 1000 GB Storage with at least 3000 IOPS. However, please reach out to your sales team for a tailored configuration custom to your projected workloads.
Is there a default StorageClass defined on the Kubernetes cluster?
The Kubernetes cluster must have a default StorageClass defined before starting the Arthur platform installation. If a default StorageClass does not exist, adding the storageclass.kubernetes.io/is-default-class: "true" annotation to a StorageClass should remedy this requirement.
What Ingress Controller are you planning to use to access the Arthur platform? Is it already installed?
The Arthur platform needs to expose a couple of services so the application is accessible outside the cluster. All compatible Kubernetes Ingress controllers should work, though {ref}Nginx Ingress Controller <k8s_install_prep_install_ingress> installed in a separate namespace is recommended.
Are there any SecurityContext requirements on the Kubernetes cluster?
The Arthur platform is architected to leverage as few permissions as deemed necessary for optimal functioning. No container is run as root. All processes are owned by non-system users. Please reach out to your sales team if you have specific SecurityContext requirements.
Does Arthur support running on SELinux environments?
The Arthur platform requires SELinux to be running in permissive mode, if enabled.
Are there any Network Policies configured on the Kubernetes cluster?
Pods of the Arthur platform will need to communicate with each other. By default, pods can communicate with each other. Please reach out to your sales team if you have custom Network Policies configured on the Kubernetes cluster.
How many IP addresses should be available for the Arthur Platform?
The Arthur platform is architected to be scalable, using resources on-demand. Given the dynamic nature of the infrastructure involved, we recommend at least 128 IP address CIDR blocks attached to the relevant subnets. However, this number can increase as more models are onboarded to the platform.
Are there any namespace-level constraints enforced on the Kubernetes cluster?
Please let your sales team know if there are any constraints configured at the namespace-level on the Kubernetes cluster, as this will help prepare for a smooth installation experience.
Are there any cluster-level constraints enforced on the Kubernetes cluster?
Please let your sales team know if there are any specific cluster-level contraints configured on the Kubernetes cluster, as this will help prepare for a smooth installation experience.
Does the Kubernetes cluster have access to a private/public container registry?
The Kubernetes cluster on which the Arthur platform will be installed must have connectivity to a container registry. The SysAdmin performing the installation must also have Read/Write access to the same container registry.
Does the Kubernetes cluster have access to a private/public Pypi/Conda registry?
The Kubernetes cluster on which the Arthur platform will be installed must have connectivity to a PyPI/Conda registry, which ensures optimum utilization of the features of the platform.
Other Considerations
Does your enterprise have a software procurement process?
Please keep your sales team informed of any software procurement process that maybe in place before installing new software, and potential turnaround times for such processes.
Do you want to deploy Arthur on infrastructure that isn't mentioned above (eg: Cloud Providers, Kubernetes Distributions, etc.)?
If so, please inform your sales team as soon as possible so we can setup an architecture review between your platform team and Arthur's platform team.
Can any of the Arthur platform components be externalized, so its not managed by Arthur?
The platform supports the use of AWS S3 as well as most S3 compatible systems as the Object/Blob store. The embedded metadata database can be replaced by a recent version of Postgres.
A managed service for S3 and/or Postgres is recommended for production-grade installs.
Can the Arthur platform be deployed on a Kubernetes cluster that is shared with other applications?
The Arthur platform has been architected to be highly scalable and reliable. Based on usage (number of models) and load (data ingestion), pods are scaled in short periods of time to ensure efficient operation. As such, if other applications will be installed on the same Kubernetes platform, talk to your sales team about provisioning dedicated nodegroups for the cluster.
Does the Arthur platform support different organizations/business units using the same application?
Yes. See our guide on User and Org Management.
Updated 3 months ago Table of Contents
Arthur Cluster Installs FAQs
SysAdmin
Cloud Providers
Kubernetes Server
Other Considerations
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/image-multi-class-classification-2
 Multiclass Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Multiclass ClassificationSuggest EditsMulticlass classification models predict one class from more than two potential classes. In Arthur, these models fall into the classification category and are represented by the Multiclass model type.
Some common examples of Image multiclass classification are:
What breed of dog is in this photo?
What part of the car is damaged in this photo?
Similar to binary classification, these models frequently output the predicted class and a probability for each class predicted. The highest probability class is then the predicted output. In these cases, a threshold does not need to be provided to Arthur, and it will automatically track the highest probability class as the predicted output.
Formatted Data in Arthur
Image multiclass classification models require three things to be specified in their schema: the image input, the predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e., any information you want to track about your inferences) as non-input attributes.
Attribute (Image Input)Probability of Prediction AProbability of Prediction BProbability of Prediction CGround TruthNon-Input Attribute (numeric or categorical)image_1.jpg.90.05.05AMaleimage_2.jpg.46.14.40BFemaleimage_3.jpg.16.17.71CFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their multiclass classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 2 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Option 1:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
output_mapping_1 = {
'pred_class_one_column':'one',
'pred_class_two_column':'two',
'pred_class_three_column':'three'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=output_mapping_1
)
## Option 2:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
output_mapping_2 = {
'pred_class_one_column':'gt_class_one_column',
'pred_class_two_column':'gt_class_two_column',
'pred_class_three_column':'gt_class_three_column'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping_2
)
## Example prediction function for binary classification
def predict(x):
return model.predict_proba(x)
Available Metrics
When onboarding multiclass classification models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a multiclass classification model. Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the image attribute is the only attribute required to onboard a model. So, monitoring non-input attributes for fairness in image models is only possible.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/custom-rbac
 Custom RBAC
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Custom RBACSuggest EditsManaging RBAC and Organizations for SSO users
For customers using SSO (on-prem only), Arthur can set up a fully customizable RBAC.
Please follow the below:
When setting up your identity provider via the YAML configuration, supply a global
role name and set of permissions under globalRoleDefs that your identity provider will authenticate users with.
This configuration will create the global role in the Arthur authorization system when it is applied. See the
{ref}Creating Global Roles for Managing Organizations and RBAC Policies Guide <creating_global_roles_in_arthur_config> for
more information.
That global role can then create custom role mappings for each organization:
During organization creation, including the role configuration JSON (see below, for example) in the request body when calling the organizations endpoint.
After an organization is created, create or add custom_roles by sending the role configuration JSON (see below, for example) in the request body when calling authorization custom roles
endpoint.
Users logging in through your IdP must now have a valid known role in their token when accessing the Arthur
Platform. Arthur will use this role to authenticate that the user is a member of the organization and
determine their permissions.
Managing Roles and Permissions
Understanding Permissions
A permission is a combination of a resource and an action. For example raw_data read, users write, models delete.
For a full list of available permissions. please see Arthur Permissions by Standard Roles.
For a directory of permissions by API endpoint, please see Arthur Permissions by Endpoint.
Create Custom Roles
The Create Organization Custom Roles endpoint is available for customers using SSO to operate on custom roles for each organization. A few notes:
This endpoint only operates on permission scopes within each organization. Permissions with global scope (such as creating a new organization) cannot be granted via this endpoint, those permissions must be assigned to a role with global privileges via the Arthur IdP configuration YAML. See
{ref}Creating Global Roles for Managing Organizations and RBAC Policies Guide <creating_global_roles_in_arthur_config> for more
information.
Roles can have a list of permissions to allow and/or a list of other roles to inherit permissions from.
Role names cannot conflict with Arthur Permissions by Standard Roles
Supplied permissions must be valid, known as Arthur permissions.
Roles can inherit the permissions of other roles that are either Arthur Permissions by Standard Roles or roles also defined in the same organization. Unknown inherited role names will be rejected.
Get Custom Roles
To retrieve a list of roles defined for an organization, use: Get Organization Custom Roles. To filter on specific roles, pass a comma-separated list of role names in a roles query parameter. For example: /authorization/custom_roles?roles=role1,role2. If you wish to return all roles, simply leave out the query parameter or pass "*" as role.
Delete Custom Roles
To delete a role or multiple roles from an organization, use Delete Organization Custom Roles. Specify which roles to delete in the JSON request body. For example, to delete a single role:
JSON{
"roles": [
"role3"
]
}
To delete all roles pass "*."
📘If you do not specify an organization_id, this will delete all custom roles you have created
JSON{
"roles": [
"*"
]
}
Example Role Configuration JSON
Below is an example JSON request body that creates three roles. role1 has 3 permissions defined, role2 gets
additional permission and then inherits the 3 permissions from role1, and role3 inherits the permissions from Arthur's
default "Model Owner" role. For more details on the expected schema for each endpoint, see API Documentation.
JSON{
"roles": [
{
"role_name": "role1",
"permissions": [
{
"resource": "metric_data",
"action": "read"
},
{
"resource": "metric_data",
"action": "write"
},
{
"resource": "tag",
"action": "read"
}
]
},
{
"role_name": "role2",
"permissions": [
{
"resource": "user_self",
"action": "read"
}
],
"inherited_role_names": [
"role1"
]
},
{
"role_name": "role3",
"inherited_role_names": [
"Model Owner"
]
}
]
}
Updated 3 months ago Table of Contents
Managing RBAC and Organizations for SSO users
Managing Roles and Permissions
Understanding Permissions
Create Custom Roles
Get Custom Roles
Delete Custom Roles
Example Role Configuration JSON
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/single-sign-on
 Access Control
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Access ControlSuggest EditsArthur supports a variety of mechanisms for authentication, who a user is, and authorization, what a user can do (RBAC). By default, Arthur will use a built-in authentication and authorization system. In on-prem installations, cluster administrators can optionally configure an external Identity Provider (IdP) to control user authentication and authorization. See the sections below for an overview of Arthur Standard Access Control and Arthur SSO Access Control.Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/arthur-permissions-by-endpoint
 Arthur Permissions by Endpoint
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur Permissions by EndpointSuggest EditsFor more details on the expected schema for each endpoint, see Authorization API Documentation. If an endpoint is not listed here, there is no authorization restriction on that endpoint, and all authenticated users should be able to access it.
For more details on the expected schema for each endpoint, see authorization. If an endpoint is not listed here, there is no authorization restriction on it, and all authenticated users should be able to access it.
Org, User, and Authorization Related Permissions
PermissionEndpointRESTResource in SystemActionGet all organizations/organizationsGETorganization_globalreadCreate an organization/organizationsPOSTorganization_globalwriteGet an organization/organizations/{organization_id}GETorganizationreadDelete an organization/organizations/{organization_id}DELETEorganizationdeleteAdd users to an organization/organizations/{organization_id}/usersPATCHuserwriteCreate user/usersPOSTuserwriteGet all users/usersGETuserreadGet current user/users/meGETN/AN/AUpdate current user/users/mePATCHN/AN/AGet user/users/{user_id}GETuserreadUpdate user/users/{user_id}PATCHuserwriteDelete user/users/{user_id}DELETEuserdeleteInvite a new user/users/invite_usersPOSTinvite_userwriteGet Current Authentication Info/users/me/auth_infoGETN/AN/AGet Permssions for a role/authorization/permissionsGETN/AN/ACheck Authorization for a permission and role/authorization/authorizePOSTN/AN/ACreate Custom roles for RBAC/authorization/custom_rolesPOSTcustom_roleswriteDelete Custom Roles for RBAC/authorization/custom_rolesDELETEcustom_rolesdeleteGet configured custom roles for RBAC/authorization/custom_rolesGETcustom_rolesreadGet current organization of session/organizations/currentGETN/AN/ASet current organization of session/organizations/currentPUTN/AN/AGet usage/usage/{rollup}GETorganization_metricsread
Model and Data Related Permissions
PermissionEndpointRESTResource in SystemActionGet models/modelsGETmodelreadCreate a model/modelsPOSTmodelwriteGet a model's health score (most recent)/models/healthGETmodelreadGet model/models/{model_id}GETmodelreadUpdate model/models/{model_id}PUTmodelwriteDelete model/models/{model_id}DELETEmodeldeleteGet model bias groups/models/{model_id}/bias_groupsGETmodelreadGet attributes/models/{model_id}/attributesGETmodelreadUpdate attributes/models/{model_id}/attributesPUTmodelwriteEdit model attributes/models/{model_id}/attributesPATCHmodelwriteDelete model attributes/models/{model_id}/attributesDELETEmodeldeleteGet an attribute/models/{model_id}/attributes/{attribute_id}GETmodelreadUpdate an attribute/models/{model_id}/attributes/{attribute_id}PUTmodelwriteDelete an attribute/models/{model_id}/attributes/{attribute_id}DELETEmodeldeleteGet tags/tagsGETtagreadUpdate a tag/tags/{tag_name}PUTtagwriteDelete a tag/tags/{tag_name}DELETEtagdeleteGet model groups/model_groupsGETmodelreadGet a model group/model_groups/{model_group_id}GETmodelreadUpdate a model group/model_groups/{model_group_id}PATCHmodelwriteDelete a model group/model_groups/{model_group_id}DELETEmodeldeleteGet a model group's versions/model_groups/{model_group_id}/versionsGETmodelreadGet latest version for a model group/model_groups/{model_group_id}/versions/latestGETmodelreadRetrieve the prediction and explanation for an inference/models/{model_id}/what_ifPOSTraw_datareadGenerate on-demand explanation for an inference/models/{model_id}/inferences/{partner_inference_id}/explanationGETraw_datareadSave inferences/models/{model_id}/inferencesPOSTraw_datawriteSave inferences from file/models/{model_id}/inferences/filePOSTraw_datawriteUpdate inferences/models/{model_id}/inferencesPATCHraw_datawriteUpdate ground truth/models/{model_id}/ground_truthPATCHground_truthwriteGet image inference/models/{model_id}/inferences/images/{image_id}GETraw_datareadGet batch information for batch of a model/models/{model_id}/batches/{batch_id}GETraw_datareadCloses a batch/models/{model_id}/batches/{batch_id}PATCHraw_datawriteGet inference/models/{model_id}/inferences/query/{partner_inference_id}GETraw_datareadGet all datasets for a model/models/{model_id}/datasetsGETraw_datareadGet reference data information for model/models/{model_id}/reference_dataGETreference_datareadCloses a reference dataset/models/{model_id}/reference_dataPATCHreference_datawriteUploads a parquet or json file containing reference set data/models/{model_id}/reference_dataPOSTreference_datawriteExecute query/models/{model_id}/inferences/queryPOSTquery (✱ see footnote)executeExecute query and retrun data drift values/models/{model_id}/inferences/query/data_driftPOSTqueryexecuteExecute query and return psi buckets/models/{model_id}/inferences/query/data_drift_psi_bucket_calculation_tablePOSTqueryexecuteExecute query and return scatterplot distributions/models/{model_id}/inferences/query/distributionPOSTqueryexecuteGet bias mitigation curves/models/{model_id}/bias_mitigation_curves/attributes/{attribute_id}GETraw_datareadGet bias mitigation curves/models/{model_id}/enrichments/bias_mitigation/curvesGETraw_datareadFind Hotspots/models/{model_id}/enrichments/hotspots/findGETraw_datareadGet metric queries/models/{model_id}/metricsGETmetric_queryreadCreate a metric query/models/{model_id}/metricsPOSTmetric_querywriteGet a metric query/models/{model_id}/metrics/{metric_id}GETmetric_queryreadUpdate a metric query/models/{model_id}/metrics/{metric_id}PUTmetric_querywriteDelete a metric query/models/{model_id}/metrics/{metric_id}DELETEmetric_querydeleteGet all enrichment configs/models/{model_id}/enrichmentsGETenrichment_configreadUpdate an enrichment config/models/{model_id}/enrichmentsPATCHenrichment_configwriteGet explainability config/models/{model_id}/enrichments/explainabilityGETenrichment_configreadUpdate explainability config/models/{model_id}/enrichments/explainabilityPATCHenrichment_configwriteGet anomaly detection config/models/{model_id}/enrichments/anomaly_detectionGETenrichment_configreadUpdate anomaly detection config/models/{model_id}/enrichments/anomaly_detectionPATCHenrichment_configwriteGet bias mitigation config/models/{model_id}/enrichments/bias_mitigationGETenrichment_configreadUpdate bias mitigation config/models/{model_id}/enrichments/bias_mitigationPATCHenrichment_configwriteGet hotspots config/models/{model_id}/enrichments/hotspotsGETenrichment_configreadUpdate hotspots config/models/{model_id}/enrichments/hotspotsPATCHenrichment_configwriteGet Pinned Columns/models/{model_id}/pinned_columnsGETpinned_columnsreadSet Pinned Columns/models/{model_id}/pinned_columnsPUTpinned_columnswriteAdd/Remove Pinned Columns/models/{model_id}/pinned_columnsPATCHpinned_columnswriteDelete Pinned Columns/models/{model_id}/pinned_columnsDELETEpinned_columnsdelete
✱ query requires both queryexecute permissions as well as EITHER raw_dataread OR reference_dataread permissions depending on what the posted query is.
Alert Related Permissions
PermissionEndpointRESTResource in SystemActionCreate an alert rule/models/{model_id}/alert_rulesPOSTalert_rulewriteGet alert rules/models/{model_id}/alert_rulesGETalert_rulereadDelete an alert rule/models/{model_id}/alert_rules/{alert_rule_id}DELETEalert_ruledeleteEdit an alert rule/models/{model_id}/alert_rules/{alert_rule_id}PATCHalert_rulewriteBulk resolve alert for an alert rule/models/{model_id}/alert_rules/{alert_rule_id}/bulk_alertsPATCHalertresolveGet alerts/alertsGETalertreadGet alert counts by model/alerts/model_countsGETalertreadUpdate alert status/alerts/{alert_id}PATCHalertresolveSend manual alert notification/alerts/{alert_id}/notificationsPOSTalertnotifyGet alert notification configurations/alert_notification_configurationsGETalert_notification_configreadCreate an alert notification configuration/alert_notification_configurationsPOSTalert_notification_configwriteGet an alert notification configuration/alert_notification_configurations/{configuration_id}GETalert_notification_configreadEdit an alert notification configuration/alert_notification_configurations/{configuration_id}PATCHalert_notification_configwriteDelete an alert notification configuration/alert_notification_configurations/{configuration_id}DELETEalert_notification_configdeleteGet insights/models/{model_id}/insightsGETinsightreadUpdate the status of insights/models/{model_id}/insightsPATCHinsightresolveGet an insight/models/{model_id}/insights/{insight_id}GETinsightwriteUpdate the status of an insight/models/{model_id}/insights/{insight_id}PATCHinsightresolveGet all alert summary configurations/alert_summary_configurationsGETalert_summary_configreadCreate an alert summary configuration/alert_summary_configurationsPOSTalert_summary_configwriteGet an alert summary configuration/alert_summary_configurations/{configuration_id}GETalert_summary_configreadDelete an alert summary configuration/alert_summary_configurations/{configuration_id}DELETEalert_summary_configdeleteUpdate an alert summary configuration/alert_summary_configurations/{configuration_id}PATCHalert_summary_configwriteGet all alert summary subscribers/alert_summary_configurations/{configuration_id}/subscribersGETalert_summary_subscriberreadCreate an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribersPOSTalert_summary_subscriberwriteGet an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribers/{subscriber_id}GETalert_summary_subscriberreadUpdate an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribers/{subscriber_id}PATCHalert_summary_subscriberwriteDelete an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribers/{subscriber_id}DELETEalert_summary_subscriberdeleteSend manual alert summary notifications to subscribers/alert_summary_configurations/{configuration_id}/notificationsPOSTalert_summarynotifyUpdated about 2 months ago Table of Contents
Org, User, and Authorization Related Permissions
Model and Data Related Permissions
Alert Related Permissions
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/nlp-onboarding
 NLP Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by NLP OnboardingSuggest EditsThis page walks through the basics of setting up a natural language processing (NLP) model and onboarding it to
Arthur Scope to monitor language-specific performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur Scope.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>")
Registering an NLP Model
Each NLP model is created with a name and with input_type = InputType.NLP. Here, we register a classification model on text specifying a text_delimiter of NOT_WORD:
Pythonarthur_nlp_model = arthur.model(name="NLPQuickstart",
input_type=InputType.NLP,
model_type=OutputType.Multiclass,
text_delimiter=TextDelimiter.NOT_WORD)
The different OutputType values currently supported for NLP models are classification, multi-labeling, and regression.
Text Delimiter
NLP models optionally allow specifying a text_delimiter, which specifies how a raw document is split into tokens.
If a text delimiter is not provided, a default text_delimiter will be TextDelimiter.NOT_WORD. This delimiter will ignore punctuation and tokenize text based only on the words present. However, suppose punctuation and non-word text needs to be considered by your model. In that case, you should consider using other options for a delimiter to ensure those other pieces of text are processed by your NLP model.
For a full list of available text delimiters with examples, see the
TextDelimiter constant documentation in our SDK reference.
Additionally, Arthur supports sending the pre-tokenized text. For steps on registering tokens with Arthur, see our generative text walkthrough.
Formatting Reference/Inference Data
Column names can contain only alphanumeric and underscore characters. The rest of the string values can have
additional characters as raw text.
Python
text_attr
pred_value
ground_truth
non_input_1
0
'Here-is some text'
0.1
0
0.2
1
'saying a whole lot'
0.05
0
-0.3
2
'of important things!'
0.02
1
0.7
3
'With all kinds of chars?!'
0.2
0
0.1
...
4
'But attribute/column names'
0.6
1
-0.6
5
'can only use underscore.'
0.9
1
-0.9
...
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save(), you can call arthur_model.review() the model schema to check that your data is parsed correctly.
For an NLP model, the model schema should look like this:
Python
name
stage
value_type
categorical
is_unique
0
text_attr
PIPELINE_INPUT
UNSTRUCTURED_TEXT
False
True
1
pred_value
PREDICTED_VALUE
FLOAT
False
False
...
2
ground_truth
GROUND_TRUTH
INTEGER
True
False
3
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using arthur_model.review(), you are finished registering your model and its attributes - so you are ready to complete onboarding your model.
To finish onboarding your NLP model, the following steps apply, which is the same for NLP models as it is for models
of any InputType and OutputType:
finishing_onboarding.md
Enrichments
For an overview of configuring enrichments for NLP models, see the {doc}/user-guide/walkthroughs/enrichments guide.
For a step-by-step walkthrough of setting up the explainability Enrichment for NLP models, see
{ref}nlp_explainability.Updated 3 months ago Table of Contents
Getting Started
Registering an NLP Model
Text Delimiter
Formatting Reference/Inference Data
Reviewing the Model Schema
Finishing Onboarding
Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/aggregation-functions
 Aggregation Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Aggregation FunctionsSuggest EditsFor an explanation of nested functions, see the Composing Functions guide
Mathematical Functions
Average
Take the average of a property.
Query Request:
JSON{
"select": [
{
"function": "avg",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<avg_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "avg",
"alias": "avgAge",
"parameters": {
"property": "age"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"avgAge": 55.45
}
]
}
back to top
Average of Absolute Values
Take the average of absolute values of a property.
Query Request:
JSON{
"select": [
{
"function": "avgAbs",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<avg_abs_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "avgAbs",
"alias": "avgafloat",
"parameters": {
"property": "afloat"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"avgafloat": 55.45
}
]
}
back to top
Max
Take the max of a property.
Query Request:
JSON{
"select": [
{
"function": "max",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<max_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "max",
"alias": "maxAge",
"parameters": {
"property": "age"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"maxAge": 55.45
}
]
}
back to top
Min
Take the min of a property.
Query Request:
JSON{
"select": [
{
"function": "min",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<min_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "min",
"alias": "minAge",
"parameters": {
"property": "age"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"minAge": 17
}
]
}
back to top
Sum
Take the sum of a property.
Query Request:
JSON{
"select": [
{
"function": "sum",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<sum_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "sum",
"alias": "sumAge",
"parameters": {
"property": "age"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"sumAge": 1745
}
]
}
back to top
Variance
Take the variance of a property.
Query Request:
JSON{
"select": [
{
"function": "variance",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<variance> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "variance",
"alias": "varAge",
"parameters": {
"property": "age"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"varAge": 10.234
}
]
}
back to top
Standard Deviation
Take the standard deviation of a property.
Query Request:
JSON{
"select": [
{
"function": "stdDev",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<stdDev> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "stdDev",
"alias": "stdDevAge",
"parameters": {
"property": "age"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"stdDevAge": 3.199
}
]
}
back to top
Count
Count
Count a property.
Query Request:
JSON{
"select": [
{
"function": "count",
"alias": "<alias_name> [optional string]"
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<count> [int]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "count"
}
],
"filter": [
{
"property": "income",
"comparator": "lt",
"value": 90000
}
]
}
Sample Response:
JSON{
"query_result": [
{
"count": 5432
}
]
}
back to top
Count If
Conditionally count a property.
Query Request:
JSON{
"select": [
{
"function": "countIf",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"comparator": "<comparator> [string]",
"value": "<string or number to compare with property>"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<count> [int]"
}
]
}
See Endpoint Overview for a list of valid comparators.
Sample Request:
JSON{
"select": [
{
"function": "countIf",
"alias": "michigan_count",
"parameters": {
"property": "state",
"comparator": "eq",
"value": "michigan"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"michigan_count": 5432
}
]
}
back to top
Categories Count
Count the number of categories of a property. To be used for discrete features.
Query Request:
JSON{
"select": [
{
"function": "categoriesCount",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<categories_count> [int]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "categoriesCount",
"alias": "categoriesCountZipcode",
"parameters": {
"property": "zipcode"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"categoriesCountZipcode": 732
}
]
}
back to top
Rate
Calculates the rate of a condition on a column.
Query Request:
JSON{
"select": [
{
"function": "rate",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"comparator": "<comparator> [string]",
"value": "<string or number to compare with property>"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<rate_value> [float]"
}
]
}
See Endpoint Overview for a list of valid comparators.
Sample Request: Calculate the positive predictive rate, with predictions classified as positive when pos_class above .5 (standard definition of positive predictive rate).
JSON{
"select": [
{
"function": "rate",
"alias": "pos_rate",
"parameters": {
"property": "pos_class",
"comparator": "gt",
"value": 0.5
}
}
]
}
Response:
JSON{
"query_result": [
{
"pos_rate": "0.1"
}
]
}
back to top
Distributions
Distribution
Return the distribution of a column with a specified number of bins.
You may specify one of either num_bins or bin_thresholds.
For num_bins,, if there is not enough data, fewer than the specified number of bins will be returned.
For bin_thresholds, there will be a bucket below your lowest bin and a bucket above your highest bin, i.e. n+1 buckets when supplied n thresholds.
Query Request with num_bins:
JSON{
"select": [
{
"function": "distribution",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"num_bins": "<number_of_bins> [int]"
}
}
]
}
Query Request with bin_thresholds:
JSON{
"select": [
{
"function": "distribution",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"bin_thresholds": "<list_of_floats> [List[int]]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": [
{
"lower": "<bin_lower_bound> [float]",
"upper": "<bin_upper_bound> [float]",
"count": "<bin_count> [float]"
}
]
}
]
}
Lower bounds are inclusive while upper bounds are exclusive, so a bucket response like this:
JSON{
"lower": 30,
"upper": 50,
"count": "<bin_count> [float]"
}
would include values such as 30, 40, or 49.999, but not 50.
Sample Request with num_bins:
JSON{
"select": [
{
"function": "distribution",
"parameters": {
"property": "FICO_predicted",
"num_bins": 50
}
}
]
}
Sample Response with num_bins:
JSON{
"query_result": [
{
"distribution": [
{
"lower": 500,
"upper": 600,
"count": 1000
},
{
"lower": 600,
"upper": 700,
"count": 5000
},
{
"lower": 700,
"upper": 800,
"count": 2000
},
{
"lower": 800,
"upper": 850,
"count": 550
}
]
}
]
}
Sample Request with bin_thresholds:
JSON{
"select": [
{
"function": "distribution",
"parameters": {
"property": "FICO_predicted",
"bin_thresholds": [600, 700]
}
}
]
}
Sample Response with bin_thresholds:
JSON{
"query_result": [
{
"distribution": [
{
"upper": 600,
"count": 1000
},
{
"lower": 600,
"upper": 700,
"count": 5000
},
{
"lower": 700,
"count": 2550
}
]
}
]
}
back to top
Quantile
Get the quantile of a column at the specified point. The level parameter between 0 and 1 specifies the cut point, with 0.5 representing the median, 0.9 the 90th percentile, and so on.
Query Request:
JSON{
"select": [
{
"function": "quantile",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"level": "<quantile level> [float (0.0, 1.0)]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<quantile> [constant]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "quantile",
"alias": "medianAge",
"parameters": {
"property": "age",
"level": "0.5"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"medianAge": 47
}
]
}
Decile
Get the decile of a property.
Query Request:
JSON{
"select": [
{
"function": "deciles",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": {
"max": "<max_value> [number]",
"min": "<min_value> [number]",
"q1": "<first decile> [number]",
"q2": "<second decile> [number]",
"q3": "<third decile> [number]",
"q4": "<fourth decile> [number]",
"q5": "<fifth decile> [number]",
"q6": "<sixth decile> [number]",
"q7": "<seventh decile> [number]",
"q8": "<eighth decile> [number]",
"q9": "<ninth decile> [number]",
}
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "deciles",
"alias": "likelihoodDeciles",
"parameters": {
"property": "likelihood"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"likelihoodDeciles": {
"max": 0.9427181028155817,
"min": 0.04451819608908539,
"q1": 0.5422967935750383,
"q2": 0.6379949435686291,
"q3": 0.7381396364893235,
"q4": 0.802484647469614,
"q5": 0.8282093987794898,
"q6": 0.8474456853909271,
"q7": 0.8662622658358209,
"q8": 0.884452391809424,
"q9": 0.901106156817971
}
}
]
}
back to top
Arg Values
Arg Max
Take the value of a property at which a different property as at its maximum.
Query Request:
JSON{
"select": [
{
"function": "argMax",
"alias": "<alias_name> [optional string]",
"parameters": {
"argument": "<attribute_name> [string or nested]",
"value": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<arg_max_value> [constant]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "argMax",
"alias": "mostExpensiveZipCode",
"parameters": {
"argument": "zipCode",
"value": "homePrice"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"mostExpensiveZipCode": 94027
}
]
}
back to top
Arg Min
Take the value of a property at which a different property as at its minimum.
Query Request:
JSON{
"select": [
{
"function": "argMin",
"alias": "<alias_name> [optional string]",
"parameters": {
"argument": "<attribute_name> [string or nested]",
"value": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<arg_min_value> [constant]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "argMin",
"alias": "leastExpensiveZipCode",
"parameters": {
"argument": "zipCode",
"value": "homePrice"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"leastExpensiveZipCode": 46953
}
]
}
back to top
Any If
Returns the selected property for any row which matches the provided condition. A common use case for this function is querying data with unique identifiers (as described in this guide Grouped Inference Queries).
Query Request:
JSON{
"select": [
{
"function": "anyIf",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"comparator": "<comparator> [string]",
"value": "<string or number to compare with property>",
"result": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<result> [constant]"
}
]
}
See Endpoint Overview for a list of valid comparators.
Sample Request:
JSON{
"select": [
{
"function": "anyIf",
"alias": "some_michigan_price",
"parameters": {
"property": "state",
"comparator": "eq",
"value": "michigan",
"result": "homePrice"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"some_michigan_price": 459312
}
]
}
back to top
Regional Feature Importance
Returns the regional importance score for a particular attribute. This is the average of the absolute value of the explainability value for all the inferences. This is only available if Explainability has been enabled for the model.
Query Request:
JSON{
"select": [
{
"function": "regionalFeatureImportance",
"alias": "<alias_name> [optional string]",
"parameters": {
"attribute_name": "<pipeline_input_attribute_name> [string]",
"predicted_attribute_name": "<predicted_attribute_name> [string]",
"explanation_algorithm": "[limeshap]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<feature_importance_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "regionalFeatureImportance",
"parameters": {
"attribute_name": "AGE",
"predicted_attribute_name": "prediction_0",
"explanation_algorithm": "lime"
}
}
],
"filter": [
{
"property": "inference_timestamp",
"comparator": "gte",
"value": "2020-12-01T10:00:00Z"
},
{
"property": "inference_timestamp",
"comparator": "lt",
"value": "2020-12-22T11:00:00Z"
}
]
}
Sample Response:
JSON{
"query_result": [
{
"AGE": 0.001406118694451489
}
]
}
back to top
Regional Feature Importances
Returns the regional importance scores for all of the pipeline input attributes. This is the average of the absolute value of the explainability value for all the inferences for each pipeline input attribute. This is only available if Explainability has been enabled for the model.
Query Request:
JSON{
"select": [
{
"function": "regionalFeatureImportances",
"alias": "<alias_name> [optional string]",
"parameters": {
"predicted_attribute_name": "<predicted_attribute_name> [string]",
"explanation_algorithm": "[limeshap]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<attribute_name>": "<feature_importance_value> [float]",
"<attribute_name>": "<feature_importance_value> [float]",
"<attribute_name>": "<feature_importance_value> [float]",
"<attribute_name>": "<feature_importance_value> [float]",
"<attribute_name>": "<feature_importance_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "regionalFeatureImportances",
"parameters": {
"predicted_attribute_name": "prediction_0",
"explanation_algorithm": "lime"
}
}
],
"filter": [
{
"property": "inference_timestamp",
"comparator": "gte",
"value": "2020-12-01T10:00:00Z"
},
{
"property": "inference_timestamp",
"comparator": "lt",
"value": "2020-12-22T11:00:00Z"
}
]
}
Sample Response:
JSON{
"query_result": [
{
"explainer_attribute": "PAY_0",
"regionalFeatureImportance": 0.055036517803945396
},
{
"explainer_attribute": "PAY_2",
"regionalFeatureImportance": 0.026880464089676884
},
{
"explainer_attribute": "PAY_3",
"regionalFeatureImportance": 0.024027941129616155
},
{
"explainer_attribute": "LIMIT_BAL",
"regionalFeatureImportance": 0.022367882999425544
},
{
"explainer_attribute": "PAY_AMT2",
"regionalFeatureImportance": 0.019145911247181836
},
{
"explainer_attribute": "PAY_AMT1",
"regionalFeatureImportance": 0.019052984358794038
},
{
"explainer_attribute": "PAY_AMT3",
"regionalFeatureImportance": 0.012942233755875516
},
{
"explainer_attribute": "PAY_5",
"regionalFeatureImportance": 0.011911442095349226
},
{
"explainer_attribute": "PAY_4",
"regionalFeatureImportance": 0.010464962507962139
},
{
"explainer_attribute": "PAY_6",
"regionalFeatureImportance": 0.00891260261770653
},
{
"explainer_attribute": "BILL_AMT4",
"regionalFeatureImportance": 0.007211523900878019
},
{
"explainer_attribute": "BILL_AMT5",
"regionalFeatureImportance": 0.006279087267628024
},
{
"explainer_attribute": "BILL_AMT1",
"regionalFeatureImportance": 0.006221344024007549
},
{
"explainer_attribute": "PAY_AMT4",
"regionalFeatureImportance": 0.005310133724715099
},
{
"explainer_attribute": "PAY_AMT6",
"regionalFeatureImportance": 0.004135643379284112
},
{
"explainer_attribute": "MARRIAGE",
"regionalFeatureImportance": 0.004089899824740581
},
{
"explainer_attribute": "EDUCATION",
"regionalFeatureImportance": 0.003931984513777395
},
{
"explainer_attribute": "PAY_AMT5",
"regionalFeatureImportance": 0.0033734464617669853
},
{
"explainer_attribute": "SEX",
"regionalFeatureImportance": 0.0029222783744727687
},
{
"explainer_attribute": "BILL_AMT6",
"regionalFeatureImportance": 0.002707692309829875
},
{
"explainer_attribute": "BILL_AMT2",
"regionalFeatureImportance": 0.001955133839877692
},
{
"explainer_attribute": "BILL_AMT3",
"regionalFeatureImportance": 0.001779632159224476
},
{
"explainer_attribute": "AGE",
"regionalFeatureImportance": 0.001406118694451494
}
]
}
rankedItemMaxK
Returns the maximum position of a specified ranked list item across all ranked list data for a model. Maximum position in this case refers to the largest index the item sits at in a ranked list array, not the highest ranking. If the specified item is not in any array, the function will return None. For use with ranked list type attributes only. The property name should have the _item_id suffix appended to it.
Query Request:
JSON{
"select": [
{
"function": "rankedItemMaxK",
"parameters": {
"property": <ranked_list_attr_name>_item_id,
"item_filter": <item_identifier>
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"rankedItemMaxK": <maxK>
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "rankedItemMaxK",
"parameters": {
"property": "recommendations_item_id",
"item_filter": "item1"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"rankedItemMaxK": 4
}
]
}
back to topUpdated about 2 months ago Table of Contents
Mathematical Functions
Average
Average of Absolute Values
Max
Min
Sum
Variance
Standard Deviation
Count
Count
Count If
Categories Count
Rate
Distributions
Distribution
Quantile
Decile
Arg Values
Arg Max
Arg Min
Any If
Regional Feature Importance
Regional Feature Importances
rankedItemMaxK
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/arthur-algorithms
 Arthur Algorithms
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur AlgorithmsSuggest EditsAnomaly Detection
For an explanation of how to enable Anomaly Detection with Arthur, please see the Anomaly Detection
Anomaly scores are computed by training a model on the reference set you provide to Arthur and using that model to assign an Anomaly Score to each inference you send to Arthur. Scores of 0.5 are given to "typical" examples from your reference set, while higher scores are given to more anomalous inferences, and lower scores are given to instances that the model judges as similar to the reference data with high confidence.
How Anomaly Detection Works
We calculate Anomaly Scores with an Isolation Forest algorithm. This algorithm works by building what is essentially a density model of the data by iteratively isolating data points from one another. Because anomalies tend to be farther away from other points and occur less frequently, they are easier to isolate from other points, so we can use a data point's "ease of isolation" to describe its anomaly. The method is based on the paper linked here.
The Isolation Forest "method takes advantage of two [quantitative properties of anomalies]:
i) they are the minority consisting of fewer instances and
ii) they have attribute-values that are very different from those of normal instances.
In other words, anomalies are 'few and different', which make them more susceptible to isolation than normal points."
The idea is to build a binary tree that randomly segments the data until each instance can be uniquely selected (or a maximum height is reached). Anomalous instances will take fewer steps to become isolated on the tree because of the abovementioned properties.
In the example above, we can see that the in-distribution x_i takes many more steps to reach than the out-of-distribution x_0
Of course, using a single randomly generated tree would be noisy. So we train multiple trees to construct an Isolation Forest of multiple trees and use the average path length, noting that average path lengths converge:
The Isolation Forest algorithm is highly efficient compared to other density estimation techniques because the individual trees can be built from data samples without losing performance.
When you add a reference set to your model in Arthur, we fit an Isolation Forest model to that data to compute an anomaly score for the inferences your model receives.
Generating Anomaly Scores
The path length, or the number of steps taken to reach the partition that a data point belongs to, varies between 0 and n-1 where n is the number of data points in the training set. Following our intuition above, the shorter the path length, the more anomalous a point is. To measure anomaly, an anomaly score between 0 and 1 is generated by normalizing the path length by the average path length and applying an inverse logarithmic scale.
In particular, the anomaly scores algorithm shown below is the average path length to data pointx from a collection of isolation trees and c(n) is the average path length given the number of data points in the dataset n.
Every inference you send to Arthur will be evaluated against the trained Isolation Forest model and given an anomaly score.
This can be seen in the anomaly score contour of 64 normally distributed points:
At Arthur, we also visualize the distribution of anomaly scores among all the inferences your model has retrieved since training. When you select an inference in our UI you'll see where it falls on this distribution:
Interpreting Anomaly Scores
The resulting anomaly scores can be interpreted in the following way:
if instances return s very close to 1, then they are definitely anomalies
if instances have s smaller than 0.5, then they are quite safe to be regarded as normal instances
if all the instances return s approximately at 0.5, then the entire sample does not really have any distinct anomaly
Bias Mitigation
If you are interested in mitigation capabilities, we're happy to discuss your needs and what approaches would work best for you. Within the Arthur product, we offer postprocessing methods and encourage the exploration of alternate (pre- or in-processing) methods if your data science team has the bandwidth to do so.
We currently have one postprocessing method for the product: the Threshold Mitigator.
Threshold Mitigator
This algorithm is an extension of the Threshold Optimizer implemented in the fairlearn library, which in turn is based on Moritz Hardt's 2016 paper introducing the method.
The intuition behind this algorithm is based on the idea that if the underlying black box is biased, then different groups have different predicted probability distributions — let's call the originally predicted probability, for example, x the score s_x. Then, for a fixed threshold t, P(s_x > t  x in A) > P(sx > t  x in B), where _B indicates membership in the disadvantaged group. For example, the default threshold is t = 0.5, where we predict Y^ = 1 if s > 0.5.
However, we might be able to mitigate bias in the predictions by choosing group-specific thresholds for the decision rule.
So, this algorithm generally proceeds as follows:
Try every possible threshold t from 0 to 1, where for any x, we predict Y^ = 1 if and only if s_x > t.
Calculate each threshold's group-specific positivity, true positive, and false positive rates.
Let's say we're trying to satisfy demographic parity. We want the group-wise positivity rates to be equal —
P(Y^ = 1  A) = P(Y^
= 1  B). Then, for any given positivity rate, the threshold t_a that achieves this positivity rate might differ from the threshold t_b that achieves this positivity rate.
This also hints at why we need the notion of tradeoff curves rather than a single static threshold: if we want to achieve a positivity rate of 0.3, we'll need different t_a, t_b than if we wanted to achieve a positivity rate of 0.4.
Then, once we've generated the curves, we pick a specific point on the curve, such as a positivity rate of 0.3. When making predictions on the future
and using the corresponding t_a, t_b to make predictions
What's Compatible?
Fairness constraints: We can generate curves satisfying each of the following constraints (fairness metrics):
Demographic Parity (equal positivity rate)
Equal Opportunity (equal true positive rate)
Equalized Odds (equal TPR and FPR)
Sensitive attributes: This algorithm can handle sensitive attributes that take on any number of discrete values (i.e., not limited to binary sensitive attributes). We can specify buckets for the continuous values for continuous sensitive attributes, then treat them like categorical attributes.
The tradeoff curve
Generating the Curve
Each group has its own curve.
To generate a single curve, we generate all possible thresholds (i.e. [0, 0.001, 0.002, 0.003, ..., 0.999, 1.00]) — default 1000 total thresholds; as described above, we calculate many common fairness metrics for each of those thresholds for the result if we were to use that threshold to determine predictions.
Visualizing the Curve
The tradeoff curve will look different depending on the fairness constraint we're trying to satisfy.
For equal opportunity and demographic parity, where a single quantity must be equalized across the two groups, the tradeoff curve plots the quantity to equalize on the x-axis and accuracy on the y-axis.
Understanding the Curve Artifact
The curve artifact can be pulled down according to our docs here. This mitigation approach is somewhat constrained because we will generate a separate set of curves for each sensitive attribute to mitigate on; for each constraint to follow. Then, each set of curves entails a single curve for each sensitive attribute value.
Conceptually, the curves are organized something like the one below; in the API, however, you'll be getting a single < list of (x, y) coordinate pairs that define the curve > at a time, with additional information about the attribute being mitigated, the constraint being targeted, and the feature value the specific list of pairs is for.
**mitigating on gender -- demographic parity**
gender == male
< list of (x, y) coordinate pairs that define the curve >
gender == female
< list of (x, y) coordinate pairs that define the curve >
gender == nb
< list of (x, y) coordinate pairs that define the curve >
**mitigating on gender -- equalized odds**
gender == male
< list of (x, y) coordinate pairs that define the curve >
gender == female
< list of (x, y) coordinate pairs that define the curve >
gender == nb
< list of (x, y) coordinate pairs that define the curve >
**mitigating on age -- demographic parity**
age < 35
< list of (x, y) coordinate pairs that define the curve >
age >= 35
< list of (x, y) coordinate pairs that define the curve >
Summary of Curves by Constraint
Equal opportunity (equal TPR): TPR vs. accuracy; the accuracy-maximizing solution is the point along the x-axis with the highest accuracy (y-axis) for both groups.
Demographic parity (equal selection rates): selection rate vs. accuracy; the accuracy-maximizing solution is the point along the x-axis with the highest accuracy (y-axis) for both groups.
Equalized odds (equal TPR and FPR): FPR vs. TPR (canonical ROC curve); the accuracy-maximizing solution is the point on the curves that are closest to the top left corner of the graph (i.e., low FPR and high TPR).
Choosing a Set of Thresholds (Point on Curve)
As mentioned above, a single "point" on the solution curve for a given constraint and sensitive attribute corresponds to several thresholds mapping feature value to a threshold. But how do we choose which point on the curve to use?
The default behavior — and, in fact, the default in fairlearn — is to automatically choose the accuracy-maximizing thresholds subject to a particular fairness constraint. This might work in most cases; however, accuracy is not necessarily the only benchmark an end user will care about. One client, for example, needs to satisfy a particular positivity rate and is fine with sacrificing accuracy to do so. What our feature introduces that goes beyond what's available in fairlearn is the ability to try different thresholds; see what hypothetical predictions would be; and see what hypothetical results/metrics (e.g., positivity rate, TPR, etc.) would look like if
a certain set of thresholds was applied. The ultimate choice of thresholds is up to data scientists' needs, etc.
Hotspots
For an explanation of how to enable Hotspots with Arthur, please see the Hot Spots
We might have an ML model deployed in production and some monitoring in place. We might notice that performance degrades from classic performance metrics or drift monitoring combined with explainability techniques. We’ve identified that our model is failing; the next step is to identify why our model is failing.
This process would involve slicing and dicing our input data that caused model degradation. We want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks or retraining our models on those regions.
This basically boils down to the time-consuming task of finding needles in a haystack. What if we could reverse engineer the process and surface all of the needles, i.e., input regions associated with poor performance, directly to the user?
We can with Hotspots!
How Hotspots Works
The outputs of a model are encoded as a special classification task to partition data to separate out the poor-performing data points from the correct predictions/classifications on a per-batch basis for batch models or a per 7-day window for streaming models.
Hotspot enrichments are used to surface input regions where the model is currently underperforming for inferences. Hotspots are extracted from a custom Arthur tree model, where nodes are associated with particular input regions and have associated performance metrics, e.g., a node with 70% accuracy with data points where variable X is less than 1000. Nodes are candidates for hotspots. Depending on user-specified thresholds, e.g., a threshold of 71% accuracy, the tree is traversed until all nodes with less than 71%, such as our node with 70% accuracy, have been identified and returned to the user as hotspots, not including the hotspot nodes' children, which would be either (1) purer than the hotspot node and therefore in further violation of the, e.g., 71% threshold or (2) pure nodes with correct inferences, which are not of interest to the user for remediation purposes.
See our blog post here for a more in-depth overview.Updated 5 months ago Table of Contents
Anomaly Detection
How Anomaly Detection Works
Generating Anomaly Scores
Interpreting Anomaly Scores
Bias Mitigation
Threshold Mitigator
Hotspots
How Hotspots Works
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/alerting-dashboard
 Managing Alerts
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Managing AlertsSuggest EditsAny team that has suffered from alert fatigue knows that alerts are only as good as your ability to manage and take action on them. Knowing this, there are a few ways in which teams can work and interact with alerts within Arthur, all focused on the ability for teams to derive value and track their alerts.
Recent Organization Alert Overview
Upon opening the UI, teams begin to interact with alerts at an organizational level. On the control panel home page, teams can see the most recent critical alerts triggered within their organization and a high-level description.
From this page, teams can quickly assess which model's to dive deeper into from a recent alerting perspective.
Alert Management Per Model
There is an Alert tab within each Arthur Model dashboard, as well, that provides a management dashboard for all alerts triggered for that particular model.
Here teams can:
View Recent History of Alerts: Visualize the recent patterns of alerting over the past two weeks. Understand when new critical alerts or warnings appeared and in what quantity.
Create Alert Rules: Described in the Alerting overview tab. This allows teams to create some of the most common alert rules that we see in an easy, predefined structure.
Filter and Sort Triggered Alerts: Sort alerts by metrics and severity to immediately find the alerts of interest.
Operationalize Alert Management: Teams can tag alerts into different categories through the JIRA board-like structure. While teams can determine the best way to operationalize these categories for these teams, we typically see:
CategoryUse Case DescriptionNewNew alerts coming into the model that require investigation.AcknowledgedAlerts being investigated or looked into by a team member, reducing the repetition of investigation or work within teams.ResolvedAlerts that have been investigated and resolved, either through the team triggering retraining, solving an upstream data issue, or even flagging the alert as a false positive and potentially changing the threshold/alert rules
Managing Alert Rules
Within each model's alerting dashboard, teams can view and manage all model alert rules by clicking the Manage Alert Rules button. Understand all alert rules created for your model (enabled or disabled), their severity, and the date they were last updated in one central place.
Investigating a Specific Alert - (Root Cause Analysis)
After clicking on an alert, users are taken to an overview page for the alert. This page describes more about this alert, including a historical list of when it was triggered. It also provides three common analysis charts for the alert to allow teams to begin their root cause analysis into the cause.
These charts include:
The inference count over time: See how many inferences have been affected in creating this alert
Alerted metric over time: See how the specific metric of interest
Alerted metric over time (for the attributes that experienced the most drift): Immediately evaluate if recent data distributional drifts may have been a root cause of the alert
Beyond some initial drilled-down charts for exploration, this alert details tile provides insights
View in Overview: Jump to the Model Overview Tab in the Arthur UI to investigate further. This tab will have global filtered automatically applied to the window of time and/or data segments the alert was triggered on.
Send to Relevant Team-members or stakeholders: You can easily share the alert with others by hitting the send button on the top corner. This alert can be shared by email to anyone who has enabled email alert notifications for this model. Otherwise, users can select Copy Link to copy a link to this alert details tile.Updated 3 months ago Table of Contents
Recent Organization Alert Overview
Alert Management Per Model
Managing Alert Rules
Investigating a Specific Alert - (Root Cause Analysis)
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/default-access-control
 Default Access Control
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Default Access ControlSuggest EditsIn both SaaS and On-prem installations, Arthur ships with a built-in access control system that can be used to manage users, permissions, and access to organizations. This system has different capabilities than the SSO-based paradigm. If your installation is using SSO, please see the Platform Access Control.
Authentication
Users authenticate to Arthur using a username and password set when their account is created and can be changed later in the UI. Users can also use the Login API endpoint to retrieve a token with Arthur APIs.
Applications and automated systems can authenticate with Arthur using API keys, which can be created in the Arthur UI from the organization menu in the upper right corner, then clicking on Manage API Keys.
🚧Note on using Session KeysIt is not recommended to use API-keys for non-automated use cases as they are not tied to user
identities and can obscure who is performing actions. As a best practice, use API keys minimally only in the systems that need automated access, and be sure to create a rotation practice to ensure safe keeping.
Authorization (RBAC)
The Arthur standard access control system uses role-based access control (RBAC) with a set of pre-defined roles. The available roles for users are User, Model Owner, Administrator, and SuperAdmin. If enrolled in multiple organizations, the user can have a different role in each organization. For a full list of permissions for these 4 standard roles, please reference Arthur Permissions by Standard Roles.
User: Has read-only access to the models and data within the organization.
Model Owner: Can onboard new models in the enrolled organization as well as send data, including reference data,
inferences, and ground truth.
Administrator: Organization-level administrator that has access to manage users and models within the organization.
Super Admin: Has full access to all data, models, and actions on the platform. Can create new organizations and manage users. Only available on-prem.
📘Custom RolesIf your installation uses SSO, you can take advantage of creating custom roles to fine-tune user
access to Arthur resources. See the documentation on Custom RBAC for more information.
Adding Users to an Organization in the UI
To complete this section, you must have the "Administrator" role in your organization.
Click on the organization menu in the upper right corner and then "Manage Members." From this screen, you can enter the emails of additional users to add to the organization, manage the roles of existing users, and remove users from the organization.
🚧In order for email-based user invites to work, your installation must have an email integration set up. If not, you can use the Arthur APIto create user accounts directly in your organization.
Adding Users to an Organization in the API
Arthur also supports managing users via automated workflows using the REST API. To create a user in your organization, you must have Administrator privileges or access to the super admin user for your Arthur on-prem installation. The following APIs are helpful for managing users:
Create User
Update User
Send User Invites
Switching Between Organizations
If a user is invited to multiple organizations, they can switch between them in the UI. Users can click on the organization menu in the upper right corner and choose one of the other available organizations from that menu to switch to it. If no other organizations appear, that user cannot access any other organizations.Updated 3 months ago Table of Contents
Authentication
Authorization (RBAC)
Adding Users to an Organization in the UI
Adding Users to an Organization in the API
Switching Between Organizations
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/getting-started
 Welcome to Arthur Scope!
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Welcome to Arthur Scope!As your team's data science operations center, Arthur helps enterprise teams monitor, measure and optimize AI performance at scale.Suggest EditsLooking to operationalize your machine learning systems in production? Arthur's observability platform helps enterprise teams monitor, measure, and improve machine learning at scale.
What Happens When AI Meets the Real World?
As the AI performance company, Arthur helps data scientists, product owners, and business leaders accelerate model operations at scale. Our platform monitors, measures, and improves machine learning models for better results across three core areas: accuracy, explainability, and fairness.
Video Overview
Got 4 minutes? Check out a video overview of our product:
Get Started Today
Jump into the Quickstart Guide or start learning about the Arthur Platform.Updated 3 months ago What’s NextQuickstartTable of Contents
What Happens When AI Meets the Real World?
Video Overview
Get Started Today
=====================
Content type: arthur_scope_docs
Source: https://legacy.docs.arthur.ai/api-documentation/v3-api-docs.html
 API Documentation - Arthur Documentation
API Documentation  Arthur Documentation
Link
Overview
Getting Started
Arthur Model
Arthur Attribute
Reference Set
Model Inferences
Concepts and
Terminology
User
Guide
Algorithmic Bias
Explainability
Enrichments
Batch Ingestion from S3
SparkML Integration Guide
Authentication and Deployment
SDK
Install Guide
SDK Docs
API
API
Query Guide
Endpoint Overview
Aggregation Functions
Model Evaluation Functions
Transformation Functions
Explainability
Data Drift
Alert Rules Guide
Examples on
Github
ReDoc
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/pages-in-the-arthur-platform
 Pages in the Arthur Scope Platform
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Pages in the Arthur Scope PlatformSuggest EditsOrganization Wide
In Arthur Scope, an organization is the highest level of grouped control within Arthur utilized to organize and manage access to resources that exist on the platform. Users can be added to multiple organizations and given roles that provide them with Read and/or Write access to some subset of that organization's resources, as defined by their user role.
Home Page
Each organization's home page works as a control panel for all the models in production.
Organization-Wide Model Dashboard
The organization-wide model dashboard provides a single pane of glass view into all your organization's models in production.
Model Onboarding Status
Model onboarding status provides a look into your model's creation phase. This is the place to check after onboarding to ensure your model is ready to receive inferences.
Pending: If your model status is Pending, model creation will begin soon. Please check back for updates or contact support if you don’t see any updates within a few minutes.
Creating: If your model status is Creating, the model creation process is in-progress. Please check back for updates or contact support if you don’t see any updates within 10 minutes.
Ready: If your model status is Ready, the model creation process has been completed successfully! Your model is now ready to ingest data.
Creation Failed: If your model status is Creation Failed, the model creation process was unsuccessful, and the necessary infrastructure was not provisioned fully. While in this state, your model cannot infrastructure ingest any data. You may try re-saving the model or contact support if the problem persists.
Health Scores
Health scores provide a quick insight into the model's overall performance from three important aspects.
Accuracy: Classification: F1 Score. Regression: 1 - Normalized MAE.
Ingestion: Variance of normalized time periods between ingestion events. The variance of normalized volume differences between ingestion events.
Drift: 1 - average anomaly score
These scores are classified into three color-coded sections: green, yellow, and red.
Settings
Users can access the settings for their organization by clicking on their initial button in the lower left corner.
Pages available within Settings:
Account: Change account setting information, such as your user password
API Keys: Create and copy new API Keys from
Members: A look at all members and roles in the Arthur organization
Usage: Evaluate monthly Arthur platform usage
Notifications: Organization-wide alert configuration, learn more at Managing Alerts
Changing Organizations: Users can have access to multiple organizations. To see what organizations they can access and/or switch between, they can click on the switch button in the Settings tab.
Arthur Scope - Model Dashboards
Overview Tab
The overview tab provides a look into your model's performance over time, in particular Performance Metrics, Fairness Metrics, and Data Drift Metrics.
Inference Deep Dive
The inference deep dive page provides a look into the individual inferences.
Certain administrator-level permissions will have the ability to save and edit column presets by pinning or unpinning columns on this page. These pinning selections happen at a per-model level. For those with non-adminstrator permissions, users will see the columns pinned by the admin by default and are able to pin columns to their view only.
Alert Dashboard
A control panel for all things Alerting for your Arthur model, including creating, managing, and triaging alerts in one dashboard.
Insights
Insights is a page dedicated to the information stored from enrichments within Arthur. Currently, insights only contain insights about
Hot Spots.
Updated about 2 months ago Table of Contents
Organization Wide
Home Page
Organization-Wide Model Dashboard
Settings
Arthur Scope - Model Dashboards
Overview Tab
Inference Deep Dive
Alert Dashboard
Insights
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/token-likelihood
 Token Likelihood
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Token LikelihoodSuggest EditsThere are currently no standard post-hoc explainability techniques for generative text (or token sequence) models. However, teams looking to better understand their models' outputs can turn to Token Likelihood for insights.
📘Token Likelihood Availability by LLM TypeTeams may choose from a number of LLM providers (OpenAI, Anthropic, Cohere, etc) to build their model. All of these model types can be monitored with Arthur. However, not every LLM provider allows the option for outputting token likelihood. To track token likelihood in Arthur, teams must provide token likelihood outputs from an LLM that outputs them.
Understanding Token Likelihood
The token likelihood is a number between 0 and 1 that quantifies the model’s level of surprise that this token was the next predicted token of the sentence. If a token has a low likelihood (close to 0), it means that the model is more unsure about selecting this token. While a likelihood close to 1 indicates that the model is very confident in predicting this token.
For example, if I was writing the sentence: I need to pack my backpack and ____
We can see from this example how token likelihood works. Textbooks is seen as a much more likely next word for the sentence than other terms like umbrella.
In LLM models, each token predicted has a likelihood.
What it looks like in Arthur
In Arthur Scope, Token Likelihood can be found, per inference provided for, in the Inferences Tab or the UI for text token sequence types.
The color of tokens ranges from green to red, with bright green being the most likely and bright red being the least for tokens in each inference.
Tracking Likelihood Metrics as Performance
We've seen how looking at individual inferences Token Likelihood can provide insight into a single prediction, but what if we wanted to use likelihood to assess trends?
Average Token Likelihood
The average token likelihood is created for each inference by taking the average likelihood score of predicted tokens in that inference. The average of these scores is then calculated for inferences within whatever time interval specified, i.e., daily, weekly, snapshot, etc.
The average token likelihood is a way to evaluate how confident your model is in its overall predictions. By tracking this over time, teams can better track this confidence.
Likelihood Stability
Likelihood stability looks at how stable your likelihood is between tokens for each inference.
Updated 3 months ago Table of Contents
Understanding Token Likelihood
What it looks like in Arthur
Tracking Likelihood Metrics as Performance
Average Token Likelihood
Likelihood Stability
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece
 Registering Model Attributes Manually
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Registering Model Attributes ManuallySuggest EditsThis guide is useful for teams that want to manually register all their model attributes or register a model without a reference dataset.
Setting Up To Individually Onboard Model Attributes
Setting up your model to add each attribute individually is similar to the typical starting point for Creating Arthur Model Objects. Teams must define their Arthur Model Object:
Pythonmodel = arthur.model(partner_model_id=f"CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}",
display_name="Credit Risk Batch",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass,
is_batch=True)
More information on defining this and all the steps can be found on the Creating Arthur Model Objects page here.
Set up Arthur Input Attributes
Teams looking to manually onboard models can start by manually adding all PipelineInput and NonInput attributes to their Arthur Model.
🚧Sending Null Values for AttributesUnless otherwise specified below in different types, Null values are allowed for different input types. Null and NaN values are allowed for onboarding through the Arthur SDK. On the other hand, Null values are allowed when onboarding with the API, but Null values are not.
Numerical Attribute
Numerical attributes are input attributes meant to track continuous numerical values. They can be manually added to a model in Arthur using the add_attribute function.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name="Num_Attr_Name",
value_type=ValueType.Float,
stage=Stage.ModelPipelineInput
)
📘Inferring Numerical Attributes as CategoricalWhen Arthur is inferring the model schema, Float and Integer columns are assumed to be categorical if there are fewer than 20 unique values and if Float values are all whole numbers. String and boolean columns are always assumed to be categorical for Tabular models.
Python## Ensure that numerical attributes are valued as numerical and not categorical
arthur_model.get_attribute("Num_Attr_Name", stage=Stage.ModelPipelineInput).categorical = False
Teams may also choose to specify the exact value type. The options are "Integer" or "Float".
Pythonarthur_model.get_attribute("Num_Attr_Name", stage=Stage.ModelPipelineInput).value_type = 'INTEGER'
Categorical Attribute
Categorical Attributes are attributes that represent a finite group of values (or categories). They can be manually added to a model in Arthur using the add_attribute function.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name="Cat_Attr_Name",
value_type=ValueType.String,
stage=Stage.ModelPipelineInput
)
👍Ensure All Possible Production Attributes Are SpecifiedAttributes that are set to categorical must have at least one column. In edge cases where this is not possible, the category list can be set using a single "dummy" category (e.g., ["n/a"]). While new categories will be taken in by the platform, they will not be utilized in drift calculations or segmented visualization in the UI. So, it is important to ensure that all potential categories are listed before onboarding.
Setting Possible Categories
Based on the callout above, teams may manually specify the potential categories.
Python## Set Categories to N/A
arthur_model.get_attribute("Cat_Attr_Name", stage=Stage.ModelPipelineInput).categories = ["n/a"]
## Set Categories to a List
arthur_model.get_attribute("Cat_Attr_Name", stage=Stage.ModelPipelineInput).categories = ["n/a", "bachelors","masters","highschool"]]]
Setting Attribute Labels
When teams have set up numerical encoding for their categorical variables, providing the mapping back to human understanding for the Arthur platform may be useful. This will make it easier for end users to utilize the UI to understand categorical attributes better.
Python# labels the value 0 for the attribute 'education_level'
# to have the label 'elementary', etc.
arthur_model.set_attribute_labels(
'education_level',
{0 : 'elementary', 1 : 'middle', 2 : 'high', 3 : 'university'}
)
Timestamp Attribute
Timestamp Attributes are model features that represent a date/time. These are frequently found in time series models.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a timestamp input attribute directly to the model
arthur_model.add_attribute(
name="Timestamp_Attribute_Name",
value_type=ValueType.Timestamp,
stage=Stage.ModelPipelineInput
)
It is important to note that the DateTime object being put into Arthur, must be in a DateTime format and include a timezone. A common example of how to set up these transformations can be seen below:
Python## Example of Converting to Pandas DateTime
## This function will need to change depending on how your time strings are formatted
def get_timestamps(x):
new_time = x.split('.')[0]
return datetime.strptime(new_time, '%Y-%m-%d %H:%M:%S')
df['timestamps'] = df['timestamps'].apply(lambda x: get_timestamps(x))
## Ensure Appropriate tzinfo Timezone Added
df['timestamp'] = df['timestamp'].apply(lambda x: x.replace(tzinfo=pytz.UTC))
❗️Null and NaN Values Are Not AllowedFor Timestamp attributes, Null and NaN values are not allowed within Arthur.
Time Series Attribute
Time Series Attributes are model features that represent a value over time.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a time series input attribute directly to the model
arthur_model.add_attribute(
name="Time_Series_Attribute_Name",
value_type=ValueType.TimeSeries,
stage=Stage.ModelPipelineInput
)
It is important to note that the TimeSeries object being put into Arthur must be formatted as a list of dicts with "timestamp" and "value" keys. The timestamps must be formatted according to the restrictions on Timestamp attributes. The values must be floats.
❗️There must be data for every timestamp on a regular time intervalEach time series attribute should be considered to have some regular time interval (eg. 1 day, 1 week, etc.) at which the value it is recording is polled. The value thus must be recorded on consistent time intervals, and if data is not recorded on a given timestamp in that consistent interval, a data point with a Null value must still be recorded.
Text (NLP) Attribute
Unstructured Text Attributes refer to input text for NLP models. They can be manually added to a model in Arthur using the add_attribute function. The ArthurAttribute type of UnstructuredText is designed to be used for NLP models only.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name="NLP_Input_Attribute_Name",
value_type=ValueType.UnstructuredText,
stage=Stage.ModelPipelineInput
)
Generative Text Models
Teams wanting to monitor generative text models should refer to the Generative Text Model Onboarding Guide. This provides a step-by-step walkthrough in manually onboarding those models.
Image Attribute
Image Attributes refer to the images input to computer vision models. They can be manually added to a model in Arthur using the add_image_attribute function.
Pythonmodel.add_image_attribute("ImageColumnName")
The ImageColumnName string contains the column's name in your future reference or inference data frames containing the path to each image.
Unique Identifier
A Unique Identifier Attribute within Arthur is created to specify unique values within the platform. These String-type categorical attributes within Arthur specify unique values for every category.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name="Cat_Unique_Attr_Name",
value_type=ValueType.String,
stage=Stage.ModelPipelineInput
)
## Specify this category as unique
arthur_model.get_attribute("Cat_Unique_Attr_Name", stage=Stage.ModelPipelineInput).unique = True
## Specify no categories
arthur_model.get_attribute("Cat_Unique_Attr_Name", stage=Stage.ModelPipelineInput).categories = []
🚧Do Not Onboard Your Partner Inference ID with Reference DataWhile the Partner Inference ID (your internal teams inference identifier) is the most common unique identifier within Arthur, this is something that you can specify (or Arthur will create) when you send inferences onto the platform. This is not specified when building out a reference dataset.
Set up Arthur Predicted/Ground Truth Attributes
After sending all input attribute information, teams can specify their model's predicted and ground truth attributes. This will depend on the model task type they are trying to onboard.
❗️Null Values Are Not Allowed for Predicted or Ground Truth AttributesNull values are not supported for predicted or ground truth attributes.
Classification
To add output attributes for classification tasks, teams must first specify what type of classification model they want to onboard. They can choose between:
Binary Classification with columns for each predicted attribute and ground truth value
Multi-Class Classification with columns for each predicted attribute and ground truth value
Either Binary or Multi-Class Classification with columns for each predicted attribute but only a single column for ground truth
The type of classification you choose should be based on the schema you expect for onboarding reference or inference data later.
Binary Classification
If you expect your inference schema to consist of two predictions and two ground truth columns for your binary classification task, then you should utilize the add_binary_classifier_output_attributes function. In this function, you need to provide:
Prediction to Ground Truth Mapping: Mapping of each predicted column to its corresponding ground truth column
Positive Predicted Attribute: The positive class is the class that is related to your objective function. For example, if you want to classify whether the objects are present in a given scenario. So all the data samples where objects are predicted present will be considered positively predicted.
Python# map PredictedValue attributes to their corresponding GroundTruth attributes
PRED_TO_GROUND_TRUTH_MAP = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# add the ground truth and predicted attributes to the model
# specifying that the `pred_1` attribute is the
# positive predicted attribute, which means it corresponds to the
# probability that the binary target attribute is 1
arthur_model.add_binary_classifier_output_attributes(positive_predicted_attr='pred_1',
pred_to_ground_truth_map=PRED_TO_GROUND_TRUTH_MAP)
Multi-Class Classification
If you expect your inference schema to consist of multiple predictions and their corresponding multiple ground truth columns for your classification task, then you should utilize the add_multiclass_classifier_output_attributes function. In this function, you need to provide:
Prediction to Ground Truth Mapping: Mapping of each predicted column to its corresponding ground truth column
Positive Predicted Attribute: The positive class is the class that is related to your objective function. For example, if you want to classify whether the objects are present in a given scenario. So all the data samples where objects are predicted present will be considered positively predicted.
Python# map PredictedValue attributes to their corresponding GroundTruth attributes
PRED_TO_GROUND_TRUTH_MAP = {
"dog": "dog_gt",
"cat": "cat_gt",
"horse": "horse_gt"
}
# add the ground truth and predicted attributes to the model
arthur_model.add_multiclass_classifier_output_attributes(
pred_to_ground_truth_map = PRED_TO_GROUND_TRUTH_MAP
)
Single Column Classification
Single-column classification is very similar to previous techniques; however, there is only a single ground truth column in this technique.
Prediction to Ground Truth Mapping: Mapping of each predicted column to its corresponding ground truth value
Positive Predicted Attribute: The positive class is the class that is related to your objective function. For example, if you want to classify whether the objects are present in a given scenario. So all the data samples where objects are predicted present will be considered positively predicted.
Ground_Truth_Column: You must specify the single-column ground truth
Python# Map PredictedValue attribute to its corresponding GroundTruth attribute value.
# This tells Arthur that the `pred_survived` column represents
# the probability that the ground truth column has the value 1
PRED_TO_GROUND_TRUTH_MAP = {
"pred_value": 1
}
# Add the ground truth and predicted attributes to the model,
# specifying which attribute represents ground truth and
# which attribute represents the predicted value.
arthur_model.add_classifier_output_attributes_gtclass(
positive_predicted_attr = 'pred_value',
pred_to_ground_truth_class_map = PRED_TO_GROUND_TRUTH_MAP,
ground_truth_column = 'gt_column'
)
Regression
To manually specify your regression output, teams need to specify a prediction to ground truth mapping with the following:
Predicted Value: The column that contains your numerical predicted output
Ground Truth Value: The column that contains the ground truth
Pythonfrom arthurai.common.constants import ValueType
# map PredictedValue attributes to their corresponding GroundTruth attributes
PRED_TO_GROUND_TRUTH_MAP = {
"pred_value": "gt_value",
}
# add the ground truth and predicted attributes to the model
arthur_model.add_regression_output_attributes(
pred_to_ground_truth_map = PRED_TO_GROUND_TRUTH_MAP,
value_type = ValueType.Float
)
Object Detection
To manually specify your object detection models, teams need to specify the following:
Predicted Attribute Name: This is the column name that will store your predicted bounding boxes
Ground Truth Attribute Name: This is the name of the column with the true labeled bounding boxes
Class Labels: All potential object labels for that your model is detecting
predicted_attribute_name = "objects_detected"
ground_truth_attribute_name = "label"
class_labels = ['cat', 'dog', 'person']
arthur_model.add_object_detection_output_attributes(
predicted_attribute_name,
ground_truth_attribute_name,
class_labels)
Generative Text (LLM)
Teams wanting to monitor generative text models should refer to the Generative Text Model Onboarding Guide. This provides a step-by-step walkthrough in manually onboarding those models.
Setting Reference Data Later
For teams that have chosen to manually onboard all of their model attributes to ensure that they were inferred correctly but still want to include a reference dataset for drift calculations, they can! After manually creating the model schema above, this can be done by setting the reference dataset.
Python# reference dataframe of model inputs
reference_set = pd.DataFrame(....)
# produce model predictions on reference set
# in this example, the predictions are classification probabilities
preds = model.predict_proba(reference_set)
# assign the column corresponding to the positive class
# as the `pred` attribute in the reference data
reference_set["pred"] = preds[:, 1]
# set ground truth labels
reference_set["gt"] = ...
# configure the ArthurModel to use this dataframe as reference data
arthur_model.set_reference_data(data=reference_set)
Updated 2 months ago Table of Contents
Setting Up To Individually Onboard Model Attributes
Set up Arthur Input Attributes
Numerical Attribute
Categorical Attribute
Timestamp Attribute
Time Series Attribute
Text (NLP) Attribute
Image Attribute
Unique Identifier
Set up Arthur Predicted/Ground Truth Attributes
Classification
Regression
Object Detection
Generative Text (LLM)
Setting Reference Data Later
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/enabling-enrichments
 Enabling Enrichments
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Enabling EnrichmentsSuggest EditsAs discussed in detail in the Enrichments section above, many teams require more than standard inference logging monitoring. For this reason, Arthur provides Enrichments.
Functions within Enrichment Enablement
There are three main functionalities for toggling enrichments on and off within Arthur. These are:
Getting the current configuration of the enrichment: understand whether or not the enrichment is enabled / disabled. This can be validated in a notebook environment (as shown in the below examples) or within the Arthur UI for the model under details.
Enable the enrichment: functionality to turn the enrichment on. This is not currently available in the UI and must be done with the Python SDK or an API call.
Disable the enrichment: functionality to turn the enrichment off. This is not currently available in the UI and must be done with the Python SDK or an API call.
Anomaly Detection
Anomaly detection allows users to go beyond univariate drift analysis and look at complex interactions between features that may cause drift. To learn more in detail about anomaly detection, please refer to its doc's section here:
Enable Anomaly Detection
Anomaly detection is the only enrichment automatically enabled within Arthur, as long as your model has a reference dataset attached to it.
Python## Using the Arthur Python SDK
# view current configuration
arthur_model.get_enrichment(Enrichment.AnomalyDetection)
# enable
arthur_model.update_enrichment(Enrichment.AnomalyDetection, True, {})
# disable
arthur_model.update_enrichment(Enrichment.AnomalyDetection, False, {})
Explainability
One of the most commonly enabled enrichments is explainability. Explainability allows teams to build trust with valuable insights into how models make decisions. It allows teams to understand why predictions are being made and evaluate how changes to model input change predictions.
You can learn more about how to use our explainability capabilities here, but
Model Asset Requirements
Arthur can automatically calculate explanations (feature importances) for every prediction your model makes. To make this possible, we package up your model in a way that allows us to call it'spredict function, which allows us to calculate explanations. We require a few things from your end:
A Python script that wraps your model's predict function
For Image models, a second function, load_image is also required.
A directory containing the above file, along with any serialized model files, and other supporting code
A requirements.txt with the dependencies to support the above
More detailed explanations about the model assets required here.
PythonEntrypoint FileExample Requirements.txt## enabling explainability
arthur_model.enable_explainability(
df=X_train.head(50),
project_directory="/path/to/model_folder/",
requirements_file="requirements.txt",
user_predict_function_import_path="model_entrypoint",
ignore_dirs=["folder_to_ignore"] # optionally exclude directories within the project folder from being bundled with predict function
)
## Common examples per model type can be found in the model input/output pages
# example_entrypoint.py
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(x)
boto3>=1.0
numpy>=1.0
pandas>=1.0
scikit-learn>=0.23.0
Find more information about troubleshooting explainability here.
Hot Spots
Hot spots help teams automatically surface rule-based segments of your data for underperformance. Learn more about how in the hot spots section of the documentation here.
Enable Hot Spots
Since Hot Spots rely only on inference data, no additional configuration is needed to enable them within the platform.
Python# view current configuration
arthur_model.get_enrichment(Enrichment.Hotspots)
# enable
arthur_model.update_enrichment(Enrichment.Hotspots, True, {})
# disable
arthur_model.update_enrichment(Enrichment.Hotspots, False, {})
Bias Mitigation
As a reminder about a few of bias mitigation's "only's" that affect enrichment enablement.
Bias Mitigation is only available for binary classification models
It can only be enabled if at least one model attribute is marked as monitor_for_bias=True
So by default, any binary classifier that you want to enable bias mitigation for will automatically train a mitigation model for all attributes marked as monitor_for_bias=True.
Enable Bias Mitigation
Default Bias Mitigation (all marked attributes)Bias Mitigation for a Specific Attribute# view current configuration
arthur_model.get_enrichment(Enrichment.BiasMitigation)
# enable
arthur_model.update_enrichment(Enrichment.BiasMitigation, True, {})
# or
arthur_model.enable_bias_mitigation()
Updated 3 months ago What’s NextSending InferencesAssets Required For ExplainabilityTroubleshooting ExplainabilityTable of Contents
Functions within Enrichment Enablement
Anomaly Detection
Enable Anomaly Detection
Explainability
Model Asset Requirements
Hot Spots
Enable Hot Spots
Bias Mitigation
Enable Bias Mitigation
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/preparing-for-onboarding
 Data Preparation for Arthur
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Data Preparation for ArthurWhat type of data do I need to prepare for the Arthur platform ?Suggest EditsAt a high level, teams often want to know what they need to set up to onboard Arthur before starting the onboarding process. As shown below, the assets teams need to prepare to use Arthur are directly related to how they want to use Arthur for their models.
We can see that the only assets required to monitor inferences actively are your model structure and inference data. However, teams that wish to use more aspects of the Arthur platform than seeing inputs and outputs need to onboard/send additional assets to Arthur.
Model Structure
A model structure, otherwise known as an Arthur model schema, defines a wireframe for what Arthur should expect as inputs and outputs to your model. By recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built for your model in Arthur.
Example model input and output
To define this wireframe, teams must provide either example model input and output data (as in a reference dataset) or define all model inputs (features) and outputs (predictions or ground truth) as well as their expected values. These values must be a type allowed by Arthur.
At a high level, these are the available Arthur Inputs:
Available Input Value TypesAllowed Data TypesAdditional Information RequiredNumericalInteger, FloatCategoricalInteger, String, Boolean, FloatSpecified Available CategoriesTimestampDateTimeMust be time zone aware. Cannot be Null or NaNText (NLP)StringGenerative Sequence (LLM)StringA tokens (and optional token likelihood) column is also requiredImage.jpg, .png, .gifUnique IdentifierStringTime SeriesList of Dictionaries with "timestamp" and "value" keysTimestamps must be timezone aware DateTimes and values must be floats.
These are the available Arthur Outputs:
Arthur Output TypeAllowed Data TypesNotesClassificationInteger, FloatCannot be Null or NaNRegressionInteger, FloatCannot be Null or NaNObject DetectionLiteral Array of Bounding Box ValuesCannot be Null or NaNGenerative Sequence Unstructured TextStringCannot be Null or NaNGenerative Sequence Token LikelihoodArray of Float ValuesThis is optional. Cannot be Null or NaNRanked ListList of Dictionaries with "item_id", "label", and "score" keys"item_id" and "label" values must be strings, "score" value must be a float. "label" and "score" keys are optional.
For more specific examples of troubleshooting onboarding for specific model input and output types, please refer to Registering Model Attributes Manually.
Map of Data Relationship
As well as information about how our model predictions relate to our ground truth attributes. This is used to help prepare the Arthur platform to calculate performance metrics correctly.
Teams can register this structure about their models manually ( Registering Model Attributes Manually). However, the most common way teams define their model structure in Arthur is by having it automatically inferred by onboarding their Reference dataset with the Python SDK.
Reference Dataset
A reference dataset is a representative sample of input features for your model. In other words, it is a sample of what is typical or expected for your model. Typically, teams onboard their model's training or validation data for reference.
In the Arthur platform, your reference dataset provides the baseline for data drift and anomaly metrics. Therefore, for those techniques to appear, a reference dataset must be set for your Arthur model object.
The only required stage to be included in the reference dataset is ModelPipelineInput. But we also recommend including data from the PredictedValue, GroundTruth, and NonInputData stages so that Arthur can also measure drift in those attributes over time.
Note: As mentioned in the data drift metric section, univariate data drift metrics can be calculated in the Python SDK by comparing inferences to one another without a reference dataset.
For more information on best practices/guidelines for selecting your reference dataset, please refer to the Creating Arthur Model Object page.
Model Assets
Model assets are only required for teams that wish to enable explainability for their model. Explainability, as described better in the Explainability section, can be used to understand the model's decisions better. These assets are further described in (add explainability section), but they include:
Requirements.txt File: a file containing all
the requirements needed to run your predict function
Model: the compressed runnable model used to generate predictions (typically .pkl format)
Entrypoint Python File: this file contains your Python predict function that will be used to generate predictions to produce explanations
Inputs: List
Outputs: List
Pythonimport …
model = load.model(‘pkl_model’)
## Python Predict Function
## takes in and returns a list
def predict(x):
return model.predict_proba(x)
Inference Data
For teams that want to use Arthur for the primary purpose of monitoring, it is essential to set up active monitoring of the data and predictions your model is running on in production. In Arthur, these rows of data are called inferences. Data associated with each inference might include (1) input data, (2) model predictions, and/or (3) corresponding ground truth.
Teams with the most success monitoring work to automate or create a consistent process for sending inferences to the platform.
Inference Ground Truth
While ground truth can be sent simultaneously as inference data, many ML models do not receive ground truth labels during prediction. In these instances, teams can set up a process to match ground truth in the platform with outside labels.
Note on sending large data to Arthur
While Arthur supports ingesting reference data, inferences, and ground truth labels, there are some constraints on ingestion that are worth being aware of. Clients are responsible for ensuring that data sent to Arthur respects these limits except for where called out below:
Arthur's ingress proxy has a 4Gb limit on the size of any request payload that's made to Arthur. Requests which exceed this 4Gb limit will error out indicating that the body was too large.
Arthur's ingestion-service (responsible for ingesting data) has a fixed amount of memory available. By default, this is 2Gb, but is configurable through the administrative interface or deployment scripts. Requests exceeding this limit may cause ingestion-service to run out of memory and crash.
SDK users that are ingesting Images using send_bulk_inferences, send_bulk_ground_truths or set_reference_data will have their files chunked so that they respect these limits.
Updated 2 months ago Table of Contents
Model Structure
Example model input and output
Map of Data Relationship
Reference Dataset
Model Assets
Inference Data
Inference Ground Truth
Note on sending large data to Arthur
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/pagerduty
 PagerDuty
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by PagerDutySuggest EditsWith the Arthur + PagerDuty integration, you can notify on-call teams in PagerDuty of alerts Arthur triggers. To set up this integration, follow these steps:
Step 1: Set up your email integration in PagerDuty
There are three ways to configure an email integration in PagerDuty. Please follow one of the three options in the PagerDuty Email Integration Guide to retrieve your integration email address.
Step 2: Configure your integration in Arthur
To configure the PagerDuty integration for a model in Arthur, you can send a POST request to the/alert_notification_configurations.
model_id - UUID of the model this alert notification configuration belongs to.
type - Type of notification to send. In this case, "PagerDuty".
destination - The integration email address obtained in Step 1.
enabled - Whether or not the notification configuration is enabled. It defaults to true.
Example Query Request:
JSON{
"model_id" : "<model_id> [string]",
"type" : "[PagerDuty]",
"destination" : "<[email protected]> [string]",
}
For more information on configuring alert notifications, please see the notification section of the Alerting page.
Step 3: Start monitoring!
Your integration is now ready to use! When an alert is triggered in Arthur for this model, an incident will be created in your PagerDuty.Updated 3 months ago Table of Contents
Step 1: Set up your email integration in PagerDuty
Step 2: Configure your integration in Arthur
Step 3: Start monitoring!
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/transformation-functions
 Transformation Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Transformation FunctionsSuggest EditsFor transformation functions, it will be helpful to include theproperty in the request to help associate the transformation function values, for example:
JSON{
"select": [
{
"property": "<attribute_name> [string]"
},
{
"function": "roundTimestamp",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string]",
"time_interval": "[secondminutehourdaymonthyear]"
}
}
]
}
We omit property for brevity in the following examples.
For an explanation of nested functions, see the guide Composing Functions.
Round Timestamp
Rounds a timestamp property to the provided time interval. This function requires one property
which must be an attribute of type datetime and one parameter, time_interval.
Query Request:
JSON{
"select": [
{
"function": "roundTimestamp",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"time_interval": "[secondminutehourdaymonthyear]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<rounded_timestamp> [string]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "roundTimestamp",
"parameters": {
"property": "inference_timestamp",
"time_interval": "day"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"roundTimestamp": "2020-08-10T00:00:00.000Z"
},
{
"roundTimestamp": "2020-08-09T00:00:00.000Z"
},
{
"roundTimestamp": "2020-08-08T00:00:00.000Z"
}
]
}
back to top
Label By Max Column
Given a list of columns, returns a string column with the name of the column containing the max value for the row. For example, this function can be used to extract the max probability from a vector of probability properties.
Query Request:
JSON{
"select": [
{
"function": "labelByMaxColumn",
"alias": "<alias_name> [optional string]",
"parameters": {
"properties": [
"<property name> [string]"
]
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<one of the properties from the given list> [string]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "labelByMaxColumn",
"alias": "classPrediction",
"parameters": {
"properties": [
"class_1",
"class_2",
"class_3"
]
}
}
]
}
Sample Response:
JSON{
"classPrediction": [
{
"classPrediction": "class_1"
},
{
"classPrediction": "class_1"
},
{
"classPrediction": "class_2"
}
]
}
back to top
If/Then/Else
Given a column and a condition, returns the "then" value if the condition is true on that column, otherwise returns the "else" value.
Query Request:
JSON{
"select": [
{
"function": "if",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<property name> [string or nested]",
"comparator": "<gtltgtelteeqne>",
"value": "<any value to compare to>",
"then": "<any value to return when true>",
"else": "<any value to return when false>"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<then or else value based on the conditional>"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "if",
"alias": "predicted_class",
"parameters": {
"property": "positive_probability",
"comparator": "gte",
"value": 0.6,
"then": "positive",
"else": "netivate"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"predicted_class": "positive"
},
{
"predicted_class": "negative"
},
{
"predicted_class": "positive"
}
]
}
back to top
Bin Continuous
This function bins a continuous value based on supplied thresholds. The bins will be formed as:
[< threshold_1, threshold_1 <= x < threshold_2, ... , threshold_(n-1) <= x < threshold_(n), threshold_(n) < x].
The response bins will be labeled with an integer id corresponding to the ordered bin, starting at 1.
If n thresholds are given, n+1 bins will be returned.
Query Request:
JSON{
"select": [
{
"function": "binContinuous",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<property name> [string or nested]",
"bin_thresholds": [
"<threshold_1> [number]",
"<threshold_2> [number]",
"<threshold_3> [number]"
]
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<bin_id> [int]"
}
]
}
Sample Request:
JSON{
"select": [
{
"property": "age"
},
{
"function": "binContinuous",
"alias": "ageBin",
"parameters": {
"property": "age",
"bin_thresholds": [
18,
65,
95
]
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"age": 10,
"ageBin": 1
},
{
"age": 20,
"ageBin": 2
},
{
"age": 70,
"ageBin": 3
}
]
}
back to top
Bins To Quantiles
Returns an array of values representing the quantiles based on the number of bins passed to the function. For example if you supply "num_bins": "10", then this query will return the value in your data at the 10%, 20%, ... , 90%, 100% quantiles.
Query Request:
JSON{
"select": [
{
"function": "binsToQuantiles",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]",
"num_bins": "<num_bins> [int]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": [
"float"
]
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "binsToQuantiles",
"alias": "quantiles",
"parameters": {
"property": "age",
"num_bins": 10
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": [
19,
28,
37,
46,
55,
64,
73,
82,
91
]
}
]
}
back to top
Date Diff
Returns the difference of two timestamps in units. Valid units are:
second, minute, hour, day, week, month, quarter, and year.
Query Request:
JSON{
"select": [
{
"function": "dateDiff",
"alias": "<alias_name> [optional string]",
"parameters": {
"unit": "[secondminutehourdayweekmonthquarteryear]",
"start_date": "<attribute_name> [string or nested]",
"end_date": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "difference [int]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "dateDiff",
"alias": "date_diff",
"parameters": {
"unit": "second",
"start_date": "inference_timestamp",
"end_date": "prev_timestamp"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"date_diff": 100
}
]
}
back to top
Neighbor
Returns the value of the column offset rows next to this row in the ordering.
default is the value that is returned when the offset goes out of bounds on the row set.
It is recommended to use this function in a subquery with an order_by clause to get consistent ordering.
Query Request:
JSON{
"select": [
{
"function": "neighbor",
"alias": "<alias_name> [optional string]",
"parameters": {
"offset": "<offset> [int]",
"property": "<attribute_name> [string or nested]",
"default": "<default_value> [any]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "neighbor_value [any]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "neighbor",
"alias": "prev_timestamp",
"parameters": {
"property": "inference_timestamp",
"offset": -1,
"default": null
}
}
],
"order_by": [
{
"property": "inference_timestamp",
"direction": "desc"
}
]
}
Sample Response:
JSON{
"query_result": [
{
"prev_timestamp": "2021-06-15T00:00:00.000Z"
}
]
}
back to top
Arithmetic
add, subtract, multiply, and divide are valid arithmetic functions.
Each takes two columns as input and returns the result of the arithmetic expression.
Query Request:
JSON{
"select": [
{
"function": "[addsubtractmultiplydivide]",
"alias": "<alias_name> [optional string]",
"parameters": {
"left": "<attribute_name> [string or nested]",
"right": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "expression_result [number]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "multiply",
"alias": "double_home_value",
"parameters": {
"left": "Home_Value",
"right": 2
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"double_home_value": 20000
}
]
}
Sample nested request to compute (Home_Value + Car_Value) * 2
JSON{
"select": [
{
"function": "multiply",
"alias": "double_loans",
"parameters": {
"left": {
"nested_function": {
"function": "add",
"alias": "total_loan",
"parameters": {
"left": "Home_Value",
"right": "Car_Value"
}
}
},
"right": 2
}
}
]
}
Sample Nested Response:
JSON{
"query_result": [
{
"double_loans": 20000
}
]
}
back to top
Absolute Value
Take the absolute value of a property.
Query Request:
JSON{
"select": [
{
"function": "abs",
"alias": "<alias_name> [optional string]",
"parameters": {
"property": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<abs_value> [float]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "abs",
"alias": "abs_delta",
"parameters": {
"property": "delta"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"abs_delta": 55.45
}
]
}
back to top
Logical Functions
equals, and, and or are valid logical functions. Each takes two columns as input and returns the result of the logical expression. These follow the same API as Arithmetic Functions
Query Request:
JSON{
"select": [
{
"function": "[equalsandor]",
"alias": "<alias_name> [optional string]",
"parameters": {
"left": "<attribute_name> [string or nested]",
"right": "<attribute_name> [string or nested]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "expression_result [0 or 1]"
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "equals",
"alias": "has_phd",
"parameters": {
"left": "education",
"right": 4
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"has_phd": 1
}
]
}
Sample nested request to compute has_phd or has_masters
JSON{
"select": [
{
"function": "or",
"alias": "has_higher_education",
"parameters": {
"left": {
"nested_function": {
"function": "equals",
"alias": "has_phd",
"parameters": {
"left": "education",
"right": 4
}
}
},
"right": {
"nested_function": {
"function": "equals",
"alias": "has_masters",
"parameters": {
"left": "education",
"right": 3
}
}
}
}
}
]
}
Sample Nested Response:
JSON{
"query_result": [
{
"has_higher_education": 1
}
]
}
back to topUpdated 3 months ago Table of Contents
Round Timestamp
Label By Max Column
If/Then/Else
Bin Continuous
Bins To Quantiles
Date Diff
Neighbor
Arithmetic
Absolute Value
Logical Functions
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/time-series
 Time Series
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Time SeriesSuggest EditsTime Series input models are a type of machine learning model that operates on time series data, or data which measures a certain value over time, such as credit card balance over time. These models can perform tasks such as predictions or recommendations based on past patterns.
Formatted Data in Arthur
Time Series input models require the following data formatting:
JSON[
{
"timestamp": "2023-10-05T00:00:00Z",
"value": 1
},
{
"timestamp": "2023-10-06T00:00:00Z",
"value": 4
}
]
Arthur requires that all times will be present in a given series according to a regular interval (eg. one value each day).
There is an upper bound of 500 timestamps in a single time series inference.
Arthur supports sending time series data in JSON files or DataFrames.Updated about 2 months ago Table of Contents
Formatted Data in Arthur
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/config-template
 Config Template
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Config TemplateSuggest EditsThe Configuration template for Arthur version 3.4.0 is below:
YAMLapiVersion: kots.io/v1beta1
kind: ConfigValues
metadata:
creationTimestamp: null
name: arthur
spec:
values:
IAM_permission_type:
default: access_keys
advanced_cache_options:
default: "0"
advanced_messaging_connect_cpu_limits:
default: "2"
advanced_messaging_connect_cpu_limits_not_validate: {}
advanced_messaging_connect_cpu_requests:
default: "1"
advanced_messaging_connect_cpu_requests_not_validate: {}
advanced_messaging_connect_heap_options:
default: -Xms1g -Xmx3g
advanced_messaging_connect_memory_limits:
default: 4Gi
advanced_messaging_connect_memory_limits_not_validate: {}
advanced_messaging_connect_memory_requests:
default: 2Gi
advanced_messaging_connect_memory_requests_not_validate: {}
advanced_olap_options:
default: "0"
advanced_other:
default: "0"
value: "1"
alert_service_update_rule_metrics:
default: "0"
api_token_ttl:
default: "24"
arthur_user_id:
default: "1000"
audit_log_event_bridge_bus_name: {}
audit_log_event_bridge_bus_region: {}
audit_log_event_bridge_detail_type:
default: events.arthur.ai
audit_log_event_bridge_source:
default: arthur-audit-log
audit_log_sink_destination:
default: none
batch_workflow_parallelism:
default: "120"
beta_ui:
default: "0"
beta_ui_alternate_site:
default: "0"
beta_ui_hostname: {}
bootstrap_job_backoff_limit:
default: "100"
bootstrap_job_ttl:
default: "86400"
cache_cpu_limits: {}
cache_cpu_limits_not_validate: {}
cache_cpu_requests: {}
cache_cpu_requests_not_validate: {}
cache_memory_limits: {}
cache_memory_limits_not_validate: {}
cache_memory_requests: {}
cache_memory_requests_not_validate: {}
cache_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
cache_replicas:
default: "0"
cicd_credentials:
default: "0"
cluster_nodes:
# Only Relevant for "fixed" cluster sizes.
Enter the number of nodes in the cluster. This number cannot be decreased from the current value unless it's greater than `6`.
default: "1"
value: "3"
config_job_and_workflow_retention:
default: "0"
database_admin_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
database_hostname:
# Leave the default configuration to use the embedded database. If you would like to use an external Postgres instance, provide the hostname here and follow this guide: https://docs.arthur.ai/platform-management/installation/externalize_postgres.html.
default: database-primary
database_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
database_port:
value: "5432"
database_ssl_mode:
# This option allows you to enable SSL communication between services and the postgres database.
See https://www.postgresql.org/docs/10/libpq-ssl.html for full descriptions of each option.
By default, the postgres database has ssl disabled.
default: disable
database_username:
default: arthurai
default_messaging_partition_count:
default: "3"
value: "1"
disable_ssl_redirect_on_ingress:
default: "0"
email_selection:
default: none
enable_audit_log:
default: "0"
enable_olap_backup:
default: '"0"'
enable_olap_backup_user:
default: "0"
enable_password_rotation_cache:
default: "0"
enable_password_rotation_olap:
default: "0"
existing_database_primary_pvc: {}
existing_or_vm:
default: existing_cluster
fixed_or_autoscale:
# The `fixed` mode is recommended for clusters with a fixed number of nodes. The `autoscale` mode is used for clusters that can autoscale and automatically expand their node count.
value: fixed
full_name_override:
default: arthurai
global_identity_provider:
default: none
global_model_limit_Count:
default: "500"
global_model_limits:
default: "0"
global_workflow_parallelism:
default: "150"
http_proxy: {} # Relevant if you are using Explainability and your organization is behind a proxy server.
If PIP and/or Conda need to route through the proxy server to pull down public packages this will set the environment variable HTTP_PROXY to the supplied value. Ex. http://sysproxy.my-company.com:port
http_proxy_user: {}
https_proxy: {}
https_proxy_user: {}
ingestion_service_cpu_limits: {}
ingestion_service_cpu_limits_not_validate: {}
ingestion_service_cpu_requests: {}
ingestion_service_cpu_requests_not_validate: {}
ingestion_service_memory_limits: {}
ingestion_service_memory_limits_not_validate: {}
ingestion_service_memory_requests: {}
ingestion_service_memory_requests_not_validate: {}
ingress_ambassador_enabled:
default: "false"
ingress_class:
default: nginx
ingress_hostname:
value: arthur.mydomain.ai
ingress_namespace_label_key:
value: name
ingress_namespace_label_value:
value: ingress-system
ingress_nginx_additional_hostname:
value: ""
irsa_annotations: {}
irsa_annotations_user:
default: 
eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-role
k8_storageclass:
# Provide Kubernetes StorageClass profile. Use 'gp2' for Amazon EKS, 'default' if you're using embedded Kubernetes provided by the installer
value: default
kafka_ecosystem_common_replication_calc:
default: "1"
max_arthur_replicas:
default: "1"
max_messaging_partition_count:
default: "3"
max_model_server_replicas:
default: "2"
messaging_cpu_limit:
default: "1"
messaging_heap:
default: -Xmx2G -Xms1G
messaging_memory_limit_and_request:
default: 2560Mi
messaging_rack_aware_enabled:
default: "0"
messaging_rack_label:
default: topology.kubernetes.io/zone
messaging_replicas:
default: "3"
messaging_sa_create:
default: "0"
messaging_sa_fullnameoverride: {}
messaging_zookeeper_timeout:
default: "20000"
meta_replicas:
default: "0"
metric_service_update_default_metrics:
default: "0"
min_arthur_replicas:
default: "1"
model_servers_always_on:
# For use with what-if and on-demand explainability. See https://docs.arthur.ai/user-guide/explainability.html
If set to "true", then on-demand and what-if explanations are available, but uses additional cluster resources, 1 CPU and 1 GB memory per model with explainability enabled. If set to "false", on-demand and what-if explanations are unavailable, but less cluster usage when there is no data being sent. Regardless of the setting here, streaming explainability will be available if enabled. This only effects what-if and on-demand explanations.
default: "true"
network_policy_enabled:
default: "0"
no_proxy: {} # Relevant if you are using Explainability and your organization is behind a proxy server.
If PIP and/or Conda need to route through the proxy server to pull down public packages this will set the environment variable NO_PROXY to the supplied value. Ex. localhost,127.0.0.1,.my-company.com
no_proxy_user: {}
number_of_olap_backups_to_keep:
default: "7"
oidc_identity_provider_config_yaml: {}
oidc_identity_provider_config_yaml_user: {}
olap_backup_s3_bucket:
default: arthurai
olap_backup_s3_bucket_region:
default: us-east-1
olap_backup_s3_endpoint:
default: s3.us-east-1.amazonaws.com
olap_backup_s3_path:
default: olap_backups
olap_backup_service_account:
default: arthurai-arthurai
olap_cpu_limits: {}
olap_cpu_limits_not_validate: {}
olap_cpu_requests:
default: 1000m
olap_cpu_requests_not_validate: {}
olap_database_operator_password:
# The OLAP database is installed along with a Kubernetes Operator to manage it.
This operator needs credentials to access the database.
We recommend overwriting the default password below.
default: 5ugYLDJ2uLhRdEgz5t
value: ch/0gntnboTNbQpxmzx4GuPCRnjqSNwTpOT6FwgQ9q4iY7CHiQLeFQ3snnZgxYnFt4gSyInce3KhYiMR7eebBtGbe5sIuY/aBPAySrSjExfO+1VYPBp176bP+zQ=
olap_database_user_password:
# Password used internally in our application to query the olap database, currently only supports alpha-numeric characters.
default: eQ3iBo8UGh5zqJKQWuEEySrR
value: ch/0gntnboTNbQppnGJgGvCjSmPlS/l8orO+UggQ/rstcryCj2r/GRXR8UNr+u3plPIj+uLMdXGGFiRtko6pTsClBoQkoeLXqDVr1jeqsThCZI/bTfovlA==
olap_memory_limits: {}
olap_memory_limits_not_validate: {}
olap_memory_requests:
default: 1Gi
olap_memory_requests_not_validate: {}
olap_node_label_key: {}
olap_node_label_value: {}
olap_replicas:
default: "1"
olap_zookeeper_cpu_limits: {}
olap_zookeeper_cpu_limits_not_validate: {}
olap_zookeeper_cpu_requests:
default: 500m
olap_zookeeper_cpu_requests_not_validate: {}
olap_zookeeper_heap_options:
default: -Xms4G -Xmx4G
olap_zookeeper_memory_limits: {}
olap_zookeeper_memory_limits_not_validate: {}
olap_zookeeper_memory_requests:
default: 1Gi
olap_zookeeper_memory_requests_not_validate: {}
password_rotation_cron_schedule:
default: 0 0 1 */6 *
pending_batch_workflows_limit:
default: "100"
prometheus_host:
# Leave the default configuration if you're using the embedded K8s. Provide your Prometheus hostname if you're running your own K8s.
default: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local
prometheus_labels:
# If your prometheus installation requires labels to identify ServiceMonitors and PrometheusRules, add them here. They should be in yaml format just as you would specify inside the "metadata.labels" block. Do not indent.
default: 
prometheus: monitor
app: prometheus
prometheus_namespace:
default: monitoring
prometheus_port:
# Leave the default configuration if you're using the embedded K8s. Provide your Prometheus hostname if you're running your own K8s.
default: "9090"
pypi_registry_conda: {} # This is set as a channel in the '.condarc' file. Do not include 'https://' prefix (e.g. repository.arthur.ai/repository/conda-proxy/main).
pypi_registry_conda_user: {}
pypi_registry_index: {} # This maps to the 'index key' in the 'pip.conf' file. Do not include 'https://' prefix (e.g repository.arthur.ai/repository/pypi-virtual/pypi).
pypi_registry_index_url: {} # This maps to the 'index-url' key in the 'pip.conf' file. Do not include 'https://' prefix (e.g. repository.arthur.ai/repository/pypi-virtual/simple).
pypi_registry_index_url_user: {}
pypi_registry_index_user: {}
pypi_registry_password:
default: bO4Mxhdaevso/029YtUgz98Wk7qPcxEpa1P/uVqG4cy4UY1B3+YN5Q==
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
pypi_registry_password_user:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
pypi_registry_username: {}
pypi_registry_username_user: {}
raw_anaconda_config: {}
raw_anaconda_config_user: {}
raw_pypi_config: {}
raw_pypi_config_user: {}
rbac_privileges:
# Change to "cluster_scope" to install CRDs too
default: namespace_scope
run_as_root:
default: "0"
value: "0"
s3_access_key_id:
default: access_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_access_key_id_user:
default: access_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_bucket:
default: arthurai
s3_bucket_user:
default: arthurai
s3_region:
default: us-east-1
s3_region_user:
default: us-east-1
s3_secret_access_key:
default: secret_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_secret_access_key_user:
default: secret_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_url:
default: http://minio:9000
s3_url_user:
default: http://minio:9000
saml_identity_provider_config_yaml: {}
saml_identity_provider_config_yaml_user: {}
secondary_token_validation_key:
value: Aj3ziCI/YcnTT3QR3WAtMNDNEzzqTa8W9iJCoHjNFMteiO6lrcnUKw==
ses_region: {}
ses_role: {}
show_advanced_arthur_microservice_options:
default: "0"
show_advanced_messaging:
default: "0"
value: "1"
show_hidden_variables:
default: "0"
value: "0"
show_token_signing_and_validation_options:
default: "0"
signing_cert: {}
signing_cert_user: {}
signing_private_key: {}
signing_private_key_user: {}
single_or_ha:
# The `single` configuration is a minimal deployment suitable for non-production environments. For production deployment, select `ha`.
value: single
smtp_from: {} # Provide the email address to send alerts from (e.g. [email protected])
smtp_host: {} # Provide the address of the SMTP server (e.g. smtp.arthur.ai)
smtp_password:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
smtp_port: {}
smtp_user: {}
superadmin_email:
default: [email protected]
superadmin_firstname:
default: Super
superadmin_lastname:
default: Admin
superadmin_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
superadmin_username:
value: superadmin
token_signing_primary_key:
value: YSDFzjg5I83KMBJ+wHQmU/ejDQ7tTthIpaDcCRM+iqcDTofiul7DZzTblFkb0e2U0+UJ74TuIx28oGnxPM+pkmKlc1yx2uvj
use_external_blob_storage:
# Select "Yes" if and only if you are supplying your own S3 compatible storage, otherwise select "No" to use the embedded blob storage.
default: "no"
use_external_postgres:
default: "no"
use_raw_python_repository_configs:
# The PyPi registry section is only relevant when using the explainability enrichment (https://docs.arthur.ai/user-guide/enrichments.html#explainability).
# Provide your private PyPi registry if you have an airgapped enrivonment or your model requirements file includes packages only hosted in a private repository.
# Leaving this section blank will cause the public PyPi to be used. If the public PyPi is inaccessible from the cluster, the explainability feature will not work.
default: "no"
use_smtp:
default: "0"
workflow_ttl_seconds:
default: "3600"
workflow_ttl_seconds_after_success:
default: "60"
status: {}
Do note that these parameters are sorted alphabetically. Unfortunately, this is how the 'packager' software we use for our installer outputs the list of parameters. In reality, these parameters should be grouped based on their purpose.
Most of these parameters can be commented, unless you are advised by Arthur Support to configure them. It's also important to point out that all 'default' values will be ignored by the installer.
For reference, this is the same configuration template, grouped by topic, and with only the most necessary parameters uncommented:
YAMLapiVersion: kots.io/v1beta1
kind: ConfigValues
metadata:
creationTimestamp: null
name: arthur
spec:
values:
###############################################################################
# Install privileges.
# Change to "namespace_scope" for restricted permissions.
###############################################################################
# Values: namespace_scope, cluster_scope
rbac_privileges:
value: cluster_scope
###############################################################################
# Ingress
###############################################################################
ingress_hostname:
value: arthur.mlops.company.com
ingress_nginx_additional_hostname:
value: "a1788baaec5c4473aa4ec3bf4ef81bb5.XXXXXXXXXX.us-east-1.elb.amazonaws.com"
ingress_class:
# Values: "nginx", "ambassador"
value: nginx
ingress_ambassador_enabled:
value: "false"
ingress_namespace_label_key:
value: name
ingress_namespace_label_value:
value: ingress-system
disable_ssl_redirect_on_ingress:
value: "0"
# Ingress for new UI
beta_ui:
value: "1"
beta_ui_alternate_site:
value: "1"
beta_ui_hostname:
value: "arthur.mlops.company.com"
###############################################################################
# Installation Type
###############################################################################
# The `single` configuration is a minimal deployment suitable for non-production environments.
# For production deployment, select `ha`.
single_or_ha:
value: ha
# The `fixed` mode is recommended for clusters with a fixed number of nodes. The `autoscale` mode is used for
# clusters that can autoscale and automatically expand their node count.
# 'autoscale' will assume a minimum of 6 nodes, do not set for 'autoscale' for clusters with < 6 nodes
fixed_or_autoscale:
value: fixed
# Only Relevant for "fixed" cluster sizes.
Enter the number of nodes in the cluster. This number
# cannot be decreased from the current value unless it's greater than `6`.
cluster_nodes:
value: "3"
# Provide Kubernetes StorageClass profile. Use 'gp2' or 'gp3' for Amazon EKS, 'default' if you're using
# embedded Kubernetes provided by the installer
k8_storageclass:
value: gp3
# Network Policy
network_policy_enabled:
value: "0"
###############################################################################
# Security and Authentication
###############################################################################
run_as_root:
value: "0"
arthur_user_id:
value: "1000"
# Single Sigh On
# Values: "none", "oidc", "saml"
global_identity_provider:
value: none
oidc_identity_provider_config_yaml: {}
oidc_identity_provider_config_yaml_user: {}
saml_identity_provider_config_yaml: {}
saml_identity_provider_config_yaml_user: {}
# IAM Integration - Values: "access_keys", "IRSA", "IAM Node Roles"
IAM_permission_type:
value: "IRSA"
irsa_annotations:
value: 
eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/arthur-eks-role
irsa_annotations_user:
value: 
eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/arthur-eks-role
# SSL / Token Signature
show_token_signing_and_validation_options:
value: "0"
signing_cert: {}
signing_cert_user: {}
signing_private_key: {}
signing_private_key_user: {}
# token_signing_primary_key:
#
value: YSDFzjg5I83KMBJ+wHQmU/ejDQ7tTthIpaDcCRM+iqcDTofiul7DZzTblFkb0e2U0+UJ74TuIx28oGnxPM+pkmKlc1yx2uvj
# secondary_token_validation_key:
#
value: Aj3ziCI/YcnTT3QR3WAtMNDNEzzqTa8W9iJCoHjNFMteiO6lrcnUKw==
# api_token_ttl:
#
value: "24"
###############################################################################
# S3 Integration
###############################################################################
# Select "Yes" if and only if you are supplying your own S3 compatible storage,
# otherwise select "No" to use the embedded blob storage.
use_external_blob_storage:
value: "yes"
s3_access_key_id: {}
s3_access_key_id_user: {}
s3_secret_access_key: {}
s3_secret_access_key_user: {}
s3_url: {}
s3_url_user: {}
s3_bucket:
value: arthur-s3-eks
s3_bucket_user:
value: arthur-s3-eks
s3_region:
value: us-east-1
s3_region_user:
value: us-east-1
###############################################################################
# Superadmin Configuration
###############################################################################
superadmin_email:
value: [email protected]
superadmin_firstname:
value: Super
superadmin_lastname:
value: Admin
superadmin_password:
value: Password1234
superadmin_username:
value: superadmin
###############################################################################
# SMTP Configuration
###############################################################################
# Values: "ses", "smtp", "none"
email_selection:
value: none
use_smtp:
value: "0"
# Provide the email address to send alerts from (e.g. [email protected])
smtp_from: {}
# Provide the address of the SMTP server (e.g. smtp.arthur.ai)
smtp_host: {}
smtp_password:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
smtp_port: {}
smtp_user: {}
# SES Configuration
ses_region: {}
ses_role: {}
###############################################################################
# Prometheus Integration
###############################################################################
# Leave the default configuration if you're using the embedded K8s.
# Provide your Prometheus hostname if you're running your own K8s.
prometheus_host:
value: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local
# If your prometheus installation requires labels to identify ServiceMonitors and PrometheusRules,
# add them here. They should be in yaml format just as you would specify inside the "metadata.labels" block.
# Do not indent.
prometheus_labels:
value: 
prometheus: monitor
app: prometheus
prometheus_namespace:
value: monitoring
# Leave the default configuration if you're using the embedded K8s.
# Provide your Prometheus hostname if you're running your own K8s.
prometheus_port:
value: "9090"
###############################################################################
# Private Python Registry
###############################################################################
use_raw_python_repository_configs:
# The PyPi registry section is only relevant when using the explainability enrichment
# (https://docs.arthur.ai/user-guide/enrichments.html#explainability).
# Provide your private PyPi registry if you have an airgapped enrivonment or your model requirements file
# includes packages only hosted in a private repository.
# Leaving this section blank will cause the public PyPi to be used. If the public PyPi is inaccessible from the
# cluster, the explainability feature will not work.
value: "no"
# This is set as a channel in the '.condarc' file. Do not include 'https://' prefix
# (e.g. repository.arthur.ai/repository/conda-proxy/main).
pypi_registry_conda: {}
pypi_registry_conda_user: {}
# This maps to the 'index key' in the 'pip.conf' file. Do not include 'https://' prefix
# (e.g repository.arthur.ai/repository/pypi-virtual/pypi).
pypi_registry_index: {}
# This maps to the 'index-url' key in the 'pip.conf' file. Do not include 'https://' prefix
# (e.g. repository.arthur.ai/repository/pypi-virtual/simple).
pypi_registry_index_url: {}
pypi_registry_index_url_user: {}
pypi_registry_index_user: {}
pypi_registry_password:
value: password1234
pypi_registry_password_user:
value: password1234
pypi_registry_username: {}
pypi_registry_username_user: {}
raw_anaconda_config: {}
raw_anaconda_config_user: {}
raw_pypi_config: {}
raw_pypi_config_user: {}
# Relevant if you are using Explainability and your organization is behind a proxy server.
# If PIP and/or Conda need to route through the proxy server to pull down public packages this will set
# the environment variable NO_PROXY to the supplied value. Ex. localhost,127.0.0.1,.my-company.com
no_proxy: {}
no_proxy_user: {}
# Relevant if you are using Explainability and your organization is behind a proxy server.
# If PIP and/or Conda need to route through the proxy server to pull down public packages this will
# set the environment variable HTTP_PROXY to the supplied value. Ex. http://sysproxy.my-company.com:port
http_proxy: {}
http_proxy_user: {}
https_proxy: {}
https_proxy_user: {}
###############################################################################
# Postgres Integration
###############################################################################
use_external_postgres:
value: "yes"
database_admin_password:
value: password_for_RDS_admin_user
database_hostname:
# Leave the default configuration to use the embedded database. If you would like to use an external
# Postgres instance, provide the hostname here and follow this guide:
# https://docs.arthur.ai/platform-management/installation/externalize_postgres.html.
value: arthur-db.cluster-xptowtzabcd.us-east-1.rds.amazonaws.com
database_username:
value: arthurai
database_password:
value: password_for_RDS_arthurai_user
database_port:
value: "5432"
database_ssl_mode:
# This option allows you to enable SSL communication between services and the postgres database.
# See https://www.postgresql.org/docs/10/libpq-ssl.html for full descriptions of each option.
# By default, the postgres database has ssl disabled.
value: disable
existing_database_primary_pvc: {}
#
meta_replicas:
#
value: "0"
###############################################################################
# OLAP DB Settings
###############################################################################
# OLAP Password
olap_database_operator_password:
# The OLAP database is installed along with a Kubernetes Operator to manage it.
# This operator needs credentials to access the database.
We recommend overwriting the default password below.
value: gntnboTNbQpxmzx4GuPCRnjqSNwTpOT6FwgQ9q4iY7CHiQLeFQ3snnZgxYnFt4gSyInce3KhYiMR7eebBtGbe5sIuY
olap_database_user_password:
# Password used internally in our application to query the olap database,
# currently only supports alpha-numeric characters.
value: gntnboTNbQpxmzx4GuPCRnjqSNwTpOT6FwgQ9q4iY7CHiQLeFQ3snnZgxYnFt4gSyInce3KhYiMR7eebBtGbe5sIuY
# enable_password_rotation_olap:
#
value: "0"
# password_rotation_cron_schedule:
#
value: 0 0 1 */6 *
# # OLAP Backup
# enable_olap_backup:
#
value: '"0"'
# enable_olap_backup_user:
#
value: "0"
# number_of_olap_backups_to_keep:
#
value: "7"
# olap_backup_s3_bucket:
#
value: denisd-s3-eks
# olap_backup_s3_bucket_region:
#
value: us-east-1
# olap_backup_s3_endpoint:
#
value: s3.us-east-1.amazonaws.com
# olap_backup_s3_path:
#
value: olap_backups
# olap_backup_service_account:
#
value: arthurai-arthurai
# # OLAP Optimization
# advanced_olap_options:
#
value: "0"
# olap_cpu_limits: {}
# olap_cpu_limits_not_validate: {}
# olap_cpu_requests:
#
value: 1000m
# olap_cpu_requests_not_validate: {}
# olap_memory_limits: {}
# olap_memory_limits_not_validate: {}
# olap_memory_requests:
#
value: 1Gi
# olap_memory_requests_not_validate: {}
# olap_node_label_key: {}
# olap_node_label_value: {}
# olap_replicas:
#
value: "1"
# olap_zookeeper_cpu_limits: {}
# olap_zookeeper_cpu_limits_not_validate: {}
# olap_zookeeper_cpu_requests:
#
value: 500m
# olap_zookeeper_cpu_requests_not_validate: {}
# olap_zookeeper_heap_options:
#
value: -Xms4G -Xmx4G
# olap_zookeeper_memory_limits: {}
# olap_zookeeper_memory_limits_not_validate: {}
# olap_zookeeper_memory_requests:
#
value: 1Gi
# olap_zookeeper_memory_requests_not_validate: {}
###############################################################################
# Cache Settings
###############################################################################
cache_password:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
# cache_replicas:
#
value: "0"
# advanced_cache_options:
#
value: "0"
# enable_password_rotation_cache:
#
value: "0"
# # Cache Optimization
# cache_cpu_limits: {}
# cache_cpu_limits_not_validate: {}
# cache_cpu_requests: {}
# cache_cpu_requests_not_validate: {}
# cache_memory_limits: {}
# cache_memory_limits_not_validate: {}
# cache_memory_requests: {}
# cache_memory_requests_not_validate: {}
###############################################################################
# Kafka Optimization Settings
###############################################################################
# default_messaging_partition_count:
#
value: "1"
# max_messaging_partition_count:
#
value: "3"
# messaging_replicas:
#
value: "3"
# kafka_ecosystem_common_replication_calc:
#
value: "1"
# messaging_cpu_limit:
#
value: "1"
# advanced_messaging_connect_cpu_limits:
#
value: "2"
# advanced_messaging_connect_cpu_limits_not_validate: {}
# advanced_messaging_connect_cpu_requests:
#
value: "1"
# advanced_messaging_connect_cpu_requests_not_validate: {}
# messaging_heap:
#
value: -Xmx2G -Xms1G
# messaging_memory_limit_and_request:
#
value: 2560Mi
# advanced_messaging_connect_heap_options:
#
value: -Xms1g -Xmx3g
# advanced_messaging_connect_memory_limits:
#
value: 4Gi
# advanced_messaging_connect_memory_limits_not_validate: {}
# advanced_messaging_connect_memory_requests:
#
value: 2Gi
# advanced_messaging_connect_memory_requests_not_validate: {}
# messaging_rack_aware_enabled:
#
value: "0"
# messaging_rack_label:
#
value: topology.kubernetes.io/zone
# messaging_sa_create:
#
value: "0"
# messaging_sa_fullnameoverride: {}
# messaging_zookeeper_timeout:
#
value: "20000"
###############################################################################
# Audit Log Settings
###############################################################################
# enable_audit_log:
#
value: "0"
# audit_log_event_bridge_bus_name: {}
# audit_log_event_bridge_bus_region: {}
# audit_log_event_bridge_detail_type:
#
value: events.arthur.ai
# audit_log_event_bridge_source:
#
value: arthur-audit-log
# audit_log_sink_destination:
#
value: none
###############################################################################
# Admin Console Settings
###############################################################################
# advanced_other:
#
value: "1"
# show_advanced_arthur_microservice_options:
#
value: "0"
# show_advanced_messaging:
#
value: "1"
# show_hidden_variables:
#
value: "0"
# config_job_and_workflow_retention:
#
value: "0"
###############################################################################
# Backend Performance Optimization Settings
###############################################################################
# Limits
# global_model_limit_Count:
#
value: "500"
# global_model_limits:
#
value: "0"
# global_workflow_parallelism:
#
value: "150"
# # Job Limits
# batch_workflow_parallelism:
#
value: "120"
# bootstrap_job_backoff_limit:
#
value: "100"
# bootstrap_job_ttl:
#
value: "86400"
# pending_batch_workflows_limit:
#
value: "100"
# workflow_ttl_seconds:
#
value: "3600"
# workflow_ttl_seconds_after_success:
#
value: "60"
# # Ingestion Service Optimizations
# ingestion_service_cpu_limits: {}
# ingestion_service_cpu_limits_not_validate: {}
# ingestion_service_cpu_requests: {}
# ingestion_service_cpu_requests_not_validate: {}
# ingestion_service_memory_limits: {}
# ingestion_service_memory_limits_not_validate: {}
# ingestion_service_memory_requests: {}
# ingestion_service_memory_requests_not_validate: {}
# # Explainability Optimizations
# model_servers_always_on:
#
# For use with what-if and on-demand explainability. See https://docs.arthur.ai/user-guide/explainability.html
#
# If set to "true", then on-demand and what-if explanations are available, but uses additional cluster
#
# resources, 1 CPU and 1 GB memory per model with explainability enabled. If set to "false", on-demand
#
# and what-if explanations are unavailable, but less cluster usage when there is no data being sent.
#
# Regardless of the setting here, streaming explainability will be available if enabled.
#
# This only effects what-if and on-demand explanations.
#
value: "true"
# max_model_server_replicas:
#
value: "2"
# metric_service_update_default_metrics:
#
value: "0"
# alert_service_update_rule_metrics:
#
value: "0"
###############################################################################
# Internal Arthur Use
###############################################################################
# cicd_credentials:
#
value: "0"
# existing_or_vm:
#
value: existing_cluster
# full_name_override:
#
value: arthurai
# min_arthur_replicas:
#
value: "1"
# max_arthur_replicas:
#
value: "1"
status: {}
PS: This template assumes integration with S3 and RDS database, using IRSA (IAM Roles for Service Accounts) configured.
Most of these settings can be modified in the Admin Console UI. The Config screen will include fields for all the settings in the template:
Some of the settings in this template can cause an installation to fail, if they are not correctly set. Some of the critical parameters are:
YAML
k8_storageclass:
value: gp3
This parameter defines the storage class that will be used to create the Persistent Volumes for the cluster. Having the wrong storage class defined here will cause the installer to fail to provision storage, which will compromise the installation. Make sure to have the correct value set before running the installer.
YAML
ingress_hostname:
value: arthur.mlops.company.com
ingress_nginx_additional_hostname:
value: "a1788baaec5c4473aa4ec3bf4ef81bb5.XXXXXXXXXX.us-east-1.elb.amazonaws.com"
These parameters will configure which ingress addresses Arthur will accept. By definition, Arthur will reject requests sent to addresses that are not in this list, even if they are correct (for instance, accessing Arthur through https://localhost does not work). If these hostname parameters do not match existing load balancer addresses, Arthur will be inaccessible, even once the instance is successfully installed. This can be reconfigured after installation, though it is recommended to ensure the proper value at installation time.Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/restoring-the-arthur-platform
 Restoring the Arthur Platform
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Restoring the Arthur PlatformSuggest EditsThis document details restoring various Arthur platform components from existing backups.
Restore RDS Postgres
Use the AWS RDS documentation to restore the database from an existing RDS Snapshot.
Please ensure that you correctly match the following configuration of the DB from which the snapshot was taken:
The connection port
The VPC and Security Group IDs
DB Subnet Group
DB Instance Type
Any other configuration which might be overridden
This operation might take a while, and the DB must show as Available before proceeding to install the platform.
Install the Arthur Platform
🚧Ensure Database is ReadyOnly proceed to installing the Arthur platform AFTER the restored database shows as "Available" in the RDS Console.
Install the Arthur platform either using the Airgap Kubernetes Cluster (K8s) Install` or Online Kubernetes Cluster (K8s) Install. Although most configurations for the Arthur platform should remain the same, the following two configurations might need to be updated:
The "Meta Database" section of the Admin Console should point to the newly restored DB instance.
🚧Ensure restore cluster is pointing to the right locationIt is very critical to update the configuration to point to the newly restored DB Instance. Failure to complete this step WILL CAUSE DATA CORRUPTION.
Update the ingress URL in the "Network" section of the Admin Console.
Wait for the platform to come back online before proceeding to the next steps. All Deployments and StatefulSets should be completely stood up (eg: all Pods should be ready and "Running") and all Jobs should be "Completed".
$ arthur_namespace="Put your Arthur namespace here"
$ kubectl get pods -n $arthur_namespace
NAME
READY
STATUS
RESTARTS
AGE
argo-workflows-server-75fc4d9d55-wfsqc
1/1
Running
0
12h
argo-workflows-workflow-controller-7b95b66b94-66hrs
1/1
Running
0
119m
arthurai-alert-service-858784dd7f-4kgq2
1/1
Running
0
4h58m
arthurai-api-service-7fc58f4958-trcvg
1/1
Running
0
4h58m
arthurai-custom-hpa-646bb978dd-t9b68
1/1
Running
0
12h
arthurai-dataset-service-86c8dd54cc-bwwtr
1/1
Running
0
4h58m
arthurai-frontend-78cc85fbc5-ffx79
1/1
Running
0
12h
arthurai-frontend-beta-5cb8756f68-8hljq
1/1
Running
0
12h
arthurai-frontend-classic-5ff79bd579-rhqv8
1/1
Running
0
12h
arthurai-ingestion-service-5f7896bf5c-jxwvk
1/1
Running
0
12h
arthurai-ingestion-service-5f7896bf5c-vzdn2
1/1
Running
0
4h58m
arthurai-kafka-connect-monitor-54cfcc8f7d-dcgr6
1/1
Running
2
12h
arthurai-metric-service-78f85cb548-s65dj
1/1
Running
0
12h
arthurai-query-service-64d7c9f846-h2ms9
1/1
Running
0
2d12h
arthurai-schema-service-69b8c484bd-thhkr
1/1
Running
0
4h58m
cache-master-0
1/1
Running
0
12h
cache-slave-0
1/1
Running
0
17h
cache-slave-1
1/1
Running
0
119m
chi-olap-installation-arthur-0-0-0
2/2
Running
0
119m
chi-olap-installation-arthur-0-1-0
2/2
Running
0
12h
chi-olap-installation-arthur-0-2-0
2/2
Running
0
17h
database-primary-0
1/1
Running
0
12h
database-read-0
1/1
Running
0
17h
database-read-1
1/1
Running
0
119m
kafka-exporter-744dbd8476-wwztw
1/1
Running
0
45h
kotsadm-5db494c84f-b9vtq
1/1
Running
0
119m
kotsadm-minio-0
1/1
Running
0
17h
kotsadm-rqlite-0
1/1
Running
0
12h
messaging-0
2/2
Running
0
17h
messaging-1
2/2
Running
2
11h
messaging-2
2/2
Running
0
119m
messaging-connect-5db8c6fbcf-jk7st
2/2
Running
0
45h
messaging-connect-5db8c6fbcf-jstq7
2/2
Running
0
119m
messaging-cp-zookeeper-0
3/3
Running
0
17h
messaging-cp-zookeeper-1
3/3
Running
0
2d11h
messaging-cp-zookeeper-2
3/3
Running
0
119m
messaging-schema-registry-7c646d8c7-mxshj
2/2
Running
0
2d12h
messaging-schema-registry-7c646d8c7-q77bh
2/2
Running
0
119m
messaging-schema-registry-7c646d8c7-z5s4v
2/2
Running
0
45h
olap-installation-zookeeper-0
3/3
Running
0
2d12h
olap-installation-zookeeper-1
3/3
Running
0
17h
olap-installation-zookeeper-2
3/3
Running
0
119m
olap-operator-7999d4fdb8-kkprt
2/2
Running
0
119m
$ kubectl get jobs -n $arthur_namespace
NAME
COMPLETIONS
DURATION
AGE
arthurai-additional-images-bootstrap-xmmes
0/1
16s
16s
arthurai-api-key-bootstrap-cfrhy
0/1
16s
16s
arthurai-database-migration-hhlpc
0/1
16s
16s
arthurai-default-entities-bootstrap-lddho
0/1
15s
16s
arthurai-meter-events-connector-deploy-xvxgz
0/1
15s
15s
arthurai-model-health-score-connector-deploy-qhvg1
0/1
15s
15s
arthurai-query-service-migration-17ehe
0/1
15s
15s
clickhouse-v23-migration-job-xynkq
0/1
15s
15s
messaging-config-031e7f1c
0/1
15s
15s
Restore ClickHouse Data
The Arthur Platform ships with a Kubernetes CronJob that executes a ClickHouse restore that is scheduled never to run.
To restore ClickHouse data, execute the following commands:
Get the name of the clickhouse-backup that coincides with the kafka/enrichments/workflow backups that you are restoring
Using the clickhouse pod itself -
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl exec chi-olap-installation-arthur-0-0-0 -n $arthur_namespace -c backup -- clickhouse-backup list
<<<output-truncated-for-brevity>>>
2023/05/12 15:12:16.199434
info SELECT * FROM system.macros logger=clickhouse
chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-11-00-00-07
10.33MiB
11/05/2023 00:00:14
remote
tar, regular
chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-11-00-00-07
10.33MiB
11/05/2023 00:00:15
remote
tar, regular
chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-11-00-00-07
10.33MiB
11/05/2023 00:00:15
remote
tar, regular
chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-12-00-00-06
10.33MiB
12/05/2023 00:00:14
remote
tar, regular
chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-12-00-00-06
10.33MiB
12/05/2023 00:00:14
remote
tar, regular
chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-12-00-00-06
10.33MiB
12/05/2023 00:00:15
remote
tar, regular
2023/05/12 15:12:18.317324
info clickhouse connection closed logger=clickhouse
Using AWS S3 CLI -
Shell$ aws s3 ls s3://<s3-bucket-name>/<backup-path>/ --profile AWS_PROFILE
PRE chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-11-00-00-07/
PRE chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-12-00-00-06/
PRE chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-11-00-00-07/
PRE chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-12-00-00-06/
PRE chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-11-00-00-07/
PRE chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-12-00-00-06/
Extract the ARTHUR_BACKUP_NAME from the backups. The backups are named in the CLICKHOUSE_NODE_NAME-ARTHUR_BACKUP_NAME format. For example, chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2022-05-12-00-00-06 can be parsed into:
Clickhouse node name: chi-olap-installation-arthur-0-0
Arthur's backup name: arthur-clickhouse-backup-2022-05-12-00-00-06
Create the restore job, and configure it to use the Arthur backup name from above
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl create job --from=cronjob/clickhouse-restore-cronjob -n $arthur_namespace -o yaml clickhouse-restore --dry-run=client --save-config > clickhouse-restore.yaml
$ backup_name="arthur-clickhouse-backup-2022-05-12-00-00-06" # value extracted in above step
$ sed -i -e "s/insert-backup-name-here/$backup_name/" clickhouse-restore.yaml
$ cat clickhouse-restore.yaml  grep -C2 "name: BACKUP_NAME" # verify the replacement is correct
$ kubectl apply -f clickhouse-restore.yaml -n $arthur_namespace
job.batch/clickhouse-restore created
Restore Messaging Infrastructure
The Arthur Platform restores Kafka Deployment State, PersistentVolumes, and PersistentVolumeClaims using Velero.
To restore the messaging infrastructure (Kafka and ZooKeeper), run the following commands:
Delete the StatefulSets related to messaging infrastructure that was created while installing the platform
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl get sts -n $arthur_namespace  grep -i messaging # there should only be two STSs returned
$ kubectl delete sts messaging -n $arthur_namespace
$ kubectl delete sts messaging-cp-zookeeper -n $arthur_namespace
Delete the PersistentVolumeClaims related to messaging infrastructure that was created while installing the platform
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl get pvc -n $arthur_namespace  grep -i messaging # the number of PVCs returned depends on your configuration
$ kubectl get pvc -n $arthur_namespace  grep -i messaging  awk '{print $1}'  xargs kubectl delete pvc -n $arthur_namespace
Confirm the PersistentVolumes have automatically been deleted (due to a 'delete' retention policy)
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl get pv -n $arthur_namespace  grep -i messaging  wc -l # should return 0
If the PersistentVolumes still do not get deleted automatically after a few minutes, delete them manually
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl get pv -n $arthur_namespace  grep -i messaging # the number of PVs returned depends on your configuration
$ kubectl get pv -n $arthur_namespace  grep -i messaging  awk '{print $1}'  xargs kubectl delete pv -n $arthur_namespace
🚧Make sure restore steps are completeDo not proceed until the above deletion commands have fully completed. Check with the kubectl get <resource> commands.
Get the relevant Velero Backup by using the Velero CLI:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero backup get -n $velero_namespace  grep messaging
NAME
STATUS
ERRORS
WARNINGS
CREATED
EXPIRES
STORAGE LOCATION
SELECTOR
arthur-backup-2023-05-11t15.22.37-04.00-messaging
Completed
0
0
2023-05-11 15:22:48 -0400 EDT
27d
docs-demo-storage-location
app in (cp-kafka,cp-zookeeper)
$ velero restore create \
--from-backup "arthur-backup-2023-05-11t15.22.37-04.00-messaging" \
--namespace $velero_namespace
Velero will update the Pod Specs, point to the PVs using the EBS Volume Snapshots, and restore the kubernetes resources associated with Kafka.
Wait for the messaging infrastructure and Arthur platform to become "Ready"
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl kots get apps -n $arthur_namespace
SLUG
STATUS
VERSION
arthur
ready
3.4.0
Restore Enrichments
The Arthur Platform uses Velero to restore the Enrichments infrastructure and workflows, which will require running 2 separate commands.
Restore Enrichments Infrastructure
To restore the enrichments infrastructure, run the following commands:
Shell$ velero_namespace="Put your Velero namespace here"
$ velero backup get -n $velero_namespace  grep enrichments
NAME
STATUS
ERRORS
WARNINGS
CREATED
EXPIRES
STORAGE LOCATION
SELECTOR
arthur-backup-2023-05-11t15.25.24-04.00-enrichments
Completed
0
0
2023-05-11 15:25:33 -0400 EDT
27d
default
component in (kafka-mover-init-connector,model_server)
$ velero restore create \
--from-backup "arthur-backup-2023-05-11t15.25.24-04.00-enrichments" \
--namespace $velero_namespace
Restore Enrichments Workflows
Restoring the workflows is a 2-step process:
Restore the workflows from the Velero backup
Shell$ velero_namespace="Put your Velero namespace here"
$ velero backup get -n $velero_namespace  grep workflows
NAME
STATUS
ERRORS
WARNINGS
CREATED
EXPIRES
STORAGE LOCATION
SELECTOR
arthur-backup-2022-09-23t11.23.25-04.00-workflows
Completed
0
0
2022-09-23 11:24:35 -0400 EDT
27d
default
<none>
$ velero restore create \
--from-backup "arthur-backup-2022-09-23t11.23.25-04.00-workflows" \
--namespace $velero_namespace
Restore Batch Workflows which are recoverable using an Arthur Admin Endpoint
In one terminal window, port-forward to the dataset service:
Shell$ arthur_namespace="Put your Arthur namespace here"
$ kubectl port-forward -n $arthur_namespace svc/arthurai-dataset-service 7899:80
In another terminal window, run the following commands:
Shell$ curl -k -XPOST http://localhost:7899/api/v1/workflows/batch/recover
{"message":"success"}
Smoke Tests and Validation
The restore process is now complete. All data should be restored and consistent from when the backup was taken. Any data sent during or after the backup will need to be re-sent. Perform any validation/smoke tests to ensure that the platform is operating.Updated 3 months ago Table of Contents
Restore RDS Postgres
Install the Arthur Platform
Restore ClickHouse Data
Restore Messaging Infrastructure
Restore Enrichments
Restore Enrichments Infrastructure
Restore Enrichments Workflows
Smoke Tests and Validation
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/ranked-list-outputs-onboarding
 Ranked List Outputs Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Ranked List Outputs OnboardingSuggest EditsThis page walks through the basics of setting up a recommender system model (ranked list output) and onboarding it to Arthur Scope to monitor performance. The inputs to a recommender system model could be time series inputs or more traditional tabular inputs.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur Scope.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>")
Registering a Recommender System Model
Each recommender system model is created with a name and with output_type = OutputType.RankedList. Here, we register a recommender model:
Pythonarthur_model = arthur.model(name="RecSysQuickstart",
input_type=InputType.Tabular,
model_type=OutputType.RankedList)
Formatting Reference/Inference Data
Column names can contain only alphanumeric and underscore characters.
Ranked list data can be uploaded to Arthur either in a DataFrame or a JSON file. Typically, a JSON file is a more natural formatting for ranked list data. For a recommender system model recommending a loan policy, the reference data might look like this:
JSON{
"reference_data": {
"id": "6euQxGJai11qr0gENGgvgh",
"account_id": "8klQSGJil78qr4gLJKklsy",
"recommendations": [
{
"label": "Loan Policy 3",
"item_id": "64NWp2MbJXd7oHg7XmXCIa",
"score": 90
},
{
"label": "Loan Policy 1",
"item_id": "0CoZWIVqaHHGArYRTJD1V5",
"score": 83
},
],
"gt": [
"0CoZWIVqaHHGArYRTJD1V5", // ids of relevant recommendations
"72EAUBslQ047R3j9dxMCf4",
]
},
... // more inferences here
}
Data Requirements
The list of ranked list items should be sorted in rank order, such that the highest ranked item is first.
Each ranked list output model in Arthur can have max 1000 total unique recommended items in its reference dataset.
Each ranked list output model can have max 100 recommendations per inference/ground truth.
If the label or score metadata field in a ranked list item is specified for one inference, it must be specified for all of them.
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save(), you can call arthur_model.review() on the model schema to check that your data was parsed correctly in your call to arthur_model.build().
For a recommender system model, the model schema should look like this:
Python		 name
stage
value_type
categorical
is_unique
0
ranked_list_pred_attr PREDICTED_VALUE
RANKED_LIST
False
False
1
ground_truth
GROUND_TRUTH
ARRAY(STRING)
False
False
...
2
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using arthur_model.review(), you are finished registering your model and its attributes - so you are ready to complete onboarding your model.
See this guide for further details on how to save your model, send inferences, and get performance results from Arthur. These steps are the same for recommender system models as for models of any InputType and OutputType .Updated about 2 months ago Table of Contents
Getting Started
Registering a Recommender System Model
Formatting Reference/Inference Data
Data Requirements
Reviewing the Model Schema
Finishing Onboarding
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/exporting-platform-configurations
 Exporting Platform Configurations
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Exporting Platform ConfigurationsSuggest EditsAfter a successful installation of the Arthur platform, you can download the configuration that was used as a .yaml file by following the below steps:
Navigate to the Admin Console and login.
Click on the "View Files" tab.
Go to upstream → userdata and click on config.yaml file.
Copy the contents of that file and save it for future use (or check-in to source control).
Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/anomaly-detection
 Anomaly Detection
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Anomaly DetectionMonitor and alert on incoming changes to your data distribution based on complex interactions between featuresSuggest EditsGo beyond single distribution analysis and look at complex interactions between features that may cause drift (multivariate). Both in low, but especially in high dimensional spaces where there is often data sparsity, anomaly detection can help capture changes in complex interactions between features.
Anomaly Detection in Practice
Anomaly Detection as Multivariate Drift
Model-based monitoring techniques allow you to look at inferences as a whole (not split by individual distributions) to better understand how many anomalous or weird inputs your model is receiving. Special use cases such as NLP and Computer Vision have different feature relationships than traditional tabular methods, which require model-based anomaly detection to capture their drift.
Search Out Anomalous Inferences
Easily investigate anomalous inferences by filtering by individual anomaly scores in the inference tab. While also available for Tabular or Text inputs, we can see this example in Arthur below.
In the example gif provided above, we are looking into a model trained to detect whether or not a satellite image is from France or Russia. This model was only trained on satellite images of amusement parks. Here, we can see the most anomalous inferences are of stadiums (a context that the model was not trained to understand).
Teams frequently use this feature to find potential data pipeline issues, select inputs for re-training, and better understand the environment their model is running within.
Inference Anomaly Score Distribution Chart
For each inference, teams can also visualize the anomaly score distribution chart. This chart represents all of the anomaly scores in your Reference Dataset as a point of comparison. Inferences are ranked on a scale of 0-1, where 0 represents no anomaly, and 1 represents an anomalous input.
Raw Anomaly Score (For Text and Image Input Types: As you can see in the visual above, there is the ability to visualize Raw Anomaly Score in the distribution chart. This is only available for text and image model types. The raw anomaly scores are the calculated loss of the model used to predict anomaly. Due to the way anomalies are scored against the reference dataset, we recommend using the raw anomaly score to track and sort anomalies in instances such as image quality assurance where all images are the same.
Understanding the Algorithm
To learn more about the algorithms used for anomaly detection. Please refer to the Arthur Algorithms documentation section.
Available Arthur Schemas
Anomaly Detection can be enabled for models with any Input or Output Type other than Time Series input type models. Only a reference data set is required - this can be a set of the model's train or test data. Once a reference set is uploaded, anomaly scores are calculated automatically.Updated 2 months ago What’s NextLearn more about enabling enrichments for your model in the Model Onboarding section. Otherwise, click on Hot Spots to learn about another type of enrichment.Enabling EnrichmentsHot SpotsTable of Contents
Anomaly Detection in Practice
Anomaly Detection as Multivariate Drift
Search Out Anomalous Inferences
Inference Anomaly Score Distribution Chart
Understanding the Algorithm
Available Arthur Schemas
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/metrics-1
 Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by MetricsSuggest EditsMetrics are functions for measuring model performance. They might compare predicted values to ground truth, measure distributional shift, evaluate model fairness, surface explainability trends, track feature distributions, inference volumes, or anything else you can imagine. Metrics are a foundational part of evaluating and exploring your models. Arthur’s powerful Metrics API gives you the defaults you need to hit the ground running and the flexibility to define model performance however it best suits your business.
Arthur’s metrics are defined as template queries in our Query API
format. These template queries are evaluated with your specified parameters, filters, and rollup. For example, when you’re viewing a Feature Drift chart in the UI, behind the scenes, the Arthur dashboard is:
evaluating the Feature Drift metric for your model
specifying your selected Drift Metric parameter (e.g., “PSI”) and your specified Drift Attribute parameter(s) (e.g. “Age” and “FICO Score” input attributes)
specifying your selected timeframe as a filter over the “inference_timestamp” field
specifying your selected rollup of “day,” “hour”, etc., to determine the granularity of the graph (or “batch_id” for batch models)
Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://sdk.docs.arthur.ai
 Arthur SDK Reference documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
Arthur SDK Reference
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Arthur Docs
SDK Home
Permissions by Function
arthuraiToggle child pages in navigation
arthurai.clientToggle child pages in navigation
arthurai.client.apiv3
arthurai.client.authToggle child pages in navigation
arthurai.client.auth.AuthRefresher
arthurai.client.clientToggle child pages in navigation
arthurai.client.client.new_requests_client
arthurai.client.client.ArthurAI
arthurai.client.helperToggle child pages in navigation
arthurai.client.helper.construct_url
arthurai.client.helper.get_arthur_internal_user_org
arthurai.client.helper.get_auth_info
arthurai.client.helper.get_current_org
arthurai.client.helper.user_login
arthurai.client.httpToggle child pages in navigation
arthurai.client.http.arthurToggle child pages in navigation
arthurai.client.http.arthur.ArthurHTTPClient
arthurai.client.http.baseToggle child pages in navigation
arthurai.client.http.base.AbstractHTTPClient
arthurai.client.http.requestsToggle child pages in navigation
arthurai.client.http.requests.HTTPClient
arthurai.client.validationToggle child pages in navigation
arthurai.client.validation.validate_multistatus_response_and_get_failures
arthurai.client.validation.validate_response_status
arthurai.commonToggle child pages in navigation
arthurai.common.constantsToggle child pages in navigation
arthurai.common.constants.AccuracyMetric
arthurai.common.constants.Enrichment
arthurai.common.constants.EnrichmentStatus
arthurai.common.constants.ImageContentType
arthurai.common.constants.ImageResponseType
arthurai.common.constants.InferenceType
arthurai.common.constants.InputType
arthurai.common.constants.ListableStrEnum
arthurai.common.constants.ModelStatus
arthurai.common.constants.OutputType
arthurai.common.constants.Role
arthurai.common.constants.Stage
arthurai.common.constants.TextDelimiter
arthurai.common.constants.TimestampInferenceType
arthurai.common.constants.ValueType
arthurai.common.exceptionsToggle child pages in navigation
arthurai.common.exceptions.arthur_excepted
arthurai.common.logToggle child pages in navigation
arthurai.common.log.disable_debug_logging
arthurai.common.log.enable_debug_logging
arthurai.common.log.initialize_logging
arthurai.common.log.InfoFilter
arthurai.coreToggle child pages in navigation
arthurai.core.alertsToggle child pages in navigation
arthurai.core.alerts.validate_parameters_for_alert
arthurai.core.alerts.Alert
arthurai.core.alerts.AlertRule
arthurai.core.alerts.AlertRuleBound
arthurai.core.alerts.AlertRuleSeverity
arthurai.core.alerts.AlertStatus
arthurai.core.alerts.Metric
arthurai.core.alerts.MetricType
arthurai.core.attributesToggle child pages in navigation
arthurai.core.attributes.get_attribute_order_stage
arthurai.core.attributes.ArthurAttribute
arthurai.core.attributes.AttributeBin
arthurai.core.attributes.AttributeCategory
arthurai.core.auth_infoToggle child pages in navigation
arthurai.core.auth_info.AuthInfo
arthurai.core.baseToggle child pages in navigation
arthurai.core.base.ArthurBaseJsonDataclass
arthurai.core.biasToggle child pages in navigation
arthurai.core.bias.bias_metricsToggle child pages in navigation
arthurai.core.bias.bias_metrics.BiasMetrics
arthurai.core.bias.bias_wrapperToggle child pages in navigation
arthurai.core.bias.bias_wrapper.ArthurBiasWrapper
arthurai.core.bias.threshold_mitigationToggle child pages in navigation
arthurai.core.bias.threshold_mitigation.Curves
arthurai.core.bias.threshold_mitigation.ThresholdMitigation
arthurai.core.data_serviceToggle child pages in navigation
arthurai.core.data_service.DatasetService
arthurai.core.data_service.ImageZipper
arthurai.core.dataset_validation_utilsToggle child pages in navigation
arthurai.core.dataset_validation_utils.ensure_obj_matches_attr_value_type
arthurai.core.dataset_validation_utils.get_first_elem_if_valid_list
arthurai.core.dataset_validation_utils.obj_value_type_mismatch_err
arthurai.core.dataset_validation_utils.valid_rec_obj
arthurai.core.dataset_validation_utils.validate_attr_names
arthurai.core.dataset_validation_utils.validate_series_data_type
arthurai.core.dataset_validation_utils.validate_token_likelihoods_type
arthurai.core.decoratorsToggle child pages in navigation
arthurai.core.decorators.log_prediction
arthurai.core.enrichment_status_waiterToggle child pages in navigation
arthurai.core.enrichment_status_waiter.await_enrichments_ready
arthurai.core.enrichment_status_waiter.EnrichmentStatusWaiter
arthurai.core.enrichment_status_waiter.StatusForEnrichment
arthurai.core.inferencesToggle child pages in navigation
arthurai.core.inferences.add_inference_metadata_to_dataframe
arthurai.core.inferences.add_predictions_or_ground_truth
arthurai.core.inferences.nest_inference_and_ground_truth_data
arthurai.core.inferences.nest_reference_data
arthurai.core.inferences.parse_stage_attributes
arthurai.core.model_status_waiterToggle child pages in navigation
arthurai.core.model_status_waiter.ModelStatusWaiter
arthurai.core.model_utilsToggle child pages in navigation
arthurai.core.model_utils.check_attr_is_bias
arthurai.core.model_utils.check_has_bias_attrs
arthurai.core.model_utils.get_positive_predicted_class
arthurai.core.model_utils.tensors_to_arthur_inference
arthurai.core.modelsToggle child pages in navigation
arthurai.core.models.ArthurModel
arthurai.core.models.ArthurModelGroup
arthurai.core.models.ExplainabilityParameters
arthurai.core.status_waiterToggle child pages in navigation
arthurai.core.status_waiter.StatusWaiter
arthurai.core.utilToggle child pages in navigation
arthurai.core.util.can_cast
arthurai.core.util.dataframe_like_to_list_of_dicts
arthurai.core.util.intersection_is_non_empty
arthurai.core.util.is_bool_like
arthurai.core.util.is_date_like
arthurai.core.util.is_float_like
arthurai.core.util.is_int_like
arthurai.core.util.is_list_like
arthurai.core.util.is_str_like
arthurai.core.util.is_valid_datetime_obj
arthurai.core.util.retrieve_json_files
arthurai.core.util.retrieve_parquet_files
arthurai.core.util.series_to_df
arthurai.core.util.standardize_pd_obj
arthurai.core.util.update_column_in_list_of_dicts
arthurai.core.util.NumpyEncoder
arthurai.core.vizToggle child pages in navigation
arthurai.core.viz.style
arthurai.core.viz.utilsToggle child pages in navigation
arthurai.core.viz.utils.get_pred_and_gt_attrs
arthurai.core.viz.utils.savgol_filter
arthurai.core.viz.visualizerToggle child pages in navigation
arthurai.core.viz.visualizer.DataVisualizer
arthurai.datasetsToggle child pages in navigation
arthurai.datasets.arthur_exampleToggle child pages in navigation
arthurai.datasets.arthur_example.ArthurExample
arthurai.datasets.arthur_example.ArthurExampleSchema
arthurai.datasets.downloadToggle child pages in navigation
arthurai.datasets.download.download_from_s3
arthurai.datasets.download.get_file_keys_in_s3_folder
arthurai.datasets.download.load_downloaded_file
arthurai.datasets.download.ArthurDatasetSource
arthurai.datasets.download.ArthurExampleDownloader
arthurai.explainabilityToggle child pages in navigation
arthurai.explainability.arthur_explainerToggle child pages in navigation
arthurai.explainability.arthur_explainer.ArthurExplainer
arthurai.explainability.arthur_explainer.EmptyLimeExplanation
arthurai.explainability.explanation_packagerToggle child pages in navigation
arthurai.explainability.explanation_packager.ExplanationPackager
arthurai.explainability.validationToggle child pages in navigation
arthurai.explainability.validation.validate_predicted_attribute_order_matches_dataframe
arthurai.explainability.validation.validate_predicted_attribute_order_matches_remote
arthurai.utilToggle child pages in navigation
arthurai.util.format_time_series_attr_timestamps
arthurai.util.format_timestamp
arthurai.util.format_timestamps
arthurai.util.generate_timestamps
arthurai.util.is_valid_datetime_string
arthurai.util.normal_random_ints_fixed_sum
arthurai.util.value_type_to_python_type
arthurai.version
Back to top
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Arthur SDK Python API Reference#
This page contains the complete Python API reference for Arthur’s SDK. For guides and concepts, see
our main docs.
Essentials#
There are a few touch points that are most commonly needed in the SDK. These are:
The ArthurAI client to create a connection to Arthur
Its ArthurAI.model() method to create a new (empty) model
Its ArthurAI.get_model() to fetch an existing model already registered with Arthur
The ArthurModel class (returned by both of the above methods) to interact with a particular model
Its ArthurModel.build() method to construct a new model from a DataFrame
Its ArthurModel.save() method to register a model with the Arthur platform
Contents#
Arthur Docs
SDK Home
Permissions by Function
arthurai
arthurai.client
arthurai.common
arthurai.core
arthurai.datasets
arthurai.explainability
arthurai.util
arthurai.version
Indices#
Index
Module Index
Next
Permissions by Function
Copyright © 2022, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Arthur SDK Python API Reference
Essentials
Contents
Indices
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/model-input-output-types
 Model Input / Output Types
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Model Input / Output TypesUnderstanding Model Schemas within Arthur ScopeSuggest EditsRole of a Model Schema in Arthur Scope
Arthur's model schema records important properties for your model's attributes, including their value type and stage. These data types define the data that enters and exits your model.
The InputType of a model specifies whether data enter your model as a tabular data frame, image, or raw text.
The OutputType of a model specifies the modeling task: whether your model predicts values for a regression task, probabilities for a classification task, bounding boxes for a computer vision object-detection task, or token sequences (and their probabilities for generative tasks).
Supported Schemas in Arthur Scope
Arthur Scope supports several input and output types.
The input types supported are:
The output types supported are:
Updated 2 months ago What’s NextQuick look at model monitoring in Arthur by typeTabularTextImageTable of Contents
Role of a Model Schema in Arthur Scope
Supported Schemas in Arthur Scope
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/text-regression-1
 Regression
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by RegressionSuggest EditsRegression models predict a numeric outcome. In Arthur, these models are listed under the regression model type.
Some common examples of Text regression are:
What is the predicted review score for written restaurant reviews?
Predict house price from description text
Formatted Data in Arthur
Text regression models require two columns: text input and numeric output. When onboarding a reference dataset (and setting a model schema), you need to specify a target column for each inference's ground truth. Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (Text Input)Prediction (numeric)Ground Truth (numeric)Non-Input Attribute (numeric or categorical)Dulce est desipere in loco45.3462.42High School EducationSi vis amari ama55.153.2Graduate Degree
Predict Function and Mapping
These are some examples of common values teams need to onboard for their regression models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics.
Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Single Column Ground Truth
output_mapping = {
'prediction_column':'gt_column'}
# Build Arthur Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
## Example prediction function for binary classification
def predict(x):
return model.predict(x)
Available Metrics
When onboarding regression models, there are several default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a regression model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeRoot Mean Squared ErrorPerformanceMean Absolute ErrorPerformanceR SquaredPerformanceInference CountIngestionAverage PredictionIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/sending-ground-truth
 Sending Ground Truth
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Sending Ground TruthSuggest EditsOne of the greatest differences between evaluating models in experimentation vs. monitoring in production is the delayed nature of responses.
A model that predicts an action may have ground truth available immediately after prediction. One common example is models, common in the realm of advertising, used to predict whether or not a user will click on the advertisement.
A bank using a model that predicts whether or not a customer will default on their loan in the first 6 months will not know whether or not they were correct until 6 months have passed or the customer defaults on their loan.
In rarer cases, though more often in models of unstructured data types (like text or image), ground truth may never be collected.
Due to these varying timelines of receiving ground truth data, many teams use Data Drift Metrics as proxies for performance when ground truth is delayed. With these techniques, however, it is still best practice to format and send in your ground truth data when available.
Formatting Ground Truth Data
After receiving ground truth data, matching ground truth labels with the correct inferences within Arthur is essential. To ensure this, Arthur requires two values for every inference value you would like to send ground truth for:
Ground Truth Label: True label for that inference row
Partner Inference ID: A unique inference identifier meant to connect inferences with how teams keep track of inferences internally
Sending Ground Truth Data with Python SDK
With the need to wait for ground truth, there tend to be three main workflows for teams updating their Arthur Model to receive ground truth :
At the time of prediction: Some ML models run in systems where ground truth is provided nearly instantaneously after prediction. In these instances, it is best to include ground truth as an additional column to there.send_inferences() workflow.
At the time of labeling: Similar to attaching send_inferences within the Python script where your model makes inferences, teams with a receiving ground truth workflow may choose to attach update_inference_ground_truths() to their Python script
In bulk: Other teams may wait to onboard ground truth labels until a certain number of labels or time has passed. Teams that choose to send labels in bulk from a data frame may either use.update_inference_ground_truths() or send_bulk_ground_truths() for updating ground truth for more than 100k inferences at a time
Python SDK####################################################
# we can collect a set of folder names each corresponding to a batch run, containing one or
#
more Parquet or Json files with the input attributes columns, non-input attribute columns, and
#
prediction attribute columns as well as a "partner_inference_id" column with our unique
#
identifiers and an "inference_timestamp" column
inference_batch_dirs = ...
# then suppose we have a directory with one or more parquet or json files containing matching
#
"partner_inference_id"s and our ground truth attribute columns as well as a
#
"ground_truth_timestamp" column
ground_truth_dir = ...
# send the inferences to Arthur
for batch_dir in inference_batch_dirs:
batch_id = batch_dir.split("/")[-1]
# use the directory name as the Batch ID
arthur_model.send_bulk_inferences(
directory_path=batch_dir,
batch_id=batch_id)
# send the ground truths to Arthur
arthur_model.send_bulk_ground_truths(directory_path=ground_truth_dir)
Updating Ground Truth with the API
Ground truth can also be updated using the Arthur API using either:
The Update Inference Ground Truths endpoint
The Send Inference File endpoint and specifying the Ground Truth file
Updated 2 months ago Table of Contents
Formatting Ground Truth Data
Sending Ground Truth Data with Python SDK
Updating Ground Truth with the API
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/upgrading
 Upgrading
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by UpgradingSuggest EditsArthur's platform installer leverages an open-source solution called Kubernetes Off-The-Shelf (KOTS).
Please refer to the below links for upgrading the platform components.
Upgrading the platform application
Upgrading the Admin Console
Upgrading the embedded Kubernetes cluster for VM install
Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/virtual-machine-installation
 Virtual Machine Installation
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Virtual Machine InstallationSuggest EditsThis section covers the steps required for installing Arthur on a virtual machine (VM). We have included separate steps required for online and air-gapped installations.
The VM installs are not recommended for production-grade deployments. They are great for development and testing purposes.
Online Virtual Machine (VM) Install
Go to the download portal using the URL and the password provided by Arthur.
Click the "Download license" button to download your license in YAML file.
SSH into your virtual machine (VM) and run the command below to install the Admin Console:
Shellcurl -sSL https://k8s.kurl.sh/arthur  sudo bash
Log in to the Admin Console at <yourhost>:8800 using the provided password in the install output.
Follow the instruction to set up your secure connection with TLS certificate.
Upload your license file.
Provide your configurations
Review the preflight checks to ensure that your machine meets the minimum requirements before proceeding with the installation.
Monitor the dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with kubectl CLI:
Shell# Reload your shell if you haven't
bash -l
kubectl get deployment,statefulset,pod -n <yournamespace>
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
When using kubectl, you might run into a permission issue loading the kubernetes/admin.conf file. Please remediate it by running the command below.
Shellsudo chmod +r /etc/kubernetes/admin.conf
Airgap Virtual Machine (VM) Install
Go to the download portal using the URL and the password provided by Arthur.
Select the "Embedded cluster" option
Click the “Download license” button to download your license in the YAML file.
Download the "Latest kURL embedded install" and the "Latest Arthur Airgap bundle".
Preparing the embedded cluster
Arthur leverages Kubernetes as the base. This step installs the base Kubernetes cluster and Arthur's Admin Console
on your VM with a single CLI command.
First, upload the kURL embedded install bundle on your VM instance.
Example:
Shellscp -i mykey.pem ~/Downloads/arthur.tar.gz ubuntu@hostname:arthur.tar.gz
Unpack the bundle and install the embedded Kubernetes cluster on your VM instance.
Shelltar xvf arthur.tar.gz
cat install.sh  sudo bash -s airgap
Save the output from the install, including the Kotsadm Admin Console URL and the password.
You now have a K8s cluster, kubectl CLI, and the Admin Console installed on your VM.
Deploying the application to the embedded cluster
Load the Admin Console UI on port 8800 from your browser using the Kotsadm URL and the password you recorded earlier.
Follow the instructions on the Admin Console to complete your installation by providing the arthur-x.x.x.airgap bundle and necessary configurations.
Monitor the Admin Console dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with kubectl CLI on the VM:
Shell# Reload your shell if you haven't
bash -l
kubectl get deployment,statefulset,pod
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
Note: You may also follow the instructions {doc}here <vm_install_airgapped_cli> to install the Admin Console and Arthur app programmatically using the CLI only.
Airgap Virtual Machine (VM) Install with CLI
If you prefer to install programmatically using CLI only, follow the steps below.
Upload the license file and the arthur-x.x.x.airgap bundle on your VM instance.
Example:
Shellscp -i mykey.pem ~/Downloads/Test\ Customer.yaml ubuntu@hostname:license.yaml
scp -i mykey.pem ~/Downloads/arthur-x.x.x.airgap ubuntu@hostname:arthur-x.x.x.airgap
Create a config.yaml file on the VM instance using {doc}the configuration template </platform-management/reference/config_template>.
Run this install command from your VM's SSH session:
Shell
kubectl kots install arthur \
--airgap-bundle ./arthur-x.x.x.airgap \
--license-file ./license.yaml \
--config-values ./config.yaml \
--namespace arthur \
--shared-password [The Kotsadm password you saved earlier]
Updated 3 months ago Table of Contents
Online Virtual Machine (VM) Install
Airgap Virtual Machine (VM) Install
Preparing the embedded cluster
Deploying the application to the embedded cluster
Airgap Virtual Machine (VM) Install with CLI
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/creating-arthur-model-object
 Creating Arthur Model Object
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Creating Arthur Model ObjectSuggest EditsNow that we are ready, we can start onboarding a model to Arthur. This page is a walkthrough of creating an ArthurModel object to monitor your ML model.
TL;DR
An ArthurModel object sends and retrieves data important to your deployed ML system. The ArthurModel object is separate from the trained underlying model and makes predictions; it is a wrapper for the underlying model to access Arthur platform functionality.
The general steps for creating an ArthurModelObject are as followed:
Creating a production-ready ML model
Prepare your model schema/ reference dataset for Arthur:
Define build out and save your Arthur model object
A quick overview of the code needed can be seen below.
Pythonarthur_model = arthur.model(partner_model_id=f"CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}",
display_name="Credit Risk Batch",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass,
is_batch=True)
prediction_to_ground_truth_map = {
"prediction_1": 1
}
## Building out with Reference DataFrame
arthur_model.build(ref_df,
ground_truth_column="gt",
pred_to_ground_truth_map=prediction_to_ground_truth_map
non_input_columns=['age','sex','race','education'])
arthur_model.save()
Create a Production Ready Model
The first step to onboarding a model to Arthur is to create a model ready for or already deployed to production. Since Arthur is model and platform agnostic, it does not matter how the model is built or where it is deployed.
📘Pre-Production MonitoringSome teams use Arthur’s techniques or platform to evaluate models pre-production. This is definitely an option. However, the same sentiment remains that you need to have a finished model to onboard to Arthur and evaluate.
Creating Arthur Connection
To be able to send inference data to the platform, you will need to create a connection to not only your Arthur platform but also the model the inferences are being tracked for. Information about creating your API key and connecting to the Arthur platform/model objects can be found in the UI/Platform Guide.
Prepare Model Schema / Reference Dataset for Arthur
Teams are required to define the model's structure or schema. A model structure, otherwise known as an Arthur model schema, defines a wireframe for what Arthur should expect as inputs and outputs to your model. By recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built for your model in Arthur.
When you are onboarding a model, Arthur categorizes each attribute into a different Stage,
depending on the role of the attribute in the model pipeline:
Attribute StageDescriptionModelPipelineInputAll the features the model uses to create a prediction. For tabular, these are the features that go into the model. These are the text or image inputs for text or NLP.PredictedValueOutput values (or predictions) that your model producesGroundTruthValues you can provide to compare the model's outputs against performance metrics (commonly referred to as target or label)NonInputDataAny attributes that are not predictive features within the model but are additional metadata you would like to track. (i.e., protected attributes like age, race, or sex, or business-specific data like unique customer id)
Attributes are analogous to the different columns that comprise your model's data. Each attribute has a value type: these can be standard types like int and str, or datatypes for complex models like raw text and images.
Available Input Value TypesAllowed Data TypesAdditional Information RequiredNumericalInteger, FloatCategoricalInteger, String, Boolean, FloatSpecified Available CategoriesTimestampDateTimeMust be time zone awareText (NLP)StringGenerative Sequence (LLM)StringA tokens (and optional token likelihood) column is also requiredImage.jpg, .png, .gifUnique IdentifierStringTime SeriesList of Dictionaries with "timestamp" and "value" keysTimestamps must be timezone aware DateTimes and values must be floats.
As you log data over time with Arthur, the model schema is used to type-check ingested data. This prevents analytics from being skewed by scenarios like int values suddenly replacing float values causing silent bugs.
Arthur also records attribute properties in the model schema, like the range of possible values an attribute has in your data. These properties are used to understand your data’s high-level structure, not to strictly enforce that future attributes have these same properties.
Some key things to keep in mind when onboarding models schemas are:
Verify Categories Listed: Verifying the available categories for categorical attributes is important. You want to ensure that all expected categories appear or are added to each attribute's list of possible categories. This is because these categories will be used to calculate drift metrics and appear for advanced segmentation within the UI. Other category values may be sent to the platform but will not be part of this functionality.
Specifying Attribute Bounds: Teams can set specified attribute bounds for numerical attributes, such as minimum and maximum values. These are only used to make setting alert thresholds (i.e., data-bound alerts on inferences passing an acceptable maximum value) easier.
High Cardinality Models (over 1000 unique attributes) : Arthur does not allow for high-cardinality models (i.e., models with many unique columns) with more than 1000 unique attributes.
High Cardinality Variables (over 1000 unique categories) : Arthur does not allow for high-cardinality variables (i.e., variables with many unique classes) with more than 1000 unique categories. This is more common for Non-Input metadata attributes than Model Input attributes. In these cases, we recommend manually overwriting
High Cardinality Variables (over 50 unique categories) : Attributes do not need over 1000 unique categories to be considered high cardinality. An attribute with over 50 unique categories can be onboarded o Arthur, but there is some feature loss for this attribute. This attribute will not be used when calculating enrichments; this includes explainability, anomaly detection, and hot spots.
Monitoring Specific Attributes for Bias: For attributes you know you want to track using Fairness Metrics, teams must designate those inferences for bias detection tracking. This is more explained later in the document here.
Typically, teams choose to infer this schema automatically from a reference dataset. However, a more in-depth look at how teams may choose to define schemas manually can be found here.
Selecting a Reference Dataset
A reference dataset is a representative sample of input features for your model. In other words, it is a sample of what is typical or expected for your model. Typically, teams onboard their model's training or validation data for reference.
The reference dataset is a representative sample of the input features your model ingests. It is used to compute baseline model analytics. By capturing the data distribution you expect your model to receive, Arthur can detect, surface, and diagnose data drift before it impacts results. Examples of how reference data is structured for different model types can be found in the Model Input / Output Types section.
Selecting a Reference Dataset: We typically recommend using your training dataset as the reference. This is because there is no better dataset representation of the patterns your model has been built to learn than the actual dataset it learned on. However, teams do often choose to use other datasets as a reference. In particular when:
The training dataset is oversampled / not representative of real-world data patterns: This commonly occurs when your positive predicted attribute is rare (i.e., tumor detection or credit card fraud).
The training dataset is incredibly large: Reference datasets larger than 1GB can cause platform performance issues unless gradually onboarded in chunks. We recommend using a sample of your training or validation dataset for a larger dataset (typically, about 50,000 to 100,000 rows are sufficient to calculate drift metrics). It is important to ensure this is still representative of the entire dataset. In particular, we recommend including samples with extreme values included in the sample. For help onboarding in chunks or configuring your sample, please contact Arthur Support).
In these instances, we recommend using your validation dataset, or another evaluated dataset with representative patterns, as the reference.
Define Arthur Model Object
After cleaning the data, we need to start defining the Arthur model. To do this, we will need to provide both structural and user-defined information about the model:
Pythonarthur_model = arthur.model(partner_model_id=f"CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}",
display_name="Credit Risk Batch",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass,
is_batch=True)
User-Defined Information
Model Display Name
This name will appear for your model within the Arthur UI. Making this descriptive and intuitive is important, as it will be how most users search for and find your model. We recommend making this title description about that desired task your model is completing (i.e., Credit Card Fraud Classification). This is because you can create and track multiple model versions within the same display name.
Model Partner ID
Within Arthur, the Model Partner ID must be unique and cannot be duplicated across other models within your organization.
Often, teams may have an internal ID or identifying hash for their models. We do not recommend making this ID your model display name. As one, hashes are often hard to identify and search for without expert knowledge. And two, tracking multiple versions for a specific model use case is incredibly difficult. Instead, teams are encouraged to onboard that specific model's identifier as the Model Partner ID.
📘Using Timestamps Within Partner IDsTo avoid any confusion later, it is best practice to construct a partner_model_id with a timestamp embedded in the string, so that each time the model is onboarded, a new partner_model_id is generated.
Structural Information
This is information about the model's structure, i.e., how it runs. While Arthur is model agnostic, so it doesn't care how your model was built, we require you to specify some information so we can correctly set up your model's environment based on its specifications.
Data Type
Also known as the model input type, this is the type of data that the model will use to make predictions. The four possible data types are tabular, image, text, and time series.
Task Type
Also known as the model output type, this is the output your model will generate. The four possible model output types are multiclass (all classification), regression, object detection, and ranked list. This information is needed as the platform has to set up the correct default metric types within your model environment.
Batch v. Streaming
Next, teams should specify how they plan to send data to Arthur. We have two choices:
Batch: Lower frequency used (and monitored). Typical high inference load at a time
Streaming: High frequency of use (and monitoring). Typical lower inference load at a time
So, if you have users sending data frequently (e.g., daily), we recommend streaming models. Conversely, if you have users sending data infrequently (e.g., monthly), we'd recommend batching models for them. However, if you want guarantees about when metrics are calculated or alerts fire and cannot wait for the scheduled jobs to run, we recommend using batch models.
Another call-out is that batch models generate alerts by batch (i.e., a set threshold has been passed on average within a batch). Streaming models generate alerts by a designated lookback window (i.e., the average over the past day, week, etc.). More information on alerting can be found here.
Indicating a batch model means supplying an additional batch_id to group your inferences. Arthur will default to measuring performance for each batch rather than by the inference timestamps.
Prediction Mapping
Finally, when teams build out their Arthur model, they must specify a prediction mapping function.
This ensures that Arthur correctly identifies predictive and ground truth classes when setting up the model's environment and calculating metrics. This mapping needs to identify which columns are prediction and ground truth but also map the labels within the columns to one another.
Examples of each model type's prediction mapping can be found in its Model Input / Output Types section.
Register Arthur Model Object
The simplest method of registering your attributes is to use the ArthurModel.build() function parses a Pandas DataFrame of your reference dataset containing inputs, metadata, predictions, and ground truth labels. In addition, a pred_to_ground_truth_map is required, which tells Arthur which of your attributes represent your model’s predicted values and how those predicted attributes correspond to your model’s ground truth attributes.
Python## General Format Example
## pred_mapping = {"model_inputs" :"model_outputs"}
# Map our prediction attribute to the ground truth value
# This tells Arthur that in the data you send to the platform,
# the `predicted_probability` column represents
# the probability that the ground-truth column has the value 1
prediction_to_ground_truth_map = {
"prediction_1": 1
}
## Building out with Reference DataFrame
arthur_model.build(ref_df,
ground_truth_column="gt",
pred_to_ground_truth_map=prediction_to_ground_truth_map)
Examples of each model type's mapping can be found within their specific Model Input / Output Types descriptions.
Non-Input Attributes (Optional)
Many teams want to track and monitor performance around metadata related to their model but are not necessarily model inputs or outputs. These features can be added as non-input attributes in the ArthurModel and must be specified in this build function.
Python# Specifying additional non input attributes when building a model.
# This tells Arthur to monitor ['age','sex','race','education']
# in the reference and inference data you send to the platform
arthur_model.build(
reference_df,
ground_truth_column='ground_truth_label',
pred_to_ground_truth_map=pred_to_ground_truth_map,
non_input_columns=['age','sex','race','education']
)
Verify Model and Onboard to Arthur
As mentioned above, your model schema can't be updated after you've saved your Arthur model object. For this reason, many teams choose to review their model schema before sending it to the platform. This can be done in the SDK with
Pythonarthur_model.review()
This creates a dataframe that contains all the information about the model schema. An example can be shown below:
After reviewing everything, teams can save their model to Arthur. This is done with the command below.
Pythonarthur_model.save()
Once you call arthur_model.save()Arthur will handle creating the model and provisioning the necessary infrastructure to enable data ingestion for this model. If model creation fails, you may try re-saving the model or contact support if the problem persists.
(Optional) Monitoring Attributes for Bias
For some of the attributes in your model, you may want to pay particular attention to how your model’s outputs are potentially different for each subpopulation of that attribute. We refer to this as monitoring an attribute for bias.
When you set up to monitor a PipelineInput or NonInput attribute for bias enables using Fairness Metrics for those attributes within your model UI.
🚧Enable Monitoring for Bias Before Sending InferencesYou must enable monitoring attributes for bias before sending inferences to the platform.
Based on whether the attributes of interest are categorical or continuous, you can follow the steps below for each attribute of interest you'd like to monitor for bias.
Categorical Attributes
For a categorical variable, each possible level of the attribute will be treated as a distinct sub-population for analysis. For example, if you had an attribute for “Gender,” which comprised the three possible values Male, Female, and Non-Binary, then you would simply add the following to your model onboarding.
Pythonarthur_model.get_attribute("SEX", stage=Stage.ModelPipelineInput
).monitor_for_bias = True
Continuous Attributes
For a continuous variable, you need to break the continuous range into a fixed number of groupings so that we can create sub-populations. You can do this by providing cutoff thresholds for each grouping. For example, if we have a continuous attribute called AGE, we can create three age brackets, such as < 35, 35 - 55, and > 55. We create these groups by providing the upper-cutoff values for each group.
Pythonarthur_model.get_attribute("AGE", stage=Stage.ModelPipelineInput
).monitor_for_bias = True
arthur_model.get_attribute("AGE", stage=Stage.ModelPipelineInput
).set(bins = [None, 35, 55, None])
Updated 2 months ago What’s NextMove onto enabling enrichments in Arthur or learn more about specifics around model attributes and stagesEnabling EnrichmentsAttributes and StagesRegistering Model Attributes ManuallyTable of Contents
TL;DR
Create a Production Ready Model
Creating Arthur Connection
Prepare Model Schema / Reference Dataset for Arthur
Some key things to keep in mind when onboarding models schemas are:
Selecting a Reference Dataset
Define Arthur Model Object
User-Defined Information
Structural Information
Register Arthur Model Object
Non-Input Attributes (Optional)
Verify Model and Onboard to Arthur
(Optional) Monitoring Attributes for Bias
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/saml
 SAML
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by SAMLSuggest Edits📘SSO configurations are only supported in on-prem Arthur installations
This page provides a walk-through for how to configure your Arthur installation to work with a SAML compatible IdP. In order to complete this guide, you need administrator access to your IdP and access to your Arthur installation's admin console configuration. Additionally, you will either need access to the Arthur superadmin user or be able to assume a role in your IdP to give yourself RBAC management permissions in Arthur.
This guide will walk through the following steps:
Configure the IdP user groups and SAML assertion
Configure the Arthur service provider URLs in the IdP
Configure Arthur to work with your IdP
Apply the Arthur IdP YAML configuration
Create organization user roles to match the IdP user groups
Test Access
Cleaning Up
1. Configure the IdP user groups and SAML assertion
In order to properly map users to permissions, Arthur requires an attribute in your SAML assertion that contains
information about the group memberships of the user. Each group in the IdP should correspond to a role in Arthur's
{doc}custom_rbac permission system.
This process can vary depending on your IdP, but most IdP's should have a user grouping mechanism, and a mechanism to configure attributes in the SAML assertions. For example using Okta, under the SAML application settings, admins can configure the SAML assertion attributes to include group information under SAML Settings -> Configure SAML -> Group Attribute Statements, then specifying a name for the attribute and a filter for the groups to include:
Setting this configuration produces the following attribute in the SAML assertion (in Okta click "Preview the SAML Assertion" button to see a sample):
XML
<saml2:AttributeStatement>
<saml2:Attribute Name="groups">
<saml2:AttributeValue>Everyone</saml2:AttributeValue>
<saml2:AttributeValue>admins</saml2:AttributeValue>
<saml2:AttributeValue>org-1-model-owners</saml2:AttributeValue>
</saml2:Attribute>
</saml2:AttributeStatement>
2. Configure the Arthur service provider URLs in the IdP
In order for your IdP to speak to Arthur, it needs to know where to find it. Enter the following URLs in your IdP's configuration to Arthur's SAML endpoints:
ACS URL (SSO URL): https://<HOSTNAME>/api/v3/saml/sso
Entity ID: https://<HOSTNAME>/api/v3/saml/sso
Start URL: https://<HOSTNAME>/
📘If your IdP will be sending signed assertions to Arthur, you will also need to generate and upload the public key
(certificate) Arthur will be using in your IdP. This will be the same certificate you set in the Arthur configuration
below. Please follow your own company policies to obtain a certificate
for Arthur. If you have no internal guidelines, then use a tool like
ssh-keygen to
generate them
3. Configure Arthur to work with your IdP
Additionally, Arthur needs to know how to handshake with your IdP. To do that, Arthur requires the following information:
Your IdP's metadata URL or the metadata XML payload (some IdPs require it be downloaded, either is fine)
One or more IdP administrator user groups that will be paired to global custom roles in Arthur
(see here for a description of what these are for)
An understanding of your SAML assertion and how to parse user information out of it
With those three things available, it is possible to fill out Arthur's IdP configuration YAML. The next subsections explain each section of the Arthur YAML configuration, and are followed by some complete examples further down.
Configuring the IdPs metadata URL
Some IdP's host their metadata XML at a public URL, while others only have it available for download privately. To support either option, Arthur has two configurations that can be used:
YAML# use this option if your IdP has a public URL for its metadata
metadataURL: "link to IdP metadata goes here"
# use this option if your IdP does not have a public URL and include the XML payload
# make sure to indent the XML payload two spaces and make sure the X509Certificate lines
# do not have more than two leading whitespaces!
metadataXML: 
<?xml version="1.0" encoding="UTF-8"?><md:EntityDescriptor ...>
<md:IDPSSODescriptor>
...
<ds:X509Certificate>CERTIFICATE LINE 1
CERT LINE 2
CERT LINE 3
CERT LINE 4
LAST CERT LINE</ds:X509Certificate>
</md:IDPSSODescriptor>
</md:EntityDescriptor>
{warning}If using the `metadataXML` configuration option, make sure to indent the entire XML payload two spaces. YAML expects
multi-line values to be indented under the key `metadataXML`.
{warning}Additionally, the assertion's `X509Certificate` XML attribute is a multi-line value within the XML.
Any new lines in the certificate value need to be indented only two spaces (all the way to the left of the YAML value).
Otherwise, the extra whitespaces introduces characters which will invalidate the certificate value.
Configure the Arthur Global Roles
Arthur has the ability to create roles for the cluster administrators during the configuration of the IdP. These roles are often needed by admins to configure RBAC and create organizations for other users in the system. See {ref}creating_global_roles_in_arthur_config for a deep dive on how to use global roles.
This section of the YAML config is under the globalRoleDefs field. It accepts a list of role definitions that will be created when the configuration is applied. The names of the roles in this section must match the user groups in your IdP in order to be able to assume them in Arthur.
YAML
globalRoleDefs:
# Here we can specify a list to define multiple global roles
- name: "idp-admin" # change this name to match the cluster administrator group name in your IdP
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
model:
- read
- write
- delete
Parsing the IdP SAML Assertion
In order for Arthur to communicate with your IdP, it needs to understand the format of the SAML assertion your IdP uses. This section of the config falls under the assertionAttributes YAML field. This section is designed to be flexible to support a variety of assertion formats, so it has a lot of options. At its core, the goal is to tell Arthur how to be able to extract the following information from the assertion:
user roles/groups
first name
last name
email
user ID
Each field has a corresponding YAML field the defines where to find the information in the SAML assertion XML. For example:
YAMLfirstNameAttribute:
name: "employeeFirstName"
This configuration tells Arthur that it can find the user's first name under the "employeeFirstName" attribute in the XML assertion. Such an assertion might look like this:
XML
<saml2:AttributeStatement>
<saml2:Attribute Name="firstName">
<saml2:AttributeValue>Ian</saml2:AttributeValue>
</saml2:Attribute>
</saml2:AttributeStatement>
More examples of how to parse attributes out of the SAML assertion can be
found below.
Full Configuration Examples
Here is an example of a full configuration, combining each section described above.
YAMLversion: v1
kind: SAML
config:
# if your IdP hosts its metadata, provide the URL to it here
metadataURL: "link to IdP metadata goes here"
# if the IdP does not host the metadata, provide the XML payload here and comment out metadataURL
#
metadataXML: 
#
<?xml version="1.0" encoding="UTF-8"?><md:EntityDescriptor ...>
#
<md:IDPSSODescriptor>
#
...
#
</md:IDPSSODescriptor>
#
</md:EntityDescriptor>
# this section describes how Arthur will parse the SAML assertion from your IdP
# for each required attribute, Arthur will use the "name" field to match to an XML attribute in the SAML assertion
assertionAttributes:
# this roleAttribute configuration will use a "groups" attribute in the XML assertion which expects the
# roles in separate XML AttributeValues within the assertion Attribute
roleAttribute:
name: "groups"
useAllAttributeValues: True
firstNameAttribute:
name: "employeeFirstName"
lastNameAttribute:
name: "employeeLastName"
emailAttribute:
name: "company_email"
userIdAttribute:
name: "companyUserID"
globalRoleDefs:
# Here we specify a global role for the IdP user group "idp-admin" to create and manage RBAC in Arthur
- name: "idp-admin"
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
model:
- read
- write
- delete
4. Apply the Arthur IdP YAML configuration
Once you have your YAML configuration file ready, you need to add it to your Arthur installation. With the Arthur admin console open, navigate to the "Use a 3rd Party Global Identity Provider" section and select "SAML". This will expose a text box for you to paste the YAML config file assembled above. When pasting, make sure whitespace is preserved and the YAML document has consistent spacing (do not mix tabs and spaces). Here is a screenshot of the config section:
{note}If your IdP enforces signed authorization requests, this config page also provides the ability to upload a
certificate and private key for Arthur to use when making the requests. Click the "Upload a file" button for the Public
Certificate and Private Key sections of the config to upload the appropriate files for your IdP.
Once you have added your config files, scroll to the bottom and click "Save" to save the config. Then go to the latest version and click "Deploy" to roll out the change to the cluster.
5. Create organization user roles to match the IdP user groups
In order to complete this section, you will need access to the Arthur superadmin user credentials set during your install, or you will need to be able to assume the role defined in the Arthur IdP config YAML created above in the globalRoleDefs section.
In order to use the API example linked below, you will need a Bearer token (authentication token) to include with your API request. There are a few options available to retrieve a token:
Retrieve a SAML assertion from your IdP and exchange with Arthur - Most IdPs will have a method to retrieve a SAML assertion for users. Some companies make scripts or APIs to do so. If your IdP does not have an automated method to retrieve an assertion, use one of the other options below. Once you have an assertion, you can exchange it for an Arthur access token with the follow API call to Arthur:
Shellcurl --location --request POST 'https://<YOUR ARTHUR HOST>/api/v3/saml/sso' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'SAMLResponse=<INSERT URL ENCODED BASE64 ASSERTION>'
Retrieve a global role token from your browser cookies - If you sign in to Arthur as a user with a global role, the UI will not be fully functional, but it will have a valid access token in the cookies. If you navigate to your browser's developer console and then go to the Application Storage/Cookies section, you should see a cookie like Authentication. The authentication token is the value of that cookie.
Use the /login API endpoint with the superadmin user's credentials set during the Arthur install (only available on-prem).
Using either of those credentials, we can use the Arthur API to define roles in Arthur that match the user group names in your IdP. See the {ref}creating_organization_roles section for an example API request to create custom roles in Arthur. Importantly, the role names must uniquely match to a user group in your IdP in order for your users to be able to assume those permissions in Arthur. Therefore, the roles in Arthur must be globally unique in the entire Arthur installation.
6. Test Access
At this point everything should be configured correctly to sign in to Arthur via SSO. Either navigate to your IdP or the Arthur homepage to test logging in.
7. Cleaning Up
Once users are successfully able to log in to Arthur via the IdP, you should do the following to ensure proper security best-practices remain enforced:
Restrict any Arthur global roles to only allow access to essential admin functions
Set the Arthur superadmin user password securely, and either store the password in a vault, or discard the password entirely. superadmin shouldn't be used going forward.
Set up a policy to routinely rotate the superadmin password to keep it secure
Together, these practices will help ensure the security of your Arthur installation, and will give your IdP sole control over the platform and who is able to access it.
Common Troubleshooting
If after following the steps above, users are not able to log in via the IdP try some of these common troubleshooting tips:
Does the user properly redirected to the IdP's log in screen?
If not, there is likely a configuration error in the Arthur YAML config with the IdP metadata or the URL to access it. Another problem could be if your IdP expects Arthur to make signed requests to authenticate users. If that is the case, be sure you have correctly configured Arthur's certificate and private key as described above.
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
If not, there is likely a configuration error with the IdP and the URLs that it uses to communicate with Arthur. Double-check the ACS (SSO) URL is configured correctly for the Arthur installation
at https://<HOSTNAME>/api/v3/saml/sso.
A user can see the Arthur home page, but can't see any of the model in their organization
If a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to access models (see {doc}../../reference/permissions_by_endpoint) or they were not able to correctly assume the role in Arthur. Double-check the groups in their SAML assertion match the role names that have been configured in Arthur. A superadmin or global role user with permissions to manage RBAC can see a list of roles in the installation by using the following API call. Be sure to replace the HOST and AUTH TOKEN for your installation and user:
Shellcurl --location 'https://<HOST>/api/v3/authorization/custom_roles' \
--header 'Authorization: Bearer <INSERT AUTH TOKEN HERE>'
Appendix A: More examples of SAML assertion values and how to parse them
This section outlines some additional ways to use the assertionAttributes section of the Arthur IdP config YAML format. The below examples include sample SAML assertions, then corresponding YAML for how to parse them.
Basic Full Example
This example shows how to parse a user's information from a SAML assertion when each field is its own Attribute and the user's groups are each in their own AttributeValue.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name="employeeFirstName">
<saml:AttributeValue>John</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name="employeeLastName">
<saml:AttributeValue>Doe</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name="employeeEmail">
<saml:AttributeValue>[email protected]</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name="employeeID">
<saml:AttributeValue>1234567890</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name="userGroups">
<saml:AttributeValue>group1</saml:AttributeValue>
<saml:AttributeValue>group2</saml:AttributeValue>
<saml:AttributeValue>group3</saml:AttributeValue>
</saml:Attribute>
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the user information field:
YAML
assertionAttributes:
roleAttribute:
name: "userGroups"
useAllAttributeValues: True
firstNameAttribute:
name: "employeeFirstName"
lastNameAttribute:
name: "employeeLastName"
emailAttribute:
name: "employeeEmail"
userIdAttribute:
name: "employeeID"
Parsing User Groups from Multiple Attribute Values
This example shows how to parse a user's groups from a SAML assertion when each group is its own AttributeValue.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name="Idp_user_groups">
<saml:AttributeValue>role1</saml:AttributeValue>
<saml:AttributeValue>role2</saml:AttributeValue>
<saml:AttributeValue>role3</saml:AttributeValue>
</saml:Attribute>
...
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the roleAttribute field:
YAML
assertionAttributes:
roleAttribute:
name: "Idp_user_groups"
useAllAttributeValues: True
Parsing User Groups from a String Attribute Value
This example shows how to parse a user's groups from a SAML assertion when the groups are in a single string AttributeValue.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name="Idp_user_groups">
<saml:AttributeValue>role1,role2,role3</saml:AttributeValue>
</saml:Attribute>
...
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the roleAttribute field:
YAML
assertionAttributes:
roleAttribute:
name: "Idp_user_groups"
deliminator: ","
Parsing Specific Fields in a Single Attribute's AttributeValue List
This example shows how to parse a user's information from a SAML assertion when all fields are in a single assertion Attribute's list of AttributeValues.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name="employeeInfo">
<saml:AttributeValue>John</saml:AttributeValue>
<saml:AttributeValue>Doe</saml:AttributeValue>
<saml:AttributeValue>(123) 456-7890</saml:AttributeValue>
<saml:AttributeValue>42 Wallaby Way, Sydney</saml:AttributeValue>
<saml:AttributeValue>[email protected]</saml:AttributeValue>
<saml:AttributeValue>5678987654</saml:AttributeValue>
</saml:Attribute>
...
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the user information field:
YAML
assertionAttributes:
firstNameAttribute:
name: "employeeInfo"
index: 0
lastNameAttribute:
name: "employeeInfo"
index: 1
emailAttribute:
name: "employeeInfo"
index: 4
userIdAttribute:
name: "employeeInfo"
index: 5
Updated 3 months ago Table of Contents
1. Configure the IdP user groups and SAML assertion
2. Configure the Arthur service provider URLs in the IdP
3. Configure Arthur to work with your IdP
Configuring the IdPs metadata URL
Configure the Arthur Global Roles
Parsing the IdP SAML Assertion
Full Configuration Examples
4. Apply the Arthur IdP YAML configuration
5. Create organization user roles to match the IdP user groups
6. Test Access
7. Cleaning Up
Common Troubleshooting
Does the user properly redirected to the IdP's log in screen?
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
A user can see the Arthur home page, but can't see any of the model in their organization
Appendix A: More examples of SAML assertion values and how to parse them
Basic Full Example
Parsing User Groups from Multiple Attribute Values
Parsing User Groups from a String Attribute Value
Parsing Specific Fields in a Single Attribute's AttributeValue List
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/image-object-detection
 Object Detection
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Object DetectionImage Object Detection Models within ArthurSuggest EditsObject detection models analyze and classify objects within an image by placing a bounding box over the objects. In Arthur, these models are listed under the object detection model type.
Some common examples of Object Detection are:
Animal detection with wildlife cameras
Quality control in manufacturing, detecting defective pieces on the factory line
Formatted Data in Arthur
Object detection models require two columns: image input and bounding box output. When onboarding a reference dataset (and setting a model schema), you need to specify the relationship between your prediction and ground truth bounding box columns. Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Formatting Bounding Boxes
When using an Object Detection model, bounding boxes should be formatted as lists in the form:
[class_id, confidence, top_left_x, top_left_y, width, height]
The first two components of the bounding box list represent the classification being made within the bounding box. The class_id represents the ID of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction (0.0 for completely unconfident and 1.0 for completely confident).
The next four components of the bounding box list represent the location of the bounding box within the image: the top_left_x and top_left_y represent the X and Y pixel coordinates of the top-left corner of the bounding box. These pixel coordinates are calculated from the origin, which is in the top left corner of the image. This means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. The width represents the number of pixels the bounding box covers from left to right, and the height represents the number of pixels the bounding box covers from top to bottom.
Attribute (Image Input)Prediction (bounding boxes)Ground Truth (bounding boxes)Non-Input Attribute (numeric or categorical)image_1.jpg45.3462.42High School Educationimage_2.jpg55.153.2Graduate Degree
Predict Function and Mapping
Teams must provide the relationship between the prediction and ground truth column to onboard their object detection models. This is defined to help set up your Arthur environment to calculate default performance metrics.
prediction to ground truth mapping## Single Column Ground Truth
output_mapping = {
'pred_bounding_box_column':'gt_bounding_box_column'}
# Build Function for this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
Available Metrics
When onboarding object detection models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a object detection model. Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeMAPEPerformanceInference CountIngestionAverage ConfidencePerformance
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, image) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate DriftAverage Raw Anomaly ScoreMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Formatting Bounding Boxes
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/creating-alerts
 Alerting
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by AlertingSuggest EditsAn alert is a message notifying you that something has occurred with your model. With alerts, Arthur Scope makes it easy to provide a continuous view of your model by highlighting important changes in model performance. When defining alerts in Arthur, there are a few things you need to consider:
Alert Severity
Two alert severities are available in Arthur; they are warning and critical.
Teams can set their own severity for alerts. We typically recommend that teams set two different thresholds for the same value, marking the less severe as Warning and the more as Critical.
Understanding Alert Rules
An alert is triggered based on an alert rule, which you define using a metric and a threshold. So to create an alert in Arthur, you need to:
Create a Metric in Arthur Scope
A metric in Arthur is a function for evaluating model performance. These can be common functions that data scientists or ML teams are familiar with, such as accuracy or F1 Score. Or they can be functions specific to a model's use case, such as Fairness Metrics or User Defined Metrics. This means that any metric you can create in Arthur (including segmentations, filters, or logical functions) can be transformed into an alert.
For the latter, following the proper steps to create the metric within Arthur for your specific model is important.
This is because it must first be created for your model to alert on a metric.
Define Threshold & Bound for Alert
After creating your metric, it is time to decide what level of underperformance you would like to be alerted to. This numeric value is called the alert threshold. In Arthur, alert threshold values have to be manually set. Users must also define whether they want to be alerted when their function exceeds that numeric threshold. This is the upper or lower bound of the alert.
Define Notification Timelines for Alert (For Streaming Models)
For batch models, alerts are calculated per batch of data. However, teams that are running streaming models need to decide how often they would like alerts to be calculated and how much data. To make these decisions, they have to clarify two values:
Lookback Period: How much data do they want to aggregate in their function? (Do they want to be alerted when the average of just one minute of data has passed the metric threshold, or do they only care if it's affected the average of a day or week).
Alert Wait Time: How often do you want to be alerted that something is happening? This is how often they would like the alert to be calculated (and triggered if the function threshold is met).
Creating Alerts
Default Alerts
Default data drift alerts for Feature and Prediction Drift are automatically created for every feature once reference and inference data are sent to Arthur. These alerts are created with dynamic threshold values specific to your reference dataset from the “data_drift” endpoint with “metric”: “Thresholds.”
Defining Custom Alert Rules in the UI
The UI provides a clickable walk-through guide for teams to make common performance, drift, and data-bound alerts. The common practice of alerting on segments (or filters) of your data when calculating the metric is also added as an optional step.
See below an example of defining an accuracy alert rule for women in the UI to be alerted every hour for the last 24 hours of inferences.
Defining Custom Alert Rules with the Python SDK
Our predefined alerting structure is not the only way teams can create alerts. Alerts can be made for any customizable User Defined Metric teams create for their model in Arthur. Teams must first create the user-defined metric, and then they can easily set the alert from their Python SDK notebook.
Alert Notifications
The latest alerts in your organization are shown on the homepage of your Arthur organization. However, beyond being highlighted in the online Arthur UI, alerts can be delivered to teams via email and/or via integrations such as PagerDuty and Slack. You can learn more about setting up those integrations here.
Alert Endpoint in the API
The dedicated alerts endpoint is /models/{model_id}/alert_rules.Updated 3 months ago Table of Contents
Alert Severity
Understanding Alert Rules
Create a Metric in Arthur Scope
Define Threshold & Bound for Alert
Define Notification Timelines for Alert (For Streaming Models)
Creating Alerts
Default Alerts
Defining Custom Alert Rules in the UI
Defining Custom Alert Rules with the Python SDK
Alert Notifications
Alert Endpoint in the API
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli
 Airgap Kubernetes Cluster (K8s) Install with CLI
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Airgap Kubernetes Cluster (K8s) Install with CLISuggest EditsIf you prefer to install programmatically using CLI only, follow the steps below.
Prepare a config.yaml file using the Config Template
Deploy the application by running the below kubectl command:
Shellkubectl kots install arthur \
--no-port-forward \
--namespace arthur \
--shared-password [Provide an Admin Console password] \
--license-file ./license.yaml \
--config-values ./config.yaml \
--airgap-bundle ./arthur-x.x.x.airgap \
--kotsadm-registry [Your private container image repository] \
--kotsadm-namespace arthurai \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
shared-password is the Admin Console password.
Installing a specific version of Arthur
To install a specific version of Arthur, you would run the same command as above (following the same steps to prepare the configuration), with the inclusion of the --app-version-label flag. This flag allows you to specify which specific version of Arthur you want to install (e.g., to set up a sandbox environment on the same version
as production).
To determine which versions of Arthur are available, you can run the following:
kubectl kots get versions arthur -n <arthur namespace>
Updated 3 months ago Table of Contents
Installing a specific version of Arthur
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/sending-historical-data
 Sending Historical Data
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Sending Historical DataSuggest EditsAlthough optional, we've found that a useful step for teams after onboarding is to onboard historical model information to Arthur. This allows teams to immediately begin to dig into their model data and explore trends in the data, even before new sending new inferences.
To send historical data, we will need to do the following:
Collect Historical Data
Format It's Timestamp Information
Send it to your Arthur Model
Collect Historical Data
Now, we want to collect all the historical data and inferences our model has run on. Organizations may store this information in different tables, so you may need to query or merge information from wherever you store data. However, it is important to ensure that this information contains the following:
All Inputs / Outputs Expected By Arthur Model Object: During model creation, we explored the format the Arthur Model Object expects to receive information. To review, the Arthur model object expects:
All Feature Inputs to the Model
Predicted Model Outputs
(Optional) Ground Truth (True Label Output), if known
(Optional) Any Non-Input Attributes logged to the model if known
Inference TimeStamps: This is information about when the model ran the historical predictions. This is especially important to ensure that information logged into Arthur is logged in for the correct time so that you can accurately evaluate trends and/or diagnose previous issues.
Additional Inference Identification: This is discussed further in detail in Sending Inferences, but this information includes inference identification like Partner Inference ID or Batch ID (for batch models).
Format Timestamps
The Arthur model object expects timestamps to be in DateTime format. Below is a quick example of how to format timestamps into the correct format, but here are some references about converting Python information into DateTime.
Pythonimport pytz
from datetime import datetime
## Use Lambda Function to Convert to Date Time
def get_timestamps(x):
new_time = x.split('.')[0]
return datetime.strptime(new_time, '%Y-%m-%d %H:%M:%S')
historical_df['timestamps'] = historical_df['timestamps'].apply(lambda x: get_timestamps(x))
Send Inferences to Arthur
Now, we can send the inferences to Arthur. We must separate the Arthur model information from the timestamps to do this. We will see how this works below. Although we use a streaming model, we can see a commented-out example of sending historical inferences as a batch.
Python## Send Historical Streaming Data
hist_df = historical_df.drop(columns = ['timestamps'])
arthur_model.send_inferences(hist_df, inference_timestamps = historical_df['timestamps'])
When sending historical batch data, it is important to remember that you need not only to send historical timestamp information but also unique batch ids. For teams that do not have unique batch ids already stored, a common technique will be to create a unique timestamp based on the frequency that you run batches historically.
An example below is creating unique batches based on the day. (i.e., all inferences with the same date will belong to the same batch). Changing this based on your batch frequency (i.e., if you run data every hour, etc.) is important.
Python## Create Daily Batch Times
historical_df['batch_id'] = historical_df['timestamps'].apply(lambda x: 'batch_'+x.strftime('%m_%d_%Y'))
batch_df = historical_df.drop(columns = ['timestamps','batch_id'])
## Send Historical Batch Data
arthur_model.send_inferences(batch_df, batch_id=historical_df['batch_id'], inference_timestamps=historical_df['timestamps'])
Key Things to Keep in Mind
Alerts will backdate: Alerts will still trigger any alerts set up on historical data. This can be useful to evaluate when an alert may have occurred in the past. However, it may be best to make notes explore, and then mark and conclude this information in the alerts section. This way, you can gain all the information from backdated alerts but have a cleaned and operable alerting homepage for new alerts your teams will need to act on.
UI may show just a window: The UI within Arthur is automatically set to show the last month of data. It can be easiest to go into the filter and select "all time" to ensure all your data has been sent to the platform.
All enabled functionality should be available: You should be ready to explore your data using the Arthur UI or query service with all your data and any special enrichments you enabled during Arthur Model Object creation.
Beyond Arthur functionalities, when evaluating your historical data, it is also important to keep in mind that:
Your model will perform better on data it was trained on: While we encourage onboarding all the historical data you want to your model to view trends, teams often may not realize that this historical data consists of the data they used to train their more recent model. If that is the case, seeing higher performance for inferences included in your model training set is not unusual. We encourage teams to still onboard historical data to visualize trends in their feature set/performance overall, but remember to think critically when seeing high historical accuracy.
Updated 3 months ago Table of Contents
Collect Historical Data
Format Timestamps
Send Inferences to Arthur
Key Things to Keep in Mind
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/querying-explainability
 Querying Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Querying ExplainabilitySuggest EditsOne of the most popular enrichments within Arthur, Explainability values, can also be queried with Arthur's query language. Teams looking to query explainability will pull their values from the enriched table in Arthur. It's important to note that teams must also ensure that explainability has been enabled for their model; otherwise, they will be unable to pull this information.
Tabular Explainability
For tabular models, there are three ways to interact with and query explainability:
Local Explainability: Explainability is calculated for each inference in a model.
Global Explainability: This aggregates all inferences' explainability values.
Regional Explainability: This is a filtered aggregate of explainability values for a specified cohort (or region) of inferences. This is often used for comparison evaluation against the global explainability values or another cohort.
Local Explainability
The most common way to interact with local explainability is within the UI. However, teams that want to pull a local explanation score within a notebook often use the query below. In this query, we are filtering for a specific inference based on itspartner_inference_id, which is the name used for an organization's unique inference id. This is the most common way for teams to look up individual inferences.
Pythonquery = {
"select": [
{"property": "explainer_algo"},
{"property": "explainer_predicted_attribute"},
{ "property": "explainer_attribute"},
{"property": "explainer_score"}
],
"filter": [
{"property": "[inference_idpartner_inference_id]",
"comparator": "eq",
"value": "<id> [string]"
}
],
"from": "enriched"
}
response = arthur_model.query(query)
response[{ "explainer_algo": "lime",
"explainer_attribute": "AGE",
"explainer_predicted_attribute": "prediction_0",
"explainer_score": 0.0016485283063465874},
{"explainer_algo": "lime",
"explainer_attribute": "BILL_AMT1",
"explainer_predicted_attribute": "prediction_0",
"explainer_score": 0.0032036810960691183},
{ "explainer_algo": "lime",
explainer_attribute": "BILL_AMT2",
"explainer_predicted_attribute": "prediction_0",
"explainer_score": 0.002008238656596662},
{ "explainer_algo": "lime",
"explainer_attribute": "AGE",
"explainer_predicted_attribute": "prediction_1",
"explainer_score": -0.0016485283063465892},
{ "explainer_algo": "lime",
"explainer_attribute": "BILL_AMT1",
"explainer_predicted_attribute": "prediction_1",
"explainer_score": -0.0032036810960691126},
{ "explainer_algo": "lime",
"explainer_attribute": "BILL_AMT2",
"explainer_predicted_attribute": "prediction_1",
explainer_score": -0.0020082386565966667} ]
Global Explainability
Global explainability aggregates all the inferences sent to your production model. It is calculated by taking each inference's average value of all feature importance scores. Teams can pull this information using the query below:
Python## get global explanations
explanation_algo = model.explainability.explanation_algo
predicted_class_col = model.get_positive_predicted_class()
global_explanations = pd.DataFrame(model.query({
"select": [
{
"function": "regionalFeatureImportances",
"alias": "global_importance",
"parameters": {
"predicted_attribute_name": predicted_class_col,
"explanation_algorithm": explanation_algo
}
}
]})).rename(columns={'explainer_attribute': 'feature','global_importance':'global_importance'}).sort_values(by='global_importance', ascending=False)
feature
global_importance
0
PAY_0
0.092898
1
PAY_2
0.027737
2
LIMIT_BAL
0.025445
3
PAY_3
0.018831
4
PAY_AMT1
0.013233
5
BILL_AMT1
0.012518
6
PAY_AMT2
0.011359
7
PAY_AMT3
0.010639
8
BILL_AMT2
0.008891
9
BILL_AMT6
0.008058
10
PAY_6
0.007976
11
PAY_AMT6
0.007970
12
PAY_AMT4
0.007123
13
BILL_AMT4
0.006892
14
BILL_AMT5
0.006786
15
PAY_5
0.006765
16
BILL_AMT3
0.006730
17
PAY_4
0.006194
18
PAY_AMT5
0.004818
19
AGE
0.004392
20
EDUCATION
0.002111
21
MARRIAGE
0.001940
22
SEX
0.001188
Teams also often choose to plot these values using this plot function:
Pythonax = global_explanations.set_index('feature').plot(kind='bar')
ax.set_title("Global Explanations")
ax.set_ylabel("feature importance")
Regional Explainability
Regional explainability is similar to global explainability, with additional filters applied to define your region or cohort of interest. This can be seen in the example below:
Query Request:
Pythonmodel.query({
"select": [
{
"function": "regionalFeatureImportances",
"alias": "global_importance",
"parameters": {
"predicted_attribute_name": predicted_class_col,
"explanation_algorithm": explanation_algo
}
}
],
"filter": [
{
"property": "AGE",
"comparator": "gte",
"value": 18
},
{
"property": "AGE",
"comparator": "lt",
"value": 40
}
]
})
Comparing Regional Importance to Global Importance
Pythondef regional_compare_global_explainability(model, regional_filters,title):
## get model attributes
explanation_algo = model.explainability.explanation_algo
predicted_class_col = model.get_positive_predicted_class()
## get regional explanations from filters
regional_explanations = pd.DataFrame(model.query({
"select": [
{
"function": "regionalFeatureImportances",
"alias": "global_importance",
"parameters": {
"predicted_attribute_name": predicted_class_col,
"explanation_algorithm": explanation_algo
}
}
],
"filter": regional_filters
})).rename(columns={'explainer_attribute': 'feature','global_importance':'regional_importance'}).sort_values(by='regional_importance', ascending=False)
## get global explanations
global_explanations = pd.DataFrame(model.query({
"select": [
{
"function": "regionalFeatureImportances",
"alias": "global_importance",
"parameters": {
"predicted_attribute_name": predicted_class_col,
"explanation_algorithm": explanation_algo
}
}
]})).rename(columns={'explainer_attribute': 'feature','global_importance':'global_importance'}).sort_values(by='global_importance', ascending=False)
## combine dataframes
explanations = global_explanations.merge(regional_explanations, left_on='feature', right_on='feature')
ax = explanations.set_index('feature').plot(kind='bar')
ax.set_title(title)
ax.set_ylabel("feature importance")
Python## Running Regional Explainability
filters = [
{
"property": "AGE",
"comparator": "gte",
"value": 18
},
{
"property": "AGE",
"comparator": "lt",
"value": 40
}
]
regional_compare_global_explainability(arthur_model, filters,title ="Regional vs Global")
Comparing Regional Importance to Regional Importance
Pythondef regional_compare_regional_explainability(model, regional_filters1,regional_filters2,title):
## get model attributes
explanation_algo = model.explainability.explanation_algo
predicted_class_col = model.get_positive_predicted_class()
## get regional explanations from filters
regional_explanations1 = pd.DataFrame(model.query({
"select": [
{
"function": "regionalFeatureImportances",
"alias": "global_importance",
"parameters": {
"predicted_attribute_name": predicted_class_col,
"explanation_algorithm": explanation_algo
}
}
],
"filter": regional_filters1
})).rename(columns={'explainer_attribute': 'feature','global_importance':'regional_importance1'}).sort_values(by='regional_importance1', ascending=False)
## get regional explanations from filters
regional_explanations2 = pd.DataFrame(model.query({
"select": [
{
"function": "regionalFeatureImportances",
"alias": "global_importance",
"parameters": {
"predicted_attribute_name": predicted_class_col,
"explanation_algorithm": explanation_algo
}
}
],
"filter": regional_filters2
})).rename(columns={'explainer_attribute': 'feature','global_importance':'regional_importance'}).sort_values(by='regional_importance', ascending=False)
## combine dataframes
explanations = regional_explanations1.merge(regional_explanations2, left_on='feature', right_on='feature')
ax = explanations.set_index('feature').plot(kind='bar')
ax.set_title(title)
ax.set_ylabel("feature importance")
Python## Running Regional Explainability
filters1 = [
{
"property": "SEX",
"comparator": "eq",
"value": 1
}
]
filters2 = [
{
"property": "SEX",
"comparator": "eq",
"value": 2
}
]
regional_compare_regional_explainability(arthur_model, filters1,filters2,title ="Regional vs Regional")
back to top
NLP Explainability
The nlp_explanation function can be used to query and filter explanations for tokens in NLP inferences. Using this function, the user can filter and order tokens by importance. The following are available optional properties:
nlp_explanation.token - References a token within a specific inference.
nlp_explanation.location - References a token's absolute location within a specific inference.
nlp_explanation.value - References a token's explanation value within a specific inference.
Query Request:
JSON{
"select": [
{
"function": "nlp_explanation",
"alias": "<alias_name> [Optional]",
"parameters": {
"attribute_name": "<text_input_attribute_name> [string]",
"nlp_predicted_attribute": "<predicted_attribute_name> [string]",
"nlp_explainer": "[limeshap]"
}
}
],
"filter": [
{
"property": "nlp_explanation.token",
"comparator": "eq",
"value": "<token>"
},
{
"property": "nlp_explanation.location",
"comparator": "eq",
"value": "<location>"
}
],
"order_by": [
{
"property": "nlp_explanation.value",
"direction": "desc"
}
]
}
Query Response:
JSON{
"query_result": [
{
"inference_id": "<id> [string]",
"nlp_explanation": [
{
"algorithm": "[limeshap]",
"predicted_attribute_name": "<predicted_attribute_name> [string]",
"importance_scores": [
{
"attribute_name": "<input_attribute_name> [string]",
"tokens": [
{
"token": "<token> [string]",
"position": "<position_in_text> [int]",
"value": "<explanation_score> [float]"
}
]
}
]
}
]
}
]
}
back to topUpdated 3 months ago Table of Contents
Tabular Explainability
Local Explainability
Global Explainability
Regional Explainability
NLP Explainability
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/binary-classification
 Binary Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Binary ClassificationSuggest EditsBinary classification models predict a binary outcome (i.e., one of two potential classes). In Arthur, these models fall into the classification category and are represented by the Multiclass model type.
Some common examples of Tabular binary classification are:
Is this credit card transaction fraud or not?
Will a customer click on an ad or not?
Frequently, these models output both a yes/no answer and a probability for each (i.e., prob_yes and prob_no). These probabilities are then categorized into yes/no based on a threshold. In these cases, during onboarding, teams will supply their classification threshold and continuously track the class probabilities (i.e., prob_yes, prob_no).
Formatted Data in Arthur
Tabular binary classification models require three things to be specified in their schema: all predicting model attributes (or features), predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (numeric or categorical)Attribute (numeric or categorical)Probability of Prediction AProbability of Prediction BGround TruthNon-Input Attribute (numeric or categorical)High School Education34.5.95.05AMaleGraduate Degree44.1.86.14BFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their binary classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 3 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below are common examples of the required runnable predict function (that outputs two values, the probability of each potential class).
prediction to ground truth mappingExample Prediction FunctionExample Prediction Function with Transformations## Option 1:
Single Prediction Column, Single Ground Truth Column
# Map PredictedValue Column to its corresponding GroundTruth value.
# This tells Arthur that the `pred_proba_credit_default` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map_1 = {'pred_proba_credit_default' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_1,
)
## Option 2:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
pred_to_ground_truth_map_2 = {'pred_0' : 0,
'pred_1' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_2,
positive_predicted_attr = 'pred_1'
)
## Option 3:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
pred_to_ground_truth_map_3 = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# Building the Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=pred_to_ground_truth_map_3,
positive_predicted_attr = 'pred_1'
)
# example_entrypoint.py
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(x)
# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Available Metrics
When onboarding tabular classification models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
When teams onboard a binary classification model, the following metrics are automatically available in the UI (out-of-the-box). Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments are able to be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXXXUpdated 3 months ago What’s NextLearn more about how to interact with models, including binary classification, in ArthurModel OnboardingModel Monitoring Metric TypesTable of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks
 Deploying on Amazon AWS EKS
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Deploying on Amazon AWS EKSSuggest EditsThis is a guide with additional steps to help you prepare your existing Amazon Elastic Kubernetes Service (Amazon EKS) cluster for installing the Arthur platform.
Ensure the initial steps detailed Installing Arthur Pre-requisites have already been applied to the cluster.
Configure EKS EBS CSI driver
As of EKS 1.23, the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver needs to be installed explicitly. This driver allows EKS clusters to manage the lifecycle of EBS volumes for Persistent Volumes. For more information, see Amazon Docs.
If you are deploying Arthur on EKS 1.23+, you must follow the instructions on this page.
Verify that the Add-On is successfully installed by navigating to AWS Console → EKS → Cluster → Add-Ons or by running helm list -A, depending on your installation method.
Optimizing the AWS EKS StorageClass
Once the EKS EBS CSI driver is installed, you can take advantage of the gp3 StorageClass type. This StorageClass is more cost-effective and performant
than the previous gp2 StorageClass. Apply the below YAML definition to your cluster:
YAML{note}
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
annotations:
storageclass.kubernetes.io/is-default-class: "true"
name: gp3
parameters:
type: gp3
encrypted: "true" # parameter ensures created AWS EBS volumes are encrypted using AWS Managed KMS Key
kmsKeyId: <KMS Key ARN> # optional parameter ensures created AWS EBS volumes are encrypted using Customer Managed KMS Key
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
Ensure there is **_only one_** default StorageClass on the cluster. This is controlled by the `storageclass.kubernetes.io/is-default-class` annotation.
Supported AWS Service Authentication Mechanisms
If using AWS services with Arthur, such as S3 or SES, you will need to configure Arthur to authenticate with AWS.
Arthur currently supports 3 authentication mechanisms:
AWS Access Keys
Access Keys only work with S3.
If you want to use Access Keys, you must provision an IAM user and a set of keys.
Via AWS IAM, you will need to grant this user read/write access to the S3 storage bucket you plan to use with Arthur.
Selecting the Access Keys option will expand the Blob Storage section of the config page, where you will be able to enter your Access key, Secret Access key ID, and the S3 bucket.
IRSA
We recommend using IRSA to authenticate Arthur with AWS as it is the most secure and the only mechanism supporting SES.
Using this methodology will require some AWS platform work in preparation for Arthur.
You can follow these AWS docs, which will show you how to do this setup via eksctl or the AWS CLI, or you can automate this via your internal Infrastructure as Code.
The role you create will need S3 read/write privileges on the bucket you want to use with Arthur and permissions to send email via your SES entity.
Example snippets are as below:
Sample IAM policies
We provide some sample IAM policy snippets so they can be referenced easily.
Sample IAM policy for S3 access
{
"Statement": [
{
"Action": [
"s3:PutObject",
"s3:GetObject",
...
],
"Effect": "Allow",
"Resource": [
"arn:aws:s3:::<insert-s3-bucket-name>/*",
"arn:aws:s3:::<insert-s3-bucket-name>"
],
....
},
Sample IAM policy for SES access
"Action": [
"ses:SendTemplatedEmail",
"ses:SendEmail",
"ses:SendCustomVerificationEmail",
"ses:SendBulkTemplatedEmail",
"ses:SendBulkEmail",
"ses:SendBounce"
],
"Effect": "Allow",
"Resource": "*",
"Sid": "sesSendEmails"
},
This role will also need to have a trust relationship with the OIDC provider of your EKS cluster, specifying the Arthur service accounts.
See the linked docs above for a further explanation.
An example snippet of this is:
{
"Version": "2012-10-17",
"Statement": [
{
"Sid": "",
"Effect": "Allow",
"Principal": {
"Federated": "arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-2.amazonaws.com/id/ABDCEF......"
},
"Action": "sts:AssumeRoleWithWebIdentity",
"Condition": {
"StringEquals": {
"oidc.eks.us-east-2.amazonaws.com/id/ABDCEF:sub": [
"system:serviceaccount:<namespace>:arthurai-<namespace>",
"system:serviceaccount:<namespace>:arthurai-<namespace>-helm-hook"
]
}
}
}
]
}
Once this is all set up, you can pass this role to Arthur via the config page.
This sets the role in the Arthur Service Accounts specified above, which enables Arthur's pods to authenticate with AWS via the role, and the permissions you created.
Be sure to use the exact formatting shown below:
Proceed to the Blob Storage section of the Arthur config page to specify the S3 bucket
SES
To utilize AWS SES for Arthur-generated emails, you must configure IRSA as outlined in the above section.
Once this is done, navigate to the email configuration section of Arthur's config page.
Select AWS SES, then enter the region in which your SES entity is configured. As outlined above, the role associated with the cluster must have permissions on this SES entity.
If the SES entity is in the same account as your cluster, and you do not need to utilize a different role, such as for cross-account permissions, do not enter a role in the second box.
If your SES entity is in another Arthur, account, you must set up cross-account privileges between roles.
In the account of your SES entity (Account A), you must create an IAM role (Role A) that has sent email permissions to SES, as depicted above.
Role A will also need to have a trust relationship with either the account that your cluster is in (Account B), the OIDC provider on your cluster as depicted above, or the IRSA role associated with your cluster.
Additionally, the IRSA role you created above in Account B, will also need to be granted STS assume role privileges on the role you are creating in Account A.
Once all of this is set up, enter the role in the account that contains the SES entity (Account A) that the IRSA role should assume to send emails:
Updated 3 months ago Table of Contents
Configure EKS EBS CSI driver
Optimizing the AWS EKS StorageClass
Supported AWS Service Authentication Mechanisms
AWS Access Keys
IRSA
Sample IAM policies
SES
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/fundamentals
 Fundamentals
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by FundamentalsUnderstanding the Basics of Arthur Query EndpointsSuggest EditsArthur models are queried using a SQL-like wrapper, so a working query's endpoints expect a body that contains the following keys:
select (required)
from (optional)
subquery (optional)
filter (optional)
group_by (optional)
order_by (optional)
This page is going to go through these endpoints one-by-one to explain what is happening, as well as practice querying some standard information about our model.
Select
The select statement of our query allows us to choose what we want to grab out of our database. Typically, when we are grabbing information about our model, we would want to grab one of the "regular" properties within the Arthur database. These properties include:
all of the model's attributes
inference_timestamp
received_timestamp
inference_id
partner_inference_id
ground_truth_timestamp (if ground truth is included)
batch_id (if batch model)
Similar to SQL, you can also select all of these attributes at once using the * string
We can practice running the following query to select inference_id
Pythonquery = {
"select": [
{
"property": "inference_id"}
]
}
response = arthur_model.query(query)
response
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b'},
{'inference_id': '3392ac21-3cf9-48ad-9e07-e0343b76cc50'},]
We can see that this grabbed all of the inference ids from the inference dataset. It is important to note that the inference dataset is the default dataset to select from. If we wanted to grab more than one parameter at once in the query, we could do the following:
Pythonquery = {
"select": [
{
"property": "inference_id"},
{
"property": "inference_timestamp"}
]
}
response = arthur_model.query(query)
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b',
'inference_timestamp': '2022-11-10T15:22:14.959Z'},
{'inference_id': '3392ac21-3cf9-48ad-9e07-e0343b76cc50',
'inference_timestamp': '2022-11-10T15:23:14.959Z'},]
📘Transformations in Select FunctionsTeams often ask about running transformations in select functions, such as addition, subtraction, or "OR" and "AND" statements. More informationn about creating logic can be found in the Transformation Functions page.
Unique Model Types
Object Detection
Computer Vision models with an Output Type of Object Detection have some special fields you can use when querying. Bounding boxes are sent using the following form: [class_id, confidence, top_left_x, top_left_y, width, height]. While the fields aren't named when sending data, you can access these nested fields when querying.
Here we can see:
Pythonquery = {
"select": [
{"property": "inference_id"},
{"property": "objects_detected"}
]
}
arthur_model.query(query)
response
Or, to grab each value within the bounding box individually:
Pythonquery = {
"select": [
{"property": "inference_id" },
{"property": "objects_detected.class_id" }
]
}
arthur_model.query(query)
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b',
'class_id': '0'},
{'inference_id': '3392ac21-3cf9-48ad-9e07-e0343b76cc50',
'class_id': '1'},]
Generative Text with Token Likelihoods
TokenLikelihoods attributes yield two queryable columns for that attribute with suffixes “_tokens” and “_likelihoods” appended to the attribute's name. For example, a model with a TokenLikelihoods attribute named summary_token_probs yields two queryable columns: summary_token_probs_tokens and summary_token_probs_likelihoods which represent an array of the selected tokens and an array of their corresponding likelihoods.
Pythonquery =
{"select": [
{"property": "summary_token_probs_tokens"},
{"property": "summary_token_probs_likelihoods"}
]}
response[{ "summary_token_probs_likelihoods": [
0.3758265972137451,
0.6563436985015869,
0.32000941038131714,
0.5629857182502747],
"summary_token_probs_tokens": [
"this",
"is",
"a",
"summary"] }]
Property Types
There are two property types within Arthur.
"regular" properties - listed above. These are properties available for any table queried.
"enriched" properties - you must specify these by name to include them in the response and use the from value enriched:
anomaly_score
lime_importance
shap_importance
We'll learn more about querying from different datasets next.
From
In the example above, we could pull data from the inference dataset. The inference dataset is the dataset that is most often queried and therefore set as the default. However, it is only one of the datasets from where we can pull data. All of our sources include:
inference - The latest raw inference data sent to the platform. This is the default.
enriched - Every value from the inference data, with additional fields for anomaly scores and explanations. This data has some insert latency compared to the raw table.
reference - The reference data set uploaded for the model.
We will talk about the enriched dataset later. So, we can practice pulling an attribute from our reference dataset below
Pythonquery = {
"select": [
{
"property": "age"}
]
, "from": "reference"
}
response = arthur_model.query(query)
response[{'age': 23},
{'age': 48}]
Filter
The next feature we can add to queries within Arthur is filters. These allow us to specify a subset of data we are interested in from a database and grab only them. This filter command works similarly to where in traditional SQL query queries. We create this specification by using comparators to create rules within the data.
Available Comparators in Arthur
Numerical Comparison
eq - Filters where the property field equals the value field.
ne - Filters where the property field is not equal to the value field.
lt - Filters where the property field is less than the value field. Only valid for number values.
gt - Filters where the property field is greater than the value field. Only valid for number values.
lte - Filters where the property field is less than or equal to the value field. Only valid for number values.
gte - Filters where the property field is greater than or equal to the value field. Only valid for number values.
Examplequery = {
"select": [
{
"property": "*"
}
],
"filter": [
{
"property": "inference_id",
"comparator": "eq",
"value": "509f34eb-19ae-4a18-b454-6517fed4e76b"
}
]
}
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b'},
{'age': '22'},
{"inference_timestamp":"2020-07-22T10:01:23Z"},
{"attr1":"something}]
The numerical comparators can also be used to investigate
Pythonquery = {
"select": [
{ "property": "*" },
{"property": "anomaly_score"}
],
"from": "enriched",
"filter": [
{"property": "inference_timestamp",
"comparator": "gte",
"value": "2020-07-22T10:00:00Z" },
{ "property": "inference_timestamp",
"comparator": "lt",
"value": "2020-07-22T11:00:00Z"
}
]
}
response[{ "inference_id": "0001",
"attr1": "something",
"anomaly_score": 0.34,
"inference_timestamp": "2020-07-22T10:01:23Z"
},
{"inference_id": "0002",
"attr1": "something",
"anomaly_score": 0.67,
"inference_timestamp": "2020-07-22T10:02:55Z" } ]
In
in - Filters where the property field is equal to any value in a list of possible values
This comparator is typically used in count or rate functions to identify values that are in a set list of expected values.
Pythoncategory_list = ['no college', 'some college', 'bachelors', 'masters','phd',null]
query= {"select":[{
"function": "rate",
"alias": "OldCatRate",
"parameters": {
"property": feature_of_interest,
"comparator": "in",
"value":category_list
}
}
]}
arthur_model.query(query)
response[{"OldCatRate":0.89}]
Like
like - Filters where the property field is like the value field. This filter is only valid for property types of unstructured text.
Python
Null Values
NotNull - Filters where the property field is not null. The value field should be empty.
IsNull - Filters where the property field is null. The value field should be empty.
Python## Rate of Null Values in Feature
query = {"select": [
{"function": "rate",
"alias": "MarriageNullRate",
"parameters": {
"property": "Marriage",
"comparator": "IsNull",
"value":""
}
}
]
}
response = arthur_model.query(query)
response[{"MarriageNullRate":0.89}]
Group By
The group_byendpoint is used in queries to group rows based on one or more columns. It allows for data aggregation by specifying the criteria for grouping and can be used in conjunction with any of the different function types we allow in Arthur queries.
These function types are further discussed in Querying Functions, but at a high level, they contain:
Default Performance Metrics: All default performance metrics available within Arthur
Transformation Functions: Functions done on the properties selected
Aggregation Functions: Provides common data aggregation measures like average, sum, count, quantiles, etc.
Composing Functions: This enables teams to create their own function logic
Pythonquery = {
"select":[
{"function":"count",
"alias":"count"},
{"property":"age"}],
"filter":[
{"property":'income',
"comparator":"gt",
"value":30000}],
"group_by":[
{"property":"age"}],
}
response[{"count":2500},{"age":25}]
Order By
The order_by endpoint enables teams to order the outputs of their query. Within the order_by command teams will specify what property they would like to sort the output by and in which direction. The options for direction are asc and desc for ascending and descending, respectively.
Pythonquery = {
"select":[
{"property":"age"},
{"property":"income"}
],
"order_by": [
{
"property": "income",
"direction": "desc"
}
}
arthur_model.query(query)
Subquery
Subqueries are a powerful feature of modern SQL. The query endpoint can support subqueries via the subquery field in the request body. The format of the subquery is exactly the same as the full request body and can even support recursive subqueries! Here are some helpful examples that show how to use them.
Concise Queries with Subqueries
Sometimes you may have a calculation that must be aggregated in multiple ways. One option would be to repeat the calculation in each aggregation’s select, which can lead to lots of repeated JSON. Subqueries can be used to reduce duplicated expressions. In this example, we use a subquery to square the property Home_Value, then aggregate with min, max, and avg without repeating the calculation.
query = {
"select": [
{
"function": "max",
"alias": "max",
"parameters": {
"property": {
"alias_ref": "hv_squared"
}
}
},
{
"function": "min",
"alias": "min",
"parameters": {
"property": {
"alias_ref": "hv_squared"
}
}
},
{
"function": "avg",
"alias": "avg",
"parameters": {
"property": {
"alias_ref": "hv_squared"
}
}
}
],
"subquery": {
"select": [
{
"function": "multiply",
"alias": "hv_squared",
"parameters": {
"left": "Home_Value",
"right": "Home_Value"
}
}
]
}
}
response[{"avg": 33413668226.974968,
"max": 17640000000000,
"min": 0 }]
Subqueries for grouping
Subqueries can also be used to perform operations on grouped data. In this example, we get the count of the inferences in each batch in the subquery, then average those counts.
Python{
"select": [
{
"function": "avg",
"alias": "avg_count",
"parameters": {
"property": {
"alias_ref": "batch_count"
}
}
},
{
"function": "count",
"alias": "total_batches"
}
],
"subquery": {
"select": [
{
"function": "count",
"alias": "batch_count"
},
{
"property": "batch_id"
}
],
"group_by": [
{
"property": "batch_id"
}
]
}
}
response[{"avg_count": 5930.2558139534885,
"total_batches": 86}]
Updated 3 months ago Table of Contents
Select
Unique Model Types
Property Types
From
Filter
Available Comparators in Arthur
Group By
Order By
Subquery
Concise Queries with Subqueries
Subqueries for grouping
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/query-quickstart
 Common Queries Quickstart
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Common Queries QuickstartSuggest EditsTo access information about a model's performance, drift, bias, or other enabled enrichments, write a query object and submit it with the Arthur SDK using arthur_model.query(query)
For a general overview of this endpoint, including a more thorough description of its rules, power, and customizability, see the Fundamentals.
In each of the following examples, let our model be a binary classifier, and let GT1 and PRED1 be the names of our model's ground truth attribute and predicted value.
Accuracy
This is usually the simplest way to check for classifier performance. We can fetch a model's accuracy rate by querying a select on the function accuracyRate using the typical threshold 0.5.
Given the following query:
PythonGT1 = 'gt_isFraud'
PRED1 = 'pred_isFraud'
query = {
"select": [
{
"function": "accuracyRate",
"parameters":
{
"threshold" : 0.5,
"ground_truth_property" : GT1,
"predicted_property" : PRED1
}
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'accuracyRate': 0.999026947368421}]
Accuracy by batch
To expand the accuracy query by batch, add the batch_id property to the query's select, and add a group_by to the query using batch_id.
Given the following query:
PythonGT1 = 'gt_isFraud'
PRED1 = 'pred_isFraud'
query = {
"select": [
{
"function": "accuracyRate",
"parameters":
{
"threshold" : 0.5,
"ground_truth_property" : GT1,
"predicted_property" : PRED1
}
},
{
"property": "batch_id"
}
],
"group_by": [
{
"property": "batch_id"
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'accuracyRate': 0.999704, 'batch_id': 'newbatch3'},
{'accuracyRate': 0.999744, 'batch_id': 'newbatch0'},
{'accuracyRate': 0.992952, 'batch_id': 'newbatch19'},
{'accuracyRate': 0.999616, 'batch_id': 'newbatch5'},
{'accuracyRate': 0.999144, 'batch_id': 'newbatch6'},
...]
Batch IDs
Querying accuracy by batch includes the batch_id values in the query result. But to query the batch_ids on their own, only select and group_by the batch_id.
Given the following query:
Pythonquery = {
"select": [
{
"property": "batch_id"
}
],
"group_by": [
{
"property": "batch_id"
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'batch_id': 'newbatch19'},
{'batch_id': 'newbatch18'},
{'batch_id': 'newbatch13'},
{'batch_id': 'newbatch12'},
{'batch_id': 'newbatch16'},
...]
Accuracy (single batch)
To query the accuracy for only one batch, add a filter to the query according to the rule batch_id == BATCHNAME
Given the following query (for a specified batch name):
PythonGT1 = 'gt_isFraud'
PRED1 = 'pred_isFraud'
BATCHNAME = "newbatch19"
query = {
"select": [
{
"function": "accuracyRate",
"parameters":
{
"threshold" : 0.5,
"ground_truth_property" : GT1,
"predicted_property" : PRED1
}
},
{
"property": "batch_id"
}
],
"group_by": [
{
"property": "batch_id"
}
],
"filter": [
{
"property": "batch_id",
"comparator": "eq",
"value": BATCHNAME
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'accuracyRate': 0.992952, 'batch_id': 'newbatch19'}]
Confusion Matrix
A confusion matrix counts the number of true positive, true negative, false positive, and false negative classifications; knowing these values is usually more useful than just accuracy when it is time to improve your model.
To query a confusion matrix, we use the confusionMatrix function in our query's select.
📘For the confusionMatrix function, the ground_truth_property and predicted_property parameters are optional.
Given the following query:
Pythonquery = {
"select": [
{
"function": "confusionMatrix",
"parameters":
{
"threshold" : 0.5
}
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'confusionMatrix':
{'false_negative': 4622,
'false_positive': 0,
'true_negative': 4745195,
'true_positive': 183}}]
Confusion Matrix (single batch)
As we did with accuracy, to get a confusion matrix for a single batch we add the property batch_id to the query's select, add a group_by using batch_id, and then add a filter according to the rule batch_id == BATCHNAME
Given the following query (for a specified batch name):
PythonBATCHNAME = 'newbatch19'
query = {
"select": [
{
"function": "confusionMatrix",
"parameters":
{
"threshold" : 0.5
}
},
{
"property": "batch_id"
}
],
"group_by": [
{
"property": "batch_id"
}
],
"filter": [
{
"property": "batch_id",
"comparator": "eq",
"value": BATCHNAME
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'batch_id': 'newbatch19',
'confusionMatrix':
{'false_negative': 1762,
'false_positive': 0,
'true_negative': 248238,
'true_positive': 0}}]
Confusion Matrix (by group)
Instead of querying for metrics and grouping by batch, we can group by other groupings as well. Here, we use the model's non-input attribute race so that we can compare model performance across different demographics. To do this, we add the group name race to our query's select and to its group_by
Given the following query:
PythonGROUP = 'race'
query = {
"select": [
{
"function": "confusionMatrix",
"parameters": {
"threshold" : 0.5
}
},
{
"property": GROUP
}
],
"group_by": [
{
"property": GROUP
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'confusionMatrix': {'false_negative': 1162,
'false_positive': 0,
'true_negative': 1184707,
'true_positive': 44},
'race': 'hispanic'},
{'confusionMatrix': {'false_negative': 1145,
'false_positive': 0,
'true_negative': 1186659,
'true_positive': 49},
'race': 'asian'},
{'confusionMatrix': {'false_negative': 1137,
'false_positive': 0,
'true_negative': 1187500,
'true_positive': 38},
'race': 'black'},
{'confusionMatrix': {'false_negative': 1178,
'false_positive': 0,
'true_negative': 1186329,
'true_positive': 52},
'race': 'white'}]
Predictions
Here we aren't querying any metrics - we are just accessing all the predictions that have output by the model.
Given the following query:
PythonPRED1 = 'pred_isFraud'
query = {
"select": [
{
"property": PRED1
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'pred_isFraud_1': 0.005990342859493804},
{'pred_isFraud_1': 0.02271116879043313},
{'pred_isFraud_1': 0.15305224676085477},
{'pred_isFraud_1': 0},
{'pred_isFraud_1': 0.03280797449330532},
...]
Predictions (average)
To only query the average value across all these predictions (since querying all predictions and then averaging locally can be slow for production-sized query results), we only need to add the avg function to our query's select, with our predicted value PRED1 now being a parameter of avg instead of a property we directly select.
Given the following query:
PythonPRED1 = 'pred_isFraud'
query = {
"select": [
{
"function": "avg",
"parameters": {
"property": PRED1
}
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'avg': 0.016030786000398464}]
Predictions (average over time)
To get the average predictions on each day, we add the function roundTimestamp to our select using a time_interval of day - this groups the timestamp information according to day instead of options like hour or week. Then, we add a group_by to the query using the alias (DAY) specified in the roundTimestamp function.
Given the following query:
PythonPRED1 = 'pred_isFraud'
query = {
"select": [
{
"function": "avg",
"parameters": {
"property": PRED1
}
},
{
"function": "roundTimestamp",
"alias": "DAY",
"parameters": {
"property": "inference_timestamp",
"time_interval": "day"
}
}
],
"group_by": [
{
"alias": "DAY"
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'avg': 0.016030786000359423, 'DAY': '2022-07-11T00:00:00Z'},
{'avg': 0.018723459201003300, 'DAY': '2022-07-12T00:00:00Z'},
{'avg': 0.014009919280009284, 'DAY': '2022-07-13T00:00:00Z'},
{'avg': 0.016663649020394829, 'DAY': '2022-07-14T00:00:00Z'},
{'avg': 0.017791902929210039, 'DAY': '2022-07-15T00:00:00Z'},
...]
Updated 3 months ago Table of Contents
Accuracy
Accuracy by batch
Batch IDs
Accuracy (single batch)
Confusion Matrix
Confusion Matrix (single batch)
Confusion Matrix (by group)
Predictions
Predictions (average)
Predictions (average over time)
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/bias-mitigation
 Bias Mitigation
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Bias MitigationPost Process Technique to Determine the Fairest ThresholdSuggest EditsIf you are interested in bias mitigation capabilities, we’re happy to discuss your needs and what approaches would work best for you. Within Arthur Scope, we offer postprocessing methods; we encourage exploring alternate (pre- or in-processing) methods if your data science team has the bandwidth to do so.
Our currently available postprocessing method for use is the Threshold Mitigator. It automatically evaluates for demographic parity, equalized odds, and equal opportunity constraints.
Enabling Bias Mitigation will automatically train a mitigation model for all attributes marked as.monitor_for_bias=True, for the constraints of demographic parity, equalized odds, and equal opportunity.
The Onlys of Bias Mitigation
Bias Mitigation is an enrichment in Arthur with a few only's.
Bias Mitigation is only available for binary classification models
It can only be enabled if at least one model attribute is marked as.monitor_for_bias=True
It is the only enrichment that is only available in the Python SDK. This also means that it is the only enrichment you run in a notebook each time you want to use it.
Bias Mitigation with the Python SDK
As mentioned above, bias mitigation is only available through our Python SDK. Here is an example notebook that we have put together on how to use the bias mitigation capabilities: Bias Mitigation Notebook on Arthur GitHub
Understanding the Algorithm
To learn more about the algorithm used for bias mitigation. Please refer to the Arthur Algorithms documentation section.Updated 3 months ago What’s NextLearn about enabling enrichments in the model onboarding sectionEnabling EnrichmentsTable of Contents
The Onlys of Bias Mitigation
Bias Mitigation with the Python SDK
Understanding the Algorithm
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/welcome-to-platform-administration
 Welcome to Platform Administration
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Welcome to Platform AdministrationSuggest EditsThe second half of our documentation is dedicated to Platform Administration resources.
Platform administration isUpdated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/examples
 Examples
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ExamplesGet started with Arthur Scope using one our many examplesSuggest EditsThroughout our website, documentation, public GitHub, and YouTube channel, we are dedicated to providing useful and intuitive examples of how to use the Arthur platform.
Core Example Groups: We have broken out our core examples resources into four sections:
Model Onboarding
Notebook Tutorials
Interested in something else?: Here are some of our other most commonly referred to references:
Arthur Glossary
Arthur YouTube Channel
📘Can't Find What You're Looking For?If you can't find what you're looking for, please let us know either through the feedback section in the top right corner of this documentation website or submit a page review at the bottom of this page.
Model Onboarding
Getting Started - Quickstart Guides
ExampleResource PageQuickstart (Tabular)QuickstartQuickstart NLPNLP OnboardingQuickstart Computer VisionCV OnboardingQuickstart Generative Text (LLM)Generative TextOnboarding Model with APIRegistering A Model with the API
Model Input / Output Type
Model TypeBatchStreamingTabular Binary ClassificationCredit Risk Batch ExampleCredit Risk ExampleTabular Multi-class ClassificationTabular RegressionBoston Housing ExampleNLP Binary ClassificationNLP Multi-class ClassificationMedical Transcript ExampleNLP RegressionNLP - Generative Text (LLM)OpenAI ExamplesCV Binary ClassificationSatelite Image ExampleCancer Classification ExampleCV Multi-class ClassificationCV RegressionCV Object DetectionMars Rover ExampleTime Series Ranked List (Recommender System)
Partnerships and Integrations
NameResource PageSpark MLSpark MLSageMaker Data CaptureSageMakerLangchainLangchain
Extra Model Onboarding Tutorials
NameDescriptionResource Page or NotebookTesting Arthur Explainer LocallyBefore enabling explainability, teams can test their explainer locally. Ensure that it will work when onboarded to Arthur.Test Explainability LocallyConnecting Arthur Model Object in NotebookTeams must first connect to the Arthur platform to run any functionality in a notebook.Creating a Connection to ArthurOnboarding Historical Data to ArthurWant to backfill your new model on Arthur with existing data? Onboard historical inferencesSending Historical DataModel VersioningExample of creating mutliple model versionsModel Versioning Notebook
Notebook Tutorials
Metrics and Alerting
NameNotebookCreate Custom Metrics in a NotebookComing SoonCreate Custom Alerts in a NotebookComing SoonDeleting Alerts in a NotebookComing Soon
Enrichments
NameDescriptionNotebookBias MitigationRun our bias mitigation enrichment in a notebookBias Mitigation Notebook
Querying Guides
NameDescriptionNotebook or Resource PageQuery Guide NotebookOverview of querying in a notebookQuery Jumpstart NotebookQuerying ExplainabilityQuery multiple levels of explainability in a notebookQuerying ExplainabilityQuerying Data DriftQuery data drift comparing different distributionsQuery Data DriftUpdated 2 months ago Table of Contents
Model Onboarding
Getting Started - Quickstart Guides
Model Input / Output Type
Partnerships and Integrations
Extra Model Onboarding Tutorials
Notebook Tutorials
Metrics and Alerting
Enrichments
Querying Guides
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/ranked-list-output-onboarding
 Time Series Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Time Series OnboardingSuggest EditsThis page walks through the basics of setting up a time series input model and onboarding it to Arthur Scope to monitor performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur Scope.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>")
Registering a Time Series Model
Each time series model is created with a name and with input_type = InputType.TimeSeries. Here, we register a time series model:
Pythonarthur_model = arthur.model(name="RecSysQuickstart",
input_type=InputType.TimeSeries,
model_type=OutputType.RankedList)
Formatting Reference/Inference Data
Column names can contain only alphanumeric and underscore characters.
Time series data can be uploaded to Arthur either in a DataFrame or a JSON file. Typically, a JSON file is a more natural formatting for time series data. For a time series model tracking credit card balance over time as input, the reference data might look like this:
JSON{
"reference_data": {
"credit_card_balance": [
{
"timestamp": "2023-10-05T00:00:00Z",
"value": 3004.18
},
{
"timestamp": "2023-10-06T00:00:00Z",
"value": 150.19
}
],
"id": "6euQxGJai11qr0gENGgvgh",
"account_id": "8klQSGJil78qr4gLJKklsy"
},
... // more inferences here
}
Data Requirements
Arthur requires that all times will be present in a given series according to a regular interval (eg. one value each day).
There is an upper bound of 500 timestamps in a single time series inference.
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save(), you can call arthur_model.review() on the model schema to check that your data was parsed correctly in your call to arthur_model.build().
For a time series model, the model schema should look like this:
Python		 name
stage
value_type
categorical
is_unique
0
time_series_attr
PIPELINE_INPUT
TIME_SERIES
False
True
1
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using arthur_model.review(), you are finished registering your model and its attributes - so you are ready to complete onboarding your model.
See this guide for further details on how to save your model, send inferences, and get performance results from Arthur. These steps are the same for time series models as for models of any InputType and OutputType .Updated about 2 months ago Table of Contents
Getting Started
Registering a Time Series Model
Formatting Reference/Inference Data
Data Requirements
Reviewing the Model Schema
Finishing Onboarding
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/custom
 User-Defined Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by User-Defined MetricsTrack and communicate the unique ways your internal team or external stakeholders define performanceSuggest EditsBeyond Arthur's UI capabilities and APIs, Arthur's Python SDK has a built-in Query Functionality. This SQL-like query structure allows teams the ability to turn any functions they create into alert-able metrics within the Arthur platform.
Pieces of User-Defined Metrics
There are three pieces to define when creating user-defined metrics:
Metric Name: The name refers to how the user identifies or calls the metric.
Metric Type: The Arthur platform has four unique metric types. These are:
Metric TypePerformanceDataDriftDataBoundModelOutput
Typically, Arthur infers the type of metric from the function provided. However, for more advanced metrics that Arthur cannot infer or for teams that want to ensure a specific metric type, teams should define the specific metric type.
Arthur Query Function: Finally, the most important aspect of defining a metric is creating the mathematical function the metric will track. These functions are built with the Arthur Query structure. More about building out this Arthur Query function can be found below:
Building an Arthur Query Function
A more in-depth querying guide can be found in the query section. However, there are some key things to keep in mind:
The query must be built with the information contained within Arthur (per inference). Additional information beyond model inputs (features) and outputs (predictions) can be added to Arthur as Non-Input Attributes.
The query should return a single value. For example, the query should not return a row for each inference, time-series data, or score for multiple attributes.
The query should generally not include filters unless it is to define a very specific segment you wish to track (and not a further filter). This is because filters can easily be defined when evaluating a metric. Keeping the metric definition general allows different segmentations to be easily applied.
The query may include parameters, which are denoted by {{ param_name }} template values in the query definition and have corresponding entries in the parameters field on the metric definition.
Custom Data Drift Metrics
Of note for teams looking to create custom metrics for data drift, data drift uses a special query structure. So, these metrics would need to be defined with this special structure. Teams also need to add in is_data_drift = True to their definition.
Defining Metrics Examples
Using Python SDK
We typically recommend that teams create custom metrics within a notebook with our Python SDK. This is because teams can easily craft and validate their queries in a notebook before onboarding them to Arthur.
Follow this notebook example to learn more: Coming Soon
Using API
You can define a custom metric for your model by sending a POST request to the dedicated metrics endpoint at /models/{model_id}/metrics. Check out the API Reference guide for the full specification.Updated 3 months ago Table of Contents
Pieces of User-Defined Metrics
Building an Arthur Query Function
Defining Metrics Examples
Using Python SDK
Using API
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/okta
 OIDC
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by OIDCSuggest Edits📘SSO configurations are only supported in on-prem Arthur installations
This page provides a walk-through for how to configure your Arthur installation to work with an OIDC compatible IdP.
In order to complete this guide, you need administrator access to your IdP and access to your Arthur installation's admin console configuration. Additionally, you will either need access to the Arthur superadmin user or be able to assume a role in your IdP to give yourself RBAC management permissions in Arthur.
This guide will walk through the following steps:
Configure the IdP user groups and OIDC token claims
Configure the IdP OIDC Settings
Configure Arthur to work with your IdP
Apply the Arthur IdP YAML configuration
Create organization user roles to match the IdP user groups
Test Access
Cleaning Up
1. Configure the IdP user groups and OIDC token claims
In order to properly map users to permissions, Arthur requires a claim in your OIDC JSON Web Token (JWT) that contains information about the group memberships of the user. Each group in the IdP should correspond to a role in Arthur's {doc}custom_rbac permission system.
This process can vary depending on your IdP, but most IdP's should have a user grouping mechanism, and a mechanism to configure attributes in the JWT claims. For example using Okta, admins can configure the JWT claims to include group information under their account's Security -> API -> default -> Claims then the "Add Claim" button. From the popup, give the claim a name, "Groups" in this example, set the "Include in token type" to "Access Token", select "Value type" as "Groups", and include a "Matches regex" filter to select the groups to include:
Retrieving an example OIDC token is IdP-specific and may involve completing the sign-in flow via a script or API client. An alternative can be to use a 3rd party like https://oidcdebugger.com, but that will require whitelisting https://oidcdebugger.com/debug as a valid redirect URL for your Arthur SSO app (this could be enabled temporarily for debugging, then removed). Here is an example JWT after setting the group claims field:
JSON{
"iss": "https://dev.okta.com/oauth2/default",
"aud": "api://default",
"scp": [
"openid"
],
"Groups": [
"idp-admin",
"org-1-model-owner",
"org-2-model-owner"
],
"FirstName": "John",
"LastName": "Doe",
"Login": "[email protected]"
}
As the example token shows, the user's groups in the IdP are populated as a string list in the "Groups" field in the token. Arthur will use this list of groups to match the user to the corresponding roles in Arthur by name.
2. Configure the IdP OIDC Settings
In order for your IdP to speak to Arthur, it needs to know where to find it. Enter the following URL in your IdP's configuration to whitelist Arthur's callback endpoint (sign-in redirect
URL): https://<YOUR ARTHUR HOST>/login/callback.
Additionally, the IdP will need to know what OIDC protocol to speak with Arthur. Today, Arthur supports two protocol flows:
implicit
PKCE
Both flows are intended to be used with "Single Page Applications" or SPAs. Follow the configuration for your IdP that matches one of those two flows with SPAs. Additionally, note the following settings from your IdP in order to use in the Arthur configuration below:
Client ID
Resource ID (if available)
OIDC flow (PKCE or implicit)
Audience (value that is set in the token by the IdP)
3. Configure Arthur to work with your IdP
Next, Arthur needs to know how to handshake with your IdP. To do that, Arthur requires the following
information:
Your IdP's discovery URL, typically in the format <IdP path>/.well-known/openid-configuration URL that contains the relevant endpoints for your IdP.
{note}If this page isn't accessible to Arthur due to CORS or other restrictions, the values can be provided manually.
One or more IdP administrator user groups that will be paired to global custom roles in Arthur
(see here for a description of what these are for)
An understanding of your OIDC token claims (values) and how to parse user information out of it
The four configuration values captured above from your IdP
With that information available, it is possible to fill out Arthur's IdP configuration YAML. The next subsections explain each subsection of the Arthur YAML configuration, and is followed by some complete examples further down.
Configuring the IdPs discovery URL
Almost all OIDC IdP's have accessible discovery URLs, but some do not provide CORS support for them, so their contents need to be filled out manually. To support either option, Arthur has two configurations that can be used, discoveryBaseURL or endpointOverrides. If your IdP has CORS restrictions, see Appendix B for additional setup that is required.
YAML# use this option if your IdP has an accessible discovery URL
# IMPORTANT: don't include the /.well-known/openid-configuration suffix!!
# for example, if the full URL is https://<HOST>/oauth2/default/.well-known/openid-configuration
# only specify: https://<HOST>/oauth2/default
discoveryBaseURL: "https://<HOST>/oauth2/default"
# use this option if your IdP has CORS restrictions on the discovery URL, otherwise comment this out.
# fill in the values manually from the discovery endpoint's contents
endpointOverrides:
issuer: "issuer string for the IDP"
authorization_endpoint: "URL ending in /authorize"
token_endpoint: "URL ending in /token"
jwks_uri: "URL ending in /keys"
userinfo_endpoint: "URL ending in /userinfo"
# note not all IdPs will have the following endpoints, fill in as many as you can
end_session_endpoint: "URL ending in /logout"
device_authorization_endpoint: "URL ending in /devicecode"
revocation_endpoint: "URL ending in /revoke"
introspection_endpoint: "URL ending in /introspect"
registration_endpoint: "URL ending in /clients"
Configure the Arthur Global Roles
Arthur has the ability to create roles for the cluster administrators during the configuration of the IdP. These roles are often needed by admins to configure RBAC and create organizations for other users in the system. See {ref}creating_global_roles_in_arthur_config for a deep dive on how to use global roles.
📘Global Roles in the UIGlobal Roles only provide access for the Arthur API, and therefore, cannot be used to grant universal access to workflows in the Arthur UI. A user that belongs to a group that maps to a Global Role will not be able to see all organizations in the UI, nor enter and read/write data in a specific organization unless they are bound to a role within that organization through an Organization-Scoped Role.
This section of the YAML config is under the globalRoleDefs field. It accepts a list of role definitions that will be created when the configuration is applied. The names of the roles in this section must match the user groups in your IdP in order to be able to assume them in Arthur.
YAML
globalRoleDefs:
# Here we can specify a list to define multiple global roles
- name: "idp-admin" # change this name to match the cluster administrator group name in your IdP
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
model:
- read
- write
- delete
Parsing the IdP JWT claims
In order for Arthur to communicate with your IdP, it needs to understand the format of the JWT claims your IdP uses. This section of the config falls under the accessTokenValidation YAML field. This section is designed to be flexible to support a variety of claim formats, so it has a lot of options. At its core, the goal is to tell Arthur how to be able to extract the following information from the claims:
user roles/groups
first name
last name
email
user ID
Each field has a corresponding YAML configuration that defines where to find the information in the JWT claims. For example:
YAMLclaimMapping:
firstName: FirstName
This configuration tells Arthur that it can find the user's first name under the "FirstName" claim in the JWT. Such a token might look like this:
JSON{
"iss": "https://dev.okta.com/oauth2/default",
"aud": "api://default",
"scp": [
"openid"
],
"Groups": [
"idp-admin",
"org-1-model-owner",
"org-2-model-owner"
],
"FirstName": "John",
"LastName": "Doe",
"Login": "[email protected]"
}
More examples of how to parse user information out of the JWT claims can be
found below.
Full Configuration Examples
Here is an example of a full configuration, combining each section described above.
YAMLversion: v2
kind: OIDC
config:
# discovery URL without the /.well-known/openid-configuration suffix
discoveryBaseURL: https://example.com/oauth2/default
# Either "implicit" or "PKCE"
flowType: PKCE
# client ID from your IdP for the Arthur SSO application
clientID: "client id string"
# optional: resource ID from your IdP for the Arthur SSO application if required by the IdP
resourceID: ""
authorizeScopes:
- openid
# required for OIDC
# use this section to define global roles
# one example role would be to give the cluster admin permissions to create organizations and manage custom roles
globalRoleDefs:
- name: "iam-admin"
# change this to match the user group name in your IdP for administrators
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
# this section describes how to parse the user information out of the JWT returned from the IdP
# this is used by Arthur to understand who the user is and what their roles are
accessTokenValidation:
type: JWT
# only JWT is supported today
# fields in the token Arthur will use to extract the authentication information
claimMapping:
roles: Groups
# this is telling Arthur to look in the "Groups" claim to find the list of user's roles
userID: EmployeeID
username: Login
firstName: FirstName
lastName: LastName
email: Login
# one or more audiences to validate, this should match your IdP's configuration
audience:
- api://default
# optional override signature algo
# signatureAlgo: RS256
Here is an additional descriptions of the fields that need to be set in the config YAML above:
discoveryBaseURL: This is the base URL for your Identity Provider. Your IdP should have
a /.well-known/openid-configuration endpoint and the
discoveryBaseURL is simply that url minus the /.well-known/openid-configuration part.
flowType: We support both implicit and PKCE flows. Consult with your team to decide which OIDC flow type is right
for your organization.
clientID: When you create the application integration in your IdP, a Client ID will be provided to you. Paste that
value into this field.
resourceID: This is optional. If your IdP gives you a Resource ID when creating your application integration, paste
the value here.
claimMapping: We extract various pieces of authentication information from the provided JWT access token. However,
there is no common standard for
how these pieces of information should be formatted in the token. For us to extract this information from the token,
we need you to explicitly tell Arthur
where this information is stored in the token. For example, a username could be stored in a field called username
or login or email or userID.
In order to get this user information, a mapping needs to be provided for the following items
roles: This is the field for where either a single authorization role or a list of authorization roles will be
specified.
Note that this is not where you paste a list of roles, this is the name of a field in the JWT where the
user's roles are specified.
For help with role definitions, see {doc}custom_rbac.(Required)
userID: This is the field for a unique identifier for the user; this is frequently the same as username
and/or email. (Optional, omit if empty)
username: This is the field for the user's unique username; this is frequently the same as username
and/or email. (Optional, omit if empty)
firstName: This is the field for the user's first name. (Optional, omit if empty)
lastName: This is the field for the user's last name. (Optional, omit if empty)
email: This is the field for the user's email. (Optional, omit if empty)
audience: This is part of the JWT standard. The aud field for any JWT you create must be a value in this list. For
example in the above configuration, any token that has an aud field that is not set to api://defaults, the token
will be automatically rejected by Arthur. If you are having trouble finding this value, it is frequently the same as
your resourceID. Remember to format this as a list, not a single value.
{note}If your IdP has CORS restrictions see
[Appendix B](#appendix-b-setup-for-idps-with-cors-restrictions) below for a workaround.
4. Apply the Arthur IdP YAML configuration
Once you have your YAML configuration file ready, you need to add it to your Arthur installation. With the Arthur admin console open, navigate to the "Use a 3rd Party Global Identity Provider" section and select "OIDC". This will expose a text box for you to paste the YAML config file assembled above. When pasting, make sure whitespace is preserved and the YAML document has consistent spacing (do not mix tabs and spaces). Here is a screenshot of the config section:
Once you have added your config files, scroll to the bottom and click "Save" to save the config. Then go to the latest
version and click "Deploy" to roll out the change to the cluster.
5. Create organization user roles to match the IdP user groups
In order to complete this section, you will need access to the Arthur superadmin user credentials set during your install, or you will need to be able to assume the role defined in the Arthur IdP config YAML created above in the globalRoleDefs section.
In order to use the API example linked below, you will need a Bearer token (authentication token) to include with your API request. There are a few options available to retrieve a token:
Retrieve a global role token directly from your IdP - Most IdPs will have a method to retrieve tokens for users. Some companies make scripts or APIs that allow retrieving a token. If your IdP does not have an automated method to retrieve a token, you can try setting up a tool like https://oidcdebugger.com (this may involve adding https://oidcdebugger.com as an allowed URL in your IdP settings).
Retrieve a global role token from your browser cookies - If you sign in to Arthur as a user with a global role, the UI will not be fully functional, but it will have a valid access token in the cookies. If you navigate to your browser's developer console and then go to the Application Storage/Cookies section, you should see a cookie like ArthurAuth0, which is your authentication token. Note: if your user has a large number of groups, there may be multiple cookies of the form ArthurAuthN. In this case your token was too large to fit in the browser cookie, so it had to be split. You can assemble the full token by concatenating the values of the ArthurAuthN cookies in order.
Use the /login API endpoint with the superadmin user's credentials set during the Arthur install (only available on-prem).
Using either of those credentials, we can use the Arthur API to define roles in Arthur that match the user group names in your IdP. See the {ref}creating_organization_roles section for an example API request to create custom roles in Arthur. Importantly, the role names must uniquely match to a user group in your IdP in order for your users to be able to assume those permissions in Arthur. Therefore, the roles in Arthur must be globally unique in the entire Arthur installation.
6. Test Access
At this point everything should be configured correctly to sign in to Arthur via SSO. Either navigate to your IdP or the Arthur homepage to test logging in.
7. Cleaning Up
Once users are successfully able to log in to Arthur via the IdP, you should do the following to ensure proper security best-practices remain enforced:
Restrict any Arthur global roles to only allow access to essential admin functions
Set the Arthur superadmin user password securely, and either store the password in a vault, or discard the password entirely. superadmin shouldn't be used going forward.
Set up a policy to routinely rotate the superadmin password to keep it secure
Together, these practices will help ensure the security of your Arthur installation, and will give your IdP sole control over the platform and who is able to access it.
Common Troubleshooting
If after following the steps above, users are not able to log in via the IdP try some of these common troubleshooting tips:
Does the user properly redirected to the IdP's log in screen?
If not, there is likely a configuration error in the Arthur YAML config with the IdP discovery URL. Double check that the url entered resolved correctly when you append /.well-known/openid-configuration to the end of it. The full URL should be viewable in your browser or via a REST client.
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
If not, there is likely a configuration error with the IdP and the URLs that it uses to communicate with Arthur. Double-check the redirect (whitelisted) URL is configured correctly for the Arthur installation at https://<HOSTNAME>/login/callback.
A user can see the Arthur home page, but can't see any of the model in their organization
If a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to access models (see {doc}../../reference/permissions_by_endpoint) or they were not able to correctly assume the role in Arthur. Double-check the groups in their JWT claims match the role names that have been configured in Arthur. A superadmin or global role user with permissions to manage RBAC can see a list of roles in the installation by using the following API call. Be sure to replace the HOST and AUTH TOKEN for your installation and user:
Shellcurl --location 'https://<HOST>/api/v3/authorization/custom_roles' \
--header 'Authorization: Bearer <INSERT AUTH TOKEN HERE>'
Appendix A: More examples of JWT claims and how to parse them
This section outlines some additional ways to use the accessTokenValidation section of the Arthur IdP config YAML format. The below examples include sample JWTs, then corresponding YAML for how to parse them.
Basic Full Example
This example shows how to parse a user's information from JWT claims in a typical format.
Example parsed JWT claims JSON:
JSON{
"iss": "https://dev.okta.com/oauth2/default",
"aud": "api://default",
"scp": [
"openid"
],
"Groups": [
"idp-admin",
"org-1-model-owner",
"org-2-model-owner"
],
"FirstName": "John",
"LastName": "Doe",
"Login": "[email protected]",
"EmployeeID": "1234567890"
}
Corresponding settings for the Arthur IdP config YAML accessTokenValidation for the user information field:
YAMLaccessTokenValidation:
type: JWT
claimMapping:
roles: Groups
userID: EmployeeID
username: Login
firstName: FirstName
lastName: LastName
email: Login
Minimal Example
This example shows how to parse a user's information from JWT claims when many fields are missing.
Example parsed JWT claims JSON:
JSON{
"iss": "https://dev.okta.com/oauth2/default",
"aud": "api://default",
"scp": [
"openid"
],
"Groups": [
"idp-admin",
"org-1-model-owner",
"org-2-model-owner"
],
"user": "[email protected]"
}
Corresponding settings for the Arthur IdP config YAML accessTokenValidation for the user information field:
YAMLaccessTokenValidation:
type: JWT
claimMapping:
roles: Groups
userID: user
username: user
firstName: ""
lastName: ""
email: user
Appendix B: Setup for IdPs with CORS Restrictions
Completing this will require access to the Kubernetes cluster Arthur is running in, and the ability to create ingress resources in that cluster.
If your OIDC Identity Provider does not support CORS (common with Microsoft Azure AD), you will need to proxy requests via the Arthur backend. The following examples show how this can be done with a cluster using the NGINX ingress controller.
This first example YAML configures a route on NGINX that will proxy OIDC connections to your IdP. You'll need to replace the <IDP HOST> and <ARTHUR HOST> placeholders, then apply it to your cluster with kubectl apply -n <NAMESPACE> -f file.yaml. There should be two places to fill in each variable below.
YAMLapiVersion: v1
kind: Service
metadata:
name: external-idp
spec:
type: ExternalName
externalName: "<IDP HOST>"
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: external-idp
annotations:
kubernetes.io/ingress.class: nginx
nginx.ingress.kubernetes.io/rewrite-target: /$2
nginx.ingress.kubernetes.io/backend-protocol: "HTTPS" #important
nginx.ingress.kubernetes.io/upstream-vhost: "<IDP HOST>"
spec:
rules:
- host: "<ARTHUR HOST>"
http:
paths:
- backend:
service:
name: external-idp
port:
number: 443
path: /oidc(/$)(.*)
pathType: Prefix
tls:
- hosts:
- "<ARTHUR HOST>"
secretName: kotsadm-tls
After you've applied the above configuration to your cluster, you should be able to visit your IdP's /.well-known/openid-configuration endpoint at the following URL: https://<ARTHUR HOST>/oidc/<your IdP's .well-known endpoint path>. Once that is accessible, we need to modify the OIDC YAML configuration file. Fill in the following example with the correct values in the endpointOverrides section. Note, the issuer and authorization_endpoint fields should match what is in your IdP's /.well-known spec. The rest of the values should use the same path as shown in the IdP's /.well-known spec, but with the value of <ARTHUR HOST>/oidc/ substituted for the host of the IdP. The following example shows a proper CORS config for an IdP at the https://XXXX.okta.com address.
YAMLversion: v2
kind: OIDC
config:
discoveryBaseURL: https://XXXX.okta.com/oauth2/default
# if your IdP has CORS restrictions with the metadata URL,
# specify this block to prevent using the metadata endpoint to look them up
endpointOverrides:
# these first two match the IdP's .well-known spec
issuer: "https://XXXX.okta.com/oauth2/default"
authorization_endpoint: "https://XXXX.okta.com/oauth2/default/authorize"
# notice the following are all modified to add the <ARTHUR HOST>/oidc prefix in the URL
token_endpoint: "https://<ARTHUR HOST/oidc/oauth2/default/tokens"
jwks_uri: "https://<ARTHUR HOST/oidc/oauth2/default/keys"
userinfo_endpoint: "https://<ARTHUR HOST/oidc/oauth2/default/user_info"
end_session_endpoint: "https://<ARTHUR HOST/oidc/oauth2/default/logout"
# the rest of this file is unchanged from the examples above...
Once you have modified this YAML file accordingly, follow the steps
above to save it to your installation.Updated 3 months ago Table of Contents
1. Configure the IdP user groups and OIDC token claims
2. Configure the IdP OIDC Settings
3. Configure Arthur to work with your IdP
Configuring the IdPs discovery URL
Configure the Arthur Global Roles
Parsing the IdP JWT claims
Full Configuration Examples
4. Apply the Arthur IdP YAML configuration
5. Create organization user roles to match the IdP user groups
6. Test Access
7. Cleaning Up
Common Troubleshooting
Does the user properly redirected to the IdP's log in screen?
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
A user can see the Arthur home page, but can't see any of the model in their organization
Appendix A: More examples of JWT claims and how to parse them
Basic Full Example
Minimal Example
Appendix B: Setup for IdPs with CORS Restrictions
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/tabular
 Tabular
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by TabularSuggest EditsTabular input models are a type of machine learning model that operates on tabular data, which is data that is arranged in a table or spreadsheet format. These models are used to make predictions or classifications based on the input data, which may include features such as numerical or categorical variables. Tabular input models are commonly used in fields such as finance, healthcare, and marketing, where data is often structured in a tabular format.Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/integrations
 Integrations
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by IntegrationsSuggest EditsCurrently there are two available integrations for Single-Sign-On. Find these in the Single Sign On (SSO) Integrations page.Updated 3 months ago
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/glossary
 Glossary
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by GlossarySuggest EditsThe following definitions are specific to the Arthur platform, though in most cases apply to ML more broadly.
Arthur Inference
Container class for inferences uploaded to the Arthur platform. An inference comprises input features, prediction values, and (optionally) ground truth values and any Non-Input data.
Example:
Pythonground_truth = {
"Consumer Credit Score": 652.0
}
inference = arthur_model.get_inference(external_id)
inference.update(ground_truth)
Related terms: inference, ArthurModel
Arthur Model
A model object sends and retrieves data pertinent to a deployed ML system. The ArthurModel object is separate from the underlying model trained and makes predictions; it serves as a wrapper for the underlying model to access Arthur platform functionality.
An ArthurModel contains at least aname, an InputType and a OutputType.
Examples:
Pythonarthur_model = connection.model(name="New_Model",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
Pythonarthur_model = connection.get(model_id)
arthur_model.send_inference(...)
Arthur Model Group
Arthur Model Groups are an organizational construct the Arthur platform uses to track different versions of an Arthur model. Every
Arthur Model is a version of one Model Group, and a Model Group will always have at least one Arthur Model. The Model Group for an Arthur Model can only be specified during onboarding, and once the Arthur Model is saved, its group cannot be changed. If an Arthur Model is created without specifying a Model Group, a new Model Group will be created automatically with the new model as its single version.
When adding a model to a model group, the model is assigned a unique, incrementing Version Sequence Number (starting at 1) corresponding to the order in which it was added to the model group. Additionally, you can provide a Version Label to store a custom version string label along with the Version Sequence Number.
Example:
Python# retrieve the first version of a model
arthur_model_v1 = connection.get(model_id)
model_group = arthur_model_v1.model_group
# create the new version of the model
arthur_model_v2 = connection.model(name="Model_V2",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
# add the new model to the model group
model_group.add_version(arthur_model_v2, label="2.0.1")
arthur_model_v2.save()
Related terms: Version Label, Version Sequence Number
Attribute
A variable associated with a model. , be input, prediction, ground truth or ancillary information (these groupings are known as Stages in the Arthur platform).
It can be categorical or continuous.
Example:
The attribute age is an input to the model, whereas the attribute creditworthy is the target for the model.
Synonyms: variable, {predictor, input}, {ouput, target}, prediction.
Related terms: input, stage, prediction, ground truth
Bias
While bias is an overloaded term in stats&ML, we refer specifically to where the model's outcomes have the potential to lead to discriminatory outcomes.
Example:
This credit approval model tends to lead to biased outcomes: men are approved for loans at a rate 50% higher than women are.
Related terms: bias detection, bias mitigation, disparate impact
Bias Detection
The detection and quantification of {ref}algorithmic bias <bias_monitoring> in an ML system, typically as evaluated on a model's outputs (predictions) across different definitions of a sensitive attribute.
Many definitions of algorithmic bias have been proposed, including group and individual fairness definitions.
Group fairness definitions are often defined by comparing group-conditional statistics about the model's predictions. In the below definitions, the group membership feature is indicated by G, and a particular group membership value is simplified by g.
Example:
Common metrics for group fairness include Demographic Parity, Equalized Odds, and Equality of Opportunity.
Related terms: bias mitigation
Demographic Parity
A fairness metric that compares group-conditional selection rates. The quantity being compared is:
P(Y^ = 1  G = g)
There is not necessarily a normative ideal relationship between the selection rates for each group: in some situations, such as allocating resources, it may be important to minimize the disparity in selection rates across groups; in others, metrics based on group-conditional accuracy may be more relevant. However, even in the latter case, understanding group-conditional selection rates, especially when compared against the original training data, can be useful contextualization for the model and its task as a whole.
Related term: disparate impact
Equal Opportunity
A fairness metric that compares group-conditional true positive rates. The quantity being compared is:
P(Y^ = 1  Y =1, G = g)
For all groups, a true positive rate closer to 1 is better.
Equalized Odds
A fairness metric that incorporates both group-conditional true positive rates and false positive rates, equivalently, true positive and negative rates. There are a variety of implementations
due to the fact that some quadrants of the confusion matrix are complements of one another); here is one possible quantity to compare across groups:
P(Y^ = 1  Y = 1, G = g) + P(Y^ = 0  Y= 0, G = g)
In this implementation, this quantity should be as close to 2 as possible for all groups.
Bias Mitigation
Automated techniques to mitigate bias in a discriminatory model. Can be characterized by where the technique sits in the model lifecycle:
Pre-Processing: Techniques that analyze datasets and often modify/resample training datasets to make the learned classifier less discriminatory.
In-Processing: Techniques for training a fairness-aware classifier (or regressor) that explicitly trades off optimizing for accuracy and maintaining fairness across sensitive groups.
Post-Processing: Techniques that only adjust the output predictions from a discriminatory classifier without modifying the training data or the classifier.
Related terms: bias detection
Binary Classification
A modeling task where the target variable belongs to a discrete set with two possible outcomes.
Example:
This binary classifier will predict whether or not a person is likely to default on their credit card.
Related terms: output type, classification, multilabel classification
Categorical Attribute
An attribute whose value is taken from a discrete set of possibilities.
Example:
A person's blood type is a categorical attribute: it can only be A, B, AB, or O.
Synonyms: discrete attribute
Related terms: attribute, continuous, classification
Continuous Attribute
An attribute whose value is taken from an ordered continuum can be bounded or unbounded.
Example:
A person's height, weight, income, IQ can all be through of as continuous attributes.
Synonyms: numeric attribute
Related terms:
attribute, continuous, regression
Classification
A modeling task where the target variable belongs to a discrete set with a fixed number of possible outcomes.
Example:
This classification model will determine whether an input image is of a cat, a dog, or fish.
Related terms: output type, binary classification, multilabel classification
Data Drift
Refers to the problem arising when, after a trained model is deployed, changes in the external world lead to degradation of model performance and the model becoming stale.
Detecting data drift will provide a le ing indicator of data stability and integrity.
Data drift can be quantified with respect to a specific reference set (e.g., the model's training data) or, more generally, the model's temporal shifts in a variable with respect to past time windows.
Your project can {ref}query data drift metrics through the Arthur API <data_drift>. This section will provide an overview o
the available data drift metrics in Arthur's query service.
Related terms: ou Arthur'stribution
Multivariate
Arthur also offers a multivariate Anomaly Score, which you can configure via the steps detailed in Enabling Enrichments.
See Anomaly Detection for an explanation of how these scores are used and Arthur Algorithms for how they're calculated.
Disparate Impact
Legal terminology originally from Fair Lending case law. This constraint is strictly harder than Dispara e Treatment and asserts that model outcomes must not be discriminatory across protected groups. That is, the outcome of a decision process should not be substantially higher (or lower) for one group of a protected class over another.
While there does not exist a single threshold for establishing the presence or absence of disparate impact, the so-called "80% rule" is commonly referenced. However, harm certain subgroups of a population differentially r, we strongly recommend against adopting this rule-of-thumb, as these analyses should be grounded in use-case-specific analysis and the legal framework pertinent to a given industry.
Example:
Even though the model didn't take gender as input, it still results in disparate impact when we compare outcomes for males and females.
Related terms: bias, disparate treatment
Disparate Treatment
Legal terminology originally from Fair Lending case law. Disparate Treatment asserts that you are not allowed to consider protected variables (e.g., race, age, gender) when approving or denying a credit card loan application.
In practical terms, a data scientist cannot include these attributes as inputs to a credit decision model.
Adherence to Disparate Treatment is not a sufficient condition for actually achieving a fair model (see proxy and bias detedefinitionstion). "Fairness through "unawareness" is not good enough.
Related terms: bias, disparate impact
Enrichment
Generally used to describe data or metrics added to raw data after ingestion. Arthur provides various enrichments such as Anomaly Detection and Explainability. See entity enrichments for details about using enrichments within Arthur.
Feature
An individual attribute that is an input to a model
Example:
The credit scoring model has features like “home_value”, “zip_code”, “height".
Ground Truth
The true label or target variable (Y) corresponds to inputs (X) for a dataset.
Examples:
pred = sklearn_model.predict_proba(X)
arthur_model.send_inference(
model_pipeline_input=X,
predicted_values={1:pred, 0: 1-pred})
Related terms: prediction
Image Data
Imagery data is commonly used for computer vision models.
Related terms: attribute, output type, Stage
Inference
One row of a dataset. Inference refers to passing a single input into a model and the model's prediction. Data associated with that inference might include (1) the input to the model, (2) the model's prediction and (3) the corresponding ground truth.
With respect to the Arthur platform, the term inference denotes any and all of those related components of data for a single input&prediction.
Related terms: ArthurInference, stage
Input
A single data instance upon which a model can calculate an output prediction. The input consists of all relevant features together.
Example:
The input features for the credit scoring model consist of “home_value”, “zip_code”, “height".
Related terms: feature, model
Input Type
For an ArthurModel, this field declares what kind of input datatype will be flowing into the system.
Allowable values are defined in the InputType enum:
Tabular
Image
NLP
Example:
Pythonarthur_model = connection.model(name="New_Model",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
Related terms: output type, tabular data, nlp data
Model Health Score
On the UI dashboard, you will see a model health score between 0-100 for each of your models. This
score averages over a 30-day window of the following normalized metrics: performance, drift, and ingestion.
Performance:
Regression: 1 - Normalized MAE
Classification: F1 Score
Drift
1 - Average Anomaly Score
Ingestion
The variance of normalized periods between ingestion events
The variance of normalized volume differences between ingestion events
You can also extract the health score via an API call.
Model Onboarding
Model onboarding refers to the process of defining an ArthurModel, preparing it with the necessary reference dataset, passing it through a validation check, and saving it to the Arthur system.
Once your model is onboarded onto Arthur, you can use the Arthur system to track the model and view all its performance and analytics in your online Arthur dashboard.
Related terms: ArthurModel, reference dataset
Multiclass Classification
A modeling task where each input is associated with one label from a fixed set of possible labels.
Often this is a binary classifier (the output is either 0 or 1), but the output can also have more than 2 possible labels.
Example:
This NLP model applies the most relevant tag to news articles. The model is trained on example articles which are tagged with a topic like Congress.
Related terms: multilabel clasification, output type,
Multilabel Classification
A modeling task where each input is associated with two or more labels from a fixed set of possible labels.
Example:
This NLP model applies relevant tags to news articles. The model is trained on example articles which are tagged with multiple topics like Politics, Elections, Congress.
Related terms: output type, multiclass clasification
NLP Data
Unstructured text sequences are commonly used for Natural Language Processing models.
Related terms: attribute, output type, Stage
Non-Input Attribute
A non-input attribute is an attribute that an ArthurModel will track that does not actually enter the model as an input.
Common non-input attributes are protected class attributes such as age, race, or sex. By The model ending such non-input attributes to Arthur, you can track model performance based on these groups in your data to evaluate model bias and fairness.
Related terms: attribute, bias
Object Detection
The OutputType is for computer vision models to detect an object within an image and output a box that bounds the object.
This bounding box is used to identify where the object resides in the image.
Related terms: image
Out of Distribution Detection
Refers to the challenge of detecting when an input (or set of inputs) is substantially different from the distribution of a larger set of reference inferences. This term commonly arises in data drift, where we want to detect if new inputs differ from the training data (and distribution thereof) for a particular model. OOD Detection is a relevant challenge for Tabular data and unstructured data such as images and sequences.
Related terms: {ref}glossary_data_drift
Output Type
For an ArthurModel, this field declares what kind of output predictions will flow out of the system.
Allowable values are defined in the OutputType enum:
Regression
appropriate for continuous-valued targets
Multiclass
appropriate for both binary classifiers and multiclass classifiers
Multilabel
appropriate for multilabel classifiers
ObjectDetection
only available for computer vision models
Example:
Pythonarthur_model = connection.model(name="New_Model",
input_type=InputType.Tabular,
output_type=OutputType.Regression)
Related terms: input type
Prediction
The output prediction (y_hat) of a trained model for any input.
Related terms: ground truth
Protected Attribute
An attribute of an inference that is considered sensitive with respect to model bias. Common examples include race, age, and gender. The term "protected" comes from the Civil Rights Act of 1964.
Synonyms: sensitive attribute
Related terms: bias, proxy
Proxy
An input attribute in a model (or combination thereof) is highly correlated with a protected attribute such as race, age, or gender. The presence of proxies in a dataset makes it difficult to rely only on [Disparate Treatment] as a standard for fair ML.
Example:
In most US cities, zip code is a strong proxy for race. Therefore, one must be cautious when using zip code as an input to a model.
Related terms: bias, disparate impact, disparate treatment
Reference
The dataset is used as a baseline reference for an Arthur model.
A reference dataset must include a sample of the input features a model receives.
A reference dataset can optionally include a sample of model outputs, ground truth values, and other non-input attributes as metadata.
The reference dataset for a model is used to compute drift: the distribution of input features in the reference dataset makes up the baseline against which future inferences are compared to compute anomaly scores.
Related terms: inference
Regression
A modeling task (or model) where the target variable is a continuous variable.
Example:
This regression model predicts what the stock price of $APPL will be tomorrow.
Related terms: output type
Sensitive Attribute
See protected attribute
Stage
The Arthur platform uses taxonomy to delineate how attributes contribute to the model computations.
Allowable values are defined in the Stage enum:
ModelPipelineInput: Input to the entire model pipeline. This will most commonly be the Stage used to represent all model inputs. Will contain base input features familiar to the data scientist: categorical and continuous columns of a tabular dataset.
PredictFunctionInput: Potential alternative input source, representing direct input into the model's predi t() method. Therefore, data in the specific models have already undergone all relevant transformations, including scaling, one-hot encoding, or embedding.
PredictedValue: The predictions coming out of the model.
GroundTruth: The ground truth (or target) attribute or a model. Must be one-hot for classifiers
GroundTruthClass: The ground truth class for classification models, not one-hot encoded
NonInput: Ancillary data that can be associated with each inference but not necessarily a direct input t
the model. For example, sensitive attributes like age, sex, or race might not be direct model inputs, but will be useful to associate with each prediction.
Tabular Data
The data type for model inputs where the data can be thought of as a table (or spreadsheet) composed o
rows and columns. Each column represents an input attribute for the model, and each row represents a separate record that composes the training data. In supervised learning, exactly one of the columns acts as the target.
Example:
This credit scoring model is trained on tabular data. The input attributes are income, country, and age and the target is FICO score.
Related terms: Attribute, output type, Stage
Token Likelihood
The token likelihood is a number between 0 and 1 that quantifies the model’s level of surprise that this token was the next predicted token of the sentence. If a token has a low likelihood (close to 0), the model is more unsure about selecting this token. While a likelihood close to 1 indicates that the model is very confident in predicting this token.
Version Label
A Version Label is a string representing a custom version of your Arthur Model within its A thur Model Group. Version Labels are not required, and the platform will default to using the Version Sequence Number when not provided.
Example:
Python# retrieve the model group
model_group = connection.get_model_group(model_group_id)
# create the new version of the model
arthur_model_v2 = connection.model(name="Model_V2",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
# add the new model to the model group
model_group.add_version(arthur_model_v2, label="2.0.1")
label = arthur_model_v2.version_label
arthur_model_v2.save()
# label == "2.0.1"
Related terms: Arthur Model, Arthur Model Group, Version Sequence Number
Version Sequence Number
A Version Sequence Number is a unique, auto-incrementing (starting at 1) integer assigned to Arthur Models in an A thur Model Group. This number uniquely represents an Arthur Model’s Version with the Model Group. If a Version Label is not provided, the platform will show the Version Sequence Number instead.
Example:
Python# retrieve the first version of a model
arthur_model_v1 = connection.get(model_id)
num = arthur_model_v1.version_sequence_num
# num == 1
# retrieve the second version of a model
model_group = arthur_model_v1.model_group
arthur_model_v2 = model_group.get_version(sequence_num=2)
num = arthur_model_v2.version_sequence_num
# num == 2
Related terms: Arthur Model, Arthur Model Group, Version LabelUpdated 3 months ago Table of Contents
Arthur Inference
Arthur Model
Arthur Model Group
Attribute
Bias
Bias Detection
Demographic Parity
Equal Opportunity
Equalized Odds
Bias Mitigation
Binary Classification
Categorical Attribute
Continuous Attribute
Classification
Data Drift
Multivariate
Disparate Impact
Disparate Treatment
Enrichment
Feature
Ground Truth
Image Data
Inference
Input
Input Type
Model Health Score
Model Onboarding
Multiclass Classification
Multilabel Classification
NLP Data
Non-Input Attribute
Object Detection
Out of Distribution Detection
Output Type
Prediction
Protected Attribute
Proxy
Reference
Regression
Sensitive Attribute
Stage
Tabular Data
Token Likelihood
Version Label
Version Sequence Number
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/langchain
 Langchain
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by LangchainSuggest EditsThis guide walks through how to use the ArthurCallbackHandler, an integration that allows you to send LLM inferences to the Arthur platform through LangChain.
Register your ArthurModel
You can skip this step if your generative text model is already registered with Arthur.
If you do not have a model currently onboarded to Arthur, follow the steps on our Generative Text Onboarding Guide
You will need your model's ID registered to the platform before you create an ArthurCallbackHandler with your LangChain LLM.
Create your LangChain LLM with the ArthurCallbackHandler
First, get your Arthur login info and the ID of your registered ArthurModel:
Pythonarthur_url = "<https://app.arthur.ai">
arthur_login = "your-arthur-login-username-here"
arthur_model_id = "your-arthur-model-id-here"
Next, we create a LangChain ChatOpenAI LLM with your Arthur credential info passed to the ArthurCallbackHandler
Note that we are also configuring the LLM with the useful StreamingStdOutCallbackHandler from LangChain, which returns responses as a token-by-token stream instead of returning the entire result at once - this typically makes for a better UX for development & testing.
Pythonfrom langchain.callbacks import ArthurCallbackHandler
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chat_models import ChatOpenAI
chatgpt = ChatOpenAI(
streaming=True,
temperature=0.1,
callbacks=[
StreamingStdOutCallbackHandler(),
ArthurCallbackHandler.from_credentials(
arthur_model_id,
arthur_url=arthur_url,
arthur_login=arthur_login)
])
You can now use the LLM in your LangChain application, with its input text, output text, and other monitored attributes recorded to the Arthur platform for each inference.
Note that the attributes from each LLM response will only be saved to Arthur if you have registered those attributes with your ArthurModel. For more information on registered generative text models with additional attributes besides input text & output text, visit the Generative Text Onboarding Guide.
Run the model to log inferences to the Arthur platform
Here we define a run function that executes a loop for a chat between a user and an LLM until the user types q to quit
Note that this function is not required to use the ArthurCallbackHandler - it is just meant as a quick demonstration of how to use a LangChain LLM
Pythonfrom langchain.schema import HumanMessage
def run(llm):
history = []
while True:
user_input = input("\n>>> input >>>\n>>>: ")
if user_input == "q":
break
history.append(HumanMessage(content=user_input))
history.append(llm(history))
Each subsequent user <> LLM back-and-forth response will be logged as its own inference in the Arthur platform.
For example, here is an execution of this run function with the ChatGPT LLM.
Pythonrun(chatgpt)
Textinput >>>
> > > : What is a callback handler?
> > > A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.
The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.
Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.
input >>>
> > > : What do I need to do to get the full benefits of this
> > > To get the full benefits of using a callback handler, you should consider the following:
1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.
2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.
3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.
4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.
5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.
6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.
By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.
input >>>
> > > : q
Updated 3 months ago Table of Contents
Register your ArthurModel
Create your LangChain LLM with the ArthurCallbackHandler
Run the model to log inferences to the Arthur platform
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/administration
 Administration
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by AdministrationSuggest EditsBy default, the installer creates a new organization, "My Organization," for convenience. You can also create new organizations using the API with the superadmin user. Full instructions for creating new users and organizations can be found Organizations and Users.
To access the UI for the default organization dashboard, visit thehttps://your_arthur_domain from your Web browser. Login with admin username and SuperSecret password. Make sure to change the password as soon as possible.
Refer to the Quickstart guide to start onboarding your models.
Admin Console
The Admin Console can be made available via the ingress controller on port 443 by creating a subdomain DNS record that starts with admin. (e.g., admin.arthur.mydomain.com). This eliminates the port 8800 egress requirement for VM installation on the firewall.
We recommend that you rotate your Admin Console password often. You can reset the password using this command:
Shellkubectl kots reset-password -n <namespace>
Update Admin password for Embedded Postgres
The 'Postgres' admin user manages the embedded Postgres database. If you would like to update the password for this admin user, you can execute the following commands on the primary database pod:
Shellkubectl exec -it database-master-0 -- psql -U postgres
Password for user postgres: <type_current_secret>
psql (11.13)
Type "help" for help.
postgres=# ALTER ROLE postgres WITH PASSWORD '<insert_new_secret>';
postgres=# \q
$
Updated 3 months ago Table of Contents
Admin Console
Update Admin password for Embedded Postgres
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/regression
 Regression
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by RegressionSuggest EditsRegression models predict a numeric outcome. In Arthur, these models are listed under the regression model type.
Some common examples of Text regression are:
What is the predicted review score for written restaurant reviews?
Predict house price from description text
Formatted Data in Arthur
Tabular regression models require
two columns: text input and numeric output. When onboarding a reference dataset (and setting a model schema), you need to specify a target column for each inference's ground truth. Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (numeric or categorical)Attribute (numeric or categorical)Prediction (numeric)Ground Truth (numeric)Non-Input Attribute (numeric or categorical)45Graduate Degree45.3462.42Female22Bachelor's Degree55.153.2Male
Predict Function and Mapping
These are some examples of common values teams need to onboard for their regression models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics.
Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Single Column Ground Truth
output_mapping = {
'prediction_column':'gt_column'}
# Build Arthur Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
## Example prediction function for binary classification
def predict(x):
return model.predict(x)
Available Metrics
When onboarding tabular regression models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a regression model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeRoot Mean Squared ErrorPerformanceMean Absolute ErrorPerformanceR SquaredPerformanceInference CountIngestionAverage PredictionIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/changelog
 Release Notes
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchArthur Platform 3.12.0 23 days ago by ReadMe APIRelease notes for Arthur Platform 3.12.0Arthur Platform 3.11.0 about 2 months ago by ReadMe APIRelease notes for Arthur Platform 3.11.0Arthur Platform 3.10.0 about 2 months ago by ReadMe APIRelease notes for Arthur Platform 3.10.0Arthur Platform 3.9.0 4 months ago by ReadMe APIRelease notes for Arthur Platform 3.9.0Arthur Platform 3.8.0 5 months ago by ReadMe APIRelease notes for Arthur Platform 3.8.0Arthur Platform 3.7.0 7 months ago by ReadMe APIRelease notes for Arthur Platform 3.7.0addedMay 2023 10 months ago by Haley MassaNew FeaturesWelcome to Arthur about 1 year ago by Haley MassaWelcome to the developer hub and documentation for Arthur.
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/email
 Email
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by EmailSuggest EditsOne of the most popular alert notification techniques is through email. This notification technique can easily be set up in the organization or model-level UI.
Organization Level
They are typically used when teams want to manage organization-wide email alert notification access over multiple models quickly. Teams can go into the organization's setting page and configure which users will receive email alerts for each model that has alerts enabled.
Model Level
If you do not want to look at alerts at an organizational level, you can also set up email alerts per model within an individual model's Alerts tab. Here you can navigate to manage the alert rules of the model. Within this management center, you can select and edit the notification channels for this model. This is where emails are entered and saved for users that wish to receive alerts.
Resulting Alert
After configuring your alert email settings, teams are ready to receive alerts for these models to their designated emails.
An example of an email alert notification sent by Arthur
Configuring Email Alert Notifications on SSO
For users in SSO environments to subscribe other users up to email alerts notifications, configure your email domain whitelist through the KOTS admin console to be able to send email alert notifications.
In the KOTS Admin console, check “Show Other Advanced Options” navigate to the “Other Advanced Options” section
Under "Other Advanced Options," set a comma-separated list of domains in the “Email Domain Whitelist” field.
An example of the Email Domain Whitelist within the Admin ConsoleUpdated 3 months ago Table of Contents
Organization Level
Model Level
Resulting Alert
Configuring Email Alert Notifications on SSO
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/ranked-list-recommender-systems
 Ranked List (Recommender Systems)
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Ranked List (Recommender Systems)Suggest EditsRanked List output data is typically used in recommender systems, which is a type of machine learning model that generates suggestions about “relevant” ranked items based on some input data. An example of a recommender system using ranked list data is a model that recommends relevant movies to a viewer based on metadata generated from their watch history.
Formatted Data in Arthur
Ranked List output models require the following data formatting:
JSON[
{ // first recommended item
"item_id": "item1",
"score": 0.324,
"label": "apple"
},
{ // second recommended item
"item_id": "item2",
"score": 0.024,
"label": "banana"
]
In this formatting, the score must be a float value, whereas the label and item_id must be string values. The label is an optional, readable version of item_id and score is an optional score/probability for a given item. If one of these optional metadata fields are specified in one inference, it must be specified for all of them.
Arthur expects the list of ranked items to be sorted in rank order, such that the highest ranked item is first. Each ranked list output model in Arthur can have max 1000 total unique recommended items in its reference dataset. Additionally, each ranked list output model can have max 100 recommendations per inference/ground truth.
Recommender Systems Ground Truth
The ground truth for ranked list output models is an array of strings representing the items that have been determined “relevant” for a given inference.
Available Metrics
When onboarding recommender system models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) when teams onboard a ranked list model. Find out more about these metrics in the Performance Metrics section.
MetricMetric TypePrecision at kPerformanceRecall at kPerformancenDCG at kPerformanceMean Reciprocal RankPerformanceRanked List AUCPerformanceInference CountIngestion
Drift Metrics
In the Arthur platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftTime Series DriftFeature DriftPrediction DriftPrediction Drift
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.Updated about 2 months ago Table of Contents
Formatted Data in Arthur
Available Metrics
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/image
 Image
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ImageSuggest EditsImage input models are a type of machine learning model that operates on image data, such as digital photographs, medical scans, and satellite imagery. These models are designed to recognize patterns and features within images and can perform tasks such as object detection, image segmentation, and image classification. Image input models can be built using a variety of techniques, including convolutional neural networks, which are specifically designed to work with image data.
Sending Images to Arthur
Arthur takes in the raw images as input. This means that instead of taking in images as matrixes of pixel values, we take the images in with [.gif , .jpeg , .png , .tiff] format. After sending these values to Arthur, they are stored in an AWS S3 bucket.Updated 3 months ago What’s NextBinary ClassificationTable of Contents
Sending Images to Arthur
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/pre-requisites
 Pre-Requisites
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Pre-RequisitesSuggest EditsThe following configurations should be applied to the platform to use Arthur's Backup and Restore capabilities:
Arthur must be configured to use external object storage, specifically, AWS S3
The access to external storage must be configured using IRSA annotations
In order to use IRSA annotations, the cluster must be deployed using Amazon EKS
If the above are not true/possible for your deployment, please reach out to Arthur Support.
Configuring Velero
The only component that needs to be installed separately from Arthur to perform backup and restores is Velero. Instructions are provided below for setting up Velero to store backups in S3 using IRSA.
The general overview of the installation is as follows:
Setup permissions for Velero
Install Velero
Confirm Velero is installed and configured correctly
Configure the Backup Storage Destination to point to S3
Velero permissions
Generate the below policy which will grant Velero the necessary permissions:
TextShell$ cat > velero-policy.json <<EOF
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"ec2:DescribeVolumes",
"ec2:DescribeSnapshots",
"ec2:CreateTags",
"ec2:CreateVolume",
"ec2:CreateSnapshot",
"ec2:DeleteSnapshot"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"s3:GetObject",
"s3:DeleteObject",
"s3:PutObject",
"s3:AbortMultipartUpload",
"s3:ListMultipartUploadParts"
],
"Resource": [
"arn:aws:s3:::${BUCKET}/*"
]
},
{
"Effect": "Allow",
"Action": [
"s3:ListBucket"
],
"Resource": [
"arn:aws:s3:::${BUCKET}"
]
},
{
"Effect": "Allow",
"Action": [
"kms:CreateGrant*",
"kms:ReEncrypt*",
"kms:GenerateDataKey*",
"kms:Encrypt*",
"kms:DescribeKey*",
"kms:Decrypt*"
],
"Resource": "*"
}
]
}
EOF
$ aws iam create-policy \
--policy-name velero-perms \
--policy-document file://velero-policy.json
Attach this IAM policy to the IAM role that the Arthur service account (IRSA) assumes
Shell$ aws iam attach-role-policy \
--role-name <IRSA-Role-name> \
--policy-arn <policy-ARN>
📘Encrypting data at restArthur highly recommends that your EBS volumes are encrypted with KMS.
In addition to giving Velero the permission for KMS, please make sure that the IAM roles assumed by Arthur service account also have access to KMS so the restored encrypted volumes can be re-attached.
If you're using separate KMS keys on the cluster you backed-up and the cluster you're restoring to, the EBS volume snapshots must be copied with the new KMS key so the new cluster can work with the snapshots.
Install Velero
Velero can be installed on the Kubernetes cluster using helm.
Generate a velero-values file as follows (taken from the official source with defaults removed for brevity):
YAML$ cat > velero-values.yaml <<EOF
resources:
requests:
cpu: 500m
memory: 128Mi
limits:
cpu: 1000m
memory: 512Mi
initContainers:
- name: velero-plugin-for-aws
image: velero/velero-plugin-for-aws:v1.6.1
imagePullPolicy: IfNotPresent
volumeMounts:
- mountPath: /target
name: plugins
podSecurityContext:
runAsNonRoot: true
runAsUser: 1000
runAsGroup: 1000
upgradeCRDs: true
cleanUpCRDs: false
configuration:
# Cloud provider being used (e.g. aws, azure, gcp).
provider: aws
backupStorageLocation:
# name is the name of the backup storage location where backups should be stored.
name: <insert-bsl-name-here>
provider: aws
# bucket is the name of the bucket to store backups in. Required.
bucket: <insert-s3-bucket-name-here>
config:
region: us-east-2
volumeSnapshotLocation:
# name is the name of the volume snapshot location where snapshots are being taken. Required.
name: <insert-vsl-name-here>
config:
region: us-east-2
# These are server-level settings passed as CLI flags to the `velero server` command.
logLevel: debug
namespace: <insert-velero-namespace-here>
rbac:
# Whether to create the Velero role and role binding to give all permissions to the namespace to Velero.
create: true
# Whether to create the cluster role binding to give administrator permissions to Velero
clusterAdministrator: true
# Name of the ClusterRole.
clusterAdministratorName: cluster-admin
# Information about the Kubernetes service account Velero uses.
serviceAccount:
server:
create: true
name: velero
annotations:
eks.amazonaws.com/sts-regional-endpoints: "true"
eks.amazonaws.com/role-arn: <insert-IAM-Role-ARN-here>
labels:
credentials:
useSecret: false
backupsEnabled: true
snapshotsEnabled: true
deployNodeAgent: false
EOF
Install the Velero helm chart with the above values file:
Shell{note}$ velero_namespace=<insert-velero-namespace-here>
$ helm install velero vmware-tanzu/velero \
--create-namespace \
--namespace $velero_namespace \
--version 3.2.0 \
-f velero-values.yaml
Arthur recommends installing Velero in a different namespace from Arthur, so Velero can be managed separately from Arthur.
📘Where to install veleroArthur recommends installing Velero in a different namespace from Arthur, so Velero can be managed separately from Arthur.
Verify Velero Installation
To confirm that Velero is installed and configured correctly:
Open the Kots Admin Interface and navigate to the "Snapshots" tab
Click the "Check for Velero" button (see the screenshot below)
Validate the Backup Storage Location
The Backup Storage Location is a Velero resource that points to the S3 Bucket where backups will be stored. Use kubectl to validate connectivity/access to AWS S3, which should say "Available".
Shell$ velero_namespace=<insert-velero-namespace-here>
$ kubectl get backupstoragelocation -n $velero_namespace
Please do not proceed until the Backup Storage Location is Available.
Configuring clickhouse-backup
Configuring clickhouse-backup to store backups in remote storage (e.g., S3) can be done in the Admin Console.
Once your cluster is set up for Backup and Restore, you should see the "Enable OLAP Database Backup Capabilities" option in the "OLAP Database" section.
Ensure that:
The configuration that points to the bucket is correct
The Bucket Name
The Bucket Region
The ServiceAccount is the same ServiceAccount that you've configured with the IRSA annotation (if you are not sure, enter the default value)
The IAM Role that you are using for the IRSA annotation has the appropriate permissions to read/write/list from the S3 bucket
The S3 Path is where you want to be storing backups
Updated 3 months ago Table of Contents
Configuring Velero
Velero permissions
Install Velero
Verify Velero Installation
Validate the Backup Storage Location
Configuring clickhouse-backup
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/performance-metrics
 Performance Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Performance MetricsMonitor and communicate model performanceSuggest EditsWhether a performance alert has been triggered requiring the ML team's attention or an external stakeholder has requested a performance breakdown for a specific segment, Arthur provides a single location to report on and explore model performance across an organization.
Performance Metrics in Arthur
Out-of-the-Box
It can be overwhelming for external stakeholders to balance too many technical definitions for model accuracy, primarily if each team in an organization provides different baseline metrics. Onboarded model schema automatically infers performance metrics of interest for your models, easily standardizing top performance metrics across the organization.
Each model types out-of-the-box performance metrics are defined for them in their Model Types section.
User-Defined Metrics
Different stakeholders define performance differently and accommodate all measures of performance utilizing our ability to curate performance metrics by data slices and user-defined custom metrics.
Performance Metric UI Guide
Performance metrics for specific models can be found within that model's Overview tab.
Time Display
The default option for seeing performance charts in the UI is in time series mode. These charts provide an average of each metric overtime at the time interval specified at the top of the chart.
Snapshot Mode
For teams that are not interested in viewing their metrics over time, they can select Snapshot mode in the top corner.
This mode creates bar graphs of the average metric over the specified range from the global filters (discussed more below).
Segment Sections of Interest
As referenced above, users can also segment data of interest.
Global Filters
Global Filters are available in the top corner of the UI. Teams can apply time or attribute-based filter rules to generate representative charts for those groups.
Note: Global Filter Are Applied Globally. Global filters do not apply just to the Performance graph or Overview Tab. These filters are applied globally across the UI. This means that they can be used to explore the inferences in the inference deep dive for example. However, it is important to remember that the filters are applied before navigating to other tabs.
Require Ground Truth
One key thing to keep in mind is that many performance metrics require ground truth (or labels). Teams that have a lag between prediction and ground truth should look into Drift and Anomaly metrics within Arthur.Updated 3 months ago What’s NextSet alerts on your performance metrics, or learn more about different metrics within Arthur.AlertingDrift and AnomalyFairness MetricsUser Defined MetricsTable of Contents
Performance Metrics in Arthur
Out-of-the-Box
User-Defined Metrics
Performance Metric UI Guide
Time Display
Segment Sections of Interest
Global Filters
Require Ground Truth
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/versioning
 Versioning
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by VersioningSuggest EditsVersioning is crucial for production systems in machine learning as it allows for the following:
seamless updates and rollbacks of machine learning models
enables performance monitoring and A/B testing, facilitating data-driven decisions for model improvements.
and ensures reproducibility and auditability of deployed models, meeting compliance requirements and providing transparency in machine learning systems.
Model Groups == Model Tasks
We can think of versions relating to a particular model task (or use case). For example, a “Real Time Transaction Fraud Model” will be a years-long project with much evolution. Many small and large changes may occur over the years, from retraining with new data to building completely new architectures.
When you represent these new versions in Arthur Scope, they go into a
model group. Within Arthur, a model group refers to a group of versions of the models that are used for the same model task.
Creating a New Model Version with the Python SDK
New model versions within Arthur are completely new ArthurModel Objects. This means that building out a new model version consists of building a new ArthurModel object and linking it to your existing ArthurModel object through its Arthur model group. That may seem a bit confusing, so we'll break the steps down below:
Step 0: Have an Initial Model on Arthur
To put the first version of your model onto Arthur, you do not need to specify its version. You just need to follow the Creating Arthur Model Object section of the documentation.
Get the Model Group for ArthurModel you want to version
Once you have been running that version of your model on Arthur for a while, you may decide that it is time to onboard a new model version. To ensure that this new version is connected with the old version, you will use the model_group_id.
Here is a code example of how to grab the model_group_id from a running ArthurModel object.
Python# get the model group for v1 (we will save v2 in the same model group)
model_group = connection.get_model_group(arthur_model_v1)
Build a new ArthurModel Object for this version
Build a new Arthur model object for this version, following the same steps as Creating Arthur Model Object. Just make sure that you do not save the model object until you do the next step.
Python# Register V2 of the model with Arthur
arthur_model_v2 = connection.model(display_name="Magic_Model", # The model name can be the same or different
input_type=InputType.Tabular,
output_type=OutputType.Multiclass)
# creating mapping from predictions to ground truth
pred_to_ground_truth_map = {}
for i, name in enumerate(pred_df_2.columns):
pred_to_ground_truth_map[name]= i
arthur_model_v2.build(ref_df_2,
ground_truth_column='label',
pred_to_ground_truth_map=pred_to_ground_truth_map)
arthur_model_v2.get_attribute('label').set(categorical=True, categories=list(ref_df_2['label'].unique()))
arthur_model_v2.review()
Save your new version
The connection between your old ArthurModel object and the new ArthurModel object happens when you save the model. Here we can see how we are assigning our new model object the same model_group_id as our original ArthurModel object. We can also provide a version_label which will represent what this new version is in the platform.
When we finish linking all that, we can save the model to the platform.
Python# Add v2 of the model to the same model group as v1
arthur_model_v2.model_group_id = model_group.id
# Assign a version label to v2 of the Model
arthur_model_v2.version_label = "V2"
# Save v2 of the model to Arthur
model_id = arthur_model_v2.save()
Update Inference Sending Techniques
With all that done, your new ArthurModel object is now on the Arthur platform. However, you are not done just yet. You need to ensure that any system you set up for sending inferences to your old model (or want to set up for sending inferences to your new model) is implemented to begin effectively monitoring.
To do this, teams should follow the techniques listed in Sending Inferences. Know that for model versioning, teams often follow a few different patterns:
Replacing Old Inference Connection to New Model ID: If you are switching over completely to this new model version, replacing the old ArthurModel ID connection with this new version's model id can be the easiest.
Setting Up Validation in Production Pipelines: Many teams use versioning before completely switching which model is being served. Teams may run different types of validation tests in production (such as A/B testing, shadow deployments, or canary tests). These are all possible within Arthur, you will just need to create different ArthurModel objects for each test and set up inference sending to the platform for them to be effective.
Updated 3 months ago Table of Contents
Model Groups == Model Tasks
Creating a New Model Version with the Python SDK
Step 0: Have an Initial Model on Arthur
Get the Model Group for ArthurModel you want to version
Build a new ArthurModel Object for this version
Save your new version
Update Inference Sending Techniques
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/troubleshooting-explainability
 Troubleshooting Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Troubleshooting ExplainabilitySuggest EditsTroubleshooting
AttributeError When Loading Predict Function
While this can be an issue with any model type, it is common to see when using sk-learn objects that take in custom user functions.
We will use TfidfVectorizer as an example, which is a commonly used vectorizer for NLP models, that often utilizes custom user functions.
A TfidfVectorizer accepts a user defined tokenize function, which is used to split a text string into tokens.
Problem
Say this code was used to create your model.
Python# make_model.py
def tokenize(text):
# tokenize and lemmatize
doc = nlp(txt)
tokens = []
for token in doc:
if not token.is_stop and not token.is_punct \
and not token.is_space and token.lemma_ != '-PRON-':
tokens.append(token.lemma_)
return tokens
def make_model():
# here we pass a custom function to an sklearn object
vectorizer = TfidfVectorizer(tokenizer=tokenize)
vectorizer.fit(X_train)
model = LogisticRegression()
model.fit(vectorizer.transform(X_train))
pipeline = make_pipeline(vectorizer, model)
joblib.dump(pipeline, 'model.pkl')
if __name__ == "__main__":
make_model()
Now you create this entrypoint file to enable explainability:
Python# entrypoint.py
model = joblib.load("./model.pkl")
def predict(fv):
return model.predict_proba(fv)
Now when the SDK imports entrypoint to test the function, the following error gets thrown:
AttributeError: module '__main__' has no attribute 'tokenize'
What happens is that Python failed to serialize the custom function, only the reference to how it was imported. Which in this case, it was just top level in the model creation script (hence __main__.tokenize in the error).
This function doesn't exist in entrypoint, and so the error is thrown.
Solution
To solve, you need to pull out tokenize into its own module, that can be imported from both create_model.py
and also in entrypoint.py.
Python# model_utils.py
def tokenize(text):
# tokenize and lemmatize
doc = nlp(txt)
tokens = []
for token in doc:
if not token.is_stop and not token.is_punct \
and not token.is_space and token.lemma_ != '-PRON-':
tokens.append(token.lemma_)
return tokens
Python# create_model.py
from model_utils import tokenize
def make_model():
# here we pass a custom function to an sklearn object
vectorizer = TfidfVectorizer(tokenizer=tokenize)
vectorizer.fit(X_train)
model = LogisticRegression()
model.fit(vectorizer.transform(X_train))
pipeline = make_pipeline(vectorizer, model)
joblib.dump(pipeline, 'model.pkl')
if __name__ == "__main__":
make_model()
Python# entrypoint.py
from model_utils import tokenize
model = joblib.load("./model.pkl")
def predict(fv):
return model.predict_proba(fv)
Now, when Python serializes the model, it stores the reference as model_utils.tokenize, which is also imported within entrypoint.py and therefore no error is thrown.
Now everything will work, but both model_utils.py AND entrypoint.py must be included in the directory passed to enable_explainability().Updated 3 months ago Table of Contents
Troubleshooting
AttributeError When Loading Predict Function
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/registering-a-model-with-the-api
 Registering A Model with the API
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Registering A Model with the APIThis page describes the process of registering models through standard REST API calls.Suggest EditsIntroduction
As an API-first solution, Arthur Scope allows the entire model onboarding process to be fully automated, either through our SDK, or through standard REST API calls. This allows Arthur to be integrated with basically any ML platform, workflow management or automation software. This is possible because every step of the onboarding process can be achieved through API calls.
In this page, we will describe the process of onboarding a model using standard API calls. We will also be providing hints about how the SDK can be leveraged to help with some of these steps, for python-based automation environments.
The main steps required to onboard a model are:
Create the model schema definition and set basic metadata
Save the model
Upload reference dataset for model
Manage model Enrichments (Bias monitoring, Explainability, Hotspots, etc)
Create model Alert Rules (optional)
Create the model schema definition and basic metadata
To onboard a model, Arthur Scope needs information about the attributes (input, non-input, prediction and ground truth), as well as some basic metadata about the model.
When onboarding a model through the API, this information is sent in JSON format, as in the example below:
json{
"display_name": "Credit Risk",
"partner_model_id": "CreditRiskModel_FG_20230523115857",
"description": "Credit Risk Model Created Through REST API",
"input_type": "TABULAR",
"output_type": "MULTICLASS",
"attributes": [
{
"name": "LIMIT_BAL",
"value_type": "INTEGER",
"stage": "PIPELINE_INPUT",
"position": 0,
"categorical": false,
"min_range": 10000,
"max_range": 1000000,
"monitor_for_bias": false,
"is_unique": true,
"is_positive_predicted_attribute": false
},
...
],
"tags": ["Tabular", "Classification", "Credit"],
"is_batch": true,
"version_label": "Credit Risk v1"
}
PS: most of the attributes were removed from this code block, for readability.
Please refer to the documentation for more details on which metadata parameters are required. Do keep in mind that the partner_model_id attribute must be absolutely unique, even across different versions of the model, or for models that were deleted and re-created. The recommendation is to add a timestamp to the ID, in order to avoid potential duplication.
When preparing the model schema, the bulk of the work will go to the task of preparing the list of attributes. As we can see from the example above, the model attributes are sent as a list inside the JSON payload. This list must include all input, non-input (optionally), prediction, and ground truth attributes. Attributes cannot be added once the model is saved, so this list must be complete at model saving time.
Every attribute has several parameters that can be set. This is a more complete list of parameters from the API documentation:
JSON{
"name": "string",
"label": "string",
"value_type": "BOOLEAN",
"stage": "GROUND_TRUTH",
"position": 0,
"categorical": false,
"categories": [],
"min_range": 0,
"max_range": 0,
"monitor_for_bias": false,
"bins": [],
"is_unique": false,
"is_positive_predicted_attribute": false,
"attribute_link": "string",
"gt_class_link": "string",
"implicit": false
}
Do note that some of these parameters only apply to certain types of attributes. A brief description of each attribute can be found in the table below:
AttributeDescriptionnameName of the attribute. Must be unique within the model.labelA friendly label that can be set for attribute names that are encoded.value_typeType of data managed by the attribute (integer, float, etc). A list of supported value types can be found here.stageDetermines whether this attribute is input, non-input, prediction or ground truth.positionThis is an incremental counter that should start with 0 and increase by 1 for each attribute (with the exception of prediction and ground truth attributes, that must start at zero).categoricalSet it to true if the attribute has a limited number of potential values.categoriesList all the categories (potential values) for the attribute. Only relevant if categorical is set to true.min_rangeFor non-categorical, numerical attributes. Lowest numerical value this attribute should receive (PS: this will not be enforced as a threshold limit).max_rangeFor non-categorical, numerical attributes. Highest numerical value this attribute should receive (PS: this will not be enforced as a threshold limit).monitor_for_biasTrue or False. Determines whether or not this attribute should be monitored for bias.binsFor non-categorical, numerical attributes that are being monitored for bias. Describes the bins (or buckets) that Arthur should use to group inferences as it checks for bias.is_uniqueUsed to determined if the values of the attribute are unique (PS: this will not be enforced through unique constraint validation).is_positive_predicted_attributeOnly used for the predicted attribute of a binary classification model (where there's a 'positive' and 'negative' prediction.attribute_linkUsed to associate prediction attributes to their corresponding ground truth attributes.gt_class_linkUsed for single ground truth class models, where the prediction attribute is associated with the corresponding string value of the ground truth class.
Next, let's examine some attributes to see how they can be configured.
JSON{
"name": "LIMIT_BAL",
"value_type": "INTEGER",
"stage": "PIPELINE_INPUT",
"position": 0,
"categorical": false,
"min_range": 10000,
"max_range": 1000000,
"monitor_for_bias": false,
"is_unique": false,
"is_positive_predicted_attribute": false
},
This is an integer input attribute, non-categorical, that can range between 10,000 and 1,000,000. It will not be monitored for bias.
JSON{
"name": "AGE",
"value_type": "INTEGER",
"stage": "PIPELINE_INPUT",
"position": 1,
"monitor_for_bias": true,
"bins" : [
{
"continuous_start" : 0,
"continuous_end" : 35
},
{
"continuous_start" : 35,
"continuous_end" : 55
},
{
"continuous_start" : 55,
"continuous_end" : 100
}
],
"categorical": false,
"min_range": 21,
"max_range": 79,
"is_unique": false,
"is_positive_predicted_attribute": false
},
This is an integer input attribute, non-categorical, being monitored for bias. Because this attribute is non-categorical, we need to specify the bins for bias monitoring, so that Arthur knows how to group the inferences.
JSON{
"name": "SEX",
"value_type": "INTEGER",
"stage": "PIPELINE_INPUT",
"position": 2,
"monitor_for_bias": true,
"categorical": true,
"categories": [
{
"value": "1",
"label": "Male"
},
{
"value": "2",
"label": "Female"
}
],
"is_unique": false,
"is_positive_predicted_attribute": false
},
This is an integer input, categorical attribute. It will be monitored for bias; in this case, Arthur will used the defined categories to group inferences for bias monitoring.
JSON{
"name": "UNIT_CODE",
"value_type": "STRING",
"stage": "PIPELINE_INPUT",
"position": 3,
"categorical": true,
"monitor_for_bias": false,
"categories": [
{ "value": "DW00" }, { "value": "A800" }, { "value": "YZ00" }, { "value": "M 00" }, { "value": "H 00"},
{ "value": "H100" }, { "value": "N 00" }, { "value": "T800" }, { "value": "A 00" }, { "value": "GX00"},
{ "value": "RG00" }, { "value": "JL00" }, { "value": "TC00" }, { "value": "R 00" }, { "value": "LV00"},
{ "value": "E 00" }, { "value": "T 00" }
],
"is_unique": false,
"is_positive_predicted_attribute": false
},
This is a string input. All string input attributes should be categorical.
JSON{
"name": "RISK_AMT",
"value_type": "FLOAT",
"stage": "NON_INPUT_DATA",
"position": 4,
"categorical": false,
"min_range": 10152.23,
"max_range": 999990.0,
"monitor_for_bias": false,
"is_unique": false,
"is_positive_predicted_attribute": false
},
This is a float, non-input attribute. It is non-categorical and it will not be monitored for bias.
JSON{
"name": "prediction_1",
"value_type": "FLOAT",
"stage": "PREDICTED_VALUE",
"position": 1,
"categorical": false,
"min_range": 0,
"max_range": 1,
"monitor_for_bias": false,
"is_unique": false,
"is_positive_predicted_attribute": true,
"attribute_link": "ground_truth_1"
},
{
"name": "ground_truth_1",
"value_type": "INTEGER",
"stage": "GROUND_TRUTH",
"position": 1,
"categorical": true,
"categories": [
{ "value": "0" },
{ "value": "1" }
],
"is_unique": false,
"monitor_for_bias": false,
"is_positive_predicted_attribute": false,
"attribute_link": "prediction_1"
}
This is a pair of matching prediction and ground truth attributes. They must be explicitly correlated for Arthur to be able to calculate performance metrics. Regression models will have one prediction and one ground truth attributes, while multiclass models will have many. For every prediction attribute declared, a corresponding ground truth attribute must also be provided, even if this model is not expected to receive ground truth data at all.
The position of these elements should match as well. In this case, the prediction_1 is the 'positive' prediction made by the model, so we will mark it as such.
Arthur expects the prediction attribute to always be a float (the probability of this class), while the ground truth should be either an integer (0 or 1) or a String.
📘The Arthur SDK can be used to help with the process of mapping model attributes, especially for models with dozens or hundreds of attributes. The SDK provides a build() function that will create a dataframe with the model attributes, based on the reference data provided. This dataframe can then be converted to JSON and used on a direct REST API call. Consider leveraging the Arthur SDK to do the basic mapping of attributes, and then apply your specific logic to ensure the proper value types, ranges, categories, etc.
Save the Model
With the model schema and metadata in place, saving the model is a simple call to:
POST {{hostname}}/api/v3/models
Documentation: https://docs.arthur.ai/api-documentation/v3-api-docs.html#tag/models/paths/~1models/post
PS: this will require credentials with the Model Owner role, or a custom role allowed to create models.
If the call is successful, the return message will include the model ID. This model ID will be required for all subsequent calls, so make sure to save it to an environment variable.
A number of backend operations are executed at model saving time, including creating the database tables for the model, along with Kafka topics and other components. This process should only take a few seconds.
At this time, the model is saved and it has the necessary infrastructure to receive data. However, since no reference dataset is available, Drift and Anomaly Scores will not be calculated. Also, none of the other enrichments will have been enabled.
Upload Reference Dataset for Model
The reference dataset is required to calculate Drift metrics, as well as to train the Anomaly Score model that Arthur will create for each model being monitored.
When models are saved through the Arthur SDK, several tasks happen automatically behind the scenes: the SDK will save the reference data to a parquet or json file, and will upload it to Arthur once the model is done saving. Those steps must be executed explicitly when saving the model through direct API calls.
The reference data must contain all input, non-input, prediction, and ground truth attributes. It must also include the column headers:
It should also be representative of all expected values for the inferences; otherwise, the Drift scores might be misleading. For instance, if the reference dataset is only comprised of records for customers between 20 and 50 years old, any inference data about 80-year-old customers will receive a high drift score.
Setting the reference data is done in 2 steps:
Upload the reference data
Close the reference data
Upload the Reference Data
The parquet or json file can be uploaded to the following API endpoint:
POST {{hostname}}/api/v3/models/{{model_id}}/reference_data
The file should be attached as reference_data. This is a python example for that call:
payload = {}
files=[
('reference_data',('dataset.parquet',open('/Users/.../dataset.parquet','rb'),'application/octet-stream'))
]
headers = {
'Arthur-Organization-ID': '{{organization_id}}',
'Authorization': '{{access_token}}'
}
response = requests.request("POST", url, headers=headers, data=payload, files=files)
PS: The Arthur-Organization-ID header is required for environments with multiple organizations.
The return message will include the number of records that were successfully uploaded. Note this number, as you will need it for the next call.
Close the Reference Data
Arthur will wait until the reference data is closed before doing the backend processing (which includes training the anomaly score model and other tasks). The reference data can not be modified once it's closed, so ensure the proper data is in place before executing the next call.
Reference data can be closed through a PATCH call to the same endpoint as before:
PATCH {{hostname}}/api/v3/models/{{model_id}}/reference_data
The body must include the number of records uploaded, which must match the number of successful records uploaded in the previous step:
JSON{
"status": "uploaded",
"total_record_count": 30000
}
In this case, the reference data uploaded had 30,000 records.
Manage Model Enrichments
Arthur provides a set of standard Enrichments that are available for all model types. Other Enrichments will be specific to certain model types. In this section, we will review the available API endpoints for the different Enrichment capabilities.
Main Documentation page: https://docs.arthur.ai/api-documentation/v3-api-docs.html#tag/enrichments
🚧Keep in mind that most Enrichments will only be applied to new inferences. Make sure to have all Enrichments enabled before the model gets populated with data.
Retrieving a list of current Enrichments
The following API endpoint can be used to fetch a list of Enrichments configured for a model:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments
The return message (for a fully configured model) will look like this:
JSON{
"anomaly_detection": {
"enabled": true,
"config": {}
},
"bias_mitigation": {
"enabled": false
},
"explainability": {
"enabled": true,
"config": {
"sdk_version": "3.25.0",
"python_version": "3.8",
"explanation_algo": "shap",
"model_server_cpu": "1000m",
"model_server_memory": "1Gi",
"explanation_nsamples": 2000,
"shap_expected_values": [
0.8227832963996526,
0.17721670360034752
],
"inference_consumer_cpu": "100m",
"inference_consumer_memory": "256Mi",
"model_server_max_replicas": 2,
"inference_consumer_score_percent": 1,
"streaming_explainability_enabled": true,
"user_predict_function_import_path": "entrypoint",
"inference_consumer_thread_pool_size": 2
}
},
"hotspots": {
"enabled": true,
"config": {}
}
}
In this example, we can see that this model has Anomaly Detection, Explainability and Hotspots enabled, while Bias Mitigation is currently disabled.
It is possible to update the configuration for all Enrichments with a single call. The URL would be the same as described above, only using the PATCH method instead of GET. The JSON block above would be sent as part of the form-multipart request. Do keep in mind that Explainability requires additional assets to be attached (this will be covered further down in this page).
Anomaly Detection
Anomaly Detection is automatically enabled for every model other than Time Series input models, once the Reference Data is uploaded (and closed). Because of this, it does not need to be explicitly enabled after the model is saved. It can be disabled and re-enabled at any time (at which point, the anomaly score model will be re-trained from the reference data).
This is the API endpoint to check the status of the Anomaly Detection Enrichment:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments/anomaly_detection
The return payload will look like this:
JSON{
"enabled": true,
"config": {}
}
To modify the status (enable/disable), the same endpoint can be used, with the PATCH method:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/anomaly_detection
The request body will contain the desired status for the Enrichment:
JSON{
"enabled": false
}
In this case, Anomaly Detection is being disabled for this model.
Bias Mitigation
To check the status of Bias Mitigation, use this API endpoint:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments/bias_mitigation
The return payload will be similar to this:
JSON{
"enabled": false
}
To update the status, use the same endpoint, with the PATCH method:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/bias_mitigation
The request body will contain the desired status for this Enrichment:
JSON{
"enabled": false
}
Hotspots
Hotspots is a capability that aims to identify and surface regions of underperformance in the model. It is currently only available for Tabular - Classification models. Hotspots will not be automatically enabled once the model is saved. It can, however, be enabled at any time after that, and it does not require any additional data or assets.
To check the status of Hotspots, use this API endpoint:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments/hotspots
The return payload will be similar to this:
JSON{
"enabled": true,
"config": {}
}
To update the status, use the same endpoint, with the PATCH method:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/hotspots
The request body will contain the desired status for this Enrichment:
JSON{
"enabled": false
}
Explainability
With Explainability enabled, Arthur is able to provide explanations for each inference. This data is also used for Global Explainability (feature importance, etc). This capability requires Arthur to be able to generate predictions on demand, which means Arthur needs a working model that can be called at any time.
Because of that, there are a number of assets required to enable Explainability, including the model assets, and a python function that can call the model's predict function and return the probability arrays.
Also, some Explainability assets need to be generated by the Arthur SDK, which makes the SDK a required part of the Explainability process.
🚧Arthur uses LIME or SHAP, which are industry accepted algorithms, to produce explanations. This means that the Explainability assets will be python-based. These algorithms will be packaged in an Explainer object, that must be created with the Arthur SDK. This Explainer is uploaded as part of the call to enable Explainability.
In order to enable Explainability for a model, the following assets and files are required:
user_project.zip file
This is a zip file that contains all required model assets, and an 'entrypoint' file
The model assets include anything required to create a prediction: .pkl files, tokenizers, etc. Arthur must be able to generate a prediction from the inference data received.
The entrypoint file is a python file that will run the predict() function. This is the appropriate place for any data manipulation that might be required for the input data: transformations, scaling, one-hot encoding, etc. This file must contain a predict() function and return an array of probabilities.
The model assets must be in the root of the zip file (not in a subfolder). This is important, as it will fail to generate predictions otherwise.
user_requirements_file.txt (requirements.txt file)
This file contains a list of python requirements to run the model. Make sure it includes all pre-reqs to your model, or the entrypoint file will fail to load (with 'package not found' exceptions)
explainer.pkl
This is the Explainer object created by Arthur SDK. It will include the LIME or SHAP algorithm to produce feature importance data.
config
This is a json block with additional settings. It's the same as can be seen in the GET /enrichments example above.
Preparing the user_project zip file
Move all required model assets to a folder. Keep in mind that Arthur will automatically install the packages listed in the requirements.txt file, from a public or private repository.
In that folder, create the entrypoint.py file. This file must contain a predict() function, which Arthur will call at runtime. The predict() function will received a 2-D numpy array, where each item represents a perturbation of the original attribute input data. The default number of perturbations is 5000 (it is a configurable parameter). So the entrypoint file must be able to process an array of input elements. The expected return is a 2-D numpy array with the probability scores. The size of this array should match the size of the input data.
This is an example of entrypoint file:
Pythonimport joblib
import os
from pathlib import Path
model_path = os.path.join(Path(__file__).resolve().parents[0], "credit_model.pkl")
sk_model = joblib.load(model_path)
def predict(x):
return sk_model.predict_proba(x)[:,1]
In this case, the entrypoint file loads the model and uses the predict() function to wrap the predict_proba() function from the model. More complicated cases might require data transformation, one-hot encoding, etc. Keep in mind that the input of the predict() function will always be an array of input elements, and Arthur expects to receive as return an array of probabilities.
Next, prepare the requirements.txt file with all the necessary packages.
Finally, zip the folder, ensuring that the entrypoint.py and requirements.txt files are at the root.
Creating the explainer.pkl file
The easiest way to create this file is by using the Arthur SDK. Depending on the environment, it might be easier to use the enable_explainability() function of the SDK. Some customer environments, however, will prefer to run these functions as pure REST calls, without having to load the Arthur SDK. So the Explainability assets can be prepared in advance and then uploaded through direct REST calls.
The basic steps to create the Explainer object are:
Connect to the Arthur instance
Load the model definition from Arthur
Prepare the (unzipped) user project folder, with the entrypoint and requirements files
Load reference data (can be a small subset)
Create a packager object using the SDK
Extract the Explainer from the packager
Save the Explainer to a .pkl file in the user project folder
Save the model (regressor/classifier/etc) to a .pkl file in the user project folder
This code example walks through those steps:
Pythonconnection = ArthurAI(url = url, login = login, password = password)
arthur_model = connection.get_model(model_id)
import os
project_dir = os.path.join(os.getcwd() + "/explain")
from arthurai.explainability.explanation_packager import ExplanationPackager
packager = ExplanationPackager(
arthur_model,
df=df_reference,
project_directory=project_dir,
user_predict_function_import_path='entrypoint',
streaming_explainability_enabled=True,
requirements_file="requirements.txt",
explanation_algo='lime')
packager.create()
arthur_explainer = packager.explainer
# Test the Explainer by producing 1 explanation
# The input for the Explainer is a list of input values --not a dataframe
# remove non-input, prediction and ground truth attributes from this list
def get_sample(num_samples):
sample_inf = df_reference.sample(num_samples)
del sample_inf['pred']
del sample_inf['gt']
return sample_inf.values.tolist()
sample_inf = get_sample(1)
explanation = arthur_explainer.explain_tabular_lime(sample_inf)
print(explanation)
# Save Model to .pkl file
import dill as pickle
with open("./explain/model.pkl", "wb") as file:
pickle.dump(regressor, file)
# Save Explainer to .pkl file
with open("./explain/explainer.pkl", "wb") as file:
pickle.dump(arthur_explainer, file)
Currently, the project folder will contain the model .pkl file, the Explainer, the entrypoint, and the requirements.file. These are all the assets required to enable Explainability.
Enabling Explainability
This is the API endpoint to enable explainability:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/explainability
This is a multipart/form-data call that must include the elements described above.
The following python code is an example of how this call can be configured:
Pythonheaders = {
'Arthur-Organization-ID': '{{organization_id}}',
'Authorization': '{{access_token}}'
}
files = [
('user_project.zip',('explain.zip',open('./explain.zip','rb'),'application/zip')),
('user_requirements_file.txt',('requirements.txt',open('./explain/requirements.txt','rb'),'text/plain')),
('explainer.pkl',('explainer.pkl',open('./explain/explainer.pkl','rb'),'application/octet-stream'))
]
payload = {
'config': '{"enabled": true, "config":{"python_version": "3.8","sdk_version": "3.25.0","streaming_explainability_enabled": true,"user_predict_function_import_path": "entrypoint","shap_expected_values": [0],"model_server_cpu": "2","model_server_memory": "1500Mi","model_server_max_replicas": 30,"explanation_nsamples": 2000,"explanation_algo": "lime","inference_consumer_cpu": "500m","inference_consumer_memory": "512Mi","inference_consumer_score_percent": 1,"inference_consumer_thread_pool_size": 5}}'
}
exp_response = session.request("PATCH", f'{url}/models/{model_id}/enrichments/explainability',
headers = headers,
data = payload,
files = files)
The expected response is an "ok" string.
In the backend, Arthur will provision a model server, which will be able to provide explanations on demand. This process should take a few minutes to complete.
Appendix: Sending Inferences
Once the model is saved to Arthur and all enrichments are enabled, everything should be ready to receive and process inference data.
Inferences can be sent to Arthur in several different ways: individually or in batches; in JSON format or as parquet, and when using the Arthur SDK, directly as pandas dataframes.
When sending inference data, the following information is required:
Inference timestamp
Inference ID
Inference data (all input values)
Non-input data (optional, but it cannot be uploaded after the fact)
Prediction attributes
Optionally, ground truth data can also be sent (when available).
This is a JSON payload example sending 2 inferences:
JSONpayload = json.dumps([
{
"partner_inference_id": "inf_" + str(uuid.uuid4()),
"inference_timestamp": "2023-06-07T12:00:13.656449Z",
"inference_data": {
"input_1": 1000,
"input_2": "ABDC1234",
"input_3": 9999,
"pred" : 0.85
},
"ground_truth_timestamp": "2023-06-07T12:00:13.656449Z",
"ground_truth_data": {
"gt": 1
}
},
{
"partner_inference_id": "inf_" + str(uuid.uuid4()),
"inference_timestamp": "2023-05-07T12:01:13.656449Z",
"inference_data": {
"input_1": 2000,
"input_2": "ABDC12345",
"input_3": 8888,
"pred" : 0.65
},
"ground_truth_timestamp": "2022-05-07T12:01:13.656449Z",
"ground_truth_data": {
"gt": 0
}
}
])
PS: for batch models, the batch_id parameter is required.
In this example, ground truth data is being sent along with inference data. Arthur supports uploading ground truth data at any moment after receiving the inference data.
This is the API endpoint that can be used to send inferences in JSON format:
POST {{hostname}}/api/v3/models/{{model_id}}/inferences
Sending inferences as parquet files
A different API endpoint is available to process inferences in parquet files:
POST {{hostname}}/api/v3/models/{{model_id}}/inferences/file
The parquet file must contain the headers, as well as the timestamps and inference IDs:
In this case, the parquet file does not contain ground truth data.
The file must be attached to the request as inference_data, as shown in the following example:
Pythonfiles = [
('inference_data',('data.parquet', open('./data.parquet','rb'), 'application/octet-stream'))
]
Uploading Ground Truth Data
When uploading ground truth data, only the ground truth timestamp, ground truth attributes, and inference IDs are required. The inference IDs must match the existing inferences:
The endpoint to upload ground truth data is the same one used to upload inference data:
POST {{hostname}}/api/v3/models/{{model_id}}/inferences/file
The difference will be that the file should be sent as ground_truth_data, instead of inference_data:
Pythonfiles = [
('ground_truth_data',('gt.parquet', open('./gt.parquet','rb'), 'application/octet-stream'))
]
Updated 2 months ago Table of Contents
Introduction
Create the model schema definition and basic metadata
Save the Model
Upload Reference Dataset for Model
Appendix: Sending Inferences
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/full-directory-of-arthur-permissions
 Full Directory of Arthur Permissions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Full Directory of Arthur PermissionsSuggest EditsThese can be used to specify permissions in the custom role mapping configuration JSON under "resource" and "action".
EX:
* "Resource"
* "Action"
* "Action"
* "Action"
Org and User Related Permissions
organization_global
read
write
organization
read
delete
custom_roles
read
write
delete
system_config
write
user
read
write
delete
user_invite
write
organization_metrics
read
Model Related Permissions
model
read
write
delete
tag
read
write
delete
enrichment_config
read
write
metric_query
read
write
delete
raw_data
read
write
ground_truth
write
query
execute
Alert Related Permissions
alert
read
resolve
notify
insight
read
resolve
alert_rule
read
write
delete
alert_notification_config
read
write
delete
alert_summary
notify
alert_summary_config
read
write
delete
alert_summary_subscriber
read
write
delete
Updated about 2 months ago Table of Contents
Org and User Related Permissions
Model Related Permissions
Alert Related Permissions
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/querying-performance
 Default Evaluation Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Default Evaluation FunctionsSuggest EditsArthur provides common default metrics for all Model Input / Output Types. A list of each default metric available can be found within each model types page. This page will provide an overview of all of them to show how to query Arthur.
Regression
All regression evaluation metrics will follow the below request body structure.
Query Request:
JSON{
"select": [
{
"function": "[rmsemaerSquared]",
"alias": "<alias_name> [optional string]",
"parameters": {
"ground_truth_property": "<attribute_name> [string]",
"predicted_property": "<attribute_name> [string]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<evaluation_value> [float]"
}
]
}
RMSE
Get the RMSE between a prediction attribute and a ground truth attribute.
Sample Request:
JSON{
"select": [
{
"function": "rmse",
"alias": "error",
"parameters": {
"ground_truth_property": "FICO_actual",
"predicted_property": "FICO_predicted"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"error": 0.76
}
]
}
back to top
MAE
Get the Mean Absolute Error between a prediction and ground truth attributes.
This function takes an optional parameter aggregation that allows swapping the aggregation from "avg" to either "min" or "max". This can be helpful if you're looking for extremes, such as the lowest or highest absolute error. Additionally, this function supports optional params normalizationMax and normalizationMin that accept numbers and will perform min/max normalization on the values before aggregation if both params are provided.
Query Request:
JSON{
"select": [
{
"function": "mae",
"alias": "<alias_name> [optional string]",
"parameters": {
"predicted_property": "<predicted_property_name> [string]",
"ground_truth_property": "<ground_truth_property_name> [string]",
"aggregation": "[avgminmax] (default avg, optional)",
"normalizationMin": "<value> [optional number]",
"normalizationMax": "<value> [optional number]"
}
}
]
}
Sample Request:
JSON{
"select": [
{
"function": "mae",
"alias": "error",
"parameters": {
"ground_truth_property": "FICO_actual",
"predicted_property": "FICO_predicted"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"error": 0.76
}
]
}
back to top
R Squared
Get the R Squared value between a prediction and ground truth attributes.
Sample Request:
JSON{
"select": [
{
"function": "rSquared",
"alias": "rsq",
"parameters": {
"ground_truth_property": "FICO_actual",
"predicted_property": "FICO_predicted"
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"rsq": 0.94
}
]
}
back to top
Binary Classification
When using binary classification evaluation functions with a multiclass model, outputs will be calculated assuming a one vs. all approach.
Confusion Matrix
Calculates the confusion matrix for a classification model. For binary classifiers, users must specify a probability threshold to count a prediction as a positive class.
Query Request:
JSON{
"select": [
{
"function": "confusionMatrix",
"alias": "<alias_name> [optional string]",
"parameters": {
"threshold": "<value [float]> [required only for binary classifiers]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": {
"true_positive": "<count> [int]",
"false_positive": "<count> [int]",
"true_negative": "<count> [int]",
"false_negative": "<count> [int]"
}
}
]
}
Sample Request: Calculate the confusion matrix for a binary classifier with a threshold of 0.5 (the standard threshold for confusion matrix).
JSON{
"select": [
{
"function": "confusionMatrix",
"parameters": {
"threshold": 0.5
}
}
]
}
Sample Response:
JSON{
"query_result": [
{
"confusionMatrix": {
"true_positive": 100480,
"false_positive": 100076,
"true_negative": 100302,
"false_negative": 99142
}
}
]
}
back to top
Confusion Matrix Rate
Calculates the confusion matrix rates for a classification model. For binary classifiers, users must specify a probability threshold to count a prediction as a positive class.
Query Request:
JSON{
"select": [
{
"function": "confusionMatrixRate",
"alias": "<alias_name> [optional string]",
"parameters": {
"threshold": "<value [float]> [required only for binary classifiers]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": {
"true_positive_rate": "<rate> [float]",
"false_positive_rate": "<rate> [float]",
"true_negative_rate": "<rate> [float]",
"false_negative_rate": "<rate> [float]",
"accuracy_rate": "<rate> [float]"
}
}
]
}
Sample Request: Calculate the confusion matrix for a binary classifier with a threshold of 0.5 (the standard threshold for confusion matrix).
JSON{
"select": [
{
"function": "confusionMatrixRate",
"parameters": {
"threshold": 0.5
}
}
]
}
Response:
JSON{
"query_result": [
{
"confusionMatrixRate": {
"true_positive_rate": 0.5033513340213003,
"false_positive_rate": 0.49943606583557076,
"true_negative_rate": 0.5005639341644292,
"false_negative_rate": 0.4966486659786997
}
}
]
}
back to top
Confusion Matrix Variants
If you only want a specific metric derived from a confusion matrix, you can use one of the following functions:
truePositiveRate
falsePositiveRate
trueNegativeRate
falseNegativeRate
accuracyRate
balancedAccuracyRate
f1
sensitivity
specificity
precision
recall
For example, to return the truePositiveRate:
JSON{
"select": [
{
"function": "truePositiveRate",
"parameters": {
"threshold": 0.5,
"ground_truth_property":"class_a",
"predicted_property":"ground_truth_a"
}
}
]
}
Response:
JSON{
"query_result": [
{
"truePositiveRate": 0.5033513340213003
}
]
}
back to top
AUC
The Area Under the ROC Curve can also be computed for binary classifiers.
Sample Query:
JSON{
"select": [
{
"function": "auc",
"parameters": {
"ground_truth_property":"class_a",
"predicted_property":"ground_truth_a"
}
}
]
}
Response:
JSON{
"query_result": [
{
"auc": 0.9192331426352897
}
]
}
Multi-class Classification
Multi-class Accuracy Rate
Calculates the global accuracy rate.
Query Request:
JSON{
"select": [
{
"function": "accuracyRateMulticlass",
"alias": "<alias_name> [optional string]"
}
]
}
Query Response:
JSON{
"query_result": [
{
"accuracyRateMulticlass": "<rate> [float]"
}
]
}
Example:
JSON{
"select": [
{
"function": "accuracyRateMulticlass"
}
]
}
Response:
JSON{
"query_result": [
{
"accuracyRateMulticlass": 0.785
}
]
}
back to top
Multi-class Confusion Matrix
Calculates the confusion matrix for a multi-class model with regard to a single class.
The predicted attribute and ground truth attribute must be passed as parameters.
Query Request:
JSON{
"select": [
{
"function": "confusionMatrixMulticlass",
"alias": "<alias_name> [optional string]",
"parameters": {
"predicted_property": "<predicted_property_name>",
"ground_truth_property": "<ground_truth_property_name>"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": {
"true_positive": "<count> [int]",
"false_positive": "<count> [int]",
"true_negative": "<count> [int]",
"false_negative": "<count> [int]"
}
}
]
}
Example:
JSON{
"select": [
{
"function": "confusionMatrixMulticlass",
"parameters": {
"predicted_property": "predicted_class_A",
"ground_truth_property": "gt_predicted_class_A"
}
}
]
}
Response:
JSON{
"query_result": [
{
"confusionMatrix": {
"true_positive": 100480,
"false_positive": 100076,
"true_negative": 100302,
"false_negative": 99142
}
}
]
}
back to top
Multi-class Confusion Matrix Rate
Calculates the confusion matrix rates for a multi-class classification model in regards to a single predicted class.
Query Request:
JSON{
"select": [
{
"function": "confusionMatrixRateMulticlass",
"alias": "<alias_name> [optional string]",
"parameters": {
"predicted_property": "predicted_class_A",
"ground_truth_property": "gt_predicted_class_A"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": {
"true_positive_rate": "<rate> [float]",
"false_positive_rate": "<rate> [float]",
"true_negative_rate": "<rate> [float]",
"false_negative_rate": "<rate> [float]",
"accuracy_rate": "<rate> [float]",
"balanced_accuracy_rate": "<rate> [float]",
"precision": "<rate> [float]",
"f1": "<rate> [float]"
}
}
]
}
Example calculating the confusion matrix rates:
JSON{
"select": [
{
"function": "confusionMatrixRateMulticlass",
"parameters": {
"predicted_property": "predicted_class_A",
"ground_truth_property": "gt_predicted_class_A"
}
}
]
}
Response:
JSON{
"query_result": [
{
"confusionMatrixRateMulticlass": {
"true_positive_rate": 0.6831683168316832,
"false_positive_rate": 0.015653220951234198,
"true_negative_rate": 0.9843467790487658,
"false_negative_rate": 0.31683168316831684,
"accuracy_rate": 0.9378818737270875,
"balanced_accuracy_rate": 0.8337575479402245,
"precision": 0.8884120171673819,
"f1": 0.7723880597014925
}
}
]
}
back to top
If you only want a specific value from the confusion matrix rate function, you can use one of the following functions:
truePositiveRateMulticlass
falsePositiveRateMulticlass
trueNegativeRateMulticlass
falseNegativeRateMulticlass
For example, to return the truePositiveRate:
JSON{
"select": [
{
"function": "truePositiveRateMulticlass",
"parameters": {
"predicted_property": "predicted_class_A",
"ground_truth_property": "gt_predicted_class_A"
}
}
]
}
Response:
JSON{
"query_result": [
{
"truePositiveRate": 0.5033513340213003
}
]
}
back to top
Multi-class F1
Calculates the components needed to compute a F1 score for a multi-class model.
In this example, the model has 3 classes: class-1, class-2, class-3 and the corresponding ground truth labels class-1-gt, class-2-gt, class-3-gt.
Query Request:
JSON{
"select": [
{
"function": "count",
"alias": "count"
},
{
"function": "confusionMatrixRateMulticlass",
"alias": "class-1",
"parameters": {
"predicted_property": "class-1",
"ground_truth_property": "class-1-gt"
}
},
{
"function": "countIf",
"alias": "class-1-gt",
"parameters": {
"property": "multiclass_model_ground_truth_class",
"comparator": "eq",
"value": "class-1-gt"
},
"stage": "GROUND_TRUTH"
},
{
"function": "confusionMatrixRateMulticlass",
"alias": "class-2",
"parameters": {
"predicted_property": "class-2",
"ground_truth_property": "class-2-gt"
}
},
{
"function": "countIf",
"alias": "class-2-gt",
"parameters": {
"property": "multiclass_model_ground_truth_class",
"comparator": "eq",
"value": "class-2-gt"
},
"stage": "GROUND_TRUTH"
},
{
"function": "confusionMatrixRateMulticlass",
"alias": "class-3",
"parameters": {
"predicted_property": "class-3",
"ground_truth_property": "class-3-gt"
}
},
{
"function": "countIf",
"alias": "class-3-gt",
"parameters": {
"property": "multiclass_model_ground_truth_class",
"comparator": "eq",
"value": "class-3-gt"
},
"stage": "GROUND_TRUTH"
}
]
}
Query Response:
JSON{
"query_result": [
{
"count": 7044794,
"class-1-gt": 2540963,
"class-2-gt": 2263918,
"class-3-gt": 2239913,
"class-1": {
"true_positive_rate": 0.4318807475748368,
"false_positive_rate": 0.3060401245073361,
"true_negative_rate": 0.6939598754926639,
"false_negative_rate": 0.5681192524251633,
"accuracy_rate": 0.5994314383074935,
"balanced_accuracy_rate": 0.5629203115337503,
"precision": 0.4432575070302042,
"f1": 0.437495178612114
},
"class-2": {
"true_positive_rate": 0.42177322676881407,
"false_positive_rate": 0.3514795196528837,
"true_negative_rate": 0.6485204803471163,
"false_negative_rate": 0.578226773231186,
"accuracy_rate": 0.5756528863725469,
"balanced_accuracy_rate": 0.5351468535579652,
"precision": 0.3623427088234848,
"f1": 0.38980575845890253
},
"class-3": {
"true_positive_rate": 0.26144274353512836,
"false_positive_rate": 0.2805894672521546,
"true_negative_rate": 0.7194105327478454,
"false_negative_rate": 0.7385572564648716,
"accuracy_rate": 0.5737983254017079,
"balanced_accuracy_rate": 0.4904266381414869,
"precision": 0.3028268576818381,
"f1": 0.2806172238153916
}
}
]
}
With this result, you can calculate the weighted F1 score by multiplying each classes's F1 score by the count of the ground truth and dividing by the total count.
In this example, that would be
(class-1.f1 * class-1-gt + class-2.f1 * class-2-gt + class-3.f1 * class-3-gt) / count
and with numbers:
(0.437495178612114 * 2540963 +
0.38980575845890253 * 2263918 +
0.2806172238153916 * 2239913) / 7044794
= 0.3722898785
back to top
Object Detection
Objects Detected
For multiclass, multilabel, and regression models, querying model performance works the same for Arthur computer vision models as more tabular and NLP models. But Object Detection computer vision has some special fields you can use when querying.
Example query fetching all bounding box fields:
JSON{
"select": [
{
"property": "inference_id"
},
{
"property": "objects_detected"
}
]
}
The response will have 1 object per bounding box.
JSON{
"query_result": [
{
"inference_id": "1",
"objects_detected.class_id": 0,
"objects_detected.confidence": 0.6,
"objects_detected.top_left_x": 23,
"objects_detected.top_left_y": 45,
"objects_detected.width": 20,
"objects_detected.height": 30
},
{
"inference_id": "1",
"objects_detected.class_id": 1,
"objects_detected.confidence": 0.6,
"objects_detected.top_left_x": 23,
"objects_detected.top_left_y": 45,
"objects_detected.width": 20,
"objects_detected.height": 30
},
{"inference_id": 2,
"...": "..."}
]
}
You can also specify only a single nested field:
JSON{
"select": [
{
"property": "inference_id"
},
{
"property": "objects_detected.class_id"
},
{
"property": "objects_detected.confidence"
}
]
}
The response will have 1 object per bounding box.
JSON{
"query_result": [
{
"inference_id": "1",
"objects_detected.class_id": 0,
"objects_detected.confidence": 0.6
},
{
"inference_id": "1",
"objects_detected.class_id": 1,
"objects_detected.confidence": 0.6
},
{"inference_id": 2,
"...":
"..."}
]
}
When supplying the bounding box specific fields in filters, group bys, or order bys the columns must also be supplied in the select clause in order for the query to succeed.
Mean Average Precision
Calculates Mean Average Precision for an object detection model. This is used as measure of accuracy for object detection models.
threshold determines the minimum IoU value to be considered a match for a label. predicted_property and ground_truth_property are optional parameters and should be the names of the predicted and ground truth attributes for the model.
They default to "objects_detected" and "label" respectively if nothing is specified for these parameters.
Query Request:
JSON{
"select": [
{
"function": "meanAveragePrecision",
"alias": "<alias_name> [Optional]",
"parameters": {
"threshold": "<threshold> [float]",
"predicted_property": "<predicted_property> [str]",
"ground_truth_property": "<ground_truth_property> [str]"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<result> [float]"
}
]
}
Example:
JSON{
"select": [
{
"function": "meanAveragePrecision",
"parameters": {
"threshold": 0.5,
"predicted_property": "objects_detected",
"ground_truth_property": "label"
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"meanAveragePrecision": 0.78
}
]
}
Generative Text
Token Likelihood
TokenLikelihoods attributes yield two queryable columns for that attribute with suffixes “_tokens” and “_likelihoods” appended to the attribute's name. For example, a model with a TokenLikelihoods attribute named summary_token_probs yields two queryable columns: summary_token_probs_tokens and summary_token_probs_likelihoods which represent an array of the selected tokens and an array of their corresponding likelihoods.
Pythonquery =
{"select": [
{"property": "summary_token_probs_tokens"},
{"property": "summary_token_probs_likelihoods"}
]}
response[{ "summary_token_probs_likelihoods": [
0.3758265972137451,
0.6563436985015869,
0.32000941038131714,
0.5629857182502747],
"summary_token_probs_tokens": [
"this",
"is",
"a",
"summary"] }]
Bias
Bias Mitigation
Calculates mitigated predictions based on conditional thresholds, returning 0/1 for each inference.
This function returns null for inferences that don't match any of the provided conditions.
Query Request:
JSON{
"select":
[
{
"function": "biasMitigatedPredictions",
"alias": "<alias_name> [Optional]",
"parameters":
{
"predicted_property": "<predicted_property> [str]",
"thresholds":
[
{
"conditions":
{
"property": "<attribute_name> [string or nested]",
"comparator": "<comparator> [string] Optional: default 'eq'",
"value": "<string or number to compare with property>"
},
"threshold": "<threshold> [float]"
}
]
}
}
]
}
Query Response:
JSON{
"query_result": [
{
"<function_name/alias_name>": "<result> [int]"
}
]
}
Example:
JSON{
"select":
[
{
"function": "biasMitigatedPredictions",
"parameters":
{
"predicted_property": "prediction_1",
"thresholds":
[
{
"conditions":
[
{
"property": "SEX",
"value": 1
}
],
"threshold": 0.4
},
{
"conditions":
[
{
"property": "SEX",
"value": 2
}
],
"threshold": 0.6
}
]
}
}
]
}
Response:
JSON{
"query_result":
[
{
"SEX": 1,
"biasMitigatedPredictions": 1
},
{
"SEX": 2,
"biasMitigatedPredictions": 0
},
{
"SEX": 1,
"biasMitigatedPredictions": 0
}
]
}
Ranked List Outputs
Performance Metrics
Precision at k
Precision is an indicator of the efficiency of a supervised machine learning model. If one model gets all the relevant items by recommending fewer items than another model, it has a higher precision. For item recommendation models, precision at k measures the fraction of all relevant items among top-k recommended items.
Query Request
JSON{
"select": [
{
"function": "precisionAtK",
"parameters": {
"predicted_property": "predicted_items",
"ground_truth_property": "relevant_items",
"k": 5
}
}
]
}
Query Response
JSON{
"query_result": [
{
"precisionAtK": 0.26666666666666666
}
]
}
Recall at k
Recall is an indicator of the effectiveness of a supervised machine learning model. The model which correctly identifies more of the positive instances gets a higher recall value. In case of recommendations, the recall at k is measured as the fraction of all relevant items that were recovered in top k recommendations.
Query Request
JSON{
"select": [
{
"function": "recallAtK",
"parameters": {
"predicted_property": "predicted_items",
"ground_truth_property": "relevant_items",
"k": 5
}
}
]
}
Query Response
JSON{
"query_result": [
{
"recallAtK": 0.26666666666666666
}
]
}
Mean Average Precision at k (MAP @ k)
The MAP@K metric is the most commonly used metric for evaluating recommender systems. It calculates the precision at every location 1 through k where there is a relevant item. Average Precision is calculated per inference, then the per inference values are averaged across a group of inferences to create Mean Average Precision.
Query Request
JSON{
"select": [
{
"function": "mapAtK",
"parameters": {
"predicted_property": "predicted_items",
"ground_truth_property": "relevant_items",
"k": 5
}
}
]
}
Query Response
JSON{
"query_result": [
{
"mapAtK": 0.26666666666666666
}
]
}
Normalized Discounted Cumulative Gain at k (nDCG @ k)
nDCG measures the overall reward at all positions that hold a relevant item. The reward is an inverse log of the position (i.e. higher ranks for relevant items would lead to better reward, as desired).
Similar to MAP@k, this metric calculates a value per inference, then is aggregated across inferences using an average.
Query Request
JSON{
"select": [
{
"function": "nDCGAtK",
"parameters": {
"predicted_property": "predicted_items",
"ground_truth_property": "relevant_items",
"k": 5
}
}
]
}
Query Response
JSON{
"query_result": [
{
"nDCGAtK": 0.26666666666666666
}
]
}
AUC
In the case of ranked list metrics, AUC measures the likelihood that a random relevant item is ranked higher than a random irrelevant item. Higher the likelihood of this happening implies a higher AUC score meaning a better recommendation system. We calculate this likelihood empirically based on the ranks given by the algorithm to all items — out of all possible pairs of type (relevant-item, non-relevant-item), AUC is a proportion of pairs where the relevant item was ranked higher than the irrelevant item from that pair.
This metric is calculated per-inference, then aggregated as an average over a group of inferences.
Query Request
JSON{
"select": [
{
"function": "rankedListAUC",
"parameters": {
"predicted_property": "predicted_items",
"ground_truth_property": "relevant_items"
}
}
]
}
Query Response
JSON{
"query_result": [
{
"rankedListAUC": 0.26666666666666666
}
]
}
Mean Reciprocal Rank (MRR)
Mean Reciprocal Rank quantifies the rank of the first relevant item found in the recommendation list. It takes the reciprocal of this “first relevant item rank”, meaning that if the first item is relevant (i.e. the ideal case) then MRR will be 1, otherwise it will be lower.
Query Request
JSON{
"select": [
{
"function": "meanReciprocalRank",
"parameters": {
"predicted_property": "predicted_items",
"ground_truth_property": "relevant_items"
}
}
]
}
Query Response
JSON{
"query_result": [
{
"meanReciprocalRank": 0.26666666666666666
}
]
}
back to topUpdated 3 months ago Table of Contents
Regression
RMSE
MAE
R Squared
Binary Classification
Confusion Matrix
Confusion Matrix Rate
Confusion Matrix Variants
AUC
Multi-class Classification
Multi-class Accuracy Rate
Multi-class Confusion Matrix
Multi-class Confusion Matrix Rate
Multi-class F1
Object Detection
Objects Detected
Mean Average Precision
Generative Text
Token Likelihood
Bias
Bias Mitigation
Ranked List Outputs
Performance Metrics
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/multi-class-classification
 Multiclass Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Multiclass ClassificationSuggest EditsMulticlass classification models predict one class from more than two potential classes. In Arthur, these models fall into the category of classification and are represented by the Multiclass model type.
Some common examples of Tabular multiclass classification are:
What breed of dog is in this photo?
What part of the car is damaged in this photo?
Similar to binary classification, these models frequently output not only the predicted class but also a probability for each class predicted. The highest probability class is then the predicted output. In these cases, a threshold does not need to be provided to Arthur and it will automatically track the highest probability class as the predicted output.
Formatted Data in Arthur
Tabular binary classification models require three things to be specified in their schema: all predicting model attributes (or features), predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (numeric or categorical)Attribute (numeric or categorical)Probability of Prediction AProbability of Prediction BProbability of Prediction CGround TruthNon-Input Attribute (numeric or categorical)High School Education34.5.90.05.05AMaleGraduate Degree44.1.46.14.40BFemaleGraduate Degree33.5.16.17.71CFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their multi-class classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 2 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Option 1:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
output_mapping_1 = {
'pred_class_one_column':'one',
'pred_class_two_column':'two',
'pred_class_three_column':'three'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=output_mapping_1
)
## Option 2:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
output_mapping_2 = {
'pred_class_one_column':'gt_class_one_column',
'pred_class_two_column':'gt_class_two_column',
'pred_class_three_column':'gt_class_three_column'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping_2
)
## Example prediction function for classification
def predict(x):
return model.predict_proba(x)
Available Metrics
When onboarding multiclass classification models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a multiclass classification model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/quickstart
 Quickstart
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by QuickstartSuggest EditsFrom a Python environment with the arthurai package installed, this quickstart code will:
Make binary classification predictions on a small dataset
Onboard the model with reference data to Arthur
Log batches of model inference data with Arthur
Get performance results for our model
Imports
The arthurai package can be pip-installed from the terminal, along with numpy and pandas:
Bashpip install arthurai numpy pandas
Then you can import the functionality we'll use from the arthurai package like this:
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
from arthurai.util import generate_timestamps
# Other libraries used in this example
import numpy as np
import pandas as pd
Model Predictions
We write out samples from a Titanic survival prediction dataset explicitly in Python,
giving the age of each passenger, the cost of their ticket, the passenger class of their ticket, and the ground-truth label of whether they survived. Our model's outputs are given by a predict function using only the age variable. We split the data into
reference_data for onboarding the model
inference_data for in-production inferences the model processes
Python# Define Titanic sample data
titanic_data = pd.DataFrame({
"age":[19.0,37.0,65.0,30.0,22.0,24.0,16.0,40.0,58.0,32.0],
"fare":[8.05,29.7,7.75,7.8958,7.75,49.5042,86.5,7.8958,153.4625,7.8958],
"passenger_class":[3,1,3,3,3,1,1,3,1,3],
"survived":[1,0,0,0,1,1,1,0,1,0]})
# Split into reference and inference data
reference_data, inference_data = titanic_data[:6].copy(), titanic_data[6:].copy()
# Predict the probability of Titanic survival as inverse percentile of age
def predict(age):
nearest_age_index = np.argmin(np.abs(np.sort(reference_data['age']) - age))
return 1 - (nearest_age_index / (len(reference_data) - 1))
# reference_data and inference_data contain the model's inputs and outputs
reference_data['pred_survived'] = reference_data['age'].apply(predict)
inference_data['pred_survived'] = inference_data['age'].apply(predict)
Onboarding
This code will only run once you enter a valid username.
First we connect to the Arthur API and create an arthur_model with some high-level metadata: a classification model operating on tabular data with the name "TitanicQuickstart".
Python# Connect to Arthur
arthur = ArthurAI(url="https://app.arthur.ai",
login="<YOUR_USERNAME_OR_EMAIL>",
password=os.environ['ARTHUR_PASSWORD'])
# Register the model type with Arthur
arthur_model = arthur.model(display_name="Example: Titanic Quickstart",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass)
Next, we infer the model schema from thereference_data, specifying which attributes are in which {ref}stage <basic_concepts_attributes_and_stages>. Additionally, we configure extra settings for the passenger_class attribute. Then we save the model to the platform.
Python# Map PredictedValue attribute to its corresponding GroundTruth attribute value.
# This tells Arthur that the `pred_survived` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map = {'pred_survived' : 1}
# Build arthur_model schema on the reference dataset,
# specifying which attribute represents ground truth
# and which attributes are NonInputData.
# Arthur will monitor NonInputData attributes even though they are not model inputs.
arthur_model.build(reference_data,
ground_truth_column='survived',
pred_to_ground_truth_map=pred_to_ground_truth_map,
non_input_columns=['fare', 'passenger_class'])
# Configure the `passenger_class` attribute
# 1. Turn on bias monitoring for the attribute.
# 2. Specify that the passenger_class attribute has possible values [1, 2, 3],
# since that information was not present in reference_data (only values 1 and 3 are present).
arthur_model.get_attribute(name='passenger_class').set(monitor_for_bias=True,
categories=[1,2,3])
# onboard the model to Arthur
arthur_model.save()
Sending Inferences
Here we send inferences from inference_data to Arthur. We'll oversample inference_data and use Arthur's utility function to generate some fake timestamps as though the inferences were made over the last five days.
Python# Sample the inference dataset with predictions
inferences = inference_data.sample(100, replace=True)
# Generate mock timestamps over the last five days
timestamps = generate_timestamps(len(inferences), duration='5d')
# Send the inferences to Arthur
arthur_model.send_inferences(inferences, inference_timestamps=timestamps)
Inferences usually become available for analysis in seconds, but it can take up to a few minutes. You can wait until they're ready for your analysis like this:
Python# Wait until some inferences land in Arthur
arthur_model.await_inferences()
Performance Results
With our model onboarded and inferences sent, we can get performance results from Arthur. View your model in your Arthur dashboard, or use the code below to fetch the overall accuracy rate:
Python# Query overall model accuracy
query = {
"select": [
{
"function": "accuracyRate"
}
]
}
query_result = arthur_model.query(query)
print(query_result)
You should see [{'accuracyRate': 0.8}] or a similar value depending on the random sampling of your inference set.Updated 3 months ago What’s NextLearn more about important terms with the Core Concepts in Arthur page, try out in-depth examples in our Arthur Github Sandbox, or start your in-depth onboarding walkthrough with the Data Preparation for Arthur page.Core Concepts in ArthurArthur Sandbox GitHub RepositoryData Preparation for ArthurTable of Contents
Imports
Model Predictions
Onboarding
Sending Inferences
Performance Results
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/grouped-inference-queries
 Grouped Inference Queries
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Grouped Inference QueriesSuggest EditsInitial analyses that treat inferences as independent of one another can provide tremendous value. But over time, models often make multiple predictions about the same real-world entities. No matter what you're predicting, it can be helpful to compare the inputs and outputs of your model on an entity-by-entity basis.
For example, let's say that your model makes predictions about whether customers will make a purchase in the next 30 days. You might have the following attributes:
customer_id: a non-input attribute
will_purchase_pred: the prediction attribute: whether a customer will make a purchase in the next 30 days
will_purchase_gt: the ground truth attribute: whether a customer actually did make a purchase within 30 days
recent_purchase_count: an input attribute with the total number of purchases the customer made in the last 90 days
newsletter_subscriber: an input attribute depicting whether the customer subscribes to the deals newsletter
Your model might be run on the full universe of Customer IDs at some regular interval. With Arthur's powerful Query API, you can follow inferences for each Customer ID through time and answer questions like:
How does recent_purchase_count tend to change for each customer, from the first to last time inference is conducted?
What is the per-customer variance of recent_purchase_count across time?
How many customers changed their newsletter subscription status, from one month ago to today?
What is the distribution of the lifetimes of Customer IDs?
Example Queries
We'll walk through some example queries for these entity-by-entity comparisons, exploring the sample case outlined above.
Per-Customer Variance
We can look at how consistent recent_purchase_count is for each customer across time. We'll compute the variance in recent_purchase_count for each customer across all their inferences, and then roll those individual variances up into a distribution.
JSON{
"select": [
{
"function": "distribution",
"alias": "recent_purchase_count_variance_distribution",
"parameters": {
"property": {
"nested_function": {
"function": "variance",
"parameters": {
"property": "recent_purchase_count"
}
}
},
"num_bins": 20
}
}
],
"subquery": {
"select": [
{
"property": "recent_purchase_count"
},
{
"property": "customer_id"
}
],
"group_by": [
{
"property": "customer_id"
}
]
}
}
Change Across Batches
If our model is a batch model, we might want to compare the values for each customer between two difference batches. We'll again look at the distribution of change in the recent_purchase_count, but this time look at the difference for each customer between two specific batches.
JSON{
"select": [
{
"function": "distribution",
"alias": "recent_purchase_count_difference_distribution",
"parameters": {
"property": {
"nested_function": {
"function": "subtract",
"parameters": {
"left": "batch1_recent_purchase_count",
"right": "batch2_recent_purchase_count"
}
}
},
"num_bins": 20
}
}
],
"subquery": {
"select": [
{
"property": "customer_id"
},
{
"property": "batch1_recent_purchase_count"
},
{
"property": "batch2_recent_purchase_count"
}
],
"subquery": {
"select": [
{
"property": "customer_id"
},
{
"function": "anyIf",
"parameters": {
"result": "recent_purchase_count",
"property": "batch_id",
"comparator": "eq",
"value": "batch1"
},
"alias": "batch1_recent_purchase_count"
},
{
"function": "anyIf",
"parameters": {
"result": "recent_purchase_count",
"property": "batch_id",
"comparator": "eq",
"value": "batch2"
},
"alias": "batch2_recent_purchase_count"
}
],
"group_by": [
{
"property": "customer_id"
}
]
},
"where": [
{
"property": "batch1_recent_purchase_count",
"comparator": "NotNull"
},
{
"property": "batch2_recent_purchase_count",
"comparator": "NotNull"
}
]
}
}
Change Across First to Last Inference Per Customer
We can again compare the difference between two absolute points, but instead of comparing fixed batches compute it for the earliest and latest inference for each customer:
JSON{
"select": [
{
"function": "distribution",
"alias": "recent_purchase_count_difference_distribution",
"parameters": {
"property": {
"nested_function": {
"function": "subtract",
"parameters": {
"left": "newest_recent_purchase_count",
"right": "oldest_recent_purchase_count"
}
}
},
"num_bins": 20
}
}
],
"subquery": {
"select": [
{
"property": "customer_id"
},
{
"function": "argMax",
"parameters": {
"argument": "inference_timestamp",
"value": "recent_purchase_count"
},
"alias": "newest_recent_purchase_count"
},
{
"function": "argMin",
"parameters": {
"argument": "inference_timestamp",
"value": "recent_purchase_count"
},
"alias": "oldest_recent_purchase_count"
}
],
"group_by": [
{
"property": "customer_id"
}
]
}
}
Change in Categorical Variables
We can also look at change in categorical variables on an entity-by-entity basis. Let's look at the distribution of customers who remained subscribed, remained unsubscribed, newly subscribed, or newly unsubscribed from one batch to the next.
JSON{
"select": [
{
"alias": "batch1_not_subscribed",
"function": "equals",
"parameters": {
"left": "batch1_newsletter_subscriber",
"right": 0
}
},
{
"alias": "batch1_is_subscribed",
"function": "equals",
"parameters": {
"left": "batch1_newsletter_subscriber",
"right": 1
}
},
{
"alias": "batch2_not_subscribed",
"function": "equals",
"parameters": {
"left": "batch2_newsletter_subscriber",
"right": 0
}
},
{
"alias": "batch2_is_subscribed",
"function": "equals",
"parameters": {
"left": "batch2_newsletter_subscriber",
"right": 1
}
},
{
"alias": "stayed_unsubscribed_count",
"function": "and",
"parameters": {
"left": {
"alias_ref": "batch1_not_subscribed"
},
"right": {
"alias_ref": "batch2_not_subscribed"
}
}
},
{
"alias": "did_subscribe_count",
"function": "and",
"parameters": {
"left": {
"alias_ref": "batch1_not_subscribed"
},
"right": {
"alias_ref": "batch2_is_subscribed"
}
}
},
{
"alias": "stayed_subscribed_count",
"function": "and",
"parameters": {
"left": {
"alias_ref": "batch1_is_subscribed"
},
"right": {
"alias_ref": "batch2_is_subscribed"
}
}
},
{
"alias": "did_unsubscribe_count",
"function": "and",
"parameters": {
"left": {
"alias_ref": "batch1_is_subscribed"
},
"right": {
"alias_ref": "batch2_not_subscribed"
}
}
}
],
"subquery": {
"select": [
{
"property": "customer_id"
},
{
"property": "batch1_newsletter_subscriber"
},
{
"property": "batch2_newsletter_subscriber"
}
],
"subquery": {
"select": [
{
"property": "customer_id"
},
{
"function": "anyIf",
"parameters": {
"result": "newsletter_subscriber",
"property": "batch_id",
"comparator": "eq",
"value": "batch1"
},
"alias": "batch1_newsletter_subscriber"
},
{
"function": "anyIf",
"parameters": {
"result": "newsletter_subscriber",
"property": "batch_id",
"comparator": "eq",
"value": "batch2"
},
"alias": "batch2_newsletter_subscriber"
}
],
"group_by": [
{
"property": "customer_id"
}
]
},
"where": [
{
"property": "batch1_newsletter_subscriber",
"comparator": "NotNull"
},
{
"property": "batch2_newsletter_subscriber",
"comparator": "NotNull"
}
]
}
}
Updated 3 months ago Table of Contents
Example Queries
Per-Customer Variance
Change Across Batches
Change Across First to Last Inference Per Customer
Change in Categorical Variables
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/audit-log
 Audit Log
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Audit LogSuggest EditsThe Arthur platform has the ability to produce an audit log of all calls to sensitive endpoints that include models, organizations, RBAC, and uploading / modifying data.
Event Format
Each event in the audit log has the following fields:
FieldTypeNotesevent_categorystringA description of the overarching category for this event. See the table below for a breakdown of the various categories.event_typestringAn explanation of what kind of event occurred within the event_category. See the table below for a breakdown of the various types.event_idstringA unique ID for this event, currently in UUID format but this may change in the future.timestamp[string, int]A timestamp in either Unix Epoch millisecond integer format or RFC 3339 string format, depending on the point of integration.organization_id[string, null]A string UUID of the organization if there is one associated with the event.model_id[string, null]A string UUID of the model if there is one associated with the event.user_id[string, null]A string ID of the user if there is one associated with the event.user_type[string, null]A string description of the kind of user if there is one associated with the event. This can be one of: service-account, arthur-managed, or idp-managed.http_path[string, null]A string HTTP path of the request that triggered the event if one exists.http_method[string, null]A string HTTP method of the request that triggered the event if one exists.http_status_code[int, null]An integer HTTP status code of the request that triggered the event if one exists.
Logged Endpoints
When enabled, Audit Logging will track all requests made to the following endpoints and set the Event Category and Event Type respectively in the audit log events.
EndpointMethodEvent CategoryEvent Type/organizationsPOSTevents.arthur.ai/organizationcreated/organizations/{organization_id}DELETEevents.arthur.ai/organizationdeleted/modelsPOSTevents.arthur.ai/modelcreated/models/{model_id}PUTevents.arthur.ai/modelupdated/models/{model_id}DELETEevents.arthur.ai/modeldeleted/alerts/{alert_id}/notificationsPOSTevents.arthur.ai/alertcreated/models/{model_id}/inferencesPOSTevents.arthur.ai/ingestioninference_data_received/models/{model_id}/inferencesPATCHevents.arthur.ai/ingestionground_truth_data_received/models/{model_id}/inferences/filePOSTevents.arthur.ai/ingestioninference_data_received/models/{model_id}/reference_dataPOSTevents.arthur.ai/ingestionreference_data_received/models/{model_id}/batches/{batch_id}PATCHevents.arthur.ai/ingestioninference_data_batch_completed/models/{model_id}/reference_dataPATCHevents.arthur.ai/ingestionreference_data_upload_completed/models/{model_id}/metricsPOSTevents.arthur.ai/metricscreated/models/{model_id}/metrics/{metric_id}PUTevents.arthur.ai/metricsupdated/models/{model_id}/metrics/{metric_id}DELETEevents.arthur.ai/metricsdeleted/authorization/custom_rolesPOSTevents.arthur.ai/rbacupdated/authorization/custom_rolesDELETEevents.arthur.ai/rbacupdated
A more thorough description of these endpoints is available at our API Documentation.
Integration with EventBridge
The on-prem installation provides support for shipping the Audit Log to AWS EventBridge. To configure this, you will need the following:
Bus Name: Required. The name of the EventBridge bus. This should not be the full ARN of the bus.
Region: Required. This is the AWS region where your EventBridge bus is located.
Source: Optional. This value will be added to the EventBridge events "source" for all events. This defaults to "arthur-audit-log".
Detail Type: Optional. This value will be added to the EventBridge events "detail-type" for all events. This defaults to "events.arthur.ai."
An example of the events that are written to EventBridge looks like the following (this was captured via an EventBridge to CloudWatch Log Group rule and target):
JSON{
"version": "0",
"id": "b87f2a3a-6be1-e1d9-bc94-720d60e0a9d8",
"detail-type": "events.arthur.ai",
"source": "arthur-audit-log",
"account": "1234567890",
"time": "2022-07-21T22:07:00Z",
"region": "us-east-2",
"resources": [],
"detail": {
"event_type": "created",
"event_category": "events.arthur.ai/model",
"event_id": "da2ec82d-f581-4e72-bb66-fc82504f2a7e",
"timestamp": "2022-07-21T22:06:59.683+0000",
"organization_id": "d579359a-7259-4397-a08b-3e36c212350f",
"model_id": "a950c9ad-6a1e-4042-8e47-461d13072da5",
"user_id": "df3fe374-26d7-4bd8-bf62-e04a6e078e2b",
"user_type": "arthur-managed",
"http_path": "/api/v3/models",
"http_method": "POST",
"http_status_code": 200
}
}
Configuration
The EventBridge integration can be enabled on the Admin Console Config Page by:
Checking "Show Other Advanced Options" under the Other Advanced Options section
After that is checked, a new section will appear called "Audit Logging"
Check "Enable Audit Log"
Next, a choice of persistence methods appears. Choose "AWS EventBridge"
Fill out the "Bus Name," "Region," "Event Source," and "Detail Type" fields that appear.
Click "Save config" and deploy the updated version
Required IAM Permissions
To send events to AWS EventBridge, the Arthur IAM credentials or role will require the events:PutEvents permission. Here is an example policy that grants that permission on an EventBridge bus called arthur-events in the us-east-2 region, in the 0123456789 AWS account.
JSON{
"Statement": [
{
"Action": "events:PutEvents",
"Effect": "Allow",
"Resource": "arn:aws:events:us-east-2:0123456789:event-bus/arthur-events",
"Sid": ""
}
],
"Version": "2012-10-17"
}
Updated 3 months ago Table of Contents
Event Format
Logged Endpoints
Integration with EventBridge
Configuration
Required IAM Permissions
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install
 Online Kubernetes Cluster (K8s) Install
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Online Kubernetes Cluster (K8s) InstallSuggest EditsMake sure your K8s cluster is ready for Arthur platform installation by following the Kubernetes Preparation guide.
Download Installation Files
Go to the download portal using the URL and the password provided by Arthur.
Click the "Download license" button to download your license in the YAML file.
Setup for Installation
Make sure you're in the correct kubectl environment context before running the installer.
Install the KOTS kubectl extension on your local machine:
Shellcurl https://kots.io/install  bash
Start Installation
Run the Admin Console installer and login on to your browser at localhost:8800 via the provided port forwarding tunnel:
Shellkubectl kots install arthur
For Namespace-Scoped Installs, follow this {doc}guide <k8s_install_namespace_scoped>.
When you need to re-create the tunnel to Admin Console, run:
Shellkubectl kots admin-console --namespace <your_name_space>
Upload your license file.
On the following screen, click on the link to install Arthur from the Internet.
Configure Arthur.
Review the preflight checks to make sure that your machine meets the minimum requirements before you proceed with the installation.
Verify Installation
Monitor the admin console dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with thekubectl CLI:
Shellkubectl get deployment,statefulset,pod -n <yournamespace>
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
Customize Installation
Configure graphs on Admin Console by clicking on the Configure graphs button and providing your Prometheus endpoint (e.g.,http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090).Updated 3 months ago Table of Contents
Download Installation Files
Setup for Installation
Start Installation
Verify Installation
Customize Installation
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/querying-data-drift
 Querying Data Drift
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Querying Data DriftSuggest EditsQuerying Drift in Python
The basic format of a drift query using the Python SDK involves specifying that the
query_type parameter has the value 'drift':
Pythonquery = {...}
arthur_model.query(query, query_type='drift')
Data Drift Endpoint
Data drift has a dedicated endpoint at Query Data Drift.
Returns the data drift metric between a base dataset with a target dataset. This endpoint can support up to 100 properties in one request.
num_bins - Specifies the granularity of bucketing for continuous distributions and will be ignored if the attribute is categorical.
metric - Specify one metric among {ref}the data drift metrics Arthur offers <glossary_data_drift>.
filter - Optional blocks specific to either reference or inference set to specify which data should be used in the data drift calculation.
group_by - Global and applies to both the base and target data.
rollup - Optional parameter aggregating the calculated data drift value by the supported time dimension.
For HypothesisTest, the returned value is transformed as -log_10(P_value) to maintain directional parity with the other data drift metrics. A lower P_value is more significant and implies data drift, reflected in a higher -log_10(P_value). Further mathematical details are in the Glossary
Query Request:
JSON{
"properties": [
"<attribute1_name> [string]",
"<attribute2_name> [string]",
"<attribute3_name> [string]"
],
"num_bins": "<num_bins> [int]",
"metric": "[PSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest]",
"base": {
"source": "[inferencereference]",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"target": {
"source": "[inferencereferenceground_truth]",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"group_by [Optional]": [
{
"property": "<group_by_attribute_name> [string]"
}
],
"rollup [Optional]": "minutehourdaymonthyearbatch_id"
}
Query Response:
JSON{
"query_result": [
{
"<attribute1_name>": "<attribute1_data_drift> [float]",
"<attribute2_name>": "<attribute2_data_drift> [float]",
"<attribute3_name>": "<attribute3_data_drift> [float]",
"<group_by_attribute_name>": "<group_by_attribute_value> [stringintnull]",
"rollup": "<rollup_attribute_value> [stringnull]"
}
]
}
Example: Reference vs. Inference
Sample Request: Calculate data drift for males, grouped by country, rolled up by hour.
JSON{
"properties": [
"age"
],
"num_bins": 10,
"metric": "PSI",
"base": {
"source": "reference",
"filter": [
{
"property": "gender",
"comparator": "eq",
"value": "male"
}
]
},
"target": {
"source": "inference",
"filter": [
{
"property": "gender",
"comparator": "eq",
"value": "male"
},
{
"property": "inference_timestamp",
"comparator": "gte",
"value": "2020-07-22T10:00:00Z"
},
{
"property": "inference_timestamp",
"comparator": "lt",
"value": "2020-07-23T10:00:00Z"
}
]
},
"group_by": [
{
"property": "country"
}
],
"rollup": "hour"
}
Sample Response:
JSON{
"query_result": [
{
"age": 2.3,
"country": "Canada",
"rollup": "2020-07-22T10:00:00Z"
},
{
"age": 2.4,
"country": "United States",
"rollup": "2020-07-22T10:00:00Z"
}
]
}
Example: Inference vs. Inference
Sample Request: Compare data drift between two batches, with no grouping, filters, or rollups.
JSON{
"properties": [
"age"
],
"num_bins": 10,
"metric": "PSI",
"base": {
"source": "inference",
"filter": [
{
"property": "batch_id",
"comparator": "eq",
"value": "5"
}
]
},
"target": {
"source": "inference",
"filter": [
{
"property": "batch_id",
"comparator": "eq",
"value": "6"
}
]
}
}
Sample Response:
JSON{
"query_result": [
{
"age": 2.3
}
]
}
back to top
Example: Reference vs. Ground Truth
Sample Request: Calculate data drift for individual ground truth class prediction probabilities, rolled up by hour.
JSON{
"properties": [
"gt_1"
],
"num_bins": 10,
"metric": "PSI",
"base": {
"source": "reference"
},
"target": {
"source": "ground_truth",
"filter": [
{
"property": "ground_truth_timestamp",
"comparator": "gte",
"value": "2020-07-22T10:00:00Z"
},
{
"property": "ground_truth_timestamp",
"comparator": "lt",
"value": "2020-07-23T10:00:00Z"
}
]
},
"rollup": "hour"
}
Sample Response:
JSON{
"query_result": [
{
"gt_1": 0.03,
"rollup": "2020-07-22T10:00:00Z"
},
{
"gt_1": 0.4,
"rollup": "2020-07-22T11:00:00Z"
}
]
}
back to top
Data Drift PSI Bucket Table Values
This metric has a dedicated endpoint at Query PSI Bucket Table.
Returns the PSI scores by bucket using the reference set data. This query for this endpoint omits the need for metric and takes in a single property but otherwise is identical to the data drift endpoint
Note when using this endpoint with categorical features, the bucket_min and bucket_max fields will not be
returned in the response. Instead, the bucket field will contain the category name.
Query Request:
JSON{
"property": "<attribute_name> [string]",
"num_bins": "<num_bins> [int]",
"base": {
"source": "[inferencereference]",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"target": {
"source": "[inferencereference]",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"group_by [Optional]": [
{
"property": "<group_by_attribute_name> [string]"
}
],
"rollup [Optional]": "minutehourdaymonthyearbatch_id"
}
Query Response:
JSON{
"query_result": [
{
"bucket": "string",
"rollup": "stringnull",
"group_by_property_1": "stringnull",
"base_bucket_max": "number",
"base_bucket_min": "number",
"base_count_per_bucket": "number",
"base_ln_probability_per_bucket": "number",
"base_probability_per_bucket": "number",
"base_total": "number",
"target_bucket_max": "number",
"target_bucket_min": "number",
"target_count_per_bucket": "number",
"target_ln_probability_per_bucket": "number",
"target_probability_per_bucket": "number",
"target_total": "number",
"probability_difference": "number",
"ln_probability_difference": "number",
"psi": "number"
}
]
}
Sample Request: Calculate data drift bucket components for males, grouped by country, rolled up by hour.
JSON{
"property": "age",
"num_bins": 2,
"base": {
"source": "reference",
"filter": [
{
"property": "gender",
"comparator": "eq",
"value": "male"
}
]
},
"target": {
"source": "inference",
"filter": [
{
"property": "gender",
"comparator": "eq",
"value": "male"
},
{
"property": "inference_timestamp",
"comparator": "gte",
"value": "2020-07-22T10:00:00Z"
},
{
"property": "inference_timestamp",
"comparator": "lt",
"value": "2020-07-23T10:00:00Z"
}
]
},
"group_by": [
{
"property": "country"
}
],
"rollup": "hour"
}
Sample Response:
JSON{
"query_result": [
{
"bucket": "bucket_1",
"rollup": "2020-01-01T00:00:00Z",
"country": "Canada",
"base_bucket_max": 0.9999971182990177,
"base_bucket_min": 0.5009102069226075,
"base_count_per_bucket": 4988,
"base_ln_probability_per_bucket": -0.6955500651756032,
"base_probability_per_bucket": 0.4988,
"base_total": 10000,
"target_bucket_max": 0.9999971182990177,
"target_bucket_min": 0.5009102069226075,
"target_count_per_bucket": 2487,
"target_ln_probability_per_bucket": -0.6701670131762315,
"target_probability_per_bucket": 0.5116231228142357,
"target_total": 4861,
"probability_difference": -0.012823122814235699,
"ln_probability_difference": -0.025383051999371742,
"psi": 0.00032548999318807485
},
{
"bucket": "bucket_2",
"rollup": "2020-01-01T00:00:00Z",
"country": "United States",
"base_bucket_max": 0.9999971182990177,
"base_bucket_min": 0.5009102069226075,
"base_count_per_bucket": 4988,
"base_ln_probability_per_bucket": -0.6955500651756032,
"base_probability_per_bucket": 0.4988,
"base_total": 10000,
"target_bucket_max": 0.9999971182990177,
"target_bucket_min": 0.5009102069226075,
"target_count_per_bucket": 2487,
"target_ln_probability_per_bucket": -0.6701670131762315,
"target_probability_per_bucket": 0.5116231228142357,
"target_total": 4861,
"probability_difference": -0.012823122814235699,
"ln_probability_difference": -0.025383051999371742,
"psi": 0.00032548999318807485
},
{
"bucket": "bucket_1",
"rollup": "2020-01-01T01:00:00Z",
"country": "Canada",
"base_bucket_max": 0.9999971182990177,
"base_bucket_min": 0.5009102069226075,
"base_count_per_bucket": 4988,
"base_ln_probability_per_bucket": -0.6955500651756032,
"base_probability_per_bucket": 0.4988,
"base_total": 10000,
"target_bucket_max": 0.9999971182990177,
"target_bucket_min": 0.5009102069226075,
"target_count_per_bucket": 2487,
"target_ln_probability_per_bucket": -0.6701670131762315,
"target_probability_per_bucket": 0.5116231228142357,
"target_total": 4861,
"probability_difference": -0.012823122814235699,
"ln_probability_difference": -0.025383051999371742,
"psi": 0.00032548999318807485
},
{
"bucket": "bucket_2",
"rollup": "2020-01-01T01:00:00Z",
"country": "United States",
"base_bucket_max": 0.9999971182990177,
"base_bucket_min": 0.5009102069226075,
"base_count_per_bucket": 4988,
"base_ln_probability_per_bucket": -0.6955500651756032,
"base_probability_per_bucket": 0.4988,
"base_total": 10000,
"target_bucket_max": 0.9999971182990177,
"target_bucket_min": 0.5009102069226075,
"target_count_per_bucket": 2487,
"target_ln_probability_per_bucket": -0.6701670131762315,
"target_probability_per_bucket": 0.5116231228142357,
"target_total": 4861,
"probability_difference": -0.012823122814235699,
"ln_probability_difference": -0.025383051999371742,
"psi": 0.00032548999318807485
}
]
}
Sample Request: Compare data drift bucket components between two batches, with no grouping, no filters, and no rollups.
JSON{
"property": "age",
"num_bins": 10,
"base": {
"source": "inference",
"filter": [
{
"property": "batch_id",
"comparator": "eq",
"value": "5"
}
]
},
"target": {
"source": "inference",
"filter": [
{
"property": "batch_id",
"comparator": "eq",
"value": "6"
}
]
}
}
Sample Response:
JSON{
"query_result": [
{
"bucket": "bucket_1",
"base_bucket_max": 0.9999971182990177,
"base_bucket_min": 0.5009102069226075,
"base_count_per_bucket": 4988,
"base_ln_probability_per_bucket": -0.6955500651756032,
"base_probability_per_bucket": 0.4988,
"base_total": 10000,
"target_bucket_max": 0.9999971182990177,
"target_bucket_min": 0.5009102069226075,
"target_count_per_bucket": 2487,
"target_ln_probability_per_bucket": -0.6701670131762315,
"target_probability_per_bucket": 0.5116231228142357,
"target_total": 4861,
"probability_difference": -0.012823122814235699,
"ln_probability_difference": -0.025383051999371742,
"psi": 0.00032548999318807485
},
{
"bucket": "bucket_2",
"base_bucket_max": 0.9999971182990177,
"base_bucket_min": 0.5009102069226075,
"base_count_per_bucket": 4988,
"base_ln_probability_per_bucket": -0.6955500651756032,
"base_probability_per_bucket": 0.4988,
"base_total": 10000,
"target_bucket_max": 0.9999971182990177,
"target_bucket_min": 0.5009102069226075,
"target_count_per_bucket": 2487,
"target_ln_probability_per_bucket": -0.6701670131762315,
"target_probability_per_bucket": 0.5116231228142357,
"target_total": 4861,
"probability_difference": -0.012823122814235699,
"ln_probability_difference": -0.025383051999371742,
"psi": 0.00032548999318807485
}
]
}
back to top
Data Drift for Classification Outputs
For classification outputs, one may want to examine drift among a collection of different classes, i.e., the system of outputs, instead of the drift of the probability predictions of a single class. The query uses one of "predicted_classes": ["*"] or "ground_truth_classes": ["*"] but otherwise is identical to a standard data drift query. Rather than using the star operator to select all prediction or ground truth classes, respectively, in a model, a list of string classes can be provided for looking at the drift of a subset of multiclass outputs.
predicted_classes - Specifies which prediction classes to use for predictedClass data drift.
ground_truth_classes - Specifies which prediction classes to use for groundTruthClass data drift.
properties can be included in the same query as long as the target sourcecorresponds to the classification output tag. For example, one can query drift on input attributes and predictedClass in the same query with target source of inference; one can query drift on individual ground truth labels and groundTruthClass in the same query with target source of ground_truth.
Query Request:
JSON{
"properties [Optional]": [
"<attribute1_name> [string]",
"<attribute2_name> [string]",
"<attribute3_name> [string]"
],
"[predicted_classesground_truth_classes]": [
"<class0_name> [string]"
"<class1_name> [string]"
],
"num_bins": "<num_bins> [int]",
"metric": "[PSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest]",
"base": {
"source": "[inferencereference]",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"target": {
"source": "[inferencereferenceground_truth]",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"group_by [Optional]": [
{
"property": "<group_by_attribute_name> [string]"
}
],
"rollup [Optional]": "minutehourdaymonthyearbatch_id"
}
Query Response:
JSON{
"query_result": [
{
"<attribute1_name>": "<attribute1_data_drift> [float]",
"<attribute2_name>": "<attribute2_data_drift> [float]",
"<attribute3_name>": "<attribute3_data_drift> [float]",
"[predictedClassgroundTruthClass]": "<classification_data_drift> [float]",
"<group_by_attribute_name>": "<group_by_attribute_value> [stringintnull]",
"rollup": "<rollup_attribute_value> [stringnull]"
}
]
}
Sample Request: Calculate data drift on all prediction classes.
JSON{
"predicted_classes": [
"*"
],
"num_bins": 20,
"base": {
"source": "reference"
},
"target": {
"source": "inference"
},
"metric": "PSI"
}
Sample Response:
JSON{
"query_result": [
{
"predictedClass": 0.021
}
]
}
Sample Request: Calculate data drift on ground truth using the first and third ground truth classes.
JSON{
"predicted_classes": [
"gt_1",
"gt_3"
],
"num_bins": 20,
"base": {
"source": "reference"
},
"target": {
"source": "ground_truth"
},
"metric": "PSI"
}
Sample Response:
JSON{
"query_result": [
{
"groundTruthClass": 0.021
}
]
}
back to top
Automated Data Drift Thresholds
What is a sufficiently high data drift value to suggest that the target data has actually drifted from the base data? For HypothesisTest, we can reverse engineer -log_10(P_value) and plug in the conventional .05 alpha level to establish a lower bound of -log_10(.05).
For the other data drift metrics, pining a constant is insufficient. We abstract this away for the user and allow queries to obtain automatically generated data drift thresholds (lower bounds) based on a model's data. These thresholds can be used in alerting. For more information, see: Automating Data Drift Thresholding in Machine Learning Systems.
The query uses the"metric": "Thresholds" and does not require nor use "target" and "rollup" fields but otherwise is identical to a standard data drift query.
Query Request:
JSON{
"properties": [
"<attribute1_name> [string]",
"<attribute2_name> [string]",
"<attribute3_name> [string]"
],
"num_bins": "<num_bins> [int]",
"metric": "Thresholds",
"base": {
"source": "reference",
"filter [Optional]": [
{
"property": "<filter_attribute_name> [string]",
"comparator": "<comparator> [string]",
"value": "<filter_threshold_value> [stringintfloat]"
}
]
},
"group_by [Optional]": [
{
"property": "<group_by_attribute_name> [string]"
}
]
}
Query Response:
JSON{
"query_result": [
{
"<attribute1_name>": {
"HellingerDistance": "<threshold> [float]",
"JSDivergence": "<threshold> [float]",
"KLDivergence": "<threshold> [float]",
"PSI": "<threshold> [float]"
},
"<attribute2_name>": {
"HellingerDistance": "<threshold> [float]",
"JSDivergence": "<threshold> [float]",
"KLDivergence": "<threshold> [float]",
"PSI": "<threshold> [float]"
}
}
]
}
Sample Request:
JSON{
"properties": [
"AGE"
],
"num_bins": 20,
"base": {
"source": "reference"
},
"metric": "Thresholds"
}
Sample Response:
JSON{
"query_result": [
{
"AGE": {
"HellingerDistance": 0.00041737395239735647,
"JSDivergence": 2.959228131592643,
"KLDivergence": 0.001893866910388703,
"PSI": 0.0018945640055550161
}
}
]
}
back to topUpdated 3 months ago Table of Contents
Querying Drift in Python
Data Drift Endpoint
Data Drift PSI Bucket Table Values
Data Drift for Classification Outputs
Automated Data Drift Thresholds
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/alert-summary-3
 Alert Summary Reports
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Alert Summary ReportsSuggest EditsAn alert summary is a way for teams to send alert reports about different models in their organization to members of their internal team or external stakeholders interested in alert governance.
What Is Sent Out with An Alert Summary?
An alert summary report contains information about the status of alerts triggered for a particular model.
What Needs to Be Configured?
An alert summary report in Arthur is an aggregated report that can be shared with members of your organization on some regular cadence. The report can be configured to control the following:
Name of Your Alert Summary Report: Teams can create as many unique alert summary configurations as they want. Each alert configuration will have a unique ID, which will be used when maintaining the report (keeping up-to-date subscriber lists, maintaining models to be alerted on, etc).
Which Arthur Models Are Included: List of Arthur Model UUIDs
Who Gets Sent the Report: known as a subscriber, this contains email addresses of users who would like to receive the report
Frequency of Sent Reports: There are currently two options: daily and weekly. For weekly, teams can also configure what day of the week they would like to receive the report.
Time of Day to Send Report: Do you want to receive the report on Monday mornings at a specific time? Set up the time of day you would like your report to be sent
This can be configured through the Create Alert Summary Configuration.
This will return:
Alert Summary Configuration ID: This unique ID is used when maintaining the report (keeping up-to-date subscriber lists, maintaining models to be alerted on, etc.).
Maintaining Your Alert Summary Report
After creating a specific alert summary report, teams can maintain these reports to keep them current. A few common API calls and actions that teams will make include:
Adding and Deleting Alert Summary Subscribers: Teams will use these API calls to add new subscribers
and remove subscribers from specific alert summary configurations.
Updating Alert Summary Configuration: Another piece of alert summary maintenance is being able to make changes to an existing configuration. One of the most common reasons teams make changes is to add in a newly onboarded model to the group's alert summary. This can be done with the Update Alert Summary Configuration API call.
Pulling Reports on Your Alert Summary Report: The level of Meta-governance often needed in governance systems. Teams can call the Arthur API to pull reports on their alert summary configurations, such as lists of alert summaries past ,
get subscribers,, and subscriber notification configurations.
Updated 3 months ago Table of Contents
What Is Sent Out with An Alert Summary?
What Needs to Be Configured?
Maintaining Your Alert Summary Report
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/servicenow
 ServiceNow
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ServiceNowService Now Integration GuideSuggest EditsWith the Arthur + ServiceNow integration, you can set up email automation to notify on-call teams in ServiceNow of alerts Arthur triggers. To set up this integration, follow these steps:
Step 1: Set up your email integration in ServiceNow
An inbound email action in ServiceNow can be configured to receive Arthur alerts and create incidents in response to those alerts. Please see the ServiceNow Email Actions Guide for details on how to do this. Once you have set up an email action to handle incoming Arthur alerts and generate incidents from those alerts, retrieve the ServiceNow instance email address for Step 2.
Step 2: Configure your integration in Arthur
To configure the ServiceNow integration for a model in Arthur, you can send a POST request to the/alert_notification_configurations.
model_id - UUID of the model this alert notification configuration belongs to.
type - Type of notification to send. In this case, "ServiceNow".
destination - The integration email address obtained in Step 1.
enabled - Whether or not the notification configuration is enabled. It defaults to true.
Example Query Request:
JSON{
"model_id" : "<model_id> [string]",
"type" : "[ServiceNow]",
"destination" : "<[email protected]> [string]",
}
For more information on alert notifications, please see the notifications section of the
Alerting page.
Step 3: Start monitoring!
Your integration is now ready to use! When an alert is triggered in Arthur for this model, an incident will be created in your ServiceNow instance.Updated 3 months ago Table of Contents
Step 1: Set up your email integration in ServiceNow
Step 2: Configure your integration in Arthur
Step 3: Start monitoring!
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/arthur-faq
 Arthur Scope FAQ
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur Scope FAQSuggest Edits1. Can I use Arthur Scope without using the Python SDK?
Yes! The Arthur Scope platform is API-first. You can use our Arthur API to onboard models, send predictions, and query metrics and insights.
2. Does Arthur need a copy of my model?
Arthur doesn’t generally need access to your actual model but only captures the inputs to the model and the predictions it makes. This means that you can even use Arthur to monitor models you cannot access, such as models hosted by third-party services.
To enable explainability, Arthur does need access to your model. When enabling explainability, you will need to provide access to the modelspredict function.
3. What if my data is proprietary? Can I still use Arthur?
Yes! Arthur offers on-premises installation for customers with data security requirements. By integrating Arthur into your business's on-premises stack, you can be confident that all security requirements are met while still getting the benefits of the computation and analytics Arthur provides.
4. What if I don’t have ground truth labels for my data? Or what if I will have the ground truth labels in the future, but they are not available yet?
You don't need ground truth labels to log your model's inferences with Arthur.
If your ground truth labels become available after your model's inferences, whether seconds later or years later, Arthur can link these new ground truth values to your model's past predictions, linking the new values by ID to their corresponding inferences already in the Arthur system.
In the meantime, Arthur’s data drift metrics can offer leading indicators of model underperformance to keep you covered if your ground truth labels are delayed or never become available.
5. I got an error using the SDK. What do I do?
If the error message says "an internal exception occurred, please report to Arthur" that means there was a problem on our side. Please email the Arthur support team at [email protected] to let us know what happened.
Otherwise, the error message should provide helpful instructions for how to resolve the issue. If you don’t find the error message actionable, please let Arthur know so we can improve it.
6. Do I have to type my credentials in every time I use the SDK?
No! Instead of manually entering them, you can specify an ARTHUR_ENDPOINT_URL and ARTHUR_API_KEY environment variable to be used to create the ArthurAI connection object.
7. What are streaming and batch models?
Streaming and batch are two model types with different patterns of ingesting data to send to Arthur.
A streaming model processes data as a stream of individual inferences: data is logged with Arthur directly as individual inferences when the data flows into the model.
A batch model processes data as a sequence of grouped inferences, which are usually grouped over time: data is logged with Arthur as a group of inferences as the model processes the batch.
8. Which drift metric should I use?
Population stability index (PSI) is typically a good default drift metric.
In some cases, one wants a drift metric with a certain property, e.g., using a drift metric with the unit nats for interpretability or using a drift metric bounded between 0 and 1 so that drift values don't increase arbitrarily for outliers. In these cases, other metrics may be preferable to PSI.
For a review of the data drift metrics Arthur offers and their properties, see the data drift section of our
Glossary. Furthermore, see our blog post for an overview of data on how Arthur automates the choice of thresholding for drift metrics.Updated about 2 months ago Table of Contents
1. Can I use Arthur Scope without using the Python SDK?
2. Does Arthur need a copy of my model?
3. What if my data is proprietary? Can I still use Arthur?
4. What if I don’t have ground truth labels for my data? Or what if I will have the ground truth labels in the future, but they are not available yet?
5. I got an error using the SDK. What do I do?
6. Do I have to type my credentials in every time I use the SDK?
7. What are streaming and batch models?
8. Which drift metric should I use?
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/backup-and-restore
 Backup and Restore
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Backup and RestoreSuggest EditsWARNINGS
🚧Only tested on AWS, as writtenThese instructions have been tested as written for an AWS deployment. If you find they do not work for your use-case, please reach out to Arthur Support before modifying them. We cannot guarantee reliable operation if these instructions are not followed exactly as written.
🚧Ensure no network connection between backup/restore environmentsWhen restoring into a new cluster, you must ensure that the new cluster is unable to communicate with any services or data store in the old cluster.
If you took a backup on cluster Apple, and performed a restore into cluster Banana, cluster Banana must point to its own RDS Instance, ClickHouse Database, and Kafka Store (note: it is ok if clusters share the S3 bucket).
To ensure this, you must re-configure via the Admin Interface when restoring into a new cluster. Failure to do this WILL CAUSE DATA CORRUPTION on both clusters that is unrecoverable.
🚧Backup everything at the same timeIf you are either manually taking a backup or scheduling a backup, you MUST take a backup of the full platform. You CANNOT use a ClickHouse snapshot taken at midnight with an RDS snapshot taken at 0400 AM (or any other time). All backup operations must be performed at the same time, and when restoring, the data you are using must all belong to the same backup operation. This is to ensure data consistency across the different data stores. IGNORING THIS WILL CAUSE DATA CORRUPTION.
Overview
The overall backup and restore process for the Arthur Platform is as follows:
Backing up the Arthur platform
Take a backup of ClickHouse Data
Take a backup of Kubernetes Deployment State and Persistent Volumes
Enrichments infrastructure
Model Servers
Data Pipeline Services
Enrichment / Delete Enrichment Workflows
Kafka Deployment State and EBS Volumes (using EBS Snapshots)
Take a backup of RDS Postgres
Restore the Arthur platform
Restore RDS Postgres
Update configuration and install the platform
Restore ClickHouse Data
Restore the Kafka Deployment State and Persistent Volumes
Restore Enrichments infrastructure
Restore Workflows
Smoke Tests and Validation
Overview - clickhouse-backup
The Arthur Platform stores inference data, data built from the enrichments pipeline, reference and ground truth data in ClickHouse. ClickHouse is an open-source OLAP Database which enables SQL-like query execution, replication, sharding and many additional features.
To backup ClickHouse, the Arthur Platform uses a tool called clickhouse-backup. clickhouse-backup is a sidecar-container included on the ClickHouse pods and is responsible for taking backups, performing restores, and coordinating with remote storage (in this case S3) to store and retrieve backups. clickhouse-backup uses built-in functionality of ClickHouse to take backups and perform restores.
Overview - Velero
The Arthur Platform uses Velero, which is an industry-standard, battle-tested tool for backing up Kubernetes Resources including Persistent Volumes.
Arthur uses Velero to backup necessary namespaced Kubernetes resources, as well as the EBS Volume Snapshot backups for each PersistentVolumes claimed by the StatefulSets (eg: via PVCs).
Backup data (not including EBS Volume Snapshots) is stored in an S3 bucket which is accessible via a ServiceAccount that is provisioned for the Backup and Restore agent. Backups and Restores are managed by Velero using Kubernetes Custom Resource Definitions (CRDs), which are consumed by the Velero Backup Controller.
Velero has a feature which also allows backups to be scheduled, using a cron-like configuration. It also provides ServiceMonitors which expose metrics via Prometheus, so that operators can monitor backup and restore status and set up alerts for when backup or restore operations fail.
Overview - Arthur (Argo)Workflows
The Arthur Platform uses Argo Workflows as a workflow orchestration engine for running certain jobs. Argo installs a handful of Custom Resource Definitions (CRDs) which enable the Argo Workflow services to schedule, execute and update these jobs.
Workflows are dynamically managed, meaning that their definitions are not stored in the Arthur installer script. The Backup and Restore operation accounts for this by treating restoration of Workflows on a case-by-case basis, as follows:
Enrichments and Delete Enrichments workflows
These workflows are created to stand-up and tear-down infrastructure necessary for processing enrichments data (eg: kafka topics, pods which manage the data pipeline for enrichments, etc.)
These workflows are idempotent and safe to recover
Therefore, these workflows are backed up and restored just like any other Kubernetes Resource during the backup stage
Batch workflows
These workflows are created to manage batch jobs, which are used by clients when uploading large data files to models (inferences and/or ground truths).
These workflows are sometimes safe to recover
Therefore, these workflows are restored selectively based on what state they were in when the backup was taken
Workflows for which Arthur received all the data from the client are resumed by manually re-submitting them (this is done via an Administrative HTTP endpoint that needs to be invoked manually)
Workflows for which Arthur did not receive all the data from the client will need to be re-submitted. Operators restoring the cluster will need to reach out to affected clients to communicate that their batch workflows should be re-submitted.
Reference and Cron Workflows
Reference Workflows are created for monitoring the upload of reference datasets to S3
Reference datasets that were in-flight during a backup will need to be re-uploaded via the SDK.
Cron Workflows are scheduled workflows which perform some regular processing (eg: triggering alerts for non-batch inferences)
Cron Workflows are meant to be run on a regular schedule. It is safe to wait for the next workflow to be triggered, and therefore, these workflows are not backed up nor restored.
Overview - S3
The Arthur Platform uses AWS S3 object storage for storing inference data, reference data, as well as data and trained models for the enrichments pipeline.
Arthur recommends enabling Cross-Region Replication on the AWS S3 buckets, so that objects are available in the rare event of an AWS region outage.
The Arthur Backup solution does not manage consistency with the S3 bucket and other backup data.
The data in S3 is only used in conjunction with data that is stored in Postgres (eg: model definitions), so it's ok if there's data in S3 that isn't represented in Postgres.
Therefore, the S3 bucket for a cluster will always reflect the most up-to-date state, regardless of when a backup was taken.Updated 3 months ago Table of Contents
WARNINGS
Overview
Overview - clickhouse-backup
Overview - Velero
Overview - Arthur (Argo)Workflows
Overview - S3
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/edit/getting-started
 Welcome to Arthur Scope!
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchDiscardSubmit Suggested EditsWelcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by SuggestAs your team's data science operations center, Arthur helps enterprise teams monitor, measure and optimize AI performance at scale.
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/fairness-metrics
 Fairness Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Fairness MetricsMonitor and track fairness metrics in production to take action on underperformance in sensitive segments.Suggest EditsEnabling Fairness Sections
A dedicated fairness section within the UI enables teams to track fairness performance between groups easily. The UI does not infer these trackable groups and must be explicitly defined. For attributes to show up (and be tracked) within the fairness section of the dashboard, they must be enabled for bias monitoring. This can be done with the Python SDK and is further explained in Enabling Enrichments under bias mitigation.
Tracking Fairness in the UI
Marked sensitive attributes are tracked in the Fairness section in the model's Overview Tab. This section operationalizes systematic comparisons for critical groups.
Metrics: Easily compare different accuracy rates between groups by selecting from standard fairness metrics within our drop-down selection. These metrics include:
MetricDescriptionAccuracy Ratethe proportion of correctly classified instances out of the total number of instancesTrue Positive Ratethe proportion of actual positive instances correctly identified by a machine learning model out of the total number of actual positive instancesTrue Negative Ratethe proportion of actual negative instances correctly identified by a machine learning model out of the total number of actual negative instancesFalse Positive Ratethe proportion of actual negative instances that are incorrectly classified as positive by a machine learning model out of the total number of actual negative instancesFalse Negative Ratethe proportion of actual positive instances incorrectly classified as negative by a machine learning model out of the total number of positive instances
Baseline: Within Arthur, the fairness section allows comparisons within groups of different attributes of interest. The baseline group is the group that all other groups in that attribute will be compared against. This selection is made by clicking on the Set Baseline button, and the selected group can be seen in the fairness table under the baseline column for each comparison.
Fairness Threshold Toggle: A fairness threshold is the acceptable rate of disparate performance. An appropriate fairness threshold heavily depends on the team and use case, so Arthur does not apply strict parameters. Instead, teams can toggle the threshold to their model's acceptable disparate performance rate.
Fairness Status: Based on the threshold provided, a visual representation of whether a group's performance rate has passed the allowed threshold is provided. A green check represents that all groups' performance for the rate selected is within the threshold compared to the baseline group. On the other hand, red exclamation points mean that one or more comparison groups have points beyond the specified fairness threshold.
Tracking Attribute Disparity
By default, attribute disparity rates are tracked over time.
Using Snap Shot Mode
While tracking performance over time is incredibly helpful for debugging or evaluating when disparate impact occurred, another popular way teams use the fairness section in reporting is with Snapshot mode. Snapshot mode is a toggle at the top of the UI that converts all charts from time series to average bar charts (for the time range selected in the Global Filters). This allows teams to easily create shareable charts for reports on the average impact rate between different groups.
Querying Fairness in a Notebook
Beyond the metrics enabled in the Arthur UI, Arthur can query additional fairness metrics in a notebook. Using the bias metrics submodule, teams can call demographic_parity, group_confusion_matrices, or group_positivity_rates to be calculated on a specified attribute.
Pythonarthur_model.bias.metrics.demographic_parity('<attr_name>')
arthur_model.bias.metrics.group_confusion_matrices('<attr_name>')
A description of these bias metrics:
MetricDescriptionDemographic ParityGet group-conditional positivity rates for all inferences, with the option to filter for a batch_id or a particular chunk of time.Group Confusion MatricesGet group-conditional confusion matrices for all inferences, with the option to filter for a batch_id or a particular chunk of time.Group Positivity RatesGet group-conditional positivity rates for all inferences, with the option to filter for a batch_id or a particular chunk of time.
Available Model Types
Since fairness metrics are calculated with accuracy rates, they are only available for classification models within Arthur. Additionally, since fairness metrics are a different visual way of tracking accuracy between sensitive groups, they require ground truth labels.Updated 3 months ago Table of Contents
Enabling Fairness Sections
Tracking Fairness in the UI
Tracking Attribute Disparity
Using Snap Shot Mode
Querying Fairness in a Notebook
Available Model Types
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/model-metric-definitions
 Model Metric Definitions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Model Metric DefinitionsUnderstanding Model Metrics Monitored with ArthurSuggest EditsPerformance
Accuracy Rate
Accuracy is the most common metric for classification tasks. Accuracy is the measure of how many predictions were correct out of all the predictions made.
Accuracy Rate = (# correct predictions) / (total # predictions)
We can also think of accuracy in terms of common confusion matrix rates
Accuracy Rate = (TP + TN) / (TP + TN + FP + FN)
AUC
The Area Under the Curve (AUC) is a metric that measures the performance of a classification model by calculating the area under the receiver operating characteristic (ROC) curve. It provides a single score that summarizes the trade-off between the true positive rate and false positive rate across different thresholds for defining positive cases.
Average Prediction
This is a metric used by regression models. It returns the average prediction your model has output over the
Average Token Likelihood
The token likelihood is a number between 0 and 1 that quantifies the model’s level of surprise that this token was the next predicted token of the sentence. In token sequence models, this metric quantifies the average
In Arthur, this metric is available for token sequence models.
Average Sequence Length
This metric is available for token sequence models. This is the average count of tokens for each inference ingested by Arthur.
Balanced Accuracy Rate
The balanced accuracy rate is a metric used for classification tasks.
Balanced Accuracy Rate = (Sensitivity + Specificity) / 2
Confusion Matrix Rates
A confusion matrix is a table that summarizes the performance of a classification model by comparing its predicted outputs to the true outputs across different classes. It contains information about the number of true positives, true negatives, false positives, and false negatives, which can be used to calculate various metrics for evaluating model performance.
False Negative Rate
The false negative rate is a metric that measures the proportion of actual positive cases that are incorrectly predicted as negative by a classification model. A high false negative rate indicates that the model is missing a significant number of positive cases, which can have serious consequences in some applications.
False Positive Rate
The false positive rate is a metric that measures the proportion of negative cases that are incorrectly predicted as positive by a classification model. A high false positive rate indicates that the model is producing a large number of false alarms, which can be costly or lead to unnecessary actions in some applications.
True Negative Rate
The true negative rate, also known as specificity, is a metric that measures the proportion of actual negative cases that are correctly identified as negative by a classification model. A high true negative rate indicates that the model is able to effectively identify negative cases, which is important in applications where avoiding false positives is critical.
True Positive Rate
The true positive rate, also known as sensitivity or recall, is a metric that measures the proportion of actual positive cases that are correctly identified as positive by a classification model. A high true positive rate indicates that the model is able to effectively identify positive cases, which is especially important in applications where detecting all positive cases is critical.
F1
The F1 score is a single metric that combines precision and recall, two performance metrics used in classification tasks. It ranges from 0 to 1, with higher values indicating better model performance in balancing precision and recall.
F1 = (precision * recall) / (precision + recall)
F1 = TP / (TP + 0.5 * (FP + FN))
Likelihood Stability
This metric is available for token sequence models.
Mean Average Error (MAE)
Mean Absolute Error (MAE) is a metric that measures the average magnitude of the differences between the predicted and actual values of a numerical variable in a regression model. It is calculated as the average absolute difference between the predicted and actual values and is one of the most popular measures of the model's predictive accuracy.
Mean Average Precision
In object detection, Mean Average Precision (MAP) is a commonly used metric that measures the precision and recall of a model in detecting objects of different classes in an image. It takes into account the localization accuracy of the predicted bounding boxes and is often used to compare the performance of different object detection models.
Recall
Recall, also known as sensitivity or true positive rate, is a metric that measures the proportion of true positive cases among all actual positive cases in a classification model. It provides information about the model's ability to detect positive cases and is especially important in applications where detecting all positive cases is critical.
RSME
The Root Mean Squared Error (RMSE) is a metric that measures the average magnitude of the differences between the predicted and actual values of a numerical variable in a regression model. It is calculated as the square root of the average squared difference between the predicted and actual values and is often used as a measure of the model's predictive accuracy.
R Squared
R-squared, also known as the coefficient of determination, is a statistical metric that measures the proportion of variation in the dependent variable that is explained by the independent variable(s) in a regression model. It ranges from 0 to 1, with higher values indicating a better model fit to the data.
Inference Count
Available for all model types. This is the number of predictions that have been sent to Arthur.
Inference Count = # of predictions
Inference Count by Class
For classification models, this metric counts the number of predictions ingested by Arthur per each class label.
Overall Accuracy Rate
The overall accuracy rate is a metric that measures the proportion of correctly classified cases in a classification model, across all classes. It provides a general sense of how well the model is performing, but it may not capture the performance of individual classes or the cost of misclassification errors.
Precision
Precision is a metric that measures the proportion of true positive cases among all predicted positive cases in a classification model. It provides information about the model's ability to minimize false positives and is especially important in applications where avoiding false positives is critical.
Precision = TP / (TP + FP)
Data Drift
Background
P and Q
We establish some mathematical housekeeping for the below metrics. Let P be the reference distribution and Q
be the target distribution. Binning the underlying reference and target sets can approximate these probability distributions. Generally, P is an older dataset, and Q is a new dataset of interest. We'd like to quantify how far the distributions differ to see if the reference set has gone stale, and algorithms trained on it should not be used to perform inferences on the target dataset.
Entropy
Let H(P) be the entropy of distribution P. It is interpreted as the expected (i.e., average) number of bits (if log base 2) or nats (if log base e) required to encode information of a datapoint from the distribution P. Arthur applications use log base e, so interpretation will be in nats.
KL Divergence
Let D(PQ) be the Kullback-Leibler (KL) Divergence from P to Q. It is interpreted as the nats of information we expect to lose using Q instead of P for modeling data X, discretized over probability space K. KL Divergence is not symmetrical, i.e., D(PQ) does not equal D(QP) and should not be used as a distance metric.
Population Stability Index (PSI)
Let PSI(P,Q) be the Population Stability Index (PSI) between P and Q. It is interpreted as the roundtrip loss of na s of information we expect to lose from P to Q and then from Q returning back to P, and vice versa. PSI smooths out KL Divergence since the return" trip information loss is included, and this metric is popular in financial applications.
JS Divergence
Let JSD(P,Q) be the Jensen-Shannon (JS) Divergence between P and Q. It smooths out KL divergence using a mixture of the base and target distributions and is interpreted as the entropy of the mixture M = (P+Q)_/2 minus the mixture of the entropies of the individual distributions.
Hellinger Distance
Let HE(P,Q) be the Hellinger Distance between P and Q. It is interpreted as the Euclidean norm of the difference of the square root distributions of P and Q.
Hypothesis Test
Hypothesis testing uses different tests depending on whether a feature is categorical or continuous.
For categorical features, let the equation below be the chi-squared test statistic for P and Q, with K being the number of categories of the feature, i.e., K-1 are the degrees of freedom. Let NPK and NQK be the count of feature occurrences being k, with 1<= k <= K, for P and Q, respectively. The chi-squared test statistic summarizes the standardized differences of expected counts between P and Q.
For continuous features, let KS(P, Q) be the Kolmogorov-Smirnov test statistic for P and Q. Let FP and FQ be the empirical cumulative density for P and Q, respectively. The Kolmogorov-Smirnov test is a nonparametric,
i.e., distribution-free test that compares the empirical cumulative density functions of P and Q.
The returned test statistic is then compared to cutoffs for significance. A higher test statistic indicates more data drift. We've abstracted the calculations away within our query endpoint.
For HypothesisTest, the returned value is transformed as -log_10(P_value) to maintain directional parity with the other data drift metrics. A lower P_value is more significant and implies data drift, reflected in a higher -log_10(P_value).
Fairness
Demographic Parity
Demographic parity is a fairness metric that measures whether the proportion of positive outcomes is the same across different demographic groups in a classification model. It aims to ensure that the model is not systematically biased towards or against certain groups based on demographic characteristics like race or gender.
Equalized Odds
Equalized Odds is a fairness metric that measures the true positive and false positive rates for a given group, such as a protected group defined by a demographic characteristic like race or gender, in a classification model. It ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or unequal treatment issues in both true positive and false positive rates.
Equal Opportunity
Equal Opportunity is a fairness metric that measures the true positive rate for a given group, such as a protected group defined by a demographic characteristic like race or gender, in a classification model. It ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or unequal treatment issues.Updated 3 months ago Table of Contents
Performance
Accuracy Rate
AUC
Average Prediction
Average Token Likelihood
Average Sequence Length
Balanced Accuracy Rate
Confusion Matrix Rates
F1
Likelihood Stability
Mean Average Error (MAE)
Mean Average Precision
Recall
RSME
R Squared
Inference Count
Inference Count by Class
Overall Accuracy Rate
Precision
Data Drift
Background
KL Divergence
Population Stability Index (PSI)
JS Divergence
Hellinger Distance
Hypothesis Test
Fairness
Demographic Parity
Equalized Odds
Equal Opportunity
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/on-prem-deployment-requirements
 On-Prem Deployment Requirements
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by On-Prem Deployment RequirementsSuggest EditsGeneral
A DNS hostname
TLS private key & certificate
SMTP server (StartTLS supported)
The minimum compute resource requirements in this documentation is for running a few small models in a non-production environment. Your production deployment will likely use more compute resources to achieve higher availability, performance and scalability.
Arthur’s horizontally elastic architecture allows high throughput processing in both streaming and batch. The platform's auto-scaler mechanism self-manages resource utilization in optimized and cost-effective fashion. It automatically scales up and down based on compute resource requests by the platform activities as well as the lag observed in the data pipeline queue within the limits of the allocated hardware. This works best in a cloud infrastructure with a managed Kubernetes service that enables Arthur to also auto-scale the provisioned hardware (e.g. AWS EKS, Azure ASK).
Storage volumes used for Arthur deployment should be encrypted with a data key using industry-standard data encryption (e.g. AES-256). This applies to the mounted disk volumes as well as the externalized storage, such as the S3 object storage and the relational database if any.
Kubernetes Install
Kubectl-ing workstation: Linux or MacOS
Kubernetes: 1.25 to 1.27
Runtime: containerd or Docker
Namespace
Storage class
Minimum Node Group Resource
16 CPUs
32 GB RAM
Storage with at least 3000 IOPS (>100GB recommended)
Permissions
When Arthur platform is installed, Kubernetes RBAC resources are created to allow the Admin Console to manage the application.
The kubectl-ing user who installs Arthur must have the wildcard privileges in the cluster.
Refer to this documentation for the ClusterRole and ClusterRoleBinding that
will be created for the Admin Console.
Components
Prometheus
Ingress Controller (Nginx or Ambassador)
Kubernetes Metrics Server
Velero with Restic (Optional for managed backup and restore feature)
For Airgapped installation only:
An existing private container registry
Existing private Python registries (PyPI, Anaconda) - only required for the model explanation feature
VM Install
Minimum Server Resource
16 CPUs
32 GB RAM
Storage with at least 3000 IOPS (>100GB recommended)
Supported Operating Systems
The latest versions of the following Linux operating systems are supported.
Ubuntu
RHEL
Please do the following before running the installer on your VM for a smoother deployment experience:
If SELinux is enabled, set it to the permissive mode
Make sure the VM doesn't have any container runtime pre-installed, such as Docker or containerd
Firewall Configurations
Ingress
The TCP port 443 is the only entry point that Arthur exposes.
Egress
The platform requires access to any integrations (e.g. SMTP, IdP) as well as the components you externalize (e.g. Postgres, S3).
For Airgap Installation
Your private container and Python registries must be accessible.
(requirements_for_online_installation)=
For Online Installation
Access to container images and deployment manifest files from the below public registries are required.
HostExisting ClusterEmbedded ClusterDocker HubRequiredRequiredproxy.replicated.comRequiredRequiredreplicated.appRequiredRequiredk8s.kurl.shNot RequiredRequiredamazonaws.comNot RequiredRequiredUpdated about 2 months ago Table of Contents
General
Kubernetes Install
Minimum Node Group Resource
Permissions
Components
VM Install
Minimum Server Resource
Supported Operating Systems
Firewall Configurations
Ingress
Egress
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/assets-required-for-explainability
 Assets Required For Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Assets Required For ExplainabilitySuggest EditsArthur can automatically calculate explanations (feature importances) for every prediction your model makes. To make this possible, we package up your model in a way that allows us to call it's predict function, which allows us to calculate explanations. We require a few things from your end:
A Python script that wraps your models predict function
For Image models, a second function, load_image is also required (see CV Explainability).
A directory containing the above file, along with any serialized model files, and other supporting code
A requirements.txt with the dependencies to support the above
This guide will set everything up and then use the SDK to enable explainability.
Setting up Project Directory
Project Structure
Here is an example of what your project directory might look like.
-- model_folder/

-- data/


-- training_data.csv


-- testing_data.csv

-- requirements.txt

-- model_entrypoint.py

-- utils.py

-- serialized_model.pkl
Requirements File
Your project requirements and dependencies can be stored in any format you like, such as the typical requirements.txt file, or another form of dependency management.
This should contain all packages your model and predict function need to run.
📘You do not need to include the arthurai package in this requirements file. We supply that
# example_requirements.txt
pandas==0.24.2
numpy==1.16.4
scikit-learn==0.21.3
torch==1.3.1
torchvision==0.4.2
It is advised to pin the specific versions your model requires. If no version is pinned, we will use the latest version. This can cause issues if the latest version is incompatible with the version used to build your model.
Prediction Function
We need to be able to send new inferences to your model to get predictions and generate explanations. For us to have access to your model, you need to create an entrypoint file that defines a predict() method.
The exact name of the file isn't strict, so long as you specify the correct name when you enable explainability (see below). The only thing that does matter is that this file implements a predict() method. In most cases, if you have a previously trained model, this predict() method will likely just invoke the prediction from your trained model.
Python# example_entrypoint.py
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(x)
This predict method can be as simple or complicated as you need, so long as you can go from raw input data to a model output prediction.
Specifically, in the case of a binary classifier, we expect a 2-d array where the first column indicates probability_0 for each input, and the second column indicates probability_1 for each input. In the case of a multiclass classifier with n possible labels, we expect an n-d array where column i corresponds to the predicted probability that each input belongs to class i.
Preprocessing for Prediction
Commonly, a fair amount of feature processing and transformation will need to happen before invoking your actual model.predict(). This might include normalizations, rescaling, one-hot encoding, embedding, etc. Whatever those transformations are, you can make them a part of this predict() method. Alternatively, you can wrap all those transformations into a helper function.
Python# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load("./serialized_model.pkl")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Enabling Explainability
Enabling explainability can be done using the SDK function arthur_model.enable_explainability, which takes as input a sample of your model's data (to train the explainer), and which takes as input the files that contain your model's predict function and necessary environment.
Pythonarthur_model.enable_explainability(
df=X_train.head(50),
project_directory="/path/to/model_folder/",
requirements_file="requirements.txt",
user_predict_function_import_path="model_entrypoint",
ignore_dirs=["folder_to_ignore"] # optionally exclude directories within the project folder from being bundled with predict function
)
The above provides a simple example. For a list of all configuration options and details around them, see the explainability section in Enabling Enrichments.
Notes about the above example:
joblib is a Python library allowing you to reconstruct your model from a serialized pickle file.
X_train is your trained model data frame.
user_predict_function_import_path is the Python path to import the entry point file as if you imported it into the Python program running enable_explainability.
Configuration Requirements
When going from disabled to enabled, you will need to include the required configuration settings. Once the explainability enrichment has been enabled, you can update the non-required configuration settings without re-supplying the required fields.
You must not pass in any config settings when disabling the explainability enrichment.
Configuration
SettingRequiredDescriptiondfXThe dataframe passed to the explainer. It should be similar to, or a subset of, the training data. Typically small, ~50-100 rows.project_directoryXThe path to the directory containing your predict function, requirements file, model file, and any other resources needed to support the predict function.user_predict_function_import_pathXThe name of the file containing the predict function. Do not include .py extension. Used to import the predict function.requirements_fileXThe name of the file containing pip requirements for the predict function.python_versionXThe Python version to use when executing the predict function. This is automatically set to the current Python version when usingmodel.enable_explainability().sdk_versionXThe arthurai version used to make the enable request. This is automatically set to the currently installed SDK version when using themodel.enable_explainability().explanation_algoThe explanation algorithm to use. Valid options are 'lime' or 'shap'. The default value of 'lime'.explanation_nsamplesThe number of perturbed samples used to generate the explanation. The result will be calculated more quickly for a smaller number of samples but may be less robust. It is recommended to use at least 100 samples. The default value of 2000.inference_consumer_score_percentThe number between 0.0 and 1.0 sets the percent of inferences for which to compute an explanation score. Only applicable when streaming_explainability_enabled is set to true. The default value of 1.0 (all inferences explained).streaming_explainability_enabledIf true, every inference will have an explanation generated for it. If false, explanations are available on-demand only.ignore_dirsList of paths to directories within project_directory that will not be bundled and included with the predict function. Use to prevent including irrelevant code or files in larger directories.
CV Explainability
📘Explainability is currently available as an enrichment for classification, multi-labeling, and regression CV models, but not object detection CV models.
In your model_entrypoint.py for Multiclass Image models, in addition to the predict() function, there is a second function which is required: load_image(). This function should take in a string, which is a path to an image file. The function should return the image in a numpy array. Any image processing, such as converting to greyscale, should also happen in this function. This is because Lime (the explanation algorithm used behind the scenes) will create variations of this array to generate explanations. However, any transformation resulting in a non-numpy array should happen in the predict function, such as converting to a Tensor.
No image resizing is required. As part of onboarding an image model, pixel_height and pixel_width are set as metadata on the model. When ingesting, Arthur will automatically resize the image to the configured size and pass this resized image path to the load_image function.
Below is a full example file for an Image model, with both load_image and predict defined.
Imports and class definitions are omitted for brevity.
Python# example_entrypoint.py
import ...
class MedNet(nn.Module):
...
# load model using custom user defined class
net = MedNet()
path = pathlib.Path(__file__).parent.absolute()
net.load_state_dict(torch.load(f'{path}/pretrained_model'))
# helper function for transforming image
def quantize(np_array):
return np_array + (np.random.random(np_array.shape) / 256)
def load_image(image_path):
"""Takes in single image path, and returns single image in format predict expects
"""
return quantize(np.array(Image.open(image_path).convert('RGB')) / 256)
def predict(images_in):
"""Takes in numpy array of images, and returns predictions in numpy array.
Can handle both single image in `numpy` array, or multiple images.
"""
batch_size, pixdim1, pixdim2, channels = images_in.shape
raw_tensor = torch.from_numpy(images_in)
processed_images = torch.reshape(raw_tensor, (batch_size, channels, pixdim1, pixdim2)).float()
net.eval()
with torch.no_grad():
return net(processed_images).numpy()
👍Note on Enabling Explainability for CV ModelsExplainability for CV, at least for CV models, should be configured with 4 CPUs and 4 GB RAM (default 1) to avoid long explanation times (which could break the UI). It’s done per model when enabling explainability in the notebook.
This enabling explainability configuration can be seen here:
Pythonarthur_model.enable_explainability(
project_directory=project_dir,
user_predict_function_import_path='entrypoint',
streaming_explainability_enabled=False,
requirements_file="requirements.txt",
explanation_algo='lime',
explanation_nsamples=2000,
model_server_num_cpu="4",
model_server_memory="4Gi"
)
NLP Explainability
Enabling explainability for NLP models follows the same process for Tabular models
📘An important choice for NLP explainability is the text_demiliter parameter, since this delimiter determines how tokens will be perturbed when generating explanations.
Here is an example entrypoint.py file which loads our NLP model and defines a predict function that the explainer will use:
Pythonmodel_path = os.path.join(os.path.dirname(__file__), "model.pkl")
model = joblib.load(model_path)
def predict(fvs):
# our model expects a list of strings, no nesting
# if we receive nested lists, unnest them
if not isinstance(fvs[0], str):
fvs = [fv[0] for fv in fvs]
return model.predict_proba(fvs)
Updated 3 months ago Table of Contents
Setting up Project Directory
Project Structure
Requirements File
Prediction Function
Preprocessing for Prediction
Enabling Explainability
Configuration Requirements
CV Explainability
NLP Explainability
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/monitoring-best-practices
 Monitoring Best Practices
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Monitoring Best PracticesSuggest EditsThe Arthur Scope product is used to monitor machine learning models. It runs on Kubernetes and is able to scale on-demand. There are several components that should be monitored so the platform stays healthy.
Some recommended best practices for monitoring the various Scope components are as follows:
Kubernetes
Pods
CPU/Memory utilization
Pods are the smallest building blocks in Kubernetes. It's always advised to ensure pods have sufficient resources(CPU and Memory) available for them to run.
Number of Restarts
Pods getting restarted frequently is a sign of an buggy code or bad configuration.
Pods in Pending/Unknown/Unavailable/CLBO state
Pods not in a Ready state is a sign of hardware degradation or connectivity failures to external systems.
Persistent Volumes
IOPS
Ensure the storage backing Persistent Volumes have enough throughput provisioned and there is no throttling being experienced.
Available Disk Space
Ensure attached Persistent Volumes have enough disk space.
VolumeAttachment Errors
Ensure there are no VolumeAttachment errors observed in Persistent Volumes. This is particularly critical in multi-AZ deployments.
Nodes
Sufficient nodes in each AZ
Ensure there are required number of **nodes per AZ for each deployment.
Max nodes per cluster
Monitoring the total number of nodes a cluster is scaled to ensures performance and costs are in optimal.
Datastores
Meta Database(External)
Disk Space
Ensure there is enough disk space for the database.
IOPS
Monitor for any throttling of performance for the database disk and adjust IOPS accordingly.
CPU
Monitor for any throttling of performance for the database cpu and adjust it accordingly.
OLAP Database
Replication Lag
The OLAP database is usually deployed in a 3 node setup, which are synced via replication. A lag happens when data is not consistent across all nodes.
Delayed/Rejected Inserts
This usually happens when a large number of INSERTS are sent too quickly. This can lead to data loss or corruption.
ZooKeeper Exceptions
These should generally not happen and is sometimes an indication of bad hardware.
Messaging Middleware
Kafka
Consumer Lag
Producers write data and Consumers read data from the messaging middleware. If consumers are not able to keep up with the producers, it will lead to a lag which can mean poor performance for the platform.
Under Replicated partitions
Follower replicas get data from Leader replicas using replication. Due to resource exhaustion or Leader failure, it is possible the Follower replicas don’t keep up with the Leader replicas.
Kafka Connect
Connector failures
These failures mean data is not being written to data stores, which can lead to data loss.
Task failures
These failures mean data is not matching the configurations, which can lead to data loss/corruption.
ZooKeeper
Outstanding Requests
This is the number of requests waiting to be processed by ZooKeeper.
Workflow Scheduler
Failed Steps
This usually implies a bad configuration or being unable to communicate with external systems.
Failed Workflows
Failed Steps or bad configurations could lead to failed workflows.
Queued Workflows
Workflows being queued could mean there is a lack of resources on the cluster.
Microservices
Rate of 4XX/5XX HTTP response status
Bad HTTP status codes could happen due to various reasons (bugs, pod restarts, invalid creds, access etc.).
Response times
Elevated response times can happen due to various reasons (bugs, pod restarts etc.).
Updated about 2 months ago Table of Contents
Kubernetes
Datastores
Messaging Middleware
Workflow Scheduler
Microservices
=====================
Content type: arthur_scope_docs
Source: https://docs.arthur.ai/docs/overview
 Overview
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by OverviewSuggest EditsThe Arthur Query service is a feature within Arthur that enables teams to interact with and analyze their data within Arthur Scope. Built with a SQL-like wrapper, the functionality can be used to create metric functions, pull data, create graphs for custom reports, and much more.
Interacting with Arthur Query Service
Python SDK
One of the most common ways teams can query is through the Python SDK.
Building a Queryfrom arthurai import ArthurAI
## Create Connection to Model of interest
url = ## arthur url
access_key = ## arthur access key
connection = ArthurAI(url = url, access_key = access_key, verify_ssl=False)
model_id = ## model id
arthur_model.connection.get_model(model_id)
## Build Query Function
query_function = {
"select":[ ## .....
]
}
arthur_model.query(query_function)
API
API calls that take in a query expect a JSON-formatted query.
JSON{"select":[
{"property":"*"}
],
"from":"inference"
}
Quick Common Examples
Some of the most common use cases for querying include:
Pulling Custom Data: Teams often use the query service to pull data that they are interested in experimenting with / reporting on further with custom graphs / reports.
Creating Metrics to track and alert within Arthur: User-Defined Metrics are created based on the Arthur query language. Teams can also set alerts based on these metrics in the UI, Python SDK, or API after creating a metric.
Comparing Cohorts: Finally, another common workflow of the query service is to pull cohorts of data to compare in a notebook. This can be done for any function, but some of the most common are performance, drift, or regional explainability.
Updated 3 months ago Table of Contents
Interacting with Arthur Query Service
Python SDK
API
Quick Common Examples
=====================
