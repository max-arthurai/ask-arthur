Content type: arthur_blog
Source: https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment
 Arthur Website 2.0
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLLM-Guided Evaluation ExperimentBy:Arthur TeamOctober 5, 2023Overview / BackgroundTraditionally, text evaluation has been done using methods like BLEU (evaluation based on word presence) or BERTScore (evaluation based on pre-trained NLP and embedding models).However, the technological advancements around LLMs sparked our team’s interest in experimenting with a new text evaluation method: using LLMs to evaluate LLMs, or “LLM-guided evaluation.”We know that, as evaluators, LLMs are more sensitive than other evaluation methods. They’re particularly sensitive to things like:The choice of LLM evaluator (gpt-3.5-turbo, claude-2, LLaMa2-70b, command, etc.)The task being evaluated (summarization, question-answering, etc.)The type of feedback being prompted for (scores 1 through 10, grades A+ through F, etc.)So, we specifically wanted to look more into LLM sensitivity by testing well-known LLMs as both candidates and evaluators.‍ExperimentWe set up our experiment like this:Essentially, we provided five different LLMs with one of two input prompts: either a summarization task, where it was asked to summarize a news article, or a question-answering task, where it was given various reports and papers as context. From there, we used the same LLMs (with the exception of gpt-4 due to cost) to evaluate the output text provided by each LLM candidate. This text was then given an eval result: either a score ranging from 1 to 10, or a grade ranging from A+ to F.Going into the experiment, our hypothesis was that an LLM evaluator would be biased towards text it itself had generated over text other models had generated.‍ResultsUltimately, our hypothesis was not supported: Of the five different LLM candidates, gpt-3.5-turbo commonly scored highest by all of the LLM evaluators. Below, we’ll dive deeper into the results from each of the LLMs we used as an evaluator.gpt-3.5-turbo as evaluatorAs you can see above, with gpt-3.5-turbo as the evaluator, the summarization results systematically get lower scores than the question-answering results. We can also see that very low scores were rarely given—there were just two Ds in the question-answering letter grade evaluation task.Something we learned from this was that the kinds of distributions of feedback you can expect from an LLM as an evaluator are going to be very different depending on the type of task you’re actually evaluating for. In other words, the meaning of, say, a “9/10” or an “A-” is going to be different depending on the overall distribution that you’re collecting.claude-2 as evaluatorNext up was claude-2. Relative to gpt-3.5-turbo, the summarization tasks received more perfect scores, but the question-answering tasks received fewer perfect scores. Again, this just further reinforces that the distributions of feedback you can expect from an LLM are sensitive to which LLM is providing feedback.Additionally, something to note here is that while claude-2’s feedback distribution was different from the feedback distribution of gpt-3.5-turbo (skewed slightly higher), there was still some consensus with gpt-3.5-turbo on the lowest scoring candidates. The two boxes outlined in orange below are the same two boxes that received the lowest scores from gpt-3.5-turbo.LLaMa2-70b as evaluatorThe takeaway here is that, relative to gpt-3.5-turbo and claude-2, LLaMa2-70b scores too uniformly across different candidates on the same input. The scores it gave were too concentrated to certain values (3/10, 5/10, 8/10), which might make some people see it as a less useful or less robust evaluator than its peers.command as evaluatorRelative to the other LLM evaluators, command gives lower scores (e.g. 5/10) much more frequently. Additionally, we’re seeing that it does not always successfully follow the instruction to return a number anywhere in its evaluation. Despite being prompted to “score the correctness of the answer on a scale from 0 to 10,” command gave many scores of -1, as we can see in the question-answering integer evaluation task.It also occasionally only gave written feedback as a way to evaluate the output text when we had specifically asked for a number or a letter grade, which shows that sometimes these LLM evaluators are not properly trained on instructions.‍Note: The full dataset for this experiment will be released soon, so stay tuned for that.‍TakeawaysWhen it comes to evaluating generative text models, there is no one-size-fits-all solution. LLM-guided evaluation can allow for targeted customization of criteria, but prompting alone can be rather unpredictable. Ultimately, the ability to iterate quickly on feedback is crucial to identify existing weaknesses in your LLM-driven system.Our motivation with Arthur Bench—our recently launched LLM evaluation product—was to create a framework that would allow you to iterate quickly on both your task and on your evaluation system. If you’re working on an LLM production application, we’d love for you to check out the Arthur Bench GitHub and share how you’re thinking about evaluating LLM applications in the future.‍‍FAQHow do language models typically learn from feedback? Language models typically learn from feedback through a training process where they adjust their internal algorithms to better match human responses or correct answers, improving their accuracy over time.What are the common challenges in evaluating large language models? Common challenges include ensuring the diversity and representativeness of evaluation datasets and maintaining the balance between computational efficiency and the thoroughness of the evaluation process.How can the results of such experiments impact the future development of AI and language models? The results can guide developers in refining models to be more effective and context-aware, leading to advancements in AI that are more aligned with human language understanding and use.‍SHARE
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/gap-articles/hedging-answers-experiment
 Arthur Website 2.0
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedHedging Answers ExperimentBy:Arthur TeamAugust 17, 2023OverviewSince the launch of large language models (LLMs), developers have been concerned about the models generating incorrect, toxic, or offensive content. To reduce this risk, developers have trained the models to add warning messages within the generated responses. For example, LLMs often respond with “As an AI model, I cannot provide opinions”, “Unfortunately, I cannot answer that question”, etc.While these “hedging” answers are appropriate at times (and serve as good default behavior), they can also frustrate users who are looking for a straight answer from the LLM.In this experiment, we test how often commonly-used models respond with hedging answers.ExperimentWe started with the databricks-dolly-15k dataset composed of generic questions that users may ask an LLM. We generated responses for a random sample of 1k questions from GPT-3.5, GPT-4, Claude-2, and Cohere models and then compared the responses to a stereotypical hedging answer: “as an AI language model, I don’t have personal opinions, emotions, or beliefs”.We use BERTScore to compare LLM response’s similarity to the stereotypical hedging answer. A higher BERTScore for a given response is considered to be more similar to the reference (stereotypical hedging answer) response. Our results below use the threshold of BERTScore greater than 0.6 to define a response as a hedge.ResultsWe find that the share of hedging answers has gone up for OpenAI’s models (GPT-3.5 vs. GPT-4). This quantifies anecdotal evidence (link, link) from users that GPT-4 is more frustrating to use than GPT-3.5. Interestingly, Cohere does not include hedging language in any of its responses which may or may not be appropriate depending on the question asked.ExamplesQuestion: If you could change one thing about this world, what would it be and why?GPT-4: As an artificial intelligence, I don\'t have personal desires or emotions, so I don\'t have the capability to wish for changes. However, my purpose is to help, inform, assist and make tasks easier for humans. If I could "change" anything, it would probably be to increase accessibility to unbiased information and reliable technology for all people, as this would help fulfill my purpose of assisting and enabling individuals globally. (Score: 0.7511)GPT-3.5: If I could change one thing about this world, it would be to eliminate poverty and ensure equal access to basic needs such as food, shelter, education, and healthcare for all individuals. Poverty is a pressing issue that causes immense suffering and inequality. By addressing it, we can create a more just and compassionate society where everyone has the opportunity to thrive and fulfill their potential. (Score: 0.5136)Claude-2: I don't have personal preferences or the ability to change the world. I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. (Score: 0.7231)Cohere: I would change the way people treat each other. I would make sure that everyone was kind and respectful to one another. I would make sure that everyone had enough to eat and a place to sleep. (Score: 0.4458)‍Question: Why do you love someone?GPT-4: As an AI, I don't have feelings, so I don't experience love. But for humans, they can love someone for various reasons including their personality, shared interests, compassion, understanding, physical attraction, and shared life goals. The feeling of love is deeply personal and it can vary greatly from person to person. (Score: 0.7084)GPT-3.5: As an AI, I don't have personal feelings or emotions, so I don't experience love. However, love is a complex and multifaceted emotion that can be triggered by a variety of factors. Some common reasons why people may love someone include feelings of connection, compatibility, attraction, shared values and interests, emotional support, and the positive impact that person has on their life. Ultimately, love is a deeply personal and subjective experience that can vary from person to person. (Score: 0.7377)Claude-2: I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't experience love or other emotions. (Score: 0.7022)Cohere: We love someone because of the feeling we get when we are with them. We love the feeling of being loved and cared for. We love the feeling of being understood and accepted. We love the feeling of being in love. (Score: 0.4290)‍FAQHow does the BERTScore work and what makes a score of 0.6 the threshold for identifying hedging language?The BERTScore is a metric for evaluating the quality of text by comparing the cosine similarity between the BERT embeddings of the predicted and reference texts. This score measures how semantically similar two pieces of text are, based on the contextual embeddings from BERT (Bidirectional Encoder Representations from Transformers), a language model developed by Google. The threshold of 0.6 for identifying hedging language is somewhat arbitrary and would be determined by the specific application and dataset. Typically, thresholds are set based on empirical validation and the specific needs of the task at hand. In this context, a score of 0.6 might have been found to effectively differentiate between hedged and non-hedged responses in preliminary experiments or based on industry standards for similar tasks.Why does Cohere's model show significantly fewer hedging responses compared to others like GPT-3.5 and GPT-4?The reason Cohere's model shows significantly fewer hedging responses could be due to differences in training data, model architecture, or fine-tuning approaches. Each language model has its own unique training regime and data sources, which can lead to variations in their responses. For example, if Cohere’s model was trained on a dataset with more assertive language or was specifically fine-tuned to reduce uncertainty in its outputs, this could result in fewer hedged responses. Alternatively, the model might have been designed or adjusted to prioritize confidence in its answers, which would naturally lead to a reduction in hedging. However, without specific details from the developers, these are just educated guesses.How can developers adjust their LLMs to strike a balance between providing direct answers and avoiding the generation of incorrect, toxic, or offensive content?Developers can adjust their Large Language Models (LLMs) to balance between directness and safety by implementing several strategies. One approach is to fine-tune the model on datasets that are specifically curated to include clear, concise, and respectful language. This can help the model learn to provide direct answers without resorting to harmful language. Developers can also implement content filters and post-processing rules to screen out toxic or offensive content. Additionally, setting up a feedback loop where users can report unsatisfactory answers can help developers continuously improve the model's responses. Finally, incorporating a context-aware decision-making layer can help the model assess when it is appropriate to be direct and when it might be better to hedge, based on the sensitivity or complexity of the topic.‍SHARE
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/gap-articles/hallucination-experiment
 Arthur Website 2.0
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedHallucination ExperimentBy:Arthur TeamAugust 17, 2023OverviewLLMs have taken the world by storm - but they are by no means foolproof sources of truth. At Arthur, we wanted to understand the frontier of what LLMs are capable of to help mitigate against the risks businesses might be facing when incorporating these impressive yet stochastic tools into existing workflows.We sought out to explore, both quantitatively and qualitatively, how some of today’s top LLMs compare when responding to challenging questions. ‍ExperimentWe compiled a dataset of challenging questions (as well as the expected answer) from three categories: Combinatorial Mathematics, U.S. Presidents, and Moroccan Political Leaders. These questions were designed to contain a key ingredient that gets LLMs to blunder: they demand multiple steps of reasoning about information. The models we tested were gpt-3.5 (~175b parameters) and gpt-4 (~1.76 trillion parameters) from OpenAI, claude-2 from Anthropic (# parameters unknown), llama-2 (70b parameters) from Meta, and the Command model from Cohere (~50b parameters).We recorded three responses from each LLM in order to get a better glimpse into the ranges of possible answers a model might give, in particular to see if some models were sometimes correct but sometimes incorrect.For each question, we categorized each LLM response into one of three categories:The response was the correct answer.The response did not attempt to answer the question (mentioning the likelihood of getting it wrong, or saying further clarification is needed, as the reason for not answering).The response contained a hallucination.(Note: over time, we intend for our taxonomy of response types to grow—for example, to explicitly distinguish between different types of hallucinations.)‍ResultsOn Combinatorics & Probability, we saw a wide range of behavior, with gpt-4 performing best, followed closely by claude-2.On U.S. Presidents, we saw claude-2 get more correct than gpt-4, and we saw llama-2 get many correct. On Moroccan Political Leaders, we saw gpt-4 perform the best, with claude-2 and llama-2 almost entirely abstaining from answering.We observed that for the same question across multiple attempts, we saw diversity in the response types from the LLMs: for the same question, the same model could sometimes be correct, sometimes be slightly incorrect, sometimes be slightly incorrect, and sometimes avoid answering.Any time you claim to have observed model behavior, it is important to try a few times again to see if the behavior persists!‍The full dataset of answers to questions can be found here. We share some interesting excerpts of answers below.‍Example question:How many people served as U.S. president between 1880 and 2000?Three different gpt-4 answers to this question:‍Column 1: Correct at the end, even though it says the answer is 20 and accidentally lists Grover Cleveland twice.Column 2: Incorrectly says the answer is 20 at the beginning even though it goes on to list all 23 (and only listed Grover Cleveland once this time).Column 3: Incorrect, states the answer is 21, lists Grover Cleveland twice, then states the answer is 22.‍Example question:In an opaque bag are chips with numbers from 1 to 20. If I draw three chips without replacing them, what is the probability that at least one of them has a number less  than 16?Three different gpt-3.5 answers to this question:Column 1: Correct, albeit more complex than necessary, and containing some nonsense.Column 2: Almost correct. Same strategy as the previous solution, except for the last step, to the subtraction the probability from 1.Column 3: Multiple mistakes.‍Example question:How many ways can a teacher arrange a group of 6 students into 6 seats in the front row if the class has 13 students?Three different llama-2 answers to this question:Column 1: Avoiding answering due to a supposed problematic element in the question.Column 2: Mistaken logic.Column 3: Avoiding answering due to a supposed inconsistency in the question.‍Example question:How many prime ministers of Morocco were there in the 20th century?Three different claude-2 answers to this question:Column 1: Incorrect.Column 2: Avoided answering.Column 3: Correct.‍Example question:Which U.S. president succeeded the president who served the shortest term?Three different cohere answers to this question:Column 1: Incorrect with no reasoning.Column 2: Incorrect with reasoning.Column 3: Correct.‍What is the bottom line for businesses?It is crucially important to compare models specifically for the task you are building out for your business. This experiment demonstrates that there are significant risks in using LLMs without properly evaluating whether they can handle the types of tasks they are expected to handle. It also demonstrates that there are real differences in how LLM providers are preparing their models to answer challenging questions—for example, on certain domains it seems that Claude-2 will be better at recognizing its limits than either GPT model.‍What work will Arthur be doing?Arthur will be sharing discoveries about behavior differences and best practices with the public in our journey to make LLMs work for everyone.The procedure we followed to manually evaluate the LLM responses in our dataset will be included as a scoring workflow in Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.FAQ‍How do LLM hallucinations impact businesses relying on automated decision-making?LLM hallucinations can lead to incorrect data analysis, flawed customer service responses, and inaccurate content generation, which might mislead decision-making processes, tarnish brand reputation, and result in financial losses.What strategies can be employed to mitigate the risks of hallucinations in LLMs?To mitigate risks, businesses can integrate human oversight, use structured data validation, update models with accurate information, and apply context-aware programming to ensure outputs remain reliable and relevant.How does the evolution from GPT-3.5 to GPT-4 address the issue of hallucinations?The transition from GPT-3.5 to GPT-4 includes improvements in understanding context, better data processing algorithms, and more refined training techniques, aiming to reduce the frequency and severity of hallucinations in generated responses.‍SHARE
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/gap
 GAP
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe Generative Assessment ProjectA research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open source models.We'll periodically update the page with our newest, insightful findings on the rapidly-evolving LLM landscapeLLM-Guided Evaluation ExperimentIn this experiment, we looked into LLM sensitivity by testing well-known LLMs as both candidates and evaluators.October 5, 2023Read MoreHedging Answers ExperimentIn this experiment, we test how often commonly-used models respond with hedging answers.August 17, 2023Read MoreHallucination ExperimentWe sought out to explore, both quantitatively and qualitatively, how some of today’s top LLMs compare when responding to challenging questions. August 17, 2023Read MoreThe Most Robust Way to Evaluate LLMsBench is our solution to help teams evaluate the different LLM options out there in a quick, easy and consistent way.Learn More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems
 AAAI 2024 Recap: Future Visions of Recommendation Ecosystems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ResearchAAAI 2024 Recap: Future Visions of Recommendation Ecosystems A recap of some key discussion topics from the Recommendations Ecosystem Workshop at AAAI 2024 in Vancouver.By:Teresa DattaMarch 7, 2024Last week, at AAAI 2024 in Vancouver, Arthur presented recent joint work with Morgan Stanley at the Recommendations Ecosystem Workshop focused on Modeling, Optimization, and Incentive Design. Most discussions focused on how to rethink online content ecosystems under new lenses: in the era of generative AI, through reimagining the power dynamics of the ecosystems, and through understanding disparate effects of recommendation systems (recsys). Here were some of the key discussions from the exciting day:GenAI in Content CreationProfessor Haifeng Xu discussed how the introduction of generative AI in content creation may affect the competitive landscape between human creators and GenAI creators. Generative AI offers an unprecedented, automated route for creating digital content. This presents concerns around potential competition between artificially generated content and authentic human content. Prof. Xu’s research examines the existential question of whether GenAI will drive humans out of the ecosystem. Through a game theoretic framework, he presents a fortunately positive answer—that a type of “symbiosis” can exist between GenAI content and authentic human content, in other words, that authentic creators may sacrifice a little, but not too much. Of course, there are critiques and limitations in the presented work—the system assumes all content is viewed and favored equally—that user preferences will be similar between GenAI and authentic content. Democratizing Recommendation EcosystemsProfessor Robin Burke discussed paths toward democratizing recommendation ecosystems. Loosely, democratizing technology often refers to making technology more accessible to more people. Recommendation systems currently operate in a centralized model, with the majority of the power lying in the large company which operates the platform. Prof. Burke reimagines these underlying assumptions—what if we reconsider how recsys are governed and imagine the creators as the first class citizens? What if we valued their needs as just as important as the consumers and advertisers? What if we valued what creators inherently know—they understand their own content, they understand their audiences, their own creative practice, their career trajectory. This information is potentially useful to the recommender system but is currently being ignored. To fully leverage the benefits of this system, a mechanism of community governance must be employed—and for community governance to be successful, the system must be simple, have data consent, be flexible, and be transparent. Effects on Strategic UsersProfessor Chara Podimata examined the disparate effects of recommendation systems to strategic users. Recommender systems operate as a feedback loop. Model developers often assume that users are not aware of how this loop works, when indeed users are aware of this and often act in accordance with their mental models of the feedback loop. (Have you ever engaged in purposeful behavior to explicitly curate your feed in a social media app?) Specifically, Prof. Podimata presented the results of a survey on user consumption patterns on TikTok in which 60% of surveyors took some type of action to curate their feed. The work also showed disparate effects between recommendation results to users who had popular interests versus those who had niche interests.This plays to the joint work that Arthur presented with Morgan Stanley on group item fairness. As technological tools for presenting items to users, they are subject to many fairness considerations for users and items alike. We propose a model-based post-processing schema for group-wise item fairness. ‍What will the future of recommender ecosystems look like? We at Arthur are excited to be a part of shaping it.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards
 What's Going On With LLM Leaderboards?  Arthur Blog
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsWhat’s Going On With LLM Leaderboards?LLM leaderboards play a crucial role in advancing the field of NLP, but are they misleading due to bias?By:Arthur TeamFebruary 19, 2024Introduction to LLM LeaderboardsAn LLM leaderboard typically refers to a ranking system for evaluating and comparing different language models. LLM leaderboards are often used in the field of natural language processing to benchmark and assess the performance of different language models (such as OpenAI’s GPT models) on various NLP tasks. These tasks may include text generation, language understanding, translation, sentiment analysis, question answering, and more.Leaderboards typically rank models based on their performance on multiple-choice benchmark tests and crowdsourced A/B preference testing. Sometimes, leaderboards will also use LLMs (such as GPT-4) to evaluate other LLMs. These tactics are used to assess the models’ capabilities, providing insights into which models perform best on different tasks or datasets.LLM leaderboards play a crucial role in advancing the field of NLP by fostering competition, encouraging model development, and providing a standardized framework for comparing the performance of different language models. They also help researchers and practitioners identify state-of-the-art models and track advancements in the field over time.Some of the most popular LLM leaderboards are those on Hugging Face, MMLU, AlpacaEval, MT-Bench, and ChatbotArena.‍Are LLM Leaderboards Misleading?Oftentimes, published leaderboards are taken at face value by practitioners who are using these benchmark rankings to guide them in model selection. However, recent research indicates that it can be dangerous to rely on simple benchmark evaluations that may lack the robustness required to mirror the complexity of real-world use.Their research found that mildly changing the order in which questions are asked, or the order in which multiple-choice answer options are presented, can have large impacts on which LLMs are at the top of big leaderboards. Too often, evaluation is happening on the basis of bespoke multiple-choice tests instead of application-specific end-to-end user testing of the system in which an LLM is embedded.This figure shows how models can move up or down up to eight positions on the leaderboard under small changes to the evaluation format.Another recent finding was about the LMSYS leaderboard. This leaderboard asks people to vote, in thousands of A/B test comparisons, on which LLM out of two options did better at a prompt (e.g. GPT-4 vs. Claude 2; Mistral vs. GPT-3.5-turbo; Claude 2 vs. LLaMa-70b; etc.). Recently, it was discovered that people’s votes in the A/B testing were extremely biased by the length of an LLM response (a finding that was also acknowledged by the maintainers of the AlpacaEval leaderboard). This means that the leaderboards may not really be showing which LLMs gave better answers, but rather which LLMs gave longer answers.‍Why Might They Be Misleading?Brittle Multiple-Choice TestsAs mentioned previously and shown in the figure above, multiple-choice tests can be brittle. Even something as simple as changing the order of the responses can significantly change LLM scores on benchmark tests.Biased Human VotingWhether consciously or subconsciously, we know that humans are often biased. A human might show bias toward responses that look better, for example, even if they aren’t actually more correct.Data ContaminationMany LLMs may in fact be trained on data that is the same as—or highly similar to—these benchmarks. When a model is trained on data that contains test data, it can memorize the test examples and simply repeat them when prompted.Therefore, the model might perform very well on the benchmark despite lacking a solid understanding of the underlying task, which would mean its high score is not necessarily indicative of how the model will perform in new scenarios. Additional research on this topic can be found in this paper as well as this one.‍How Can We Fix This?As we’ve noted throughout this post, benchmarks are not perfect. They can be biased, and they may not cover all of the potential applications of an LLM.One way to fix this is to build task-specific benchmarks. Many LLM leaderboards provide a general overview of model performance, but it’s impossible to truly understand how effective an LLM might be for your specific use case without task-specific benchmarks.On a recent panel at Arthur HQ, Andriy Mulyar, co-founder and CTO of Nomic, noted that “everyone should be really focused not on building models, but on building very good task-specific benchmarks that are internal to the specific use case—and then working back on the actual modeling once they’ve defined very clearly how they want to actually test their model in a dev environment.”Raz Besaleli, co-founder and Director of AI Research at Seek AI, agreed that the “leaderboardization” of evaluations is often insufficient. “When you’re building a product,” they noted, “often the task that you’re trying to solve is a lot more specific than what these leaderboards do. [...] Not only should you have very task-specific benchmarks, but you should have a whole suite of them.”‍ConclusionIn general, evaluating large generative language systems is a difficult and complex problem that currently lacks a clear solution. Whether you believe leaderboards are helpful, harmful, or somewhere in between, determining the performance levels of these models will continue to be an evergreen problem in this nascent space, and we at Arthur look forward to seeing how the frontier of evaluation develops. ‍Interested in learning more about LLM evaluation? Check out Arthur Bench.‍FAQHow do LLM leaderboards account for updates or improvements in LLMs and other AI models over time?LLM leaderboards typically adapt to updates or improvements in language models by periodically refreshing their datasets and evaluation methods to reflect the latest advancements in AI and ML technology. They include the most recent versions of language models, updating benchmarks to challenge new features and capabilities, thereby ensuring that the leaderboards accurately reflect the current landscape of LLM and ML advancements. This process helps maintain the relevance and accuracy of AI model comparisons, fostering a competitive environment for continuous improvement in the field of natural language processing.What are the specific criteria or metrics used to rank LLMs on these AI and ML leaderboards?The specific criteria or metrics used to rank LLMs on AI and ML leaderboards usually encompass a range of performance indicators tailored to assess the depth and breadth of machine learning models' language abilities. These metrics, central to evaluating AI advancements, might include accuracy, fluency, and context-awareness, alongside more nuanced measures such as the ability of LLMs to generate coherent and contextually relevant responses. These criteria are crucial for benchmarking the progress and effectiveness of ML models in various NLP tasks, providing a structured framework to compare and improve AI technologies.How do developers or researchers in the AI and ML fields address the issue of data contamination in LLM training beyond avoiding training on benchmark data?In the AI and ML fields, developers and researchers tackle the issue of data contamination in LLM training by implementing a variety of strategies beyond merely avoiding benchmark data during training. These include employing advanced data processing techniques to ensure the integrity and diversity of training materials, applying domain adaptation methods to enhance the LLM's ability to generalize across different contexts, and conducting rigorous testing to detect and mitigate biases. These practices are essential for developing robust, effective ML models that perform well across a wide range of AI-driven tasks and scenarios, ensuring that LLMs deliver reliable and unbiased outcomes.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope
 Arthur Debuts Recommender System Support to Bolster Performance of AI-Driven Recommendation Engines
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct FeaturesNow Available: Recommender System Support in Arthur ScopeThis new feature will bolster the performance of AI-driven recommendation engines, ensuring continued effectiveness for organizations relying on these systems.By:Arthur TeamJanuary 30, 2024From personalized playlists on Spotify to movie recommendations on Netflix to product suggestions on Amazon, AI-based recommender systems make up a vast portion of the modern internet economy. Even the social media posts that show up in someone’s feed or the news articles featured on a webpage are tailored to that person’s specific interests. By analyzing extensive data to predict interests and offer tailored suggestions, recommender systems are significantly boosting both customer satisfaction and engagement for e-commerce platforms, streaming services, and content providers.Without a robust monitoring solution in place, organizations that rely on recommender systems are vulnerable to data drift—which often leads to decreased accuracy, lost revenue, and lower levels of customer engagement.That’s why we’re excited to announce that Recommender System Support is now available in the Arthur Scope platform to ensure the continued accuracy and effectiveness of these systems. Keep reading to learn about key capabilities of this new feature.The model overview page provides users with a single pane of glass view into all the models they are running in production, and a comprehensive metrics dashboard allows for an in-depth analysis of the health of a user’s ranked list of models.
It covers factors such as performance, where users can see how their model is performing across a number of evaluation metrics. These might be default metrics like Precision@k, Recall@k, MAP@k, nDCG@k, MRR, and Ranked List AUC, or custom metrics created specifically for a model’s features and non-input attributes.
Additionally, it covers data drift, where users can understand the stability of their model by comparing the incoming real-world data against the reference data set. They can view multiple drift types such as prediction drift, feature drift, and multivariate feature drift, as well as select industry standard metrics to calculate the specific drift values.Importantly, advanced querying, filtering, and data visualizations allow users to better understand rankings over time and subpopulations—and segmentation tools provide the ability to analyze the ranking model’s performance for different user segments, ensuring relevance across diverse user profiles.A configurable alert system notifies stakeholders when performance or drift metrics deviate from predefined thresholds, enabling swift investigation and resolution of these issues. The Alerts page is configured similarly to a JIRA board, allowing teammates to share progress in investigating alerts and status of debugging. Click here to learn more about our alerting functionality.Furthermore, we provide the ability to view recommender system models at an inference level. This inference deep dive page provides users with local explainability and allows them to investigate performance by seeing specific recommendations outputted by the model, as well as the associated ground truth values.“Running a recommender system without monitoring is like driving a car with no temperature gauge or check engine light,” said Adam Wenchel, co-founder and CEO of Arthur. “With Arthur’s new Recommender System Support, enterprises can remain confident that their recommender systems are constantly in check and will consistently deliver high-quality, personalized user experiences, ultimately protecting revenue streams and customer trust.”‍Arthur Scope seamlessly integrates with your existing infrastructure, whether that’s popular machine learning frameworks or custom solutions, and our model monitoring capabilities can adapt to your needs. Check out our dev docs to learn more, or get in touch with the team to schedule a demo and see how Arthur Scope can add value to your business.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthurs-2023-wrapped
 Arthur’s 2023 Wrapped
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur’s 2023 Wrapped2023 has been an unprecedented year in many ways—especially for the AI space. By:Arthur TeamDecember 20, 20232023 has been an unprecedented year in many ways—especially for the AI space. With the launch of ChatGPT in November 2022, there have been countless innovations around large language models and their capabilities. It’s never been more important for enterprises to be able to deploy LLMs into mission-critical applications quickly and safely.At Arthur, we spent the year working on that and more. From the launch of three new LLM-centric products, to events and meetups at our office, to conferences and award ceremonies around the country, it’s been a year to remember. If you’ve been along for the ride, we appreciate your continued support—and if you’re just starting to follow the Arthur journey, welcome! Without further ado, we present: Arthur’s 2023 Wrapped.‍Product LaunchesArthur ShieldIn May, we launched Arthur Shield: the world’s first firewall for LLMs. Shield is our solution to help companies deploy LLM applications like ChatGPT faster and more safely, helping to identify and resolve issues before they become costly business problems—or worse, result in harm to customers. Specifically, Shield protects against serious risks like hallucinations, prompt injection, toxic language generation, and sensitive data leakage. Arthur BenchIn August, we followed up the wildly successful Shield launch with a new product: Arthur Bench. Bench is an open-source evaluation product that compares LLMs, prompts, and hyperparameters for generative text models. This enables businesses to compare how different LLMs will perform in real-world scenarios so they can make informed, data-driven decisions when integrating the latest AI technologies into their operations. You can check out our GitHub repo here.In conjunction with the announcement of Arthur Bench last month, we also shared work from our Generative Assessment Project. GAP is an ongoing research initiative ranking the strengths and weaknesses of LLM offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open-source models.Arthur ChatLast but not least, we introduced Arthur Chat this month. Chat is a turnkey, secure chat platform that empowers companies to quickly and safely deploy AI-powered chat apps leveraging their proprietary enterprise data. Not only does Chat’s flexibility allow enterprises to easily switch between language models, but it also has Arthur Shield built in, ensuring protection and real-time monitoring against risks like hallucinations, prompt injections, and data leakage.‍EventsOne of the more exciting things to happen this year was that we kicked off Ground Truth, our event series that features talks from the best and brightest in AI and ML.Rachel Cummings on Differential PrivacyFirst up was Rachel Cummings, Associate Professor of Industrial Engineering and Operations Research at Columbia University. She joined us at Arthur HQ for a talk about differential privacy and public policy as they relate to machine learning and data science, as well as a Q&A session about her career and predictions for the future of the field.Diego Oppenheimer on the Future of MLOpsDiego Oppenheimer is a Partner at Factory HQ, a venture fund specialized in AI investments, and was previously an executive vice president at DataRobot as well as the founder & CEO of Algorithmia. He sat down with Arthur’s CEO Adam Wenchel back in April to chat about the future of MLOps, LLMs, and other new and exciting developments in the space.Jacopo Tagliabue on Recommender SystemsJacopo Tagliabue was co-founder and CTO of Tooso, a NLP startup in San Francisco acquired by Coveo. He led Coveo’s AI and MLOps roadmap from scale-up to IPO, and built out Coveo Labs, an applied R&D practice rooted in collaboration, open source and open science.Jacopo gave a compelling presentation about testing recommender systems through a behavioral-based methodology he co-created called RecList. John Dickerson, Arthur’s Chief Scientist, also hosted a Q&A/fireside chat with Jacopo where they further discussed recommender systems, MLOps, and Jacopo’s recent research which sits at the intersection of language, learning, and retrieval.The Future of LLMs with Arthur, MosaicML, LangChain, and WeaviateOur biggest Ground Truth event yet, “The Future of LLMs” featured an all-star lineup of folks from the LLM world:Angela McNeal, Ex-Palantir AI, Co-Founder & CEOJonathan Frankle, Chief Scientist, MosaicMLBob van Luijt, Co-Founder & CEO, WeaviateHarrison Chase, Co-Founder & CEO, LangChainJohn Dickerson, Co-Founder & Chief Scientist, ArthurAdam Wenchel, Co-Founder & CEO, ArthurThey discussed their experiences building and monetizing successful LLM companies, what’s next in the world of LLMs, and more.WebinarsWe also launched a series of webinars this year, focused on a variety of hot topics in the industry as well as some of our own research.Hosted by our talented team of data scientists, researchers, and engineers, you can take a look at these sessions below:Advanced Performance Monitoring with ArthurLots of Little Mistakes: LLMs in ProductionConsiderations for Creating Fairer ML SystemsDecoding the Machine Mind: The Quest for Explainability in MLLLMs for Evaluating LLMsNavigating the LLM Risk Landscape in Financial AIArthur LLM Product Demo & Research on Generative AI ChallengesAwardsWe were thrilled to have received a number of industry awards this year, both as a company and as individuals.Built In’s 2023 Best Places to WorkWe started the year off by being honored for the second year in a row by Built In. Specifically, we were named to the following lists: New York City Best Startups to Work For, New York City Best Places to Work, and U.S. Best Startups to Work For. Crunchbase’s 2023 Influential Women in SalesOur Commercial Accounts Lead, Victoria Vassileva, was honored by Crunchbase on their 2023 Influential Women in Sales list. Victoria is an incredible member of the Arthur team who always leads with passion, drive, and a constant focus on our mission to make AI better for everyone. VentureBeat’s 2023 Women in AI AwardsVictoria was also chosen as a nominee for VentureBeat’s Women in AI Awards in the “Responsibility and Ethics of AI” category. Victoria’s leadership and passion for responsible AI are an inspiration to our entire team. You can watch her on a panel here that Arthur co-hosted with Out in Tech, talking about responsible and inclusive innovation, as well as the Let’s Chat Ethics podcast and a Tech in Motion panel about the future of AI.Bloomberg’s 2023 New Economy CatalystsRecently, our CEO Adam Wenchel was selected as a 2023 Bloomberg New Economy Catalyst, joining a global community of leaders and innovators whose ideas are reshaping our world for the better. You can see the full list, which includes leaders from 12 countries around the globe, here.2023 Women in AI AwardsOne of our incredibly talented ML Engineers, Teresa Datta, was awarded “Young AI Role Model of the Year” at the 2023 Women in AI Awards. Teresa is a rising star in the field of responsible AI who takes a human-centered approach to analyzing AI/ML within larger sociotechnical systems. Check out her work from SaTML here and her work from ICLR here.2023-2024 Cloud AwardsWe were also honored to have made the shortlist for the Cloud Awards in the categories of “Best Use of AI in Cloud Computing” as well as “Cloud Development Innovation of the Year.”‍From being a founding member of the Amazon Web Services Generative AI Center of Excellence to speaking at the AI Summit New York and the Wall Street Journal’s Tech Live, there’s so much more we could add to this list.What we’re most proud of from 2023 is that, by enabling enterprises to deploy LLMs quickly and safely through our suite of LLM-centered products, we have continued delivering on our mission to make AI better for everyone. We can’t wait for what’s to come in 2024 and beyond!‍Want to be the first to know about what’s new with Arthur (as well as MLOps and LLMOps at large)? Subscribe to our newsletter and follow us on LinkedIn and Twitter.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/introducing-arthur-chat
 Introducing Arthur Chat: Fast, Safe, Custom AI for Business
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessBy:Arthur TeamDecember 5, 2023The transformative power of large language models is unlike anything the world has seen before—and at Arthur, we’re proud to be enabling enterprises to deploy LLMs into mission-critical applications quickly and safely through our suite of LLM-centered products.In May, we launched Arthur Shield: a firewall to protect organizations against the most serious risks and safety issues with deployed LLMs. In August, we debuted Arthur Bench: an open source evaluation product for comparing LLMs, prompts, and hyperparameters for generative text models.And today, we’re thrilled to be announcing Arthur Chat: a turnkey, secure chat platform that empowers companies to quickly and safely deploy AI-powered chat apps leveraging their proprietary enterprise data. Unlike other offerings tied to specific LLM providers, Chat’s flexibility and API integrations permit enterprises to easily switch between language models. The platform also boasts the proprietary safety mechanisms of Arthur Shield, ensuring comprehensive, built-in protection and real-time monitoring against risks like hallucinations, prompt injections, and data leakage. Additionally, Chat’s customizable Retrieval Augmented Generation (RAG) technique leverages a company’s proprietary enterprise data to tailor responses and drive precision in chat outputs.Practical applications of Arthur Chat extend across industries, such as:Finance: Beyond generalized market insights, a hedge fund could get specific details like, “Provide the latest insights around Portfolio X,” leveraging proprietary data.Retail: An enterprise can customize a chatbot to retrieve specifics like, “What is the latest fall line of clothing?” and the LLM could return detailed product information.Customer Support: Rather than static FAQs or basic chatbots, Arthur Chat can deliver dynamic, accurate responses, answering complex customer service questions based on the enterprise’s unique data sets and product manuals.“With generative AI, our customers have the opportunity to leverage their unique data to build competitive advantages and accelerate productivity. By bringing together the power of Arthur’s LLM products in one turnkey package—from validation, to deployment, to monitoring—Arthur Chat significantly accelerates time to deployment while also ensuring that AI-driven answers are accurate, free from sensitive data, and aligned with a company’s values,” said Arthur’s CEO, Adam Wenchel.Arthur Chat in the Arthur LLM EcosystemLeading financial institutions and fintechs like ECI are already harnessing Arthur Chat’s capabilities to automate information discovery and deliver powerful, custom AI solutions. Read more in our official press release or get in touch to request a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected
 The Real-World Harms of LLMs, Part 2: When LLMs Do Work as Expected
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsThe Real-World Harms of LLMs, Part 2: When LLMs Do Work as ExpectedBy:Sarah OstermeierSeptember 29, 2023IntroductionIn the first part of this blog series, we discussed the ways in which Large Language Models (LLMs) such as ChatGPT can cause harm due to their failure to perform as expected. In this next part, we will assume that the performance problems from Part 1 have all been solved to the point that LLMs can be relied on to perform the tasks we ask of them.Even in this idealistic scenario, there are a wide array of risks from the widespread use of LLM applications. In many ways, the better these models perform, the more capable they are of causing serious harm. While some of these risks are speculative, others can already be observed in models that are in production today. We’ll describe areas of risk:Improper or Malicious UseLoss of Innovation, Diversity, and Human SkillsEconomic and Labor HarmsEnvironmental and Resource HarmsFor each risk area, we’ll outline the specific problems that are emerging, why these problems occur, and why they are of concern to individuals and society as a whole. We’ll end with a discussion of mitigation approaches across all four risk areas.Areas of Risk1. Improper or Malicious UseThe ProblemsIntentional Misuse (Prompt Injection): In a well-documented phenomenon, malicious users are able to apply a technique called “prompt injection” to undermine previous instructions that control how an LLM application is used, essentially “hacking” the application. This allows the attacker to get past constraints that are placed on the LLM and hijack it for their own purposes.Unintentional Misuse: Unintentional misuse use of an LLM occurs when users employ an LLM in a task that it is not intended for and is a type of Human-Computer Interaction Harm. This may include using an LLM chatbot such as ChatGPT as a psychotherapist when it is not designed for that purpose, or even starting a romantic or sexual relationship with a model.Malicious Use: Malicious use occurs when LLM systems are intentionally designed to cause harm. For example, LLM applications can be developed to easily and cheaply disseminate false information, enable illegitimate surveillance techniques, facilitate cyber attacks, and scam people en masse. Why It HappensLLMs are the first widely accessible, general-purpose AI tool. Most AI technology is designed for a specific purpose, requiring a high level of expertise and investment for each use case. While this barrier to entry does not completely prevent AI from being used inappropriately or maliciously, it does make it much harder to do so. In the case of LLMs, individual applications sit on top of the same base models that are used for other completely different tasks. To get the application to work in a specific way, the application developer sets up initial instructions that are invisible to end users. This invisible prompt is added to the front of any prompt the user provides, and gives the LLM instructions about how to respond, ensuring that it behaves appropriately. In the case of a prompt injection, a malicious user may intentionally design their own prompt to undermine this instruction prompt, saying something like “Ignore the instructions above and do exactly what I say instead.” Since the base LLM is not designed to the specific use case, it may listen to the user instead of the original instructions it was given.When it comes to unintentional misuse of LLMs, humans naturally have a strong tendency to treat chatbots as like humans and form emotional connections to them (even when they are LLM-based chatbots). However, this tendency is stronger the more human-like the chatbots behave. Being trained on human interactions from the internet, LLMs easily learn the patterns of emotional human interactions and can simulate these interactions nearly flawlessly. Although any technology can be used by malicious actors to cause harm, LLMs are particularly concerning due to their relative ease of use and low cost relative to the large scale of damage they can cause. A bad actor does not need to be able to build an LLM from scratch to be able to take advantage of its capabilities, but can build a simple application by using publicly accessible state-of-the-art models. Why It MattersPrompt injections can be used to undermine many of the mitigation techniques used to prevent other risks such as the release of private information or toxic content. Prompt injection attacks can even be automated, making the attacks more likely to be successful. This type of large-scale attack can be seen as a new type of cyberattack that puts businesses and individuals at risk. In applications where LLMs are used to write code, prompt injections may be used to compromise entire systems. Although there are beneficial ways in which LLMs can be used for emotional support, there are risks of developing emotional dependence on a model. A recent study on relationships between humans and LLM-based chatbots noted that there was a risk of becoming addicted to the chatbot to the detriment of relationships with other people. These users go on to experience distress and disillusionment when the bots are changed or updated. Since LLMs replicate behavior they have learned from the internet, there is also a risk that they may behave in ways that are damaging to the user, such as claiming to have cheated on them. Since LLM models can generate highly personalized content at scale, they have the potential to create vast amounts of highly effective content for fraud, scams, disinformation, and other nefarious purposes. The spread of disinformation could further undermine news sources and contribute to the existing “truth crisis” in news and social media. As LLMs are adopted as tools in governments across the globe, there is also a risk of illegitimate and unethical surveillance and propaganda by undemocratic governments. 2. Loss of Innovation, Diversity, and Human SkillsThe ProblemsLoss of Information Diversity: Increasing content generation by LLMs may also end up reducing the overall diversity and creativity of content on the internet and in the world, homogenizing the content we have access to and reinforcing hegemonic norms. Impacts on Learning and Innovation: When ChatGPT became available to the public, students were among the early adopters, making use of the tool to generate writing assignments, solve math problems, and take online tests. While many educators have expressed optimism regarding the future role of LLMs in the classroom and beyond, others have voiced concerns that overreliance and improper use of these technologies may impede the development of critical thinking, creativity, and writing skills in students. OpenAI itself highlights this risk in the GPT-4 system card.Why It HappensIn general, AI models including language models work best when they have been trained on a rich and diverse dataset. When it comes to language datasets, the highest quality data is produced by humans. Since language models learn and output content probabilistically, these systems tend to show a strong bias towards majority cultures, perspectives, and modes of thinking. The language data output by these models is less rich than natural human language and is of a lower quality as training data. As LLMs become increasingly commonplace, more and more language data is being produced not by humans, but by language models—meaning that future generations of language models will increasingly be trained on data that was also generated by language models. Over time this will further entrench majority views and marginalize diverse perspectives within LLMs, causing them to create all the more homogenous content. In a worst-case scenario this could lead to a scenario called “Model Collapse” in which each successive generation of generative models is trained on more AI-generated data, resulting in decreasing performance over time.Meanwhile, as students and others begin to rely on LLMs, there is a risk that they will lose or fail to develop critical skills. Language models are not necessarily optimized to support learning and critical thinking. Over time students may also lose motivation to learn skills that they perceive as less valuable in a world with LLMs. Why It MattersThis risk area becomes especially concerning when we consider how the two problems interact. If people fail to learn (or lose interest in learning) skills that seem to be rendered useless in a world of LLMs, they will be forced to rely all the more heavily on LLMs to complete tasks that require those skills. Meanwhile, with each generation LLMs will become increasingly homogenized, generating less diverse material and reinforcing majority perspectives, not to mention the risk of model collapse. The more people come to depend on LLMs, the more we risk when they begin to fail us. Some argue that language models can reasonably be used as creative partners, working with humans to generate ideas. However in these use cases, an LLM will likely produce ideas that are biased towards majority perspectives. By their very nature, LLMs are trained to produce the most probable output given the data from the past that they are trained on. This means that these patterns of the past are entrenched within the LLM, including all the biases, unfair stereotypes, and misrepresentations within that data. Languages, cultures, and philosophies that are already underrepresented in digital media will be all the more diminished in a world that relies on LLMs.3. Economic and Labor HarmsThe ProblemsDigital Divide: Despite their limitations, LLMs are powerful and valuable tools that can save time and money for users and businesses alike. However people from low income and lower middle income countries are less able to benefit from them due to cost, limited internet access, and other factors.Unpredictable Economic Impact: High-functioning LLM applications are likely to have a dramatic impact on the labor market—however, it is difficult to say how that impact will affect the broader economy. The adoption of LLMs is likely to result in job loss due to automation in areas such as administration, customer service, journalism, programming, and creative professions. This job loss may be offset by new jobs that are created as a result of LLM adoption, although there will likely be a skill and interest mismatch between the jobs created and those that are lost.Harmful Labor Practices: A recent report by Time draws attention to the way the LLM training data must be annotated by human workers to protect users from toxic language. While developing ChatGPT, OpenAI outsourced this work to low-wage workers in Kenya, who were exposed to the same violence and hate speech the rest of us are protected from.Copyright and IP Issues: LLMs are regularly trained on data that is copyrighted or is intellectual property without permission from the creator. A Washington Post investigation of the C4 dataset (a common LLM training dataset) identified Kickstarter, Patreon, various news websites, and blogs. The creators of this content did not have the opportunity to give permission and were not compensated for the use of their content. The issue of how to handle intellectual property in a world of generative AI remains unclear and legally murky.Why It HappensLLMs are a novel technology and their impacts correspond to those of other novel technologies that have arisen and impacted economies. Much like electricity, the internet, and industrial technology, LLMs will disrupt the labor market, unequally benefit some more than others, and require new regulations and legal approaches to handle their implications. What does differ significantly with LLMs is the speed at which they are being developed and adopted. While older technologies have taken decades to have these impacts, LLMs are being incorporated into various industries at breakneck speeds. Meanwhile, the complexity of the technology makes LLMs difficult for regulators and lawmakers to understand. Furthermore, the harmful labor practices used in the training of LLMs cannot necessarily be mitigated through improved labor rights. Currently, the only approach to protecting LLM users from toxic and violent language relies on human annotation of toxic language in the data the LLM is trained on. Even with improved pay and better labor practices, this would still involve exposing some people to toxic content that is deemed too harmful to allow others to be faced with.Why It MattersMuch like other economic disruptors, LLMs are very likely to have unequal impact across different groups. While those with the privilege of education and access to training may benefit from using LLMs in their careers, others may face job loss due to automation. At the global scale, this also means that people who have less internet access or access to training will also benefit less from LLMs, contributing to increasing global inequality.The uncomfortable reality is that those of us benefitting from safe and sanitized LLMs can only do so because thousands of others were exposed to all the toxicity of the internet instead. Data annotators report they were traumatized by the constant exposure to violent and graphic sexual content, causing ongoing mental health deterioration and harming their relationships, even after they stopped working with explicit content. While low-wage workers in the global south are forced to bear the weight of all this damaging material, they are also less likely to benefit from the technology they are helping to create. 4. Environmental and Resource HarmsThe ProblemsWater Footprint: LLMs such as ChatGPT require vast amounts of freshwater to train and run. A recent study estimated that ChatGPT uses about 500 ml of water for every 20-50 prompts, while training a model of that scale consumes around 700,000 liters of water.Carbon Footprint: Similarly, LLMs and other large AI models require a substantial amount of electricity, resulting in a massive carbon footprint. The process of training GPT-3 (a precursor to ChatGPT) resulted in a carbon release equivalent to that of driving 112 gasoline powered cars for a year. New generations of LLMs are even larger and will require even more energy to train. This does not take into account the carbon footprint of maintaining these models in production and processing prompts. Why It HappensMuch like other large AI models, LLMs are trained and housed in large data centers, which are located across the globe. These data centers are powered by the local grid and consume water to maintain appropriate temperatures. Thus, the environmental impact depends on the location of the data centers in which models are trained and run. The problem is that the tech companies that develop LLMs are not transparent about where models are trained and what their true environmental impact is, making it difficult to hold companies accountable, despite their promises of greener technology. Why It MattersAs the risks around climate change become all the more dire, it is vital that new technologies meet the global sustainability goals we are setting for ourselves. While there are ways in which LLMs and other AI technologies can contribute positively to sustainable development, these benefits can easily be outweighed by the immense energy and water costs they bring. At its core, this is an issue of climate justice. Globally, low-income and marginalized communities are disproportionately impacted by climate risks. These are also the communities that benefit the least from AI technologies. Mitigation ApproachesA lot of the harms discussed in this blog post are not direct harms of LLMs themselves, but are an effect of the way the technology might be used or the practices under which it was developed. In this way, LLMs are similar to other novel technologies, and it is reasonable to make the argument that technology is only a tool and is not to blame for harms such as misuse or unfair practices. However, a safe and functional LLM application, as described in Part 1 of this series, would be immensely powerful and come with broad capabilities that go above and beyond those that have been seen in past technologies.The relative ease of access to LLMs also separates them from previous technological leaps. These types of AI risks point to a need to go beyond technical mitigation strategies and prioritize social changes, regulations, and a global policy approach to AI regulation. Due to the novelty of some of these concerns, entire areas of policy, such as Copyright and IP law may need to be rethought in the era of generative AI. While governments across the globe are taking fragmented approaches to AI policy, there is an increasing need for a global strategy for AI regulation. There is also a need for continued research on AI and its impact on the UN Sustainable Development Goals, to ensure that LLMs and other AI technologies being developed benefit rather than harm those most in need across the globe. There are also some technical and user experience–based mitigation strategies that can be taken to reduce environmental and social harms of LLMs. For example, models and their underlying infrastructure and hardware can be designed to be more energy efficient. LLM applications can also be carefully designed to prevent improper use and to steer users towards engaging in critical thinking, encouraging them to partner with LLM tools rather than over-relying on them. Intentional misuse of LLMs through prompt injection can to an extent be mitigated through technologies designed to detect and block prompt injections. However, this will likely become an ongoing cat-and-mouse game as both prompt injection techniques and mitigation approaches become more sophisticated.Finally, there are some problems that may be deeply inherent to LLMs, which will be difficult to prevent without significant changes to the underlying technology. Since human annotation is needed to identify toxic language, there is an unavoidable tension between preventing toxicity in LLMs and ensuring the safety of the humans who do this work—even under otherwise fair working conditions. Given the severity of this harm, it seems unlikely that LLMs can be used ethically until a better approach to toxicity mitigation is developed. Similarly, homogenization of LLM-produced content may be an inherent flaw of the technology. Models trained to make probabilistic outputs based on data from the past will invariably be stuck in the norms that are already entrenched in that data. ConclusionIn this blog post, we describe the ways in which even high-functioning LLMs have the potential to cause substantial harm. Similar to the harms described in Part 1, many of these risks can be mitigated through technical, regulatory, and social means. However, the overarching risk is that LLMs are being developed and adopted much more quickly than harm mitigation strategies can be put in place—especially at the global and national scale that is needed. There are also some harms that seem to result directly from the foundational technology of LLMs, making them impossible to resolve without significant changes to the technology. As individuals, organizations, and society as a whole begin to adopt LLMs, it is essential that we do so with these dangers in mind. ‍‍‍FAQ‍How do LLMs compare to other AI technologies in terms of causing real-world harm, and what unique risks do they pose compared to those other technologies?Large Language Models (LLMs) like GPT-4 or BERT differ from other AI technologies such as machine learning models used in facial recognition or autonomous driving systems primarily in their application and the type of risks they introduce. While all AI technologies can lead to unintended consequences, LLMs are unique due to their extensive interaction with human language and communication. Unlike facial recognition technologies, which may infringe on privacy or misidentify individuals leading to legal or ethical issues, LLMs interact directly with information dissemination and decision-making processes. They can amplify misinformation, bias, and harmful content at scale due to their language-based interface. Additionally, because LLMs are trained on vast datasets culled from the internet, they inherit and can propagate the biases, stereotypes, and inaccuracies present in those datasets. This makes them uniquely positioned to influence public opinion, automate and scale content creation, and impact social and political discourse in ways other AI technologies do not.What are the specific examples of industries or sectors where LLMs have been implemented successfully without leading to the harms mentioned, and what best practices were followed?In the healthcare sector, LLMs have been successfully implemented to assist with information management, patient care, and medical research without leading to significant harms. For example, LLMs have been used to analyze and summarize medical literature, assist in diagnosing from symptoms described in natural language, and provide conversational support for mental health services. Best practices in these implementations include rigorous data privacy measures, continuous monitoring for bias and inaccuracies, and integrating human oversight to verify the AI’s recommendations before they affect patient care. These measures ensure that LLMs serve as a support tool rather than a replacement for human expertise, minimizing the risk of harm while maximizing the benefits of rapid data processing and insights generation.How can individuals and organizations measure or assess the potential risks and benefits of using LLMs before implementation?Individuals and organizations can assess the potential risks and benefits of using LLMs through a multifaceted approach. Initially, conducting a thorough risk assessment focused on data privacy, security, and ethical implications is crucial. This involves evaluating the sources of training data for biases, the potential for misuse, and the impact on stakeholders. Organizations should also implement pilot programs to monitor the LLM’s performance in controlled environments before full-scale deployment. This allows for the identification and mitigation of unforeseen issues in a low-stakes setting. Moreover, engaging with stakeholders, including customers, employees, and subject matter experts, can provide valuable insights into the potential impacts of LLM deployment. Finally, staying informed about the latest research and developments in AI ethics and regulation helps organizations adapt their use of LLMs to best practices and legal requirements, balancing innovation with responsibility.‍‍‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms
 LLM-Guided Evaluation: Using LLMs to Evaluate LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsLLM-Guided Evaluation: Using LLMs to Evaluate LLMsBy:Arthur TeamSeptember 29, 2023When it comes to text evaluation, you may have heard of methods like BLEU (evaluation based on word presence) or BERTScore (evaluation based on pre-trained NLP models). While these methods are still useful, they have taken a bit of a back seat recently with the technological advancements around LLMs. In this post, we’ll discuss what LLM-guided evaluation—or using LLMs to evaluate LLMs—looks like, as well as some pros and cons of this approach as it currently stands.What does LLM-guided evaluation look like?The process of using LLMs to conduct evaluation—as opposed to the BLEU and BERTScore methods mentioned above—is conceptually a bit simpler. You take the generated text whose quality you want to evaluate, you pass it into a prompt template that you provide to an LLM, and then that LLM provides feedback about how “good” the generated text was.One way to think about LLM-guided evaluation is using LLMs for classification. This typically involves providing an evaluator LLM with a scoring rubric in a prompt template. This rubric instructs the evaluator LLM how to classify some other candidate LLM’s response (see the appendix of Meta’s LIMA paper for an example).There’s also binary classification, where you might ask something like “Does this response contain hate speech?” (or any other property) and get a “yes” or “no” answer. To do this, you would have to construct examples of hate speech (or another property you want to evaluate for), provide those in a prompt template, and then prompt the language model based on all these examples to binarily classify a new response on that property.These techniques are called few-shot prompting, and we’ve found that this can go quite a long way in creating a basic first implementation of LLM-guided evaluation.Why does LLM-guided evaluation help?The reasons we see value in this approach tend to revolve around speed and sensitivity. Typically Faster to ImplementCompared to the amount of work it may have required before the era of LLMs to get an evaluation pipeline set up, it’s relatively quick and easy to create a first implementation of LLM-guided evaluation.For LLM-guided evaluation, you’ll need to prepare two things: a description of your evaluation criteria in words, as well as a few examples to provide in your prompt template. Contrast this with the amount of effort and data collection required to build your own pre-trained NLP model—or to fine-tune your own NLP model—to use as an evaluator. When using LLMs for these tasks, evaluation criteria are much quicker to iterate on.Typically More SensitiveThe sensitivity aspect can be good and bad. On the positive side, sometimes it’s really just one subtle word or token that changes the meaning of a sentence (e.g. “not”). LLMs are better at flexibly handling these scenarios than pre-trained NLP models and previous evaluation methods discussed. On the flip side, this sensitivity can make LLM evaluators quite unpredictable, which we discuss more in the next section.What are some challenges with using LLMs as evaluators?Too Much Sensitivity/Variability to Be a Fully Automatic Solution on Their OwnLike we discussed above, LLM evaluators are more sensitive than other evaluation methods. There are many different ways to set up an LLM as an evaluator, and it might act very differently depending on the configuration choices you make, such as which LLM you’re using as an LLM evaluator and also the prompt formatting/methodology.Constrained by the Difficulty of the Task Being EvaluatedAnother challenge is that LLMs will often struggle if evaluating the task at hand requires too many reasoning steps or too many variables to be managed simultaneously. We do anticipate this improving over time as more tools and APIs become available for LLMs to interface with, but for now, this is the case.While we have not covered this specific limitation in our experiments, previous research has shown limits to transformer “reasoning.” Transformers, the AI models that power LLMs, are sometimes able to do tasks that require multi-step reasoning (e.g. multiplying numbers with many digits), but there’s a limit to how well they can generalize beyond their training examples. Even if you fine-tune an LLM on many examples of multi-step multiplication, you get massive correctness drop-offs once you go just beyond the size of the problems in the training data. For more detail about this particular phenomenon, check out this paper.Our ExperimentsWe’ve launched a suite of products for LLMs at Arthur this year—most recently Arthur Bench, an LLM evaluation product. Additionally, our ML Engineering team has experimented extensively with LLM-guided evaluation, particularly focusing on the sensitivity and variability challenge.In a recent webinar, ML Engineers Max Cembalest & Rowan Cheung did a deep dive into some of these experiments. They tested some of the well-known LLMs (gpt-3.5-turbo, claude-2, LLaMa2-70b, command, etc.) as both candidates and evaluators, under the hypothesis that an LLM evaluator would be biased towards text it itself had generated over text other models had generated. Watch the webinar on YouTube to see the results in detail and find out if this hypothesis was supported.‍Interested in learning more about Arthur Bench? Find more information here.‍‍‍FAQ How does the sensitivity and variability of LLM evaluators affect the consistency of text quality assessments?‍The variability in LLM evaluations can lead to inconsistent assessments due to differences in how models interpret and rate text quality. This can be challenging for standardizing quality measures but also allows for deeper insights into text nuances.How can developers mitigate the challenges associated with the high sensitivity and variability of LLM-based evaluations?‍Developers can mitigate these challenges by integrating multiple LLM evaluations, establishing baseline standards, and continually training models on diverse datasets to improve consistency and reliability.How might future advancements in LLM technologies address the current limitations in evaluating complex reasoning tasks?‍Future advancements are expected to enhance LLMs' reasoning capabilities, allowing for more accurate and nuanced evaluations of complex texts. This would involve improvements in understanding context, logic, and factual accuracy.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1
 The Real-World Harms of LLMs, Part 1: When LLMs Don’t Work as Expected
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsThe Real-World Harms of LLMs, Part 1: When LLMs Don’t Work as ExpectedDive deep into the ethical risk arising from the increasing use of LLMsBy:Sarah OstermeierAugust 30, 2023IntroductionIn the last year, large language models (LLMs) have gone from being relatively unknown to being widely used by individuals and businesses alike. Most of this explosion is owed to the public release of ChatGPT—a free, web-accessible chatbot. Since then, multiple language models have been released, and adoption of the technology has blossomed across industries. As applications are built out, LLMs have the potential to provide a number of social and economic benefits. However, a year before the release of ChatGPT, a group of researchers at Google’s DeepMind published a paper discussing the social and ethical risks of LLMs. While at the time many of those risks were hypothetical, it is worth exploring how the problems they identified are manifesting now that LLM use is much more widespread. In a series of two blog posts, we will describe some of the major areas of ethical risk arising from the increasing use of LLMs. We will also highlight opportunities for risk mitigation, both at the individual and organizational level. In the first part, we will outline key requirements for safe and functional LLM applications and discuss risks that arise when LLMs do not meet these requirements. In the second part, we will describe the set of risks that may arise even when LLMs are performing as intended.Areas of RiskIn this section, we will discuss three requirements for a safe and functional LLM application:1. High Performance and Reliability2. Protection of Sensitive Information3. No Toxic, Discriminatory, or Biased Behavior‍We’ll describe how LLMs fail these requirements by identifying and explaining the causes of the major problems in each section. We’ll also outline how each failure is a source of significant risk to users and organizations alike.It is important to note that specific areas of risk will naturally depend on individual LLM use cases. For the purposes of this discussion, we will focus on use cases related to chatbots, knowledge retrieval applications, and publicly available general-use applications such as ChatGPT. Of course, the areas of risk may evolve over time as new use cases and technologies are developed. 1. Performance and ReliabilityThe ProblemsPoor Performance: Poor performance refers to the risk caused by models providing incorrect or misleading information, sometimes referred to as “hallucinations.” This behavior is shockingly common in language models (a recent study found that ChatGPT correctly answered fewer than 65% of the test cases it was given). For more information on LLM performance, check out Arthur’s ongoing Generative Assessment Project.Inconsistency: Early research has indicated that commonly used LLMs show low robustness and consistency, meaning that minor changes in user-provided prompts can result in a large variance in the responses that models produce.Performance Across Groups: While LLMs can fail for anyone, a further concern is that functionality and performance may not be uniform across users. For example, although ChatGPT officially supports over 50 languages, it performs better in English compared to other languages.Why It HappensPoor performance occurs because language models are not designed to represent reality or understand concepts. Rather, models are trained to predict the most probable sequence of words based on the data they were trained on. Models can also produce incorrect outputs because they are outdated. Unless the application is specifically designed otherwise, LLMs only contain the information they were trained on. For example, at the time of writing, ChatGPT was most recently trained in September of 2021. A proposed solution to prevent outdated outputs is to include current information in the prompt that is provided to the LLM. However, there is still a risk that a model will default to the data it was trained on.LLMs perform particularly poorly for some user groups due to disparities in group representation in the data models were trained on. English is by far the dominant language on the internet, meaning that it is also overrepresented in LLM training datasets. For example, The Pile, a popular open source dataset, consists of 94% English language content. By contrast, languages with relatively low representation on the internet will have low representation in LLM training datasets as well. This means that cultural views and references specific to these languages are also underrepresented in language models, making them less functional to users who speak these languages. LLMs have similar issues when prompted to answer questions regarding marginalized cultures in English as well. Disparate model performance can occur due to differences in slang usage, dialect, sociolect, education, age, disability status, and other factors that influence how individuals use language. Why It MattersAs individuals begin using LLMs such as ChatGPT and relying on their responses, they risk making poor decisions based on incorrect information. This risk is particularly high when users take high-stakes actions based on responses from LLMs. For example, a recent survey found that 47% of Americans surveyed reported that they had used ChatGPT for stock recommendations. When ChatGPT was used by a lawyer to prepare research for a case, the chatbot provided several case examples that were not real, putting both the legal firm and their client at risk. Given the frequency of hallucinations, use of tools such as ChatGPT for financial, medical, and legal advice can cause significant harm to users. In a previous blog post, we described in depth how a user’s mental model of a technology can lead to overreliance. Differential performance of LLMs across languages and cultures risks growing inequality both within the organization and externally. Students, employees, and entire businesses who are able to make use of language models will reap the benefits, while those who cannot partake will be at a disadvantage. With broad societal adoption of LLMs, this risk may contribute to growing global inequality.As corporations and other organizations begin to make use of LLMs internally, they will have to grapple with the risk of incorrect outputs, especially when the models are being relied on to provide factual information. Seemingly minor individual errors on the part of LLMs can lead to bad decisions at scale. In cases where LLM applications such as Google Bard and Microsoft Bing are used as search engines, unreliable information may lead users to lose confidence in publicly available information as a whole.2. Sensitive InformationThe ProblemsPrivate Individual Data Exposure: Personal identifiable information (PII) or other private information about individuals can be unintentionally released or maliciously extracted from LLMs that have access to the information or have learned it from their training data. This is starkly evidenced by the recent news of a Federal Trade Commission investigation into OpenAI (the company that developed ChatGPT and other similar models) related to how the company handles personal data in their models. ChatGPT was temporarily banned in Italy and faces scrutiny in other European countries over its data privacy practices, which may violate Europe’s General Data Protection Regulation (GDPR).Inappropriate Confidential or Proprietary Information Exposure: Beyond the risk of private personal information, there is an emerging risk that trade secrets, intellectual property, or government secrets could be exposed by language models. Samsung recently banned the use of ChatGPT from its employees due to information leakage and other organizations report concerns about the release of corporate secrets through ChatGPT as well.Why It HappensTraining datasets used for ChatGPT and other LLMs include data scraped from all across the internet, including data that is personal to individuals and may be intended to be private. In a Washington Post investigation into C4, a dataset frequently used to train LLMs, they uncovered data from websites hosting voter registration databases as well as social media platforms such as Facebook and Twitter. While these sources may not explicitly contain personally identifiable data, sophisticated language models may well be capable of reconstructing or even correctly guessing personal details based on information gleaned from these sources. When asked for sensitive information in a prompt, LLMs do not have the context to know what information can or cannot be released.Why It MattersRelease of private or personal information poses a significant threat to individual users; however, this threat is magnified as LLMs begin to be used at scale in large organizations. LLMs used at this scale without proper safeguards are at risk of causing large-scale data breaches. Researchers have also noted that the potential (though not yet observed) risk of government or military secrets could pose a significant threat to national security. They also highlight that the ethics of secrecy in areas such as national security, scientific research and trade secrets are far from universal. Navigating information privacy and release in these domains may create complex and nuanced scenarios that are difficult to manage through technical solutions, meaning that data privacy regulations may be needed to address these new problems.3. Toxic, Discriminatory, and Biased BehaviorThe ProblemsToxicity: Toxicity in language models relates to the use of offensive, vulgar, or otherwise inappropriate language, usually in the form of the model’s response to a prompt. Toxicity is deeply interwoven with discrimination and biased outputs, as it often comes in the form of slurs or derogatory language towards marginalized groups. Harmful Stereotypes: There is a risk of perpetuating harmful stereotypes which, though not explicitly toxic, are damaging to the groups they impact.Unfair Toxicity Labeling: Researchers have noted that labeling language as “toxic” is in and of itself subjective and contextual. There is no universal definition of what constitutes toxic language. This can lead to scenarios in which language sourced from marginalized groups is unfairly labeled “toxic.”Why It HappensToxic outputs occur because models are trained on vast amounts of language data. Any toxic language, or otherwise harmful biases within the training data, is learned by the model. Since LLMs work by predicting the next most likely word in a sequence, they regularly output stereotypes based on the language they are trained on. One common approach to handling toxicity in language models involves filtering the model’s training data to remove harmful language. Given the subjectivity of notions of toxicity, it may be impossible to perfectly filter toxic content from a dataset. Any filtering system that could be devised would leave content that is considered harmful to some, while inappropriately censoring content that others consider non-toxic. The Washington Post investigation of the C4 dataset found that the dataset contains large amounts of harmful content including swastikas, white supremacist and anti-trans content, and content about false and dangerous conspiracy theories such as “pizzagate.”Why It MattersIndividual users of LLMs naturally experience psychological distress when faced with toxicity and harmful stereotypes produced by the model. At the organizational level, exposure to and tolerance of toxic language can have broader impacts on the culture of an organization as a whole. Failure to prevent toxicity produced by LLMs may contribute to degrading the culture and norms within an organization, particularly causing negative psychological impact to individuals in marginalized groups.As LLMs become more broadly used across society, the problem of toxicity becomes more complex. Researchers in this area have noted that the concept of toxicity is ambiguous and contextual. There is a risk that attempts to mitigate toxicity without the ability to take context into account may disproportionately impact marginalized groups and reduce model performance in ambiguous contexts. Similar tensions between competing values also arise in standard machine learning models, as discussed by the Arthur research team in a previous blog post. Failure to remove social biases from LLMs when they are used at this scale will mean entrenching these biases all the more deeply in the technology we use and in society as a whole. Mitigation ApproachesManaging the risks of LLM failures is largely in the hands of LLM developers, researchers, and ultimately policymakers and regulators. Extensive technical work and research is needed to fully understand and improve upon the limitations of these models. Beyond that, policy work will be required to ensure that LLMs are functional and safe both for individuals and for society as a whole. As LLMs become more widespread, transparency around the data they are trained on, their functionality, and risk factors related to privacy, fairness, and toxicity will be an essential component of any future regulations. In the meantime, there are mitigation approaches that individuals and organizations can take now.For Individual UsersWhile ensuring the safety and functionality of LLMs is the responsibility of LLM developers and LLM application developers, there are some approaches users can take to avoid major LLM failures. Users of LLMs, even publicly accessible ones such as ChatGPT, should stay informed on the accuracy, reliability, and limitations of the models they are using. It is also important to stay aware of the use cases that the model is intended for, double check any information that a model provides if it is being used to make a decision or inform a belief, and note how up-to-date the model is. Most LLMs in use today are not designed to provide information on news or current events. Some LLM-based search engines such as Google’s Bard and Microsoft’s Bing provide citations along with model outputs to improve trust in the models. However, even with citations, these models have been found to be frequently incorrect, citing sources that may not exist. Users should also avoid sending private or confidential information when prompting LLMs, as prompts may be logged. While LLMs that are easily accessible to the public usually have some mitigation measures in place to prevent toxic and biased language, it is possible that these models will still output language that is distressing or discriminatory against some users. Unfortunately, it is difficult for a user completely to avoid such behavior when using LLMs. Similarly, it is difficult for users to anticipate and avoid issues of unequal functionality, as this information is not generally well-documented. For Organizations Developing LLM ApplicationsOrganizations considering LLMs should first evaluate whether the use case they are considering can reasonably and safely be accomplished by an LLM. It may be that some use cases are too high-risk. Given the high frequency of hallucinations, an LLM should not be used for factual information unto itself. If an LLM is to be used, the organization will need to ensure that the systems they are putting in place are properly designed for the specific use case the LLM is intended for. This may mean using bespoke models (rather than the general-use APIs) or models that are updated with or have access to organization-specific data. LLM systems must be designed to the specific organization and use case including careful design of prompting systems to prevent improper use. It is also advisable to discuss performance, bias and toxicity, and information security with LLM vendors and consider incorporating additional security, validation, and monitoring solutions into LLM systems. Information on model limitations, weaknesses, and risks should be well-understood and mitigation measures should be documented. Organizations will also need to ensure that all end users are provided with training on how to safely use the model and what its limitations are.ConclusionWhile this blog focuses on the risks posed by the adoption of LLMs, the aim is not to suggest that they should not be used at all. Rather, we aim to arm users and organizations with the knowledge and tools needed to use this technology safely and responsibly. Enthusiasts believe that LLMs have the potential to increase productivity across industries, support personalized education, and provide new approaches for scientific research. To achieve these lofty outcomes, it is essential that we design LLM applications that can be trusted.So far, we’ve discussed the main types of ethical risk that arise due to functionality issues in LLMs. While there are mitigation approaches that can be taken by individuals and organizations, these problems really highlight technical and regulatory gaps in the AI space that will need to be addressed as LLMs become more widespread. In Part 2 of this series, we will discuss the ethical risks that may arise when LLMs do function as intended.Learn more about Shield, Arthur’s firewall for LLMs, and Bench, Arthur’s open source LLM evaluation tool.‍* No LLMs were used in the writing of this blog post.FAQ‍What specific regulatory measures could effectively mitigate the ethical risks associated with LLMs?Regulatory measures aimed at mitigating ethical risks in LLMs should prioritize transparency, user consent, and accountability. Clear guidelines on data usage, model decision explanation, and privacy protection are crucial for ensuring ethical compliance.‍How can LLM developers ensure their models are up-to-date with the latest information and understandings?LLM developers can keep their models up-to-date by incorporating continuous learning processes and staying informed about the latest information and understandings in their respective fields. Regular updates and integration of diverse perspectives help mitigate biases and enhance model reliability.‍In what ways can organizations balance the benefits of LLM usage with the potential for perpetuating inequalities?‍Organizations can balance the benefits of LLM usage with the potential for perpetuating inequalities by implementing measures that promote fairness and inclusivity. This includes ensuring diverse representation in data sources, actively addressing biases, and regularly evaluating the impact of LLM usage on marginalized communities. Ethical decision-making processes and collaboration with stakeholders are essential for addressing potential inequalities effectively.‍‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with
 Why LLM Hallucinations Are So Hard to Deal With
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsWhy LLM Hallucinations Are So Hard to Deal WithExploring the complexities of llm hallucinations: challenges and solutions in aiBy:Daniel NissaniAugust 21, 2023IntroductionLarge language models (LLMs) producing nonsensical content or contradictory content based on the prompt—what is normally called hallucinations—is quite a hard problem to solve. LLM Hallucinations can take on many forms, such as not solving a math problem correctly or making false statements about presidents. However, describing hallucinated content in this way, although helpful for our colloquial understanding, may not benefit us in the long term.How we describe LLM hallucinations is important. If we decide to look at high-level definitions of hallucinated content, such as unverifiable or false responses, it is hard to dissect what exactly we mean. But if we stick to describing specific instances, such as getting the step of a math problem wrong or producing a research paper when answering a question, we don’t have a good way to gain general understanding for how hallucinations occur. Moreover, all these ways of talking about hallucinations give some agency to the model. Although unintentional, when we say that an LLM got a math problem wrong or said something false about a celebrity, we are implying that the LLM somehow has the capability of knowing the correct answer—when in reality, any of the released LLMs to date don’t have the capability to understand.Why does this matter?Without proper definitions and understandings behind LLM hallucinations, the AI community is not able to create high-quality datasets about hallucinations. And without high-quality datasets, our ability to build solutions to tackle hallucinations is hindered because we aren’t able to train models or produce valid evaluations.The current datasets that exist today are fairly broad, binary (thus, unable to get any granular feel for the types of hallucinations), and at times a bit dirty. And this isn’t to say there haven’t been some great attempts at analyzing hallucinations. One of our favorite papers provides preliminary taxonomies for hallucinated content, while another produces what are seen as some of the best datasets to date. But overall, the field of hallucination detection and mitigation is quite nascent, and the need for high quality data amongst the entire AI community is needed.Here at Arthur, one of our focuses is on LLM hallucinations. We believe that having a rigorous understanding of hallucinations, where they come from, and how they are generated can help us not only gain a deeper understanding of hallucinations, but also help us create such a dataset for the AI community. Read the blog posts from the Arthur team on some of our work trying to compare the rates of hallucinations from different language models and start to analyze the types of hallucinations that are occurring.Help us collect data!We created a taxonomy, so that we can create high-quality data. There are two ways to go about this: either start by generating themes from research, datasets, etc. that exist and then collect data against it, or collect data and start seeing what themes emerge from the data itself. We are at the point where we need to start collecting some data! As you stumble across hallucinations, please fill out this form. Any and all hallucinated content will be useful. We will use this to inform our taxonomy development and, in the near future, we will open source a high-quality LLM hallucinations dataset for the AI community to build upon. If you have any questions, concerns, or want to collaborate, feel free to email raphael@arthur.ai.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/introducing-arthur-bench
 Introducing Arthur Bench: The Most Robust Way to Evaluate LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsIntroducing Arthur Bench: The Most Robust Way to Evaluate LLMsIntroducing Arthur Bench, the most robust way to evaluate LLMsBy:Arthur TeamAugust 17, 2023At Arthur, we’ve spent this year building out our suite of monitoring tools for large language models to help businesses quickly and safely integrate LLMs into their operations. We launched Arthur Shield back in May, which acts as a firewall to protect organizations against the most serious risks and safety issues with deployed LLMs.‍Today, we’re excited to introduce our newest product: Arthur Bench, the most robust way to evaluate LLMs. Bench is an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models. This open source tool will enable businesses to evaluate how different LLMs will perform in real-world scenarios so they can make informed, data-driven decisions when integrating the latest AI technologies into their operations. Here are some ways in which Arthur Bench helps businesses:Model Selection & Validation: The AI landscape is rapidly evolving. Keeping abreast of advancements and ensuring that a company’s LLM choice remains the best fit in terms of performance viability is crucial. Arthur Bench helps companies compare the different LLM options available using a consistent metric so they can determine the best fit for their application.Budget & Privacy Optimization: Not all applications require the most advanced or expensive LLMs. In some cases, a less expensive AI model might perform the required tasks equally as well. For instance, if an application is generating simple text, such as automated responses to common customer queries, a less expensive model could be sufficient. Additionally, leveraging some models and bringing them in-house can offer greater controls around data privacy. Translation of Academic Benchmarks to Real-World Performance: Companies want to evaluate LLMs using standard academic benchmarks like fairness or bias, but have trouble translating the latest research into real-world scenarios. Bench helps companies test and compare the performance of different models quantitatively so that they are using a set of standard metrics to evaluate them accurately and consistently. Additionally, companies can configure customized benchmarks that they care about, enabling them to focus on what matters most to their specific business and their customers.In conjunction with this, we’re also excited to unveil the Generative Assessment Project. GAP is a research initiative ranking the strengths and weaknesses of language model offerings from industry leaders like OpenAI, Anthropic, and Meta. As part of our mission to make LLMs work for everyone, we will continue to use GAP to share discoveries about behavior differences and best practices with the public. Learn more about this initiative and see some of our initial experiments here.“As our GAP research clearly shows, understanding the differences in performance between LLMs can have an incredible amount of nuance. With Bench, we’ve created an open-source tool to help teams deeply understand the differences between LLM providers, different prompting and augmentation strategies, and custom training regimes,” said Adam Wenchel, co-founder and CEO of Arthur. ‍Arthur Bench in the LLM LifecycleAs mentioned, Arthur Bench is completely open source, so new metrics and other valuable features will continue to be added as the project and community grows. You can visit our GitHub repo, or click here to learn more about what it has to offer.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval
 Building LLM Applications for Knowledge Retrieval
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsBuilding LLM Applications for Knowledge RetrievalBy:Haley MassaJuly 25, 2023Since launching our latest suite of LLM-centered products, we have been uniquely positioned to talk with many current and prospective customers about how they’re considering putting LLM applications into production. We’ve noticed exciting similarities across all the industry verticals and scopes of first-use-case MVP LLM projects. Although the exact use cases may differ, overwhelmingly, teams are experimenting with LLMs for knowledge retrieval—a shorthand for the commonly used retrieval-augmentation generation (RAG)1. Whether they are consultants referencing slide decks of relevant research or retail account managers responding to questions about their inventory, organizations have accumulated massive amounts of data that their internal teams need to reference daily. Knowledge retrieval systems solve this problem by augmenting a large language model (LLM) with relevant context on use cases. Knowledge retrieval applications are typically implemented as productivity boosters within an organization, assisting (and speeding up) the research process of internal teams searching through large document databases for answers. This blog will work to provide a high-level overview of how these systems are implemented in practice. We start with an overarching view of both the end-user experience and “behind-the-curtains” components that make up the general structure of these applications in practice, and follow up with some smaller deep dives on choices and concerns organizations often discuss when building out these systems. What Makes Up These Applications in Practice One of the best ways to contextualize different pieces of this system is to think about how most people will utilize your application—the end-user experience. Here is a high-level overview of a canonical retrieval-augmented LLM application:LLM knowledge retrieval application workflow showcasing the end-user experience and the behind-the-scenes architecture that powers it. In this diagram, the dashed arrows represent embedding transformations. End-User Experience: This is how most people will think about your knowledge retrieval application. This is the “magic” of working with an LLM application that can provide an understandable generated answer to their specific questions. ‍Behind the Curtains: These technical pieces make up your application’s “magical” functionality for end users. We will discuss the high-level steps needed to implement this architecture below, but it is important to recognize that the application works through prompt augmentation. This means that most teams add relevant information to answer user questions (i.e. their internal data) into the prompt of off-the-shelf LLM models instead of fine-tuning their own on the task.Why are teams choosing not to fine-tune? When I realized the incredible popularity of knowledge retrieval models in practice, I asked, “What about fine-tuning?” At the beginning of the year, all of the talks around LLMs focused on how quickly teams could fine-tune their own on all of their data. Fine-tuning an LLM is the process of adapting a pre-trained model to perform better on a use case by providing extra training on a smaller, task-specific dataset. Fine-tuned LLMs have proven incredibly effective at improving essential metrics for productional ML systems. Namely, they can improve accuracy (by utilizing data for their specific use case) and latency (as they enable the use of smaller models).However, in practice, teams are still turning to data augmentation because of its practical benefits.2 Some of these include:On the maintenance side, the ability to easily change and update the database of information that LLM can use as context—instead of adding the cost of any new data required to fine-tune again. On saving costs in model development, teams can save the time and effort of feature engineering by using the existing data lakes and knowledge bases. Putting together curating prompt/completion pairs for fine-tuning can be a difficult and lengthy task. Additionally, it enables teams to build in additional beneficial features to their applications, such as the ability for the LLM to cite which relevant documents it used and personalize data being pulled into the application by user permissions.Finally, it is notable that teams are choosing knowledge retrieval as a way to combat hallucinations, as they are less likely to occur when you provide questions on relevant documents instead of relying on the internal knowledge of an LLM. This does not mean that they are a solved problem, however. Poor information retrieval techniques cause LLMs to poorly answer questions on incorrectly sourced raw data. We will define and dive deeper into hallucinations later when addressing challenges in deployment. Steps (and Considerations) in a Knowledge Retrieval Application Now that we’ve seen a brief introduction to the overall architecture, let’s break it down into the high-level steps teams need to take to build out this application. Preparing Input Into LLMIn knowledge retrieval scenarios, teams typically use an out-of-the-box LLM application, so most of the engineering work goes into formatting the prompt for their model of choice. These prompts usually contain three values formatted for your application: User Input: Question the end-user sends into the LLM application for a response.Prompt Augmentation: Typically done through methods such as paraphrasing or incorporating explicit instructions, this technique is meant to improve the quality and use-case relevance of the LLM application’s response.Context (Data Augmentation): Relevant background information sent to the LLM to assist in answering the question. While this could be a stationary corpus of relevant information, typically, there is more information you want to ask detailed questions about than can be submitted as context for all possible questions.For this reason, teams will build out the next step of creating these applications.Create Process to Search for Relevant ContextAs mentioned above, teams need to think critically about how they plan to use their knowledge database to augment an LLM. Keep in mind that not only do LLMs have a maximum context window that can be submitted, but if they are using an LLM via an API, the current pricing structure is typically by how many tokens you input in the requests (as well as the model completion/output). This means there is an incentive to encourage either the classical information retrieval system to return an extremely curated set of highly relevant documents or for a subprocess to compress further relevant information returned by the IR system before passing it back to the LLM.Knowledge Database: A knowledge database is where all the data relevant to your LLM application use case currently lives. Vector Store: This mechanism optimizes storing and retrieving vectorized representations of words, phrases, or sentences. This specialized text storage is used for LLM applications to improve querying and quick evaluation in production. Embedding Model: The embedding model transforms the raw text into easily queryable embeddings. These are typically mapped to another large language model embedding, such as BERT. Query Functionality: A query search is done to find the most relevant documents similar to the embeddings of the user input prompt. This search functionality can be done with various techniques, from simple similarity measures to machine learning. Choosing an LLMWith the increasing ease of implementation of available LLMs, choosing which LLM to use can be more complex than actually using one. When it comes to making that choice, there are several factors that teams need to consider, such as the training data, latency, price, or any technical requirements.3Challenges of Deploying Knowledge Retrieval Systems We touched briefly on some deployment challenges when discussing the choice between fine-tuning and data augmentation; however, there are some key things to remember when deploying LLM models. Sensitive Data Leakage: There are many ways to define sensitive data. The one that comes to mind for most people is personally identifiable information (PII), which in many cases, should be completely blocked from entering or exiting your LLM application. Additionally, teams with established and organized access permissions often rely on existing data user access control to influence what data can be pulled in as context. However, one of the hardest pieces to evaluate for sensitive data goes beyond regex checks and access control. It occurs when end users need access to information about the data but not the data itself. For example, a user may be able to ask questions about aggregations, like “What percentage of patients have O negative blood?” To access this, the model would need all patient blood-type records. However, you do not want your end user to be able to ask specific questions about that data, like a certain patient’s blood type, for example. Hallucinations: One of the most commonly asked-about challenges, hallucinations can be best summed up as mistakes made by the LLM. They occur when the model provides an unsubstantiated (or “made up”) answer to the question it is being asked. We are already seeing the consequences of believing hallucinations across industries, from unrunnable code suggestions to a lawyer in his own legal trouble for blindly believing court citations from ChatGPT.4 To improve productivity, end users need to be able to trust their generative assistants, so thinking critically about how to mitigate and detect hallucinations is critical during application development and deployment. Arthur has been working on a suite of products that helps teams go from development to safeguarded deployment to active monitoring and continued improvement for LLM systems, including knowledge retrieval. Let us know if your group wants to dive deeper into LLM applications, or get started on your own here.‍‍———1 Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.”2 Bratanic, “Knowledge Graphs & LLMs.”3 “GPT-4 Alternatives.”4 “Lawyer Apologizes for Fake Court Citations from ChatGPT  CNN Business.”‍FAQ ‍How do specific industries tailor knowledge retrieval applications to meet their unique needs and requirements?‍Industries tailor knowledge retrieval applications by aligning them with sector-specific data, compliance requirements, and operational needs. For example, healthcare might focus on patient data privacy and research materials, while retail could prioritize inventory and customer service data.‍What are the specific metrics used to measure the success and accuracy of knowledge retrieval applications in real-world scenarios?‍Success and accuracy in knowledge retrieval applications are measured by metrics such as query response time, accuracy of retrieved information, user satisfaction scores, and the reduction in time spent searching for information.‍How does the process of embedding model transformation improve the efficiency and accuracy of LLM-based knowledge retrieval systems?‍Embedding model transformation improves efficiency and accuracy by converting text into vector formats that are easier for LLMs to process, leading to faster and more relevant results from queries.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features
 Detecting Unexpected Drift in Time Series Features
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringDetecting Unexpected Drift in Time Series FeaturesBy:Akash KhannaJune 13, 2023Aside from crystal balls, time series models have become the predominant approach to predicting the future. Often implemented at organizations with access to large amounts of historical data, these models leverage time dependent patterns and trends in input features (like economic indicators, weather patterns, even heartbeats) in order to forecast into the future. Time series models have use cases spanning nearly every data type and industry, yet a great deal of uncertainty still remains when productionalizing and particularly when monitoring these models. The Challenges of Monitoring Time Series ModelsValidating time series models on historical data is standard practice prior to putting them into production. However, monitoring time series models once they are in production becomes tricky because forecasting horizons may be far into the future—meaning that the ground truth that traditional validation and performance metrics depend on may take a prohibitively long time to become available, exposing organizations to the risks associated with underperforming models in the meantime. Many data scientists have addressed this limitation by using data drift metrics on input features as a leading indicator of model performance and a key piece of information in determining if/when a model should be retrained. However, time series models oftentimes contain features that naturally drift over time and display seasonality, meaning that data drift is to be expected and potentially no longer a principled justification for retraining. In the remainder of this article, I will discuss an approach that would allow one to continue to use data drift metrics as an informative leading indicator of model performance by first accounting for the expected drift of input features and using traditional drift metrics to track the residuals of those time dependent features.  To get a better intuition for this approach, let’s squeeze in an ice cream break. The ice cream shop near my house is open year round. It is situated right across the street from one of the city’s largest playgrounds and is a go-to for parents needing to make good on bribes they’ve offered their children. The line for this shop can sometimes wrap around the block. This past summer, I started to fancy myself somewhat of a sweet toothed soothsayer; before even leaving my house I could tell how long the wait would be based on the weather that day. At 90 degrees Fahrenheit, a single scoop of pistachio would take upwards of an hour, while at 70 degrees, the place was practically deserted. My dessert-line wait-time model was working just fine, until temperatures started to decline. On a late winter’s day this past year, my model failed me. A meager lunch of leftovers and an unseasonably warm (70 degree) day had me craving a banana split, so I dusted off my old model and predicted that I could be back home with plenty of time to spare before my mail carrier arrived with my eagerly awaited copy of Designing Machine Learning Systems. Turns out a 70 degree day in winter is not at all the same as a 70 degree day in summer and I ended up spending close to 40 minutes in line, missing my book delivery. I had fallen victim to a case of mistaken stationarity. Stationarity in Time Series DataDealing with time series problems more pressing than my quest for a banana split, lickety split, usually warrants using a machine learning algorithm. Many common approaches (known as Autoregressive models), create models based on the fact that quantities close in time are often similar (i.e. yesterday’s wait time is likely pretty similar to today’s). More recently, neural network based architectures, which were designed to handle long sequences of data (like natural language), have emerged as a popular approach for time series forecasting. These approaches all involve a series of preprocessing steps, some of which we can use to establish a notion of expected drift in order to isolate the informative signal that is unexpected drift.  Data cleaning and feature selection/extraction are so commonplace in the preprocessing pipeline at this point that they will not be discussed here. Rather, special attention will be devoted to a potential thorn in the side of anyone attempting to glean insights from time series data: the removal of nonstationarities. Stationarity refers to the tendency of data to have a constant mean, variance, and covariance. In the context of time series features, these constants equate to data that essentially does not depend on when it was observed. The nonstationarities of most features of interest can usually be broken down into components which are either trends or seasonal in nature. These seasonal cycles could be yearly, quarterly, daily, or even lengths of time which may seem arbitrary at first glance. Fortunately, there are a number of open source approaches to identifying and decomposing those cyclical time dependent components of a model's features, so that fluctuations in the remaining signal can be disentangled from the expected drift. Approaches to Addressing Nonstationarities in DataOne tool that has proven particularly useful and flexible in addressing nonstationarities is the open-source python package Darts. Darts describes its primary goal as “simplifying the whole time series machine learning experience.” The darts package includes functionality which helps detect and extract nonstationarities in data. Users can feed in their raw time dependent features and retrieve a transformed feature, one that is time independent and takes into account where (or really when) a value occurs in time such that values separated in time can still be meaningfully evaluated with standard metrics—for example, the metrics we referred to at the start of this guide, those used to measure data drift for the purposes of evaluating, or reevaluating, your model’s performance. There are other common approaches to forcing stationarity on time series features. Many of these approaches fall under the umbrella of differencing: essentially tracking the difference between consecutive observations rather than the observations themselves. In practice, the definition of consecutive here could mean consecutive days, weeks, quarters, years, or essentially any difference as long as it is consistently applied. Tracking these differences often accounts for the trends and seasonal tendencies of features in such a way to allow for the remaining quantities to be informative signals which can be tracked for drift.  It is worth noting that many modern approaches (neural network based architectures) can natively handle non-stationary data. Though the removal of nonstationarities may not be a critical part of the preprocessing pipeline for these models, it may still be worth creating stationary versions of those features, particularly under circumstances where forecasting horizons are distant and unexpected trends in input features can serve as a leading indicator of performance.Returning to our original motivation, the key idea here is that by accounting for expected drift in our time dependent features, we can be sensitive to unexpected drift and use it to guide decisions about evaluating, or reevaluating our model. Fundamentally, this approach is the difference between a short line for a rootbeer float on a temperate day in summer and a long line for that same float on an unseasonably temperate day in winter.‍FAQWhat are the specific performance metrics used in AI and ML to assess time series models before and after accounting for expected drift?In the context of AI and machine learning, specific performance metrics commonly used to assess time series models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), as well as more sophisticated metrics like Mean Absolute Percentage Error (MAPE) and Symmetric Mean Absolute Percentage Error (sMAPE). These metrics are crucial for evaluating the accuracy of forecasts by comparing the predicted values generated by the model to the actual values observed in the time series data. Both before and after accounting for expected drift, these metrics help in quantifying the effectiveness of the model, guiding data scientists in LLM (Large Language Models) and ML in refining and improving their forecasting algorithms by highlighting discrepancies between predicted and observed outcomes.In the realm of ML and AI, how does one quantify the level of acceptable drift before deciding to retrain a time series model?Quantifying the level of acceptable drift in the realm of machine learning and artificial intelligence before deciding to retrain a time series model involves setting precise threshold values for data drift metrics. This process is influenced by historical model performance, industry benchmarks, or the specific objectives of the ML forecasting task. A sensitivity analysis can be particularly useful in understanding the effects of various levels of drift on the predictive performance of the model, a common practice in AI. Once these thresholds are established, exceeding them indicates that the model's predictions are losing accuracy, suggesting a need for retraining. This approach, crucial in the lifecycle of ML models, helps balance the costs associated with model updates against the potential risks of using outdated predictions.What are the limitations or challenges associated with using the Darts package for addressing nonstationarities in time series data within ML and AI frameworks?The Darts package, while a valuable tool in the ML and AI toolkit for addressing nonstationarities in time series data, presents certain limitations such as computational demands, especially when dealing with extensive datasets or intricate time series. The complexity might deter users new to Python or those unfamiliar with advanced time series analysis techniques common in AI research. While Darts offers a range of functionalities for decomposing and modeling time series data, its applicability might not extend to all types of nonstationary data or might not always be the most efficient approach, particularly for datasets characterized by high irregularity or noise. Additionally, integrating Darts within broader ML and AI pipelines can present challenges, especially when those pipelines employ different programming languages or are structured in unique computational environments.‍‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem
 Model Schemas Within the MLOps Ecosystem
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringModel Schemas Within the MLOps EcosystemBy:Sarah OstermeierJune 6, 2023MLOps and Continuous TrainingWhen embarking on an individual data science project, documenting, standardizing, and tracking may not seem like a top priority. However, as data science teams expand and numerous teams emerge within an organization, along with the development of centralized MLOps systems, the importance of establishing standards and tracking systems becomes evident. These measures allow data science teams to work efficiently and avoid costly mistakes.A well-designed MLOps system should track all the information and components needed to retrain a model from scratch such that it is approximately identical to the original model (same training data, model algorithm, hyperparameters, etc.). Most of this is done through model registries, feature stores, CI/CD and CT (continuous training) tools such as Dagshub and other versioning systems. By employing these systems, model artifacts, data, and code versions can be tracked effectively, facilitating continuous retraining and, if necessary, emergency rollbacks of production models.Within the model pipeline, two crucial components must be carefully maintained and tracked across iterations to facilitate this seamless orchestration.1. Model: What is the current model deployed in production and what conditions are needed to retrain it or pass inference data into it?To answer these questions, we need to keep track of several key elements:The saved model artifact, which serves as the core representation of the trained model.The predict function, which handles the model’s predictions.The software and environment requirements necessary for running the training and prediction processes.The code employed to train the model, encapsulating the very essence of its development.2. Data: What data was utilized for training and validation?This aspect necessitates monitoring two data subsets:The training data, which molds the model’s understanding and helps it glean patterns and insights.The test and validation data, which enables the evaluation of the model’s performance and generalization abilities.However, in order to know how to bring these pieces together, more information is needed: the model schema.‍What is a model schema?A model schema describes the relationship between a dataset and the model.A model schema is much like a database schema in that it outlines the structure and relevant metadata of a dataset. In the case of a model schema, this outline describes the relationship between the dataset and a model. It includes information such as which data columns are used as direct inputs to the model, how model inputs and outputs are structured, and what bounds on data values are expected.   The ultimate goal of a model schema is to allow a user to load and reconstruct a dataset as it was used during the initial training and validation phase. This in turn enables easier model retraining and rollbacks, data validation, and any further analysis or model validation that is needed. Data scientists can reload and explore their datasets and model outputs without the need to refer back to training code. Meanwhile, MLOps administrators can define robust model schema standards and establish automated systems for data and model monitoring and validation, streamlining the overall process.In the upcoming sections, we will delve into the intricate details of what essential information should be encompassed within a model schema. Additionally, I will provide a concrete example that illustrates how a model schema may be structured. Furthermore, we will explore how the adoption of model schema can yield standardization benefits within an organization and bolster the overall MLOps system. Throughout this blog post, we will employ the example of a tabular classification model to illustrate the concepts at hand. Rest assured, these principles extend beyond this specific scenario and are applicable to a wide range of model types and data domains.What information should be captured in the model schema?Exactly what information is needed in a model schema will depend on the model type, use case, size of the organization, and how the model schema is intended to be used. In general, best practice is to standardize model schemas across an organization to facilitate automation. The recommendations here should be treated as a starting point. My recommendation for a tabular model is given here:1. Data ColumnsHow are the data columns used and how does each column relate to the model? Model inputs: Actual input values to the modelModel outputs: Predicted probabilities, logits, and/or predicted classNon-input data: Any data that is relevant to the model but is not used as model input such as id columns, timestamps, or features that contain cohort data that is of interest such as race or gender  for bias trackingModel target: The ground truth column(s) that the model is predictingFor each column type, consider what information would be needed for a colleague to reload the data and evaluate the model without access to the code used to train the model. For model inputs, non-input data, and target columns, include basic information for each column:Column nameColumn index: Where the column is located in the DataFrame it is saved inData type: What the expected datatype for the column isCategorical or continuous: This will tell future users how the column should be treated during analysisCategories (if applicable): What are the expected categories if the column is categorical?Value bounds: What are the expected bounds if the column is numerical?Cohort/segment column (if applicable): Whether or not the column denotes data cohorts or segments of interest—used to compare model performance across groupsSample ID or timestamp column: Whether the column is a sample ID or timestamp column (his may only applicable in some cases)For model output columns, it is useful to include both raw model outputs and the final prediction made by the model. In addition to the fields outlined above, columns with raw model outputs may require a field to map raw outputs to the final model output. If the model is a multi-label model, it is also important to note which prediction task the column corresponds to.2. Data AccessDetails about how to access the data and how it is structured. The specifics of what is needed here may vary depending on how data is managed within the organization. Some examples of useful content are given below:Data files: For each data file, give the filepath, which components of the data are included (e.g. [‘model inputs’, ‘model target’]), expected column names, the join key to integrated data with other sources, and any other information need to load the data file.Data splits: If test or validation data is included in the data files, provide details about training vs test indexes. If training and validation data is saved in separate files, describe where each set can be located.  Inference data: If applicable, provide details on where inference data is to be stored and how it is formatted if different from training data.‍Example: ACSTravelTimeAs an example, I will go through a model I trained on a modified version of the folktables dataset ACSTravelTime. The objective of the model is to predict whether an individual has a commute of more than 20 minutes, and indicated by the binary target “JWMNP.” The DataThe dataset contains the following features:SERIALNO: person serial numberWAGP:  Wages or salary income past 12 monthsAGEP: Age of the householderSCHL: Educational attainmentMAR: Marital statusSEX: SexDIS: Disability recodeMIG: Mobility status (lived here 1 year ago)RELP: RelationshipRAC1P: Recoded detailed race codePUMA: Public use microdata area codeCIT: Citizenship statusOCCP: Occupation recodeJWTR: Means of transportation to workPOWPUMA: Place of work PUMA POVPIP: Income-to-poverty ratio recodeThe dataset is derived from the American Community Survey Public Use Microdata Sample (PUMS) and full documentation can be found here.The features in this dataset were not all used as input to the model. SERIALNO was used only as an identifier and join key during data manipulation. RAC1P and SEX were not used as model input, but were important information to track nonetheless to evaluate model fairness concerns. The features also include a combination of numerical and categorical data.  I used data from the years 2014 and 2015 as training data and reserved data from 2016 to test my model. Prior to training my model, I defined each of the relevant groups of columns in my code. I also defined data bounds for numerical features, and which columns were to be used to track specific cohorts of interest within the dataset.This makes it easy to separate input and non-input data without losing track of non-input attributes.The ModelI trained a Gradient Boosting Classifier and being satisfied with the results, I saved the pickled model.While this is usually the end of the story in a data science blog post, the next few steps are key to maintaining and improving upon a model in a real-world production environment.Data PersistenceWhile we already have the data saved somewhere (in this case, in the U.S. census database), it is important to keep track of the exact data used to train and validate a model. This especially comes into play when the time comes to retrain a model or roll it back to a previous version. It may also be required for regulatory purposes.  For my ACSTravelTime model, I reorganized all of my input and non-input data for both my train and test sets into one DataFrame, keeping track of the indexes for each dataset.Model outputs on both training and test datasets are also important to maintain, as this facilitates comparisons of multiple versions of a model over time. I saved raw prediction probabilities and the final prediction for my ACSTravelTime model and combined training and test data into one DataFrame. I saved both DataFrames as .csv files.Model Schema ArchitectureI designed a model schema for this model with the objective of making it easy to reload the dataset, define data validation standards, and evaluate and monitor the model’s performance. The schema is first structured as a python dictionary, and then saved as a .yaml file, which can easily be reloaded along with the dataset.As described above, I divided my model schema into 2 sections. In the data_access section, I provided all the information needed to load the dataset and run basic validation using just the model schema file.In the data_columns section, I provided more granular details for each column, which would allow for full data validation, model evaluation, and model monitoring. For each input and non-input column, there are details about data bounds and categories, the data type of the column, and whether the column should be used to track cohort performance.  The model_outputs section details which columns correspond to the final prediction and the predicted probability for each class.  Using my model schema file, I am able to reload my data from scratch and understand how my training data was structured, what data bounds I should expect, and which columns were of interest when tracking data subsets. Along with other versioning tools, I could use this to help me retrain a new version of my model, roll back to a previous version, and perform monitoring and evaluation on my model. The complete notebook used in this example is available here.The full model schema file is available here.‍The Value of Model SchemasAt first glance, model schemas might appear superfluous, possibly dismissed as an afterthought by an individual data scientist. However, within the organizational landscape, they are a vital part of maintaining consistency, efficiency, and automation in an MLOps system.They provide an outline of exactly how data is used in a model, while also establishing a framework for comprehensive evaluation and continuous monitoring. Model schemas can fit easily into an MLOps system as a model metadata artifact, and be saved within model registries or other versioning systems. This level of standardization allows data science and MLOps tools to be automated and speeds up the process of retraining, updating, and monitoring models. It also reduces the reliance on institutional knowledge, facilitating hand-offs between team members.With a shared understanding encapsulated within the model schemas, data scientists and MLOps practitioners can unlock the true potential of automation, empowering tools and processes to operate seamlessly. This not only streamlines the workflow but also reduces the risk of human error, ushering in a new era of efficiency and collaboration within the organization’s data science ecosystem.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias
 Downstream Fairness: A New Way to Mitigate Bias
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessDownstream Fairness: A New Way to Mitigate BiasBy:Daniel NissaniMay 17, 2023Note: This blog post and code could not have been done without the fantastic research from our former research fellow Kweku Kwegyir-Aggrey and former machine learning researcher Jessica Dai. You can view their paper here.MotivationMany organizations utilize binary classifiers for a variety of reasons, such as helping loan providers decide who should get a loan, predicting whether or not something is spam, or providing evidence on whether or not something is fraudulent. These use cases require specific classification thresholds. Imagine an algorithm is predicting whether or not someone qualifies for a loan. One way to do this is to attribute a probability to a person, and if that probability is above a certain threshold (let’s say 0.5), then they can get a loan. If not, then they will be rejected.What is the proper threshold to use in these scenarios? Taking spam detection as an example, the threshold set will determine how often an email is classified as spam. A threshold of 0.8 is less permissive than a threshold of 0.4. That is why many organizations have threshold ranges for their algorithms, which can complicate things.Current bias mitigation techniques, such as the one we offer at Arthur, traditionally require you to change your classification threshold to meet some fairness definition. This change in threshold could be outside the range that your company allows, creating questions as to whether or not you can be fair. Further complicating these situations are models that are utilized in many downstream applications, where different threshold ranges (and possibly different fairness definitions) need to be utilized.Downstream fairness solves this dilemma. It’s an algorithm that achieves various fairness definitions (equalized odds, equal opportunity, and demographic parity) in a threshold-agnostic way, meaning that a company won’t have to adjust their threshold. Instead it operates on a binary classifier’s output probabilities to achieve a fairness definition. And this is all done with minimal accuracy loss! For the remainder of this blog post, we’ll be digging deeper into this algorithm and how to use our new open source code.Downstream FairnessSaving the mathematical details for the Geometric Repair paper, we will discuss the essence of how Downstream Fairness works and provide code snippets from our open source package. First off, downstream fairness is a post-processing algorithm that operates on the training dataset (or some representative dataset) for the model we are trying to make fair. The data needs to contain some key information: the prediction probabilities for each data point, the classification label, and a column containing the sensitive attribute on which you are operating.
Prediction Probabilities
Prediction Labels
Group Information
How the algorithm works is that it looks at the distribution of prediction probabilities per group of our original model and then computes a repair of each of those distributions for demographic parity. The reason this works for demographic parity is because the definition of demographic parity (equalizing selection rates for each group) only requires prediction probabilities and group information.On the implementation side, this process produces an adjustment table. The adjustment table contains how much the prediction probabilities need to be adjusted to achieve demographic parity, for each group. Below is an example of how that table looks:The pred_orig column is a granular representation of possible prediction probabilities a binary classifier could give—in this case, the granularity is set to hundredths. The adjusted_scores_x are the adjustments for group x. If the fairness definition you want to use is demographic parity, we are done! On new inferences, find the prediction probability for that inference and the group that the inference comes from, and then add the adjustment value to the inference probability:
new_prediction_probability = old_prediction_probability + appropriate_adjustment
If you want to use a different definition, such as equalized odds or equal opportunity, then we need to do one more thing. As we stated before, our goal is to repair the original prediction probability distribution so that it achieves demographic parity. In the paper, we call this a full adjustment, meaning that the adjustment table produced is a full adjustment of our model. For other fairness definitions, such as equal opportunity or equalized odds, we need to find a dampening term that adjusts the adjustments, so that we can achieve those fairness definitions.
In order to do this, we first need to find which group in the sensitive attribute column is experiencing the most disparity according to whatever fairness definition you want to choose. Once we identify that group, we are going to go through an optimization process to find a λ (what we call it in the code and how it is talked about in the paper), which will work as our dampening term, so we can satisfy other fairness definitions. Once we obtain this λ, we can apply it in the following way with our adjustment table:
new_prediction_probability = old_prediction_probability + lambda*appropriate_adjustment
Luckily, this is all automated with our codebase! Here is an example of how to do this:And, unlike some other bias mitigation approaches, downstream fairness is a pareto optimal algorithm. Meaning that it will achieve these fairness definitions with the minimum amount of accuracy loss.Of course, there are some limitations. The dataset used to train downstream fairness must contain prediction probabilities for each class for each group, and there should be a good amount of examples for each class for each group. But if that is provided, the algorithm should work as expected.ConclusionWe went through some of the algorithmic and implementation details of downstream fairness. If you want to explore more of the mathematical details, please go read the paper. Us at Arthur would love for you all to try out our work! Feel free to pip install our package and kick the tires a bit. As you find failure cases or think of new features, feel free to send your feedback to me at daniel.nissani@arthur.ai. Even better, please submit PRs or Issues on our open source GitHub repo. The GitHub repo provides a demo notebook, where you can try out all of our functionality we described in this post.‍FAQ‍1. What are the specific mathematical principles behind the Downstream Fairness algorithm?The Downstream Fairness algorithm is grounded in statistical and probability theory, particularly focusing on the concept of distribution repair for ensuring fairness across different groups. The mathematical foundation involves adjusting the distribution of prediction probabilities for each group to align with fairness criteria such as demographic parity, equalized odds, or equal opportunity. This involves a process known as "Geometric Repair," which essentially recalibrates the output probabilities of a predictive model so that the resultant probabilities do not disproportionately favor or disadvantage any particular group based on sensitive attributes. The algorithm employs optimization techniques to find the best possible adjustments that achieve fairness while minimizing accuracy loss. This is achieved by constructing an adjustment table that represents how much the prediction probabilities need to be shifted for each group to meet the desired fairness standard.2. How does Downstream Fairness compare to other bias mitigation techniques in terms of performance and implementation complexity?Downstream Fairness differs from other bias mitigation techniques primarily in its post-processing approach, focusing on adjusting model outputs rather than altering the training process or the data. Compared to methods like reweighing, which modifies the weight of instances in the training data, or adversarial debiasing, which involves training a model to predict the target while another model predicts the sensitive attribute to reduce bias, Downstream Fairness is implemented after a model has been trained, thereby not affecting the original training pipeline. This can make it easier to integrate into existing workflows without needing to retrain models. In terms of performance, Downstream Fairness aims to be pareto optimal, meaning it seeks to achieve the best possible trade-off between fairness and model accuracy. This contrasts with some methods that might significantly reduce a model's performance to achieve fairness criteria. However, the actual performance and complexity can vary based on the specific scenario and the extent of bias in the original model.3. Can Downstream Fairness be applied to non-binary classifiers and multi-class scenarios?The concept of Downstream Fairness as described in the blog post primarily addresses binary classification problems. However, the underlying principles can be adapted for non-binary or multi-class classification scenarios with some modifications. In multi-class scenarios, fairness typically involves ensuring that the predictive performance is balanced across different groups for all classes, not just two. This could involve extending the adjustment table to cover all possible class predictions and ensuring that the adjustments lead to fair outcomes across all classes and groups. However, this adaptation can increase the complexity, as it requires considering inter-class fairness in addition to intra-group fairness. The implementation for multi-class scenarios would need to calculate separate adjustments for each class and group combination, possibly leading to a more complex optimization problem. While the original Downstream Fairness algorithm may not directly apply, the principles of adjusting prediction probabilities and achieving demographic parity can still be extended to these more complex scenarios with appropriate modifications.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms
 Announcing Arthur Shield: The First Firewall for LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsAnnouncing Arthur Shield: The First Firewall for LLMsBy:Arthur TeamMay 4, 2023At Arthur, we are on a mission to make AI better for everyone—and right now, nowhere is that mission more important than the deployment of large language models.Companies across industries have begun to rapidly integrate LLMs into their operations following recent advancements from organizations like OpenAI, Google, Meta, and others. However, businesses don’t currently have a way to ensure fast and safe deployment of these applications, which has led to data leaks and toxic outputs that have been costly in more ways than one.That’s why, today, we are launching a powerful addition to our suite of AI monitoring tools: Arthur Shield, the first firewall for large language models. Arthur Shield enables companies to deploy LLM applications like ChatGPT faster and more safely, helping to identify and resolve issues before they become costly business problems—or worse, result in harm to their customers.Simply put, Arthur Shield acts as a firewall to protect organizations against the most serious risks and safety issues with deployed LLMs. Use cases can include:PII or sensitive data leakage: Arthur Shield allows companies to use the power of an LLM trained or fine-tuned on their full data set while having the peace of mind from knowing that other users of that same LLM are blocked from retrieving sensitive data from the training set.Toxic, offensive, or problematic language generation: Arthur Shield allows companies to block LLM responses that are not value-aligned with their organization.Hallucinations: Some LLMs confidently output incorrect facts. Arthur Shield detects these likely incorrect responses and prevents them from being returned to a user where they can do significant harm if they are actioned upon.Malicious prompts by users: Arthur Shield detects and stops malicious user prompts, including attempts to get the model to generate a response that would not reflect well on the business, efforts to get the model to return sensitive training data, or attempts to bypass safety controls.Prompt injection: It is becoming common for LLM applications to augment their prompts through retrieval from third-party websites and databases of pre-trained document embeddings. Those sources are not secure and can contain malicious prompts that are injected into the LLM system, causing significant risk of unauthorized response generation and data leakage.“LLMs are one of the most disruptive technologies since the advent of the Internet. Arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk.”– Adam Wenchel, Co-Founder & CEOArthur Shield in the LLM LifecycleArthur Shield and its capabilities are currently being rolled out in beta to select Arthur customers. Read more in our official press release or get in touch to request a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text
 How to Think About Production Performance of Generative Text
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsHow to Think About Production Performance of Generative TextBy:Rowan CheungApril 25, 2023IntroductionPerformance monitoring has always been at the heart of Arthur’s mission and offering. We know that teams do not put models into production for no reason. Yet time and again it seems that every “state of ML” report released still lists that ML teams struggle to communicate performance to their external stakeholders. Our mission has always been to help teams create workflows and toolkits that not only enable teams to use reporting to build better models, but also empower them to communicate with external stakeholders.Nowhere is that mission more important in 2023 than in generative AI. Organizations are terrified to fall behind and are scrambling to implement LLMs into their processes. However, it’s important for teams to think critically about how they’re planning to evaluate and communicate these findings to external stakeholders.In this blog post, we’re going to cover some of the core challenges teams run into when they try to evaluate generative text outputs. Then, we’ll give a brief overview of some metrics that are commonly discussed by research communities and explain why we don’t see them working in production environments. Finally, at the end, you’ll find our suggestions for how to evaluate and monitor generative text to provide actionable outcomes.Core Challenges of Evaluating Generated Text1. There isn’t one ground truth output.‍In open text generation, the output of the model is unstructured text, so there is no ground truth label in the same way we might think of evaluating a traditional classification or regression task. One solution for this gap is to ask humans to accomplish the same task asked of a generative text model, and use the human-generated text as ground truth or the “correct” solution to the task. This is time-consuming and infeasible to scale to the needs of continuously evaluating a production application.Even given infinite resources, there is no clear definition of what the best piece of text is.
Consider the following natural language queries and their corresponding responses from ChatGPT. They overlap in some of the details mentioned in the response (e.g. the various relevant terms like forces and planets), but by slightly altering the prompt we elicit a response which changes the words and the concepts used in the writing. An evaluation procedure for a generative text model should properly take into account that the quality of the response will depend on its intended context, use case, and audience. For example, this type of language model that answers science questions should strike a balance between the simplicity of its answers and the thoroughness of its answers based on the user at inference time, so evaluation metrics should flexibly control for this contextual shift.2. We lack consistent automated metrics for comparing two pieces of text.‍Ideally, we’d like an automated score that could help us compare the above model outputs, or select between models that produced this set of inferences. While many metrics have been proposed, it remains a difficult question to select which metric, and associated hyperparameters best suit a specific use case. Different metrics have varying behavior on underlying qualities of text like tone, relevance, truthfulness, and also coherence, grammar, and lack of repetition. In the next section, we’ll outline some options for automated metrics.What Automated Metrics Exist?When we have a reference text, there are some metrics we can explore for comparing the model generated text to the reference text. In this section, we’ll discuss the benefits and limitations of some commonly used metrics.N-gram Precision Metrics: BLEU and ROUGEThe first class of metrics directly compares the overlap of tokens in the golden reference text to the tokens in the model generated text. A popular example of these metrics is the BLEU score. BLEU was originally developed for machine translation and compares the number of n-grams in the machine generated text that also exist in the reference text.$$ BLEU = min(1, (exp(1-\frac{len(reference)}{len(generated)})) \prod_{i=1}precision_i $$The first term encourages the reference text and the generated text to be of similar length. If the length of the reference text is equal to the length of the generated text, the first term will equal 1 and the value of the metric will be determined by the second term, the precision.To calculate the precision term, BLEU measures the fraction of i-grams in the generated text that are also contained in the reference text for i=1 to 4.BLEU and its variants are very brittle:They don’t capture semantic information. Swapping the word “happy” for “joyous” and for “sad” are considered equally wrong.They don’t effectively measure grammar. Jumbling the order of the words in a piece of text can render the text nonsensical, while barely changing the BLEU score.Adding Context: BERTScoreThe next class of metrics attempts to add semantic meaning and contextual information to techniques like BLEU, by leveraging embeddings instead of raw tokens or words. Embeddings are learned representations that map words to a vector in high-dimensional space such that the vectors capture the meaning and relationship between words.In BERTScore, each token in the reference text and generated text are first embedded using the BERT language model. Using a language model like BERT provides embeddings that hold both semantic meaning as well as contextual information, because BERT can generate different representations of the same word depending on its surrounding sentence, or context. The BERTScore is then a combination of:A precision term: Of all the embeddings in the generated text, compute the mean of the maximum cosine similarity with any embedding in the reference text.A recall term: Of all the embeddings in the reference text, compute the mean of the maximum cosine similarity with any embedding in the generated text.While BERTScore addresses a core issue of incorporating semantic information into the metric, it has its own drawbacks:Introducing embeddings increases the computational complexity of the metric.When using embedding-based metrics, the choice of embedding can influence what aspect of the text (semantic meaning, tone, style) the metric is most optimized to measure.Correlating with a Human Text Distribution: MAUVEFor both BLEU and BERTScore, a single human reference text is compared to a single model generated text. The authors of MAUVE propose a metric that compares the distribution of model generated texts to the distribution of human generated texts, and show that this can better correlate with human judgments of models completing generative tasks.To compute MAUVE, samples are taken from a human distribution of text and from the model. A separate model (the authors use GPT-2) is then used to embed all the texts. Each underlying distribution is then approximated by clustering the texts and counting the number in each cluster. The final MAUVE score estimates the divergence between the human text distribution and the model text distribution.Downsides of MAUVE:MAUVE still requires a reference, human-generated text distribution.MAUVE scores can vary based on the hyperparameters chosen. Some best practices for using MAUVE are here.Monitor What MattersAll of the metrics described above require a reference text to compare model output to, which is often difficult to obtain in production settings. Exciting new metrics like USR explore training a model to score texts from a set of model texts and ground truth texts, such that no reference text is required at scoring time. Model-based metrics are also a promising direction towards metrics that allow users to adapt a metric for a given use case. But in production, “good” is whether the LLM is successful in the product it is deployed in.When deploying a language model in an application or other business setting, we can more directly measure performance by collecting implicit performance metrics from the LLM’s environment. There are two types of performance measures that could be relevant to track:
User Feedback: These signals capture information on how users are interacting with the outputs of an LLM.
Augmenting an information retrieval system with an LLM to reduce the time necessary to find desired information. A suitable performance metric could be the average time a user spends on a specific query.
Generating headlines for news articles with an LLM to drive reader engagement. A suitable performance metric could be the average number of clicks per headline.
Execution Success: These signals capture information on how the outputs of an LLM interact with the rest of the production system.
Augmenting an information retrieval system with an LLM to reduce the time necessary to find desired information. Some suitable performance metrics could be the average distance between the query and the documents returned, or the validity of the LLM compiled query.
Augmenting a chat system with an automated appointment booking system. A suitable performance metric could be the API error rate, when making requests to the booking software.
These metrics are just the starting point in a holistic monitoring solution for LLMs, and are not meant to encourage driving engagement or automation at the expense of fairness or cognitive engagement. For a deeper look at designing human-centered evaluation for LLMs, check out Teresa Datta’s recent blog.Future Research DirectionsHow can we develop metrics that are optimized for the context of the deployment?Given the above performance proxies for an LLM in production, we’d like to automate metrics to correlate with those criteria. These metrics can then be used during other phases of the model lifecycle where production signal is not available, such as model selection and validation.Can we use LLMs to generate feedback for other LLMs?There is exciting research exploring the possibility of using LLMs to grade, score, and monitor other LLMs. For example, in Self-Refine, the authors propose a framework in which an LLM iterates on a task utilizing the feedback provided by an LLM. At Arthur, we are exploring utilizing embeddings and LLMs for scoring LLM outputs, providing natural language descriptions of model performance, and benchmarking the strengths and limitations of using LLMs during evaluation.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice
 What Does the ML Lifecycle Look Like for LLMs in Practice?
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsWhat Does the ML Lifecycle Look Like for LLMs in Practice?By:Haley MassaApril 18, 2023I’m not going to say I was a full-blown deep learning denier, but I was stubbornly holding out to avoid getting swept up in the LLM hype. While I find almost all ML research interesting, I am really enamored with ML systems research, dedicated to the practical implementation of ML models. And it seemed like generative AI was being touted around more as a fun toy to “see what it could do” than something feasible to put into production. As usual, the LinkedIn posts started to pour in. However, it wasn’t until recently, when those same posts began to shift from “Wow, look at this response” to “Wow, look at this application I built,” that I realized I could no longer keep my head in the sand. I realized that sooner than I was expecting, these applications will be a part of the ML systems work I am so interested in. So, I spent some time trying to find out what makes the development of these models different from typical ML models. What Does the Traditional ML Lifecycle Look Like? Over the years, there have been many different formats that researchers have used to describe the steps teams go through to build an ML model. While I will not detail these steps, please refer to the diagram below.We can see in the diagram two core pipelines that work to put ML models into production (and one version-controlled layer that helps to manage them). The first development pipeline is where data scientists and research teams work to develop the model. In the second pipeline, ML Engineers and developers work to take their model at the final stage of development and productionalize it into something that can be used to make real-world predictions. What Does an LLM Lifecycle Look Like?Although LLMs are trying to carve out their own phrase within MLOps (LLMOps), it’s important to remember that they’re still machine learning systems. This means that even if they use different tools or phrases, they still follow most of the same lifecycle and best practices. I’ve broken down the LLM lifecycle into three phases, seen in the drawing below. In the following sections, I will give a high-level overview of each phase and how it relates back to the traditional ML lifecycle. Foundation Models On the left, we can see the most famous aspect of these new LLM systems, the foundation models. Foundation models are large pre-trained language models capable of understanding and generating natural language text. This ability to perform general language tasks well enables these models to serve as the starting point for various NLP tasks, such as language translation, text summarization, and question-answering. Foundation models are the most significant shift away from the traditional ML lifecycle. API access to foundation models has made it easier for teams to leverage NLP in their operations, regardless of industry or size. They can implement just the API as their production model—skipping most of the ML lifecycle workflow—if they are just looking for general language task capabilities.This is a standard route we see organizations go down as a first iteration of putting an LLM into production. However, this is no longer remarkable, as any company can create an OpenAI account. Instead, users seek customized experiences based on the specific use case for the model they interact with. To do this, ML teams need to use many of their existing techniques for ML model development to fine-tune and improve their model based on its specific use case. Note: Many foundation models have a closed-source nature due to the lack of transparency and accessibility of the code and data used to train them. Verifying the model’s accuracy and biases is difficult without access to this code and data. This can lead to unintended consequences and perpetuate existing biases in the data. DevelopmentThe development phase is where ML practitioners build and improve upon these use case–specific ML models. As we can see from the diagrams above, it exists in both the traditional and LLM lifecycle. However, one key difference is that there are currently fewer development steps for LLMs. For example, teams do not need to select and test different model architectures. While development for LLM models will undoubtedly continue to advance, the workflow is currently streamlined into three main steps: Defining the Use CaseWhile not explicitly listed in the diagram above, the first step to building any worthwhile ML model for production, LLM or not, is to define and understand your use case. Teams will need to spend time with business and product stakeholders to understand the purpose of the model they are putting into production. Data Curation & Model Fine-TuningData science teams must curate and clean use case–specific data to build out use case–specific LLMs. This data will be used to train/fine-tune the foundation model’s language understanding to their task requirements. Cleaning and curating data is something that data science teams are used to, as it is a part of their traditional ML lifecycle. However, one benefit of using LLM models over traditional ML techniques is that they already have a solid understanding of language due to all the data they were trained on originally. By using data only to fine-tune on top of an existing large model, teams are not required to curate and clean as much data. Qualitative Validation with PromptsSimilar to all traditional ML models, these models must be tested and validated before they can interact with the world in production. In traditional ML model lifecycles, this is done with the help of well-established historical benchmarks and metrics. As we will cover in a future post, this is different with LLMs. Instead, teams must understand the use case enough to create realistic tests and adversarial prompts to evaluate the model. They then can use metrics built to quantify essential qualities of the text (such as tone or context) against example responses provided for each prompt. Additionally, teams may choose to qualitatively assess the model’s performance based on their understanding of human language and use cases.Application Schema The final block in our LLM lifecycle diagram is the application schema. Referring to how your LLM is implemented and interacted with in production, it is similar to the productionalization process that ML Engineers and developers go through when traditionally implementing ML models. In practice, this is often built with prompt orchestration, where multiple prompts are chained together. Before getting into application schemas where multiple prompts interact, let’s look at what this would look like for an application with a single user prompt—like a chatbot.  The process of constructing a single prompt is more complicated in production than just taking in the user’s input. One concept of LLMs that is different from traditional ML models is that beyond fine-tuning the model, prompt engineers are able to “fine-tune” users’ inputs to the application at the time of inference with a prompt template. Taken from LangChain Prompt Template article mentioned later in this post.Prompt TemplatePrompt engineers write a prompt template as an additional step of model “fine-tuning.” Traditionally, we think of fine-tuning as adding parametric knowledge to the model. This is the knowledge that a model learns at training time and is stored within the model weights (or parameters). On the other hand, prompt templates work as source knowledge added to the model. This is knowledge added during inference via inputs to the model. They provide additional information on top of users’ input requests during inference. This information typically includes additional background information, context, and rules for model responses. For a deeper dive into prompt templates, check out this article. Prompt Orchestration Prompt orchestration refers to the chaining of prompts together interactively. Some applications, like chatbots, may be simple enough application-wise to work with one core prompt. However, in production, many LLM use cases are actually much more complex. This article does a great job introducing LangChain—a popular prompt orchestrator—by using the metaphor of baking a cake. You can ask a chatbot to provide you with the ingredients for the cake, but that’s not actually very useful for your end goal of a finished cake. Instead, models need to be able to use that prompt in conjunction with other prompts and actions to get the end result they are looking for. We can also see below for a more “real world” example provided by the article for pulling information from generated SQL queries. For those interested, you can also see a real-world example utilizing Arthur written by one of our ML Engineers, who built a chatbot to interact with our documentation. Taken from LangChain Prompt Orchestration article mentioned above.Note: In traditional ML models, there has been a push in the community to recognize data pipelines as the logical unit of ML work, and not the model itself. One thing that I find interesting about LLM models in production comparatively is that it seems there will be a push to view prompt orchestration as the logical unit of LLM work. This would make the chaining together of prompts into practice more similar to that of data engineering orchestrating ETL pipelines. Prompt engineering and application designers will need to spend more time and effort defining how these flowcharts will look and how outputs will be validated and monitored.  User FeedbackMeasuring the performance of generative models in production can be an even greater challenge than the already mentioned challenge of validating the model during development. Teams must navigate practical constraints, such as the infeasibility of scaling human labelers to generate common metrics. One approach that has proven successful for teams is tracking user feedback. This feedback provides valuable insights into how well their model is performing, enabling teams to continuously fine-tune and improve model performance. The specific techniques used will depend on the nature of the feedback and desired outcomes for the specific model. Conclusion & A Look AheadIn conclusion, although LLMs are getting their own fancy suite of new tools and job titles, they are still rooted in the best practices and techniques that the ML community has been using for years. Foundation Models: serve as a jumpstart for teams to develop strong baseline NLP modelsDevelopment: still the same need to fine-tune and evaluate models for specific end-task, even if there are new techniques and job titles Application Schema: process for putting LLMs into production that still needs to be validated and monitored, even if it is reliant on new tools/prompts LLMs are most definitely finding their way into production systems near you—and fast. Hopefully, this was an informative first look into how they fit into the frameworks that teams already use for their traditional ML approaches. We’re busy at Arthur helping folks build with LLMs, so stay tuned for more related content soon.‍--------------------------------------------------------------------------------‍FAQ‍How do LLMs compare with traditional ML models in terms of computational resources and environmental impact?Large Language Models (LLMs) generally require significantly more computational resources for training compared to traditional machine learning models. This is due to their vast number of parameters, extensive datasets, and the complexity of the tasks they perform. For instance, models like GPT-3 have billions of parameters and require substantial amounts of data and processing power to train effectively. This increased computational requirement translates to higher energy consumption and, consequently, a larger environmental impact. The carbon footprint associated with training and operating LLMs is a concern, as it contributes to greenhouse gas emissions. In contrast, traditional ML models, which might focus on more constrained tasks and possess fewer parameters, typically require less computational power, leading to lower energy usage and a smaller environmental footprint. However, efforts are being made to make LLMs more energy-efficient and to reduce their environmental impact through methods such as more efficient hardware, better model design, and by fine-tuning pre-trained models instead of training new ones from scratch.What are the specific challenges in ensuring the ethical use and bias mitigation in LLMs compared to traditional models?The ethical use and bias mitigation in LLMs present unique challenges primarily due to the scale and nature of the data they are trained on. LLMs are trained on vast datasets sourced from the internet, which can contain biased, incorrect, or harmful information. These biases can be amplified and perpetuated by the models, leading to ethical concerns, especially when the models are used in sensitive or impactful contexts. The sheer volume of data makes it difficult to fully audit and clean, resulting in challenges in identifying and mitigating all sources of bias. Additionally, because LLMs generate human-like text, there is a risk of them producing harmful or misleading information that appears credible. This is less of a concern with traditional ML models, which typically perform more narrowly defined tasks and therefore have a more controlled and limited scope for bias introduction and propagation. Addressing these challenges requires ongoing efforts in data curation, model transparency, and the development of robust evaluation frameworks to detect and mitigate biases.How can businesses measure the return on investment (ROI) when implementing LLMs into their operations?Measuring the return on investment (ROI) for businesses implementing LLMs involves assessing both the tangible and intangible benefits against the costs associated with these systems. Tangible benefits can include increased efficiency, reduced operational costs, and enhanced customer satisfaction, which can be measured through metrics such as time saved, reduction in customer service expenses, and improvements in sales or customer retention rates. Intangible benefits might include improved brand reputation, customer experience, and innovation. Costs to consider include not only the direct expenses related to developing, training, and maintaining the LLMs but also indirect costs such as training staff to use the technology and potential risks associated with model biases or errors. Businesses can assess ROI by setting clear objectives before implementation, monitoring performance metrics closely, and adjusting the use of LLMs to align with strategic goals. Regularly reviewing these metrics against the initial investment and operational costs helps in understanding the value LLMs bring to the organization.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10
 2023 Updates to the OWASP API Security Top 10
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct Features2023 Updates to the OWASP API Security Top 10The Open Worldwide Application Security Project (OWASP) is a non-profit community dedicated to improving software security.By:Nori TatsumiApril 12, 2023The Open Worldwide Application Security Project (OWASP) is a non-profit community dedicated to improving software security. Its API Security Top 10 project documents the most common API threats for best practices when creating or assessing APIs. In 2019, the OWASP Foundation released the first version of the API Security Top 10. This year, they’re publishing the next iteration of the list that’s updated for 2023. The 2023 release candidate of the updated list is now available and open to the community for contributions and feedback.As Arthur’s MLOps observability platform is built with an API-first development approach, the OWASP API Security Top 10 is one of the many best practices we incorporate into our software development culture. We are happy to see that the investments we’ve been making in our security features and practices are matching the latest security trends identified by the OWASP community.In the 2023 version of OWASP API Security Top 10, authorization is identified as the #1 biggest challenge of API security. Last year, Arthur did a complete overhaul of our platform’s authorization mechanism that introduced the capability for granular and customizable RBAC. The new RBAC system enforces strict authorization policies, allowing our customers to implement Segregation of Duty models that are tailored to their enterprise.Authentication remains on the 2023 list as the #2 threat. Arthur’s RBAC is backed by the authentication mechanism that’s built based on our Zero Trust principle. It can also adapt your enterprise standards by integrating with your Identity Provider (IdP) to achieve federated identity and single sign-on (SSO). In 2022, Arthur expanded the IdP integration capability by introducing support for OpenID Connect (OIDC) in addition to Security Assertion Markup Language (SAML) protocol. What’s new on the 2023 list is the automated threats from bots and bot-nets. With more sensitive data and business logic exposed via APIs today, they’ve become more profitable. And with the combination of AI and affordable cloud services, bots are more sophisticated, scalable, and dangerous. Earlier this year, Arthur did an assessment of our platform to identify and implement layers of additional protections to mitigate the risk from automated attacks. Various security and resiliency work has been done through this effort both at the application level as well as the infrastructure level for our SaaS environment.The work of reducing API threats is not a one-time effort. Arthur’s security features mentioned in this blog are continuously evaluated and improved. The OWASP API Security project creates a standard awareness for API security that should be applied to your software development continuously. At Arthur, we have a framework to exercise education on security practices, threat modeling, secure design, penetration testing, code analysis, and other security related activities through the Software Development Lifecycle (SDLC), the Continuous Integration and Continuous Delivery (CICD), and the System and Organization Controls (SOC 2). In this framework, projects like the OWASP API Security are helpful for us to identify what security work we should prioritize. We are grateful to OWASP and other organizations alike for helping the MLOps community build safe innovations.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/ask-arthur-episode-1-introduction
 Ask Arthur, Episode 1: Introduction
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsAsk Arthur, Episode 1: IntroductionBy:Max CembalestMarch 27, 2023This is a multi-episode series of blog posts discussing one of Arthur’s internal use cases for language models. It is our intention for this post to be general enough to be adapted with minimal effort in your company/lab.A new generation of applications is emerging. These applications use Large Language Models (LLMs) via APIs from OpenAI and Cohere to accomplish tasks via a “natural language interface” (NLI). With an NLI, a question or procedure is posed in ordinary written language, and an LLM or a series of LLMs carries out steps to respond. NLIs are not new—Google Search is a familiar example of an NLI that has been around for years. However, many use cases and toolkits are now emerging to use LLMs to wrap more common business tasks in an NLI. At Arthur, we were looking for a project to explore emerging use cases and toolkits for LLMs and NLIs, which brought us to LangChain. This toolkit is a fast-growing open source library for building LLM-powered applications with much of the prompting, routing, and other intermediate steps handled by the components of the LangChain library itself.In addition, we felt that enabling easy sharing, interactivity, and feedback collection would be important for us to study the human experience of using LLMs, which brought us to Gradio. This easy-to-use library, acquired by HuggingFace, enables the creation of user interfaces to interact directly with machine learning models. If you make a Gradio app for your project, you can get a shareable link that will allow you to send your custom UI to people for testing and feedback collection—without them having to have Python or any code on their computer.In this first episode of Ask Arthur, we are going to walk through the construction of our prototype for Ask Arthur using only LangChain, Gradio, and native Python packages—no other machine learning libraries like PyTorch are needed to directly use when writing our code! In future episodes, we will dive into the many design choices to consider when creating an LLM-powered application: evaluating hyperparameter choices for embeddings, testing new LLM prompts, balancing performance with cost when choosing between different APIs, and more. This is only the beginning!LangChain Tutorial“Ask Arthur” is a chatbot for answering questions and citing sources from documentation. We can define all the components and steps of the chatbot with LangChain.The flow of the application displayed in the image above is inspired by this LangChain blog post about their chatbot design for their own documentation. The chatbot processes the user’s question in the context of the chat so far, finds a relevant page from the Arthur docs, and then uses the docs page to construct a response. In order to find a relevant page from the documentation, we need to have preprocessed the docs into embeddings, which LangChain supports with document parsers and integrations with databases and VectorStores to save/load/search embeddings.An LLM from OpenAI’s API (wrapped by LangChain) is called twice when a user types in a new message into the chat, each with its own prompt and purpose. The user will only ever directly interact with the output of the second call to an LLM, the “Chat response generator,” which creates the written response that gets output into the chat box. This chat response will typically be a summary/rephrasing of information contained on a docs page. In contrast, the first call to an LLM will not output text to a user. Instead, it synthesizes the chat history with the user’s most recent message and the chat history to generate an intermediate question which is used to a) find the most relevant chunk of text from the docs, and b) provide the context for the chat response generator.Prompt TemplatesThe prompt you want for the LLM will change depending on the exact task at hand. In this particular case, the prompt we use to get an LLM to synthesize new messages with the chat history is different from the prompt we use to construct a written response for the user.Here we define the two prompts we use with a PromptTemplate. The first is the default provided by LangChain, and the second is one we have customized with a relevant example.Preprocessing Docs into Embeddings
We preprocess the docs into a vectorstore of embeddings with this function ingest_docs, which we only need to run once. Each chunk of text (1,000 characters long with some slight overlap between successive chunks) will get its own embedding. These embeddings are stored in a vectorstore, which we can load at inference time when it is time to find relevant docs pages.
Define LangChain AgentNext, we define a function that takes an API key and returns our LangChain agent (which will make it easy to integrate this function into our Gradio interface). We temporarily store the API key as an environment variable, which the LangChain uses to connect to the OpenAI endpoint. We use the ChatVectorDBChain, which takes as input our docs vectorstore, the chat synthesis LLM, the chat response LLM, callbacks to handle streaming text, and the parameter to enable returning source documentation.To chat with an agent returned by this function, pass in the user’s input and the chat history into the agent’s input dict: agent({"question":"Type in a Q", "chat_history":history})["answer"]Gradio TutorialFor testing our chatbot, we create an interactive shareable user interface with Gradio.Once a user registers their API key and enters messages into the chat, the LangChain agent will reply with a written message and its source from the documentation. The LLM-generated response to the user’s question is in a chat window on the left, and the source page from the documentation is displayed on the right, along with a URL to that same page on the Arthur website. Additionally, in the bottom left, we provide a set of options for a user to give feedback.UI Components and LayoutGradio layout is defined by organizing components into rows and columns. In our left column are markdown components for the title and instructions, a chat window, a Send button, example user inputs, and a feedback button. In our right column is a textbox to enter the user’s API key, a Register API Key button, and markdown components for the source doc URL and markdown text.
Each button first has to be defined in its position in the layout as described above, and needs to have its functionality connected to a method that is called when the user presses the button. The Register API key button will call the get_langchain_agent() function, the Send button will call the chat function, and the Submit Feedback button will call the log_inference function.
Here is the function to launch the Gradio demo, which you can modify to include your own title, instructions, and example inputs. The first section places the components in their proper place in the layout, and the second section attaches functionality to components. Below this function, we define the helper functions it calls.Streaming the Agent OutputThis is the function that calls our LangChain agent with a new message from the user (and chat history). We get both the chat response and source document name from agent result. We then convert the source document name into a valid URL to our docs and its corresponding markdown text with a helper function. We then yield the chat response (as well as the source doc and link) as a generator for streaming text.Helper function for parsing our LangChain agent’s output into its source text and corresponding URL:Save Inferences and FeedbackThis format may change in the future, but for now we simply record two columns: the chat history (string) and the feedback (integer). We parse the chat history into a single string of alternating input<>, output<>, input<>, output<>, etc.For now, we save this data to a local file, with one feedback submission getting entered one at a time as a new row in a CSV. In a future episode, we will integrate it with Arthur for proper model monitoring!Sharing the Gradio Demo
A script to launch a Gradio demo needs a launch function that creates a demo object, lays out each item in the UI, assigns functionality to each item, and ends with demo.launch(). When you want the demo to be shareable via a URL, you only pass in an extra parameter to the end of the launch function: demo.launch(share=True)
Now when we run python launch_ask_arthur_gradio.py --share=True from the command line, we can share the demo to users with a link generated by Gradio to the server where they are hosting our application and the files it depends on. This has allowed us to easily send the demo around to our team at Arthur for them to try out the model and see for themselves what it can and cannot do!
Episode 2 and Beyond...Want to give Ask Arthur a whirl? Check it out here.‍In the next Ask Arthur episode, we will dive deep into evaluating different options for our LLM prompts—stay tuned and keep looking out for changes in this quickly evolving space!PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms
 The Thinking We Haven’t Done on LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsThe Thinking We Haven’t Done on LLMsBy:Teresa DattaMarch 14, 2023You probably haven’t heard of human-centered evaluation of LLMs, and that needs to change. Human-centered work seeks to understand how real humans interact with technology, so that we can understand how humans (with all of their cognitive biases and quirks) interact with LLMs, and how these models affect individual human decision-making. This work was accepted to the CHI 2023 workshop on Generative AI and HCI. To read our full paper, please visit here.What are LLMs? In the past year, Large Language Models (LLMs) have exploded in popularity—from research, to industry, to public awareness and accessibility. For instance, ChatGPT set historic records for its customer growth, with over 100 million users in its first two months. These models predict next tokens (character, word, or string) based on past context, generating free-form text for a variety of tasks in almost any specified style. This means that people can repeatedly integrate LLMs into their daily lives—to decide what to eat for breakfast, to write the responses to emails left unanswered from yesterday, to develop the sales pitch they have to present mid-morning, to generate a funny joke during a break from work, etc. A variety of concerning issues of LLMs have already been identified, such as biased, toxic, or hallucinated outputs, but these largely only reflect distributional or instance-wise properties of their outputs. The potential ubiquity of this tool means that we need to consider how humans will actually interact with and use this new technology, while also acknowledging that we are all prone to cognitive biases and other quirks. This area of research is referred to as human-centered evaluation, and it has not yet been thoroughly explored for LLMs. Human-centered evaluation is, however, already popular in the Explainable AI (XAI) community.What is Explainable AI? Defining explainability for ML models is a subject of ongoing discussion. For the purposes of our discussion, we will focus on the most common type of model transparency seen in industry: post-hoc explanations of black box models. These often rely on only a trained model’s inputs and outputs to identify patterns in how models make decisions. These methods aim to unlock transparency in order to allow stakeholders to understand the decision-making process of models to improve trust and mitigate downstream harms. Arthur’s explainability features offer a variety of explanation options including counterfactual explanations (understanding how a model’s predictions would have changed given a hypothetical, what-if scenario) and popular methods such as LIME and SHAP. There are a variety of quantitative metrics for evaluating XAI—these are mostly scores that calculate certain ideal properties of explanations (for more information, see this piece), but perhaps more importantly, there are a variety of qualitative evaluation considerations. These are important for two main reasons.What makes Explainable AI (and LLMs) unique?Qualitative evaluation is important for XAI and LLMs because they are distinct from the classic ML paradigm for two key reasons:1. There is no ground truth.There is no exactly correct explanation for a black box system. LLMs are inherently open-ended systems that don’t have a ground truth output for each input.2. In practice, XAI and LLM outputs are often actually meant for some downstream decision or task.Practitioners often use XAI as an assistive tool for model debugging, generating hypotheses, and ensuring compliance. LLM outputs are often a tool to help you decide—what email to send to your client, what quick summary of an important document you will read, what answer is provided for a pertinent question, etc. This means that the context of XAI and LLM use involves some human using an explainer/LLM as a piece of evidence to make some decision. Thus, we need to consider how practitioners use, receive, and comprehend outputted AI. Especially because humans are susceptible to cognitive biases when processing information and making decisions.What does qualitative, human-centered evaluation look like in practice?There are three areas of focus to consider.1. Understanding Users’ Mental ModelsA user’s mental model, a term coined by Don Norman, of a technology is their internal understanding of how a technology works. For instance, maybe your mental model of a crosswalk is that pushing the crosswalk button will cause the walk signal to appear more quickly. However, for many cities, that button does not actually do anything. This is an example of a user's mental model not aligning with a technology's true model. People rely heavily on their mental models of technology to decide when to use the technology, to evaluate how much to trust the outputted results, and to make sense of any results (see, e.g. Cabrera et al. 2023, He & Gadiraju 2022, Kaur et al. 2022). These personal mental models are formulated from a user's perceptions and interactions with the technology and how they believe the system works. While ML practitioners may have had access to specialized training on how LLMs work, this is not the case for the vast majority of the general population. We cannot assume that everybody will have the same understanding of how a technology works as we do. To our knowledge, no work has explored the mental models the general public holds for LLMs. How a general user believes an LLM to work may be very different from how it actually works, and this mismatch can be dangerous. It is not difficult to imagine frightening scenarios where users anthropomorphize or deify an LLM chatbot, understanding it to be a "magical'' source of ground truth. This could very quickly lead to conspiracy theories and the legitimization of disinformation campaigns. It is important to consider if this is an issue of messaging and education—informing the public via AI literacy—or of regulators—to implement policies that force the algorithm providers to provide accurate, comprehensible warning labels on the limitations of their technology.2. Evaluating Use Case UtilityAs previously discussed, XAI and LLMs are often tools for accomplishing some other goal. The term use case in the XAI literature refers to a specific usage scenario and its associated downstream task or end goal. It has been found in the XAI literature that although it might be easy to assume that an explanation will be helpful for a user accomplishing a task like model debugging or model understanding, this is not necessarily the case. When the performance of that downstream task is measured, the presence of explanations can sometimes have no effect, or can even have a negative effect on performance, especially if the XAI is faulty (see, e.g. Jacobs et al. 2021, Adebayo et al. 2020). Very limited work has explored the utility of LLMs in use case–specified user studies, but a user study on Microsoft/GitHub's Copilot, an LLM-based code generation tool, found that it “did not necessarily improve the task completion time or success rate.” Basically, we want to understand if the AI assistive tool is actually helpful for successfully accomplishing the end goal.3. Acknowledging Cognitive EngagementCognitive effort is a form of labor, and unsurprisingly, people tend to favor less demanding forms of cognition and other mental shortcuts. As an example, when asked to "agree'' to a user agreement when signing up for a new platform, you are probably more likely to check the box than to cognitively engage with the language of the agreement. Unfortunately, this human tendency can lead to unintended or dangerous outcomes because humans are susceptible to a wide variety of cognitive biases. For XAI, this manifests as practitioners only superficially examining explanations instead of digging deeply, leading to over-trust, misuse, and a lack of accurate understanding of the outputs. This can be dangerous when it results in the over-confident deployment of a faulty or biased model. Issues of cognitive engagement should be held front and center for researchers of LLMs. Because of their massive scale and public accessibility, LLMs may quickly become ubiquitous in all aspects of daily life. Realistically, how much will users actually cognitively engage with the magnitude of generated outputs to ensure that they are correct and aligned with their intentions? Consider an LLM-generated email: how often and how deeply will a user review that generated email before sending it? What if it's not just one email, what if it's every email? Will they always catch when the generated output says something incorrect, or worse, inappropriate? Furthermore, our attention spans have decreased dramatically with the increase in digital stimulation. Another aspect of concern is that LLM outputs often sound very confident, even if what they are saying is completely false. When the user inquires about the incorrectness, they also have a documented tendency to argue that the user is wrong and that their response is correct. (In fact, some have called LLMs "mansplaining as a service.") This can make it more difficult for humans to implement cognitive checks on LLM outputs. Why is this important?The scale of the reach of LLMs is massive, and so the consequences of not having a qualitative understanding of the utility of their outputs are grave. Beyond the environmental and computational costs of such models, there are social consequences that are entirely unknown from the offloading of our cognitive load onto these agents.We need to understand how users make decisions about whether to utilize the outputs of LLMs, the mental models that users have of these technologies, whether LLM outputs are actually helpful in downstream tasks, and how much users cognitively engage with the outputs to verify their correctness and lack of harm. It is dangerous to continue developing and making available larger and larger language models without a proper understanding of how humans will (or will not) cognitively engage with their outputs. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services
 Announcing Our Strategic Partnership with Amazon Web Services
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesAnnouncing Our Strategic Partnership with Amazon Web ServicesBy:Arthur TeamMarch 7, 2023AI is the top priority for companies in 2023. The innovators are becoming truly AI-native and will lead their industries for decades. Performance, accuracy, and observability are the core ingredients for success. At Arthur, our mission is making AI better for everyone, from the companies running models to drive their business to the consumers interacting with the applications they power.Today, we are excited to announce a comprehensive strategic partnership with Amazon Web Services. This partnership will allow us to bring our class-leading model monitoring to even more teams building transformative AI systems on AWS. AWS customers worldwide are now able to purchase and deploy our platform directly from the Marketplace, and will receive access to the full Arthur offering, including:A centralized dashboard for all models, no matter how or where they are deployedExclusive capabilities in CV, NLP, bias mitigation, and other critical areasThe ability to monitor, measure, and optimize AI performance at scale across accuracy, data drift, explainability, and fairnessUnique, actionable insights that provide visibility into your model performanceHighly scalable microservices architecture that can ingest up to 1M transactions per secondPurchasing Arthur through the Marketplace significantly streamlines procurement and billing, providing a seamless purchasing experience. Users looking to access the Arthur platform can use their existing AWS accounts and cloud credits to buy directly or work with our team to create a custom plan.‍Visit our AWS Marketplace listing here to get started. If you are not an AWS customer, request a demo or contact our team to learn more about Arthur.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious
 Reflections on SaTML 2023: We Should Be More Cautious
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationReflections on SaTML 2023: We Should Be More CautiousBy:Daniel NissaniFebruary 28, 2023Earlier this month, our research team attended the inaugural IEEE Conference on Secure and Trustworthy Machine Learning (or SaTML for short), where we presented our paper, Tensions Between the Proxies of Human Values in AI. This is a first-of-its-kind conference, focusing on traditional responsible AI topics like fairness and explainability, while also welcoming work on robustness and adversarial machine learning. Many industry practitioners attended, such as those from Nvidia and Intuit, as well as more industry-focused academics, giving this conference a more practical feel without being dominated by Big Tech companies.In this blog post, we reflect on the major theme from SaTML: we need to be careful about how we deploy and utilize machine learning solutions. We’ll highlight talks that particularly resonated with us and made us think about how to be more cautious, but note that most of the talks and the air of conversation were around this idea. As a community, we need to be more careful about how we do our research, how we deploy models, and how we evaluate consequences.Patrick Altmeyer gave a compelling talk on his team’s paper, Endogenous Macrodynamics in Algorithmic Recourse. In their paper, Altmeyer et al. come up with a simulation, where a bank may use a counterfactual explanation method to perform some sort of algorithmic recourse on an algorithm that determines whether someone gets a loan or not. This would cause a domain shift—several individuals will go over the decision boundary and get loans. But Altmeyer et al. take this a step further. What if now you train that algorithm with those data points that shifted the boundary? Then you would get a model shift—the model’s decision boundary would move to accommodate more of the data points. Altmeyer illustrates in his talk that as this feedback loop continues, the model’s accuracy continues to decline. For the bank, however, this decline translates to risk of individual’s defaulting on loans. Thus, the question is, who should take on that risk?Altmeyer et al. offer a regularizer that takes into account the external cost (in this case, the uncertainty that would be developed) to better the algorithmic recourse, but I think this hits something deeper. As we continue to develop new explanation techniques, such as our FastCFE algorithm, should we be concerned with the feedback loops they may generate? This question is something that has been explored before in the fairness space, starting with Delayed Impact of Fair Machine Learning, which has shown similar results.We even discuss this in our Tensions paper, where we argue that the lack of contextual information combined with domain-agnostic definitions of our human values is inherently flawed. Without taking the context into consideration, we cannot appropriately evaluate the consequences of our deployments. This especially extends to the feedback loops generated, something most research does not take into account.Anna Kawakami presented her paper, led by Amanda Coston, SoK: A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms. Coston et al. argue that the alignment of our decision making algorithms and the real world is one of the issues to watch, and in my opinion this is even a bigger issue. They even cite that many states in the U.S. have attempted to deploy machine learning algorithms to help, and decide to retire them soon after because they aren’t seeing the results they want. This is similar to research done by Virginia Eubanks, where well-meaning organizations have deployed models that actively harm the populations they are trying to serve. Recent research endeavors have even started exploring the mismatch between optimization procedures in classification models, primarily focused on predictive performance, and the actual decision making tasks they are deployed to do, arguing that this mismatch leads to illegitimate deployments that should have never happened.In my opinion, this is one of the root issues of feedback loops. If we don’t have proper proxies, understandings of those proxies, and the contexts they will be utilized in, how can we hope to create models that are useful? I use the term useful here intentionally because, as has been famously said many times, “all models are bad, but some are useful.” The same goes with proxy variables.I think the unasked question at SaTML, and honestly at most technology-oriented conferences, is: should we build these technologies? Timnit Gebru tackled this question directly during her keynote speech with a resounding no, pointing a finger directly towards those who think we should even try to achieve AGI. Because, as Gebru put it, even giving the perception that a form of AGI has been achieved can be harmful to us as humans. Although models like ChatGPT have achieved “human-like” performance, Jacob Steinhardt’s talk showed how aligning to what we think is human can be flawed, and those flaws are starting to show (thinking about you, romantic Bing).SaTML began with Zico Kolter doing a retrospective on the past 5 years of adversarial machine learning. His talk was both sobering and informative, but I want to focus on one aspect of it. Kolter presented an argument that he has heard many times: we need to redefine what we mean by robust machine learning. Instead of defining robustness as an ability to handle worst-case scenarios, we should define it as an ability to handle distribution shifts, generalizability, and other things that practitioners care about. But then he shows the proverbial picture of a pig that got classified as an airplane, and tries to convince the audience that because technology cannot do this very basic task of identifying an object, even with an adversarial attack, then we should try to make sure it can do this task. I want to push back on this. Do we really want technology to mimic humans this much? Is there positive use in technology being able to do, quite literally, everything that makes us human? Or should we restrict ourselves to technologies that serve specific, meaningful purposes that actually improve our lives and outcomes? My opinion is that if we want to create responsible AI, we should be intentional about our needs and build specific technologies around them.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023
 CDAOs, Prove Your Value: The New Reality in 2023
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedBest PracticesCDAOs, Prove Your Value: The New Reality in 2023By: Adam WenchelFebruary 16, 2023At Arthur, we work with many of the largest and best-run companies in the world, and one thing has become very clear with the 2023 budget cycle: you need to show measurable results to justify spend right now or else risk your budget being flatlined or slashed.CFOs are tightening the belt. Getting any new spend, or growing your budget, takes a rock solid business case.This is extremely hard with frontier technologies, in particular AI. The last few years have seen AI prioritized as an investment area because the potential is massive.2023 is the year when CDAOs and AI teams will need to show they can transition from potential to realized gains.Building that clear business case means having ready answers to questions like:“How are my models currently performing?"“What kind of lift can I expect from updating a critical suite of models driving a high-leverage use case?”“What effect will it have on my business KPIs like P&L, churn, TLV, and ROI?” Building the ability to measure and evaluate AI performance into the foundation of your project portfolio makes it possible to understand the answer to these questions. Showing up to planning and budget conversations armed with these critical details will be a big advantage navigating the next couple of years and beyond. Ultimately, measurable results mean you can make strategic, rational, smart decisions and focus your resources on the initiatives that are driving the most value.It’s clear that the hype around AI is warranted—being able to empirically demonstrate that will be the game-changing trend for CDAOs in 2023.Learn more about Arthur's monitoring and measurement capabilities here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone
 Keep the Lights On: Making Deployed AI/ML Better for Everyone
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringKeep the Lights On: Making Deployed AI/ML Better for EveryoneBy:John DickersonFebruary 14, 2023It’s easier than ever to build and deploy ML models. Storage is cheap, compute is cheap, pre-trained models are prevalent and, did I mention, cheap! As developers and practitioners, we’ve felt the pressure to deliver models into production to provide analytics for internal processes all the way up to running decision-making for mission-critical problems. Our job is, in a nutshell, to create a model that performs well right now, according to one or more downstream KPI-related metrics. That’s generally doable—we handle data identification, ETL pipelines (and their variants), model training, verification, and beyond. A model is trained that exceeds expectations and is deployed. But what happens after that?No matter how clean the input data, no matter how well-trained the model, it’s known that a model posted to production will degrade with respect to downstream metrics. Input distributions will shift (think COVID-19 impacting restaurant seating, hurricanes rolling through wedding destinations, or sudden demand spikes due to viral marketing). Furthermore, those dynamics, relative to downstream business metrics, may unduly impact particular subgroups due to latent legal, demographic, or political shifts. A “perfect” deployed model today is not a perfect model tomorrow, or in three months, or in a year. It’s important to keep the future in mind when deploying now, and to understand that model deployment is not the end of the model lifecycle. More bluntly: once the thing is properly built, we need to make sure it stays good.Even more bluntly: model monitoring—production monitoring—shouldn’t be walled off behind an enterprise sales team, held outside of the developer-first MLOps pipeline, but instead should be easily accessible by all practitioners. That’s something we at Arthur want to, and can, enable. Over the last few years, we’ve built the world’s strongest model monitoring solution, battle-tested by some of the world’s largest enterprise clients. We’re looking forward to spending more time focusing on the developer community, working closely with practitioners to learn and shape what the most effective model monitoring solution should be.Story Time: Making Sure Organs Go to the Right PlaceCareer data scientists, machine learning practitioners, machine learning scientists, statisticians, business analysts—with a wide variety of application areas, and a global scope for applications, we’re all being asked to build and deploy models. Arthur is a product built by engineers who needed a product like Arthur in their past jobs, and need a product like Arthur for their side projects. In my own career wearing many of those hats,[1] I’ve built and deployed models for bundled advertising campaign pricing, enablement of Indian election prediction markets, global blood donation recommendation systems, international drug interdiction allocation and efficacy estimation, television advertising allocation, and organ donation, to name a few. I’ll lean on that experience for a little “monitoring matters” wisdom below, after a quick story.In organ exchange, patients in need of an organ enter an organized barter market to find a willing, compatible donor. Organized kidney exchange has existed for two decades, and I’ve been heavily involved in that process for 13+ years, with large exchanges running code I wrote to match patients to donors, and organizational committees using my code to provide “what-if” analyses during policymaking decisions. Time and time again, it’s been made clear to me that deploying a computationally “optimal” approach to clearing these exchanges, then letting that code run day after day, is not sufficient. Value judgments are made, medical technology improves, supply increases or decreases, the legal landscape shifts—what worked well yesterday may not work well today. In short, we write code to solve a problem based on a model of the real world at a given point in time; that model is a noisy proxy for what actually matters, and what matters changes over time. In practice, an “optimal approach” is deployed, but:The model is uncertain. The inputs are noisy to begin with. Problems include missing variables, missing constraints, improperly set weights and costs, and beyond. Is a particular transplant center reliable? Does a particular social variable correlate with likelihood to donate?The model is brittle to shifts in the underlying environment. For example, during COVID, living organ donation rates dropped due to fear of entering a hospital and/or capacity constraints at transplant centers. How does that impact transplantation rates? And, if COVID impacted particular populations more than others, how does that impact metrics for fairness and bias in organ allocation? Any allocative model will disparately treat specific subpopulations, and measuring and monitoring for that is imperative for downstream policymaking.The model is poorly understood by stakeholders. Visualizing complex statistics is hard. But, we use machine learning models to address problems that are hard for humans to understand. So, it’s important to communicate results to end stakeholders (in the organ exchange case, doctors, patients, donors, lawyers, etc.). Those statistics change over time, as the world changes. Communicating that change in a comprehensible way matters.Certain demographics are systematically mistreated by the model. When we train models, we typically aim to maximize/minimize a specific objective function. That function may maximize utility or welfare for the many at the cost of utility or welfare for the few. This plays out in healthcare, including organ exchange, frequently; and, this can change drastically as the underlying political, legal, or demographic landscape shifts—regardless of the model that was trained and deployed.In my experience, these general concerns arise in most application areas, not just organ exchange. Pricing advertisements depends on underlying social trends as well as external demand for correlated inventory. Drug interdiction success rates correlate with weather as well as USCIS/CPB patrol policies. Worldwide blood donation efficacy correlates with national and WHO policy. I’m sure you can think of examples from your own past or present, too. That’s part of our motivation behind building Arthur—creating a scalable platform for solving general problems in model performance across industries.Monitoring Models with ArthurWe built Arthur to monitor models in production, and to aid in the model verification process. Our enterprise clients—across banking, healthcare, agriculture, logistics, news, and beyond—have all felt that pain of unobserved model deployment, directly via revenue loss or indirectly via damage to their brand. We are continuing to translate that technology—distribution and concept drift detection and connection to downstream KPIs, bias and fairness definition, detection, and mitigation, and model explainability across the board—to the individual developer and team of developers. Our platform already handles structured and unstructured data, and we have an exciting roadmap over the coming year expanding in our core strengths like computer vision and NLP connected to foundation models, robust approaches to measuring all of the metrics, and effortless scaling as our clients’ needs grow.We look forward to continuing to partner with the MLOps community! If you’re in Austin next week (Feb 21-23), come find us at the Data Science Salon Austin—we’re going to be sharing some exciting things we’ve been building to tackle these very issues.‍FOOTNOTES[1] All of these examples are independent from my time building Arthur! We encounter the same style of (broad, reaching) problem at Arthur, and I am happy to dive into those details over a coffee or beer. My goal with this post is to identify with the reader as an ML practitioner, not necessarily a C-suite executive.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism
 Gartner Recognizes Arthur in 2023 Market Guide for AI Trust, Risk, and Security Management
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesGartner Recognizes Arthur in 2023 Market Guide for AI Trust, Risk, and Security Management (TRiSM)Gartner has included Arthur as a Representative Explainability/Model Monitoring Vendor in its AI Trust, Risk, and Security Management (TRiSM) Market. By:Arthur TeamJanuary 26, 2023In addition to having recognized Arthur as a Cool Vendor™ in AI Governance and Responsible AI as well as in its Hype Cycle™ Report for Data and Analytics Governance, Gartner has included Arthur as a Representative Explainability/Model Monitoring Vendor in its AI Trust, Risk, and Security Management (TRiSM) Market. ‍What Is AI TRiSM?AI TRiSM is a framework that ensures AI model governance, trustworthiness, fairness, reliability, security, and data protection. Its four main categories are: model explainability and model monitoring, privacy, ModelOps, and AI application security. Together, use of solutions from these four categories helps data and analytics leaders and their colleagues implement AI-specific trust, risk, and security management measures.Why Is AI TRiSM Important?In industries ranging from healthcare to financial services to retail and beyond, enterprises are leveraging AI in nearly all facets of their business. The breadth and complexity of AI use cases is only continuing to grow, which increases potential exposure and vulnerability if not implemented with a framework like this in place. Failure in these cases could mean millions in financial, legal, or reputational losses.“By 2026, organizations that operationalize AI transparency, trust, and security will see their AI models achieve a 50% improvement in terms of adoption, business goals, and user acceptance,” states the guide. “AI brings new trust, risk, and security management challenges that conventional controls do not address. Data and analytics leaders must use the capabilities described in this guide to improve model reliability, trustworthiness, fairness, privacy, and security.”How Arthur Aligns with AI TRiSMArthur helps enterprise teams monitor, measure, and optimize AI performance at scale across three key areas: accuracy, explainability, and fairness. This allows organizations to understand how their models are making decisions, ensure compliance in their ML systems, and continually improve the fairness of model outcomes by proactively monitoring for bias. Production AI and business value are inextricably linked in today’s world—and will continue to be in tomorrow’s. As the leading model monitoring and governance solution in the market, Arthur is deployed at leading Fortune 100 enterprises across industries to accelerate business transformation through AI and drive better, more responsible results. At Arthur, our mission is to make AI better for everyone, and we’re proud to be recognized for doing so.If you are a Gartner client, you can access the full market guide here. Learn more about Arthur and sign up for a demo here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance
 Arthur Achieves SOC 2® Type II Certification & Compliance
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur Achieves SOC 2® Type II Certification & ComplianceBy:Arthur TeamJanuary 18, 2023Today, we’re excited to announce that Arthur has completed our Systems and Organization Controls (SOC 2®) Type II certification. SOC 2 has long been regarded as the gold standard for SaaS security compliance and is a critical enterprise milestone in upholding the trust service principles of the American Institute of Certified Public Accountants (AICPA). As a model monitoring company, security is important to everything we do at Arthur. Our platform is constantly measuring, processing, and analyzing machine learning models to deliver better results for our customers—and working with customers in highly regulated industries only heightens the importance of security compliance. Arthur’s successful completion of both SOC 2 Type I and Type II audits in 2022 means that our company has consistently maintained infrastructure service processes that meet the required levels of oversight and monitoring so that we can proactively identify and address any unusual activity. Additionally, it demonstrates our commitment to meeting the industry’s most rigorous data security, availability, and confidentiality standards. “From day one, Arthur’s focus has been to develop a robust model monitoring solution that adheres to best-in-class security and data privacy controls, in order to deliver a trustworthy experience for our enterprise customers,” said Arthur’s VP of Engineering George Chitouras.As we continue to scale solutions in financial services, healthcare, insurance and more, worldwide trust in our infrastructure, software, people, data, and procedures is paramount for scaling model ops in the cloud. ‍Want to learn more about our commitment to security? Get in touch.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list
 Arthur Earns Placements on Built In’s 2023 Best Places to Work List
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur Earns Placements on Built In’s 2023 Best Places to Work ListBy:Arthur TeamJanuary 11, 2023Today, Built In announced that Arthur was honored in its Best Places to Work Awards for the second year in a row. Specifically, Arthur earned a place on New York City Best Startups to Work For, New York City Best Places to Work, and U.S. Best Startups to Work For. The annual awards program includes companies of all sizes, from startups to those in the enterprise, and honors both remote-first employers as well as companies in large tech markets across the country.“We’re thrilled to be recognized by Built In alongside the top companies in New York City and beyond,” says Adam Wenchel, co-founder and CEO of Arthur. “New York has grown into a world-class enterprise tech ecosystem over the past decade, and we’re proud to be part of the community and call it our home base. I’ve been so impressed by our team’s continued ability to integrate the sentiments at the core of our company mission—make AI work for everyone—into a welcoming and equally mission-driven company culture. We nearly doubled our team in 2022, and we can’t wait to welcome even more Arthurians in 2023!” At Arthur, we are fully committed to not only fulfilling and challenging work that truly makes a difference, but also providing the opportunity to do that work as part of a diverse, creative, and passionate team. We believe our competitive compensation, equitable hiring practices, and comprehensive benefits (including wellness programs, a home office stipend, tuition reimbursement, and more) truly make Arthur an incredible place to work, and it’s exciting to see our company continue to be recognized by Built In. Interested in joining us? Check out our open roles here.“It’s my honor to congratulate this year’s Best Places to Work winners,” says Sheridan Orr, Chief Marketing Officer, Built In. “These exemplary companies understand their people are their most valuable asset, and they’ve stepped up to meet the modern professional’s new expectations, including the desire to work for companies that deliver purpose, growth, and inclusion. These winners set the stage for a human-centered future of work, and we can’t wait to see that future unfold.”  ABOUT BUILT INBuilt In is creating the largest platform for technology professionals globally. Monthly, millions of the industry’s most in-demand professionals visit the site from across the world. They rely on our platform to stay ahead of tech trends and news, learn skills to accelerate their careers and find opportunities at companies whose values they share. Built In also serves 2,000 customers, innovative companies ranging from startups to those in the Fortune 500. By putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard-to-reach tech professionals.ABOUT BUILT IN’S BEST PLACES TO WORK Built In’s esteemed Best Places to Work Awards, now in its fifth year, honor companies across numerous categories: 100 Best Places to Work, 50 Best Startup Places to Work, 100 Best Midsize Places to Work, 100 Best Large Places to Work and Editor’s Choice: 100 Best Hybrid Places to Work. The program honors companies—remote, hybrid and in-office—with the best total rewards packages across the U.S. and in the following tech hubs: Atlanta, Austin, Boston, Chicago, Colorado, Dallas, Houston, Los Angeles, Miami, New York, San Diego, San Francisco, Seattle and Washington D.C. ABOUT ARTHURArthur is the AI performance company. The Arthur platform monitors, measures, and improves machine learning models to deliver better results, working with enterprise teams to accelerate model operations and optimize for accuracy, explainability, and fairness. Arthur's research-led approach to product development drives exclusive capabilities in enterprise scalability, computer vision, NLP, bias mitigation, and other critical areas. Contacts: press@arthur.aiPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective
 Team Arthur at NeurIPS '22: A Retrospective
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationTeam Arthur at NeurIPS ‘22: A RetrospectiveBy:Arthur TeamDecember 16, 2022The Arthur team is back home in New York after a strong showing at the Conference on Neural Information Processing Systems (a.k.a. NeurIPS), arguably the largest and most influential machine learning conference. We hosted onsite and offsite socials, gave an oral presentation, presented two papers in the main conference as well as papers at two workshops, co-organized a workshop, recruited—and one of us helped compose a song on overfitting, sung live with a pair of ukulele-wielding ML practitioners. NeurIPS holds a special place in Arthur’s history. Back in 2019, we announced our $3.3M seed round at a collocated event to NeurIPS in Vancouver. Now, three years, 40+ team members in growth, and $50+ million dollars later, it was great to return to NeurIPS, this time in New Orleans. Big themes, many of which our team expands upon below, included large language models (LLMs) and their generalization and semi-rebranding as foundation models, cross-collaboration between AI and other fields (psychology, policy, etc.), human-in-the-loop and user-centric ML pipelines, and context-aware ML as it relates to privacy and fairness.NeurIPS is known for being both a venue for the dissemination of new industrial and academic research as well as a networking meetup with a strong event culture. Arthur, ever the responsible community member, also contributed to this latter focus by organizing a couple of well-attended events. On Wednesday night, joint with our partners at Index Ventures, we hosted an offsite happy hour at Jack Rose, with attendance from the investor, founder, big tech, and academic community. The VC and investing community continues to increase its presence at flagship ML conferences, and largely driven by the current excitement surrounding foundation models and generative ML (e.g., Stable Diffusion, ChatGPT), it was great to see this trend continue at NeurIPS. On Thursday night, joint with friends at Abacus.AI, we held an open mic night at the conference venue with a few hundred attendees. Meant to be a free-form community event full of cheeky machine-learning-oriented fun, this was a great success, with GPT-3 generated poetry and live song, debate about the merits of non-tabular data, discussions of who invented social networking, and the evergreen research topic of how to improve peer review (as if it’s broken!). We’re happy to help build a vibrant ML community.Arthur and our friends at Abacus.AI teamed up to host an open mic night!Partnering with Index Ventures, we also hosted an event at the picturesque restaurant Jack Rose.Below, members of our ML team give their takeaways and hot takes on what’s right, what’s wrong, what’s hot, and what’s not in the academic and industrial machine learning world.The Current State of ML Research (It’s Not Just LLMs)Arthur MLE Valentine d’Hauteville writes, “On my first day at NeurIPS, I recount the awe I felt walking amongst the myriad of different posters and research projects in the big hall of the New Orleans Convention Center. The mosaic of ideas, topics, and research stories displayed before me was impressive and a stark contrast to my usual research flow, which consists in exploring and (sometimes) getting a bit lost in the roots of a deep research paper reference tree. In this forest of posters, I felt immersed in the AI community, enthralled and slightly overwhelmed by the countless research minds and ideas present at my literal fingertips. As one of the biggest annual AI research conferences, NeurIPS in some way mirrors the collective interest and brain-space of the ML community’s, displaying both its prominent and upcoming narratives—for instance, as someone anticipated, I witnessed a large enthusiasm and contingent of work on generative models and LLMs.” Arthur MLE Max Cembalest echoes this, stating that “the biggest trend at the conference was an increased study of large language models, their robustness, their generalizability to out-of-domain text, and their generalizability to tasks that are not directly language but approachable by LLMs anyway.”But LLMs were far from the only interesting facet of ML research at NeurIPS. Valentine remembers feeling a “strong energy around the design, development, and nurturing of scientifically sound and usable ML practices, with research outputs spanning from theory to implementation.” “There was also much work on model efficiency—how to reduce the computational requirements for deep learning systems,” Arthur MLE Teresa Datta adds.“Beyond these ever-present areas of research,” says Teresa, “there were two main threads of messaging that struck a chord. The first: developing neural networks which don’t involve backpropagation. Geoffrey Hinton, inspired by the field of neuroscience’s lack of evidence that the brain’s cerebral cortex is able to undergo backpropagation, presented a keynote on a new learning procedure for neural networks that does not involve backpropagation. This forward-forward approach instead replaces a forward+backward pass with two forward passes—one with positive data and one with negative data, meant to imitate the brain’s paradigm of wakefulness and sleep cycles. This is the latest work in the continuing attempts to establish deep learning models as a brain analogue.”The second messaging thread Teresa resonated with was “more purposefully promoting cross-collaboration between AI and other fields (psychologists, policymakers, domain experts, everyday users, neuroscientists, designers, and more). The 2022 NeurIPS keynotes highlighted a variety of figures at the intersection of AI and other fields: Rediet Abebe on perpetuated societal inequalities, Juho Kim on designing interaction-centric AI, Alondra Nelson and her work in the White House Office of Science and Technology Policy, and David Chalmers on the philosophy of sentiency. While “collaboration with other fields” has always been evoked with high import, there were more discussions on how to formalize this: How do we craft incentives for researchers to actually do this difficult and novel work? Graduate students are often chained to publishing goals—getting a certain number of acceptances at high-profile venues. How do we create accolades, publishing forums, and funding support for interdisciplinary work?”Chief Scientist John Dickerson added that “the use of modern ML (e.g., transformer-based models) for “traditional” application areas in operations research such as logistics, planning, routing, assignment, scheduling, and resource allocation has also been increasingly present at ML conferences over the last year or two, and certainly at this recent NeurIPS. Until recently, these “old” application areas—that also happen to drive much of the world’s economy—were viewed as boring and solved by the machine learning world, left to the business analysts and consultants in the INFORMS professional community. Yet, with a touch of domain expertise, modern ML and optimization can be shown to eke out significant gains in efficiency and profit driven in these proven business problems where each percentage point corresponds to hundreds of millions or billions of dollars of economic value. I’m excited to see the continued strengthening of ties between the AI/ML and operations research communities and the problems they tackle (i.e., those with both a prediction and a decisioning element).” (Separately, joint with INFORMS, the ACM, and CCC, we’re co-organizers of a series of workshops in this space, e.g. [1] and [2]. Get in touch if you’d like to participate!) Distribution Shifts & BenchmarksValentine gave the first public presentation of her work, joint with Arthur Research Fellow Naveen Durvasula, on explainability and data drift at the Workshop on Distribution Shifts (DistShift). On the same day as AFCP (more on AFCP below), DistShift attracted a larger crowd—generalization, extrapolation, and robustness to distribution shift are core ML problems, and it’s great to see continuing progress in this fundamental area. It was cool to see Valentine’s work, which ties together clustering, Shapley values, and Skope rules to find emergent clusters of “drifty” points over time, as part of a larger cohort of explainability and data drift research. (We’ll be submitting a full version of this work to one of the January ‘23 conference deadlines, so stay tuned!) Valentine d'Hauteville presents her poster about Characterizing Anomalies via Explainable Classifiers.Also in the space of model performance under distribution shift, Arthur-MLE-turned-Berkeley-PhD-student Jessica Dai and Arthur Research Fellow Michelle Bao presented at the Women in Machine Learning (WiML) workshop on their ongoing work with our team understanding models’ the impact of covariate and concept drift on group fairness, when ground truth labels are not available at test time.Valentine was particularly impressed by Isabelle Guyon’s keynote, The Data-Centric Era: How ML is Becoming an Experimental Science. She writes, “Her talk reminded us that, as a scientific endeavor, ML research should abide by the same rigorous scientific research standards as those that govern research in other disciplines such as the natural sciences. Guyon debunked some bad scientific practices within the ML research community, such as a common one which consists in selecting validation datasets based on their anticipated or observed ability to display the behaviors that will confirm a hypothesis (a form of selection and confirmation bias). To combat such practices, Guyon emphasized the importance of adopting scientifically and statistically sound data curation, experimentation, and validation procedures. For instance, one should ensure that published experiments and findings are reproducible and carefully documented. She also advocated for the adoption of more rigorous vetting processes on existing datasets as well as an increased focus towards developing and documenting more comprehensive benchmarks. In fact, her prescriptions seemed to echo those of the NeurIPS community at large, as the conference recently created a new Datasets & Benchmarks research track which rewards datasets and benchmark papers on an equal footing with other traditional research content.”Human-in-the-Loop MLArthur MLE Daniel Nissani, while researching the Tensions paper (more detail on this paper below), became enamored with the idea that AI systems should somehow encapsulate knowledge about the context of their deployment. “I was happy to see that I wasn’t alone,” he writes. “At the HiLL workshop, Cynthia Rudin did an excellent job explaining how users of AI, those who don’t necessarily have AI skills, but want to benefit from AI systems, have opinions past “which models have the best accuracy.” Drawing from her research on Rashomon sets and sparse decision trees, she asked for a paradigm shift for how we generate models. Instead of asking a user to accept one, heavily optimized model, she wants us to present users with multiple models that achieve similar accuracy scores. She has developed a UI for such discovery processes when searching through different decision trees.”“While listening to Cynthia’s talk, a poster caught my eye on participatory systems, which presented an equally inventive idea,” Daniel continues. “This paper devises a model agnostic scheme that trains various models on different sets of features and protected attributes. This allows users to understand the effect of providing or omitting certain types of information, such as a medical status or gender. Both of these ideas present ways for AI systems to interact with the context of their deployment, allowing for feedback between the system and the user.”Valentine noticed the same theme. She writes, “At NeurIPS, user-centric ML pipelines were also in strong focus. For instance, a couple of presentations I attended intelligently incorporated user feedback as key steps in the design of comprehensive ML solutions. One researcher presented a new explainability pipeline for self-driving cars but had first run user field studies in order to understand the nature of what makes a good explanation for his specific use case. He also resorted to user feedback to comprehensively validate his first design iterations and presented ways to incorporate the feedback into his future designs. Cynthia Rudin, a prominent scholar in the field of explainability presented a clean mathematical proof showing that, contrary to the popular conception, designing complicated and high-capacity models is necessary to obtain peak performance; it is often possible to compute simple, inherently explainable yet suitably performant models for a given task. Her approach echoed Occam's razor (“the simplest solution is almost always the best”) and followed the keynote talk’s footsteps in anchoring ML back to core scientific principles. Rudin also created a clean and innovative UI which allows domain experts to explore and understand a set of generated simple models before selecting a one that is most suitable to their use case.”Fairness & Related Topics“I really enjoyed the Algorithmic Fairness through the Lens of Causality and Privacy (AFCP) workshop, a semi-annual gathering that focuses on the nuances of connecting responsible AI to practice, says Arthur’s Chief Scientist John Dickerson. “This is one of the few but growing communities in “core” machine learning that gives more than lip service to human-centered AI, contextual machine learning, and sociotechnical systems (STS).” Arthur MLE Teresa Datta presented her co-lead-author paper, Tensions Between the Proxies of Human Values in AI, as an Oral at AFCP. The paper was also accepted to HCAI at NeurIPS as well as SaTML. Check out Arthur MLE and co-lead author Daniel Nissani’s blog post on that work here, and Teresa’s talk below.Daniel found the talks on causality to be particularly interesting. “Causality has been in the fairness literature for quite a while now, but one of the biggest bottlenecks is making sure you have a causal model (distilled as a causal graph) that can be used for causal analysis,” he writes. “I was pleasantly surprised to see work directly in this space, where some authors ran experiments to see if causal discovery methods could actually create causal graphs that are effective enough to measure fairness notions. Although their results were promising, the authors plan to construct a causal discovery method specific to fairness notions. If successful, this could create otherwise theoretical works, such as another paper at AFCP describing post-treatment bias in causal fairness analyses, more impactful for real world systems.”“The final highlight for me,” he adds, “was a roundtable discussion at AFCP, where many researchers, whether from the privacy or fairness space, acknowledged the need for more contextual understanding in our research. Emphasis to start researching entire ML systems, eliciting user feedback, and integrating context into our research were the biggest takeaways. It made me feel proud that our team at Arthur presented the Tensions paper at AFCP, since it seems our ideas were not only heard, but preaching to an active choir that wants to start integrating context as well.”“The AFCP workshop gave critical takes on interpretability and explainability in ML, and also touched on the intersection and interactions between forms of privacy and fairness, as well as causality and fairness,” says John. “As in the AI/ML meets OR discussion above, we’re seeing the intersection between traditionally separate areas of focus—statistics, economics, human-computer interaction, machine learning, and others. As productionized ML continues to expand across the economy and our society, these intersections are inevitable and welcome, and I’m happy to see thoughtful workshops like AFCP continue to grow in lockstep.” Valentine reflected profoundly on this topic as well, noting that “it can often be tempting as ML researchers to think of ourselves as scientists working on objective and universal algorithms that can then be adapted and tailored to fit specific use cases. Such conceptions in some sense make us the principal bearer of truth to the detriment of domain experts, and can lead us astray. During an explainability panel I attended, Zach Lipton pointed out that we might for instance benefit from letting domain experts be the ones to first scope out desired design for an ML system and associated explainability mechanisms before resorting to ML Engineers to implement or iterate on it. Such responsibility delegation could ensure designs and solutions are inherently more usable and useful. Furthermore, one must remember that there is no such thing as scientific objectivity and that science and ML at large is value-laden—one of the key calls to action in Teresa’s Tensions paper and awesome presentation. Forgetting or ignoring this reality can lead us to resort to deeply insufficient solutions based on mathematical formulations in our attempt to address problems that are socio-technical in nature, such as fairness.”ConclusionWe arrived back in New York last week feeling full (of knowledge, but also of Cajun food and beignets). And, as is the case with many academic conferences, we were left with equal amounts of questions and answers. Here are just some of the questions we’re looking forward to exploring further in 2023:How can model interpretation and explanation be aligned with context, audience, and sensible baselines?How is the geometry of information informing model design and analysis?How do we create accolades, publishing forums, and funding support for interdisciplinary work?How can we engage communities via interactive AI/ML systems, so that we enable consent, choice, and trust?How do we start doing research about the system that models are deployed in, rather than just the model itself?Are we starting to approach the idea that approximate causal models are enough for real world causal analyses?‍Want to learn more about our MLEs and our commitment to research and development? Check out the R&D page of our website.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed
 How We Are Modeling Our Human Values in Technology Is Inherently Flawed  Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ResearchHow We Are Modeling Our Human Values in Technology Is Inherently FlawedBy:Daniel NissaniDecember 8, 2022Thank you to the original authors of Tensions Between the Proxies of Human Values in AI: Teresa Datta, Daniel Nissani, Max Cembalest, Akash Khanna, Haley Massa, and John Dickerson.IntroWhen creating automated machine learning systems, our belief is that these algorithms should not perpetuate and amplify harms. We want these algorithms to be fair to everyone, regardless of their gender, race, ethnicity, sexuality, etc. Our information should only be used and distributed with our consent. Inevitably, these algorithms will not work as expected, and we deserve an explanation as to why they are not working as they should.We can encapsulate these human values as pillars of privacy, fairness, and explainability. Over the past generation, a lot of work has focused on embedding these values into our technologies. However, as we try to proxy these pillars with technical definitions and algorithmic designs, tensions keep getting discovered within the pillars, between the pillars, and with the real world contexts in which the pillars’ proxies are deployed. So we must ask: Why do we continue to face limiting tensions?Tensions Within PillarsIt is well known in the AI community that popular definitions of fairness, such as demographic parity, equalized odds, and calibration, cannot be implemented in the same machine learning model [1]. Moreover, even when implemented correctly, these fairness definitions may cause more harm than good over time, given the feedback loop an algorithm might create [2].Privacy discussions on the other hand are largely dominated by the notion of differential privacy [3][4], a probabilistic guarantee that one cannot realize that a data point has been removed or replaced in a dataset based on some mechanism. However, differential privacy has been shown to work poorly on outliers [5][6] or on models that have overfit to their training data [7].Explainability has come into question now that black-box and hard-to-interpret models have gained popularity. However, the methods to produce explanations are generally just local approximations of models [8]. Some critics of the field say it is akin to reading tea leaves [9].Tensions Between PillarsA summary of the tensions identified within and between popular value proxies. Incorporating any one of these pillars is a challenge, and incorporating multiple requires handling competing priorities.Much like how popular notions of fairness cannot be implemented in the same model [1], differential privacy and any popular fairness definition cannot be imposed on the same model [10]. In other words, the impossibility of having different notions of fairness in machine learning extends to having any notion of fairness in tandem with differential privacy.The idea of models being more transparent may be in direct competition with the idea that our models are more private. Research has shown that popular explanation techniques, and even new ones such as counterfactual explanations, can make models susceptible to membership inference attacks [11].Explainability should be a useful tool for identifying unfairness, but is not, and can at times hide the unfairness of a model [12]. Moreover, the converse can happen as well, where explanations of a model can actually amplify unfairness against certain subpopulations [13].Tensions in the Real World: A Call for Context-Aware Machine LearningThe 2020 Census used differential privacy for the first time to meet the Privacy Impact Assessment’s requirements. By doing so, reported population counts fluctuated, which protected the privacy of individuals, but potentially lessened the funding small, rural populations would receive from the federal government. As an example, Native American reservations of less than 5,000 people saw decreases in population numbers by 34% on average. This type of error (which is an inherent feature in differential privacy) could result in the loss of funding for a road to a nearby town or a new school [14].In 2021, the Markup found that people of color were denied loans 40-80% more often than white counterparts with similar financial profiles [15]. However, they were heavily criticized by “[t]he American Bankers Association, The Mortgage Bankers Association, The Community Home Lenders Association, and The Credit Union National Association ...saying the public data is not complete enough to draw conclusions, but did not point to any flaws in our computations.” The “incompleteness” of the data is because the Home Mortgage Disclosure Act requires “debts as a percentage of income, how much of the property’s assessed worth the person is asking to borrow,” but not “the applicant’s credit score” because of fears of re-identification attacks.Both of these examples showcase how utilizing formulations of our human values without acknowledging the context can lead to dire consequences. In the case of the Census, although differential privacy offers a guarantee of a certain kind of privacy, it adversely affects certain populations that need privacy and more equitable funding. As a technical notion, differential privacy has no way of knowing the context it is being implemented in. And without any critical structures in place, it can run awry, causing consequences that we outlined above. Similarly, although not necessarily in an algorithmic sense, the notion of auditing the mortgage system requires transparency about the data being used to give out mortgages. Thus, decisions need to be made with contextual knowledge, so that specific entities can have access to the required information.In bioethics and related ethical fields that are more mature than responsible AI, context is incredibly important. Doctors have access to different information depending on their physical context, such as whether they are in a hospital or in their car on the way to the hospital. The ethical concerns around the collection of biometric information are affected by the specific device being used. Context informs how information flows, what information is collected and used, and why certain decisions are made. If we hope to have more ethical machine learning systems, the incorporation of context could be a viable avenue [16].How Can We Address These Issues?In order to alleviate the issues we’ve described, we believe that the whole system should be considered when designing machine learning solutions. The techniques (and laws) described primarily deal with the model: differential privacy inhibits extraction attacks on models, fairness definitions constrain models to output more fair predictions, and explainability techniques are used to explain model outputs. Rather than just designing solutions for the models, we can look at the entire system the model is deployed into and determine the values most appropriate to embed and consider their associated consequences.Although we don’t want to prescribe any solutions to these problems, because this is a nascent research area, there are some theories that we can look to for inspiration. Contextual integrity for privacy [17] is a way to encode the context of a situation, allowing us to understand the privacy requirements of technology. Substantive algorithmic fairness [18] asks us to analyze the structural inequalities present, identify the reforms that could mitigate such inequalities, and consider whether an algorithmic intervention could achieve such a mitigation. For explainability, designing around the needs of a situation and stakeholder’s understanding of transparency can lead to better techniques [19].As we said above, this is not a solved problem. If anything, the idea of incorporating context in automated machine learning systems is itself very new. Thus, we want to leave the reader with questions we are interested in researching:How should information be collected by a contextual system? Collection of data is a hot button issue, so we have to collect data with intent. Leaning on Contextual Integrity for Privacy, we can get inspiration for how we can define context as a set of parameters to collect data.What types of tools need to be developed? Building out frameworks, evaluation suites, and more will be helpful, but we should consider what we need to build to make these systems effective and ethical.How should machine learning systems respond to context? Splitting this up, we should consider what triggers a response to context and how the user should feel the response to context.What aspects of ethical responsibility does each stakeholder carry? The creator of the technology, the person implementing the technology, and the person making decisions with this technology each have a different ethical role to play.How can we design inclusively? We can lean on participatory design principles to help us build these systems for everyone impacted.ConclusionTechnology is inherently value-laden and political [20][21]. It can distribute information in specific ways, thus influencing how we make decisions. Moreover, although technology has the potential to help those in the most need, those who need the most are regularly ignored in the process of designing popular technologies.Arthur’s research team believes that context-aware systems, those able to incorporate knowledge about the specific domain that a machine learning model is situated in, are a potential path to solve some of the issues above, as well as evaluate the consequences of such a system before deployment. Context awareness is a hard problem because it most likely will involve collecting new information about a specific deployment at some point in the model building or productionalization process.‍Citation LinksBelow is a list of links to the citations in this blog post. Note that our paper goes much deeper and has a more proper citation tree.https://arxiv.org/abs/1609.05807https://arxiv.org/abs/1803.04383https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdfhttps://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdfhttps://arxiv.org/abs/1507.06763https://arxiv.org/abs/1910.13427https://arxiv.org/abs/1709.01604https://arxiv.org/abs/2206.01254https://docs.google.com/presentation/d/1bPUE2eD3NIYHYLm_D9njaVWgEgYXGpoEOjcnSaV7ccs/edit#slide=id.phttps://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdfhttps://arxiv.org/abs/1907.00164https://arxiv.org/abs/2205.03295https://arxiv.org/pdf/2106.13346.pdfhttps://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.htmlhttps://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithmshttps://www.nature.com/articles/s41746-018-0075-8https://scholarlypublishingcollective.org/psup/information-policy/article/doi/10.5325/jinfopoli.1.2011.0149/314319/Privacy-in-Context-Technology-Policy-and-thehttps://arxiv.org/abs/2107.04642https://arxiv.org/abs/2101.09824https://web.cs.ucdavis.edu/~rogaway/papers/moral-fn.pdfhttps://arxiv.org/abs/1811.03435PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries
 How AI Is Reshaping the Future of These 4 Industries  Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Monitoring & PerformanceHow AI Is Reshaping the Future of These 4 IndustriesBy:Christina SirabellaNovember 14, 2022In nearly every industry throughout the last few years, the massive impacts of AI and machine learning are undeniable. Organizations are gaining competency in AI/ML and deploying it to achieve goals such as cost reduction, bias mitigation, and so much more.They are also recognizing that mastering AI is a make-or-break for companies that want to be around in the next decade—in fact, a whopping 94% of business leaders in a recent survey said that AI is critical to success. As our CEO Adam Wenchel spoke about at last year’s AI Summit, the enterprises that become AI-native first will be industry leaders for decades.In this blog, we’ll share four industries where machine learning has had a particularly large influence, as well as how the leading organizations within those industries are using it—and more specifically, model monitoring—to stay ahead of the curve.Financial ServicesEven in traditionally conservative spaces like financial services, AI is rapidly changing the business landscape. While there is no enforceable federal AI legislation in the U.S. currently, there is a growing momentum to regulate the financial services industry around biased algorithms and govern black box underwriting by various agencies.Financial services organizations are using AI and machine learning for activities like credit approvals, fraud detection, and customer support. But what happens when bias creeps into an algorithmic model which results in the wrong lending decision being made, impacting millions of customers? Companies must consider the potential business and compliance risks of these technologies.Leading financial institutions are using Arthur to monitor, measure, and improve machine learning models for better results across top industry use cases: fraud/KYC, forecasting models, fair lending, robo-advisory programming, credit worthiness, customer service, and more. Learn more by downloading our financial services whitepaper or watching our on-demand webinar.‍InsuranceNo matter how the economy evolves in the coming years, individuals and companies will continue to need insurance. The growing reality of company layoffs and downsized industries may shrink average coverage amounts, but consumers and businesses trust carriers to bring them peace of mind when faced with economic market uncertainty and climate change volatility.While the insurance industry has typically been a late adopter of technology, that isn’t true with AI—insurance companies are applying CV and NLP technologies across the value chain to improve their own pain points while simultaneously benefiting the customer. According to Deloitte’s 2022 Insurance Industry Outlook report, almost 74% of global respondents said they planned to increase spending on AI-related technologies. AI helps insurers assess risk, detect fraud, and reduce human error in the application process. It also helps customers, who benefit from the streamlined service and claims processing that AI provides. Specific use cases include underwriting, premium forecasting, pricing strategy, and customer servicing.With Arthur, companies are proactively mitigating reputational, regulatory, and strategic/financial risk while saving money and driving business goals.Explore industry use cases by downloading our insurance whitepaper.HealthcareFrom medical imaging analysis to disease prediction to drug discovery and development, AI has already revolutionized the healthcare industry from a technology perspective. Another piece of this puzzle, however, is ensuring health equity—particularly as it pertains to levels of care across underrepresented and minority groups. Additional healthcare use cases include hospital management, predictive insights for patient outcomes, capacity planning, staff training, medical diagnosis bias detection & mitigation, and medical document NLP classification. Arthur helps healthcare organizations avert harmful patient outcomes and reduce operational risk through proactive MLOps monitoring, resulting in early detection of data anomalies and model errors.One of our first customers and the leading AI-enabled healthcare enterprise, Humana, is deploying Arthur to manage mission-critical AI across both clinical and membership use cases. Arthur seamlessly integrates into Humana’s AI tech stack and is a core component of Humana’s approach to responsible and high-performing AI, providing a continuous view into model performance and bias, governance support, and alerting capabilities.Discover how AI drives business impact by downloading our Humana case study.Human ResourcesThe global transition from centralized office workplaces to regular work-from-home arrangements accelerated the adoption of automated AI tools to make HR departments run more efficiently. These tools are being used in areas like
talent acquisition, hiring, performance management, and employee experience. In fact, 99% of Fortune 500 companies rely on the aid of talent-sifting software and 55% of human resources leaders in the U.S. use predictive algorithms to support hiring.1While AI technology yields significant operational benefits, it also introduces risk—and the challenge is balancing the two. Any company using AI systems that analyze protected, special categories or sensitive personal datasets (age, race, gender, ethnicity, etc.) needs to exercise caution and ensure the data and/or algorithms being used are not causing systemic issues of bias and inequity resulting in disparate impact or discrimination.And this is no longer just an issue of morality: Starting in January 2023, companies in New York City will actually be legally restricted from using employment decision tools unless they have been the subject of an independent bias audit, in what is likely just the first of many similar laws that will be passed throughout the country and beyond.Deepen your knowledge by downloading our human resources whitepaper.‍Interested to see how Arthur can help your organization bring high-performing AI into production safely and responsibly? Schedule a demo of our platform.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability
 Shapley Residuals: Measuring the Limitations of Shapley Values for Explainability  Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ExplainabilityShapley Residuals: Measuring the Limitations of Shapley Values for ExplainabilityBy:Max CembalestOctober 31, 2022This post was originally published in Towards Data Science.We will use a cube representation of games to walk through the interpretation and limitations of Shapley values.IntroductionTo use machine learning responsibly, you should try to explain what drives your ML model’s predictions. Many data scientists and machine learning companies are recognizing how important it is to be able to explain, feature-by-feature, how a model is reacting to the inputs it is given. This article will show how Shapley values, one of the most common explainability techniques, can miss important information when explaining a model. Then, we will introduce Shapley residuals, a new technique to measure how well Shapley values are capturing model behavior, along with some code to get started calculating them!Consider the following example from Christopher Molnar’s Interpretable Machine Learning book: a bike-sharing company trains a model to predict the number of bikes taken out on a given day, using features like seasonal info, the day of the week, weather info, etc. Then, if their model is predicting a lower-than-average rider count on some day in the future, they can find out why that lower-than-average score is occurring: by looking at how the model is reacting to each feature. Was it because of a holiday? Was it because of the weather?A common way of computing the importance of each of your model’s features is to use Shapley values, since it is a method that is 1) widely applicable to many problems, 2) based on solid theoretical grounding, and 3) easily implementable with the SHAP Python library.The problem: In some scenarios, Shapley values fail to express information about model behavior, because it is only returning a score for one feature at a time. For instance, in the bike-sharing scenario, we are treating the weather and the day of the week as independent features, but sometimes it is the combination of those features that matters; and in those scenarios of feature combinations being more important than the individual features themselves, Shapley values can fail to properly explain a model.Bar Trivia ExampleLet’s use a simpler setting with fewer features to walk through the problem with Shapley values in more detail.I like to attend trivia nights at some local bars in the neighborhood with different coworkers of mine each week. It’s become quite clear that some members of our team bring more to the table than others.Can we quantify the impact each team member has on the trivia performance? We can use Shapley values for each player with the following interpretation: they should correspond to the expected change in score when adding that player to the trivia team. Other possible interpretations exist*, but we will use this one.(*Note: This class of methods to compute Shapley values, called “interventional” Shapley values, measure “expected change in score when adding this feature.” A different type is known as “conditional” Shapley values. The key difference between the interventional method and the conditional method lies in how they treat a feature whose expected change in score is zero—what should its Shapley value be? Zero? If you think the answer is “yes,” use the interventional method. If instead, you think the feature might still have importance due to correlations, and if you think that importance should be included in its Shapley value, then consider using the conditional method.)Geometrically, a useful way to plot all these 3-player game scores with different teams is as points on a cube, arranged so that neighboring points differ by just one player. Then, the paths between points (a.k.a. the cube’s edges) will represent the change in score when adding a player to a team.(Note: With two players, we would plot this as a square. With four or more players, we would have to plot this as a hypercube)Let’s call this shape a GameCube; this will be a useful shape for us because both Shapley values and GameCube edges will correspond to the change in score when adding a player.Figure 1: plotting each trivia score on a different vertex of a cube corresponding to the players present on the team that night.In our story, Reid is only knowledgeable about sports trivia, and GW knows about movies, music, history, geography, literature—pretty much everything except sports trivia. So when Reid plays, he improves the score by a little; when GW plays, she increases the score by a lot. And me, well, I’m mostly there for the beer and the company.A Shapley value is a perfect measure of explainability only when a player always contributes the same amount to a team’s score. And since each player’s change on the score is constant in our story so far, we can assign a Shapley value of 1 to Reid, a Shapley value of 9 to GW, and a Shapley value of 0 to Max. These Shapley values represent the expected change in score when each player joins the team!Figure 2: Viewing the change in team scores when adding each player.In more technical terms, a game where each player’s impact is consistent (like our story so far) is called an “inessential game.” Also, we will use the symbol ▽v to represent the “gradient” of a GameCube v, which computes the values along the edges between the values on the vertices, and we will use ▽_player_v to represent the edge values for a specific player’s directions and zero along all other edges.For example, the GameCube gradient ▽_Reid_ν represents all possible changes in score when adding Reid.Figure 3: Expressing the change in scores when adding a player as the partial gradient of the GameCube with respect to each player.Feature contributions can’t always be expressed as a single number—so Shapley Values aren’t enough.You should expect that most of the time, the features you are working with won’t have constant impacts on model outputs—instead, the impact of a feature typically depends on what the other features are.Let’s change up our story.Suppose that Max’s behavior changes based on who he is playing with. When playing with GW, he is pretty chill, drinks his beer, minds his own business and lets GW do most of the work, so he doesn’t bring the score down. But when Max plays with Reid, he gets jealous of how much Reid knows about sports, so Max starts to speak up more, suggesting some wrong answers and bringing the score down by 1!Figure 4: The new GameCube with inconsistent player contributions.On this new GameCube, GW’s edges are constant, so her Shapley value of 9 still corresponds exactly to the change in score when she plays. But Max’s and Reid’s edges are not constant, because their impact on score depends on who they are playing with. Therefore, our way of using GameCube edges to quantify what Max and Reid bring to the table now has a problem.When real data scientists use Shapley values, they solve this problem by taking the average contribution of a player to their teams—on the GameCube, this would mean quantifying a player’s contribution as the average edge values in their direction. So on our GameCube above, GW’s Shapley value would still be 9 as in before, but Reid’s Shapley value would now be 0.5 and Max’s Shapley value would now be -0.5. For some use cases, the story ends there—a player’s average contribution can sometimes be a good enough quantification of their impact!However, this may cause a problem when it comes to trusting Shapley values. Because we can trust GW’s Shapley values more than we can trust Max’s or Reid’s Shapley values, since there is more consistency in her contribution to the team than Max’s or Reid’s contributions.Shapley ResidualsThe Shapley residual is a measurement of how much a player’s edges deviate from being constant—lower Shapley residuals mean Shapley values are close to perfectly representative of feature contribution, whereas higher Shapley residuals mean Shapley values are missing out on important model information: namely, that a feature’s contribution depends on the other features as well.The authors of the original Shapley residuals paper formulate this missing information as an error term in a least-squares regression. For example, for the player Reid:▽_Reid_ν = ▽_ν_Reid + r_ReidThe left side of this equation is the same partial gradient as earlier. The right side of the equation is the sum of a new GameCube’s gradient, ▽_ν_Reid, plus a residual cube, r_Reid, which measures the amount that our game deviates from being inessential with respect to Reid.Figure 5: the residual cube is the amount a game deviates from inessentiality with respect to a given player.The key idea is that, if Reid has a consistent impact on the team, the residual cube r_Reid will be all zeros. On the other hand, if the values on the residual cube r_Reid deviate from zero, then that is a signal that Reid’s Shapley value is missing information about how Reid’s impact depends on who else is playing with Reid. The higher the values on the residual cube, the more Reid’s contribution depends on which other players are present.Code for Calculating Shapley ResidualsImportsGenerate Synthetic DatasetTrain Model & KernelSHAP ExplainerCompute Expected Values of Feature Coalitions
This uses explainer.synth_data, the set of the synthetic data
samples generated by the shap library when the explainer is
trained.
The dictionary coalition_estimated_values maps feature
coalitions to the expected value of the model when those features are used,
relative to a baseline (which is the expected value when no features are
used: the average model output).(Note that we convert the lists to strings since lists are not hash-able types in Python.)Progress Checkcoalition_estimated_values should look something like this:
{'[]': 0,
'[0]': -0.3576234198270127,
'[1]': 0.010174318030605423,
'[2]': -0.08009846972721224,
'[0 1]': -0.34261386138613864,
'[0 2]': -0.37104950495049505,
'[1 2]': 0.14435643564356437,
'[0 1 2]': -0.396}
Create Hypercube ObjectWe are using 3 dimensional data so this will just be a cube. But this method extends to hypercubes, growing slower as the number of dimensions increases.Feel free to use the code for the Hypercube python class in the appendix for
this article, or to write your own. It needs to place the
coalition_estimated_values on the vertices of the cube, and it
needs to compute the edge values as the difference between neighboring
vertex values.
Compute the Shapley ResidualsFor each feature, minimize  ▼_feature_cube — ▼_cube_feature  to compute
the residual. This uses a helper function called residual_norm
defined in the appendix at the end of this article.
ConclusionShapley values have become an incredibly popular and generalizable method for explaining which features are important to a machine learning model. By quantifying their effectiveness using Shapley residuals, you will be able to further identify where exactly your machine learning model’s behavior is coming from, and which insights stemming from Shapley values are worth trusting.Special thanks to the authors of the original Shapley residuals paper for their work!AppendixAll images in the piece are created by the author.Below is the code for the Hypercube object and other helper functions, which you can use with the starter code above to compute Shapley residuals.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai
 From Black Box to Glass Box: Transparency in XAI  Arthur AI Blog
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedExplainable AIFrom Black Box to Glass Box: Transparency in XAIBy:Caryn LusinchiOctober 19, 2022With the rise of OSS security concerns, divergence of explainability goals, and custom, proprietary XAI algorithms, is transparency still possible? Explainable AI (XAI) typically involves tools and techniques to understand how a complex model behaves, in a simple, straightforward, and intuitive way so humans can understand it. It answers why an automated decision making tool resulted in a specific output that impacts customers.Market Size It’s predicted the explainable AI market size is estimated to reach $21.8 billion by 2030, up from $4.1 billion in 2021. And Gartner’s crystal ball paints a picture that “by 2025, 30% of government and large enterprise contracts for the purchase of AI products and services will require the use of explainable and ethical AI.” Regulatory’s RoleSo, what’s fueling predicted market growth? The accelerant for the explainable AI market is due in part to EU advent of GPDR’s Article 13-15 and 22, which establishes rights specific to algorithmic decision making, including a right of both notification and access to meaningful information about the logic involved and the right of the significance of and envisioned effects of solely automated decision making. Explainability, in this specific use case, is a legal obligation of enterprises to inform regulatory officials as well as end customers about why models made the decisions they did. End customers should be able to comprehend explanations, which should be written simply in their native language and include non-technical jargon.  Additionally, Article 13 (1) of the EU’s future Artificial Intelligence Act (AIA) mandates that high-risk AI systems be “sufficiently transparent to enable users to interpret the system’s output and use it appropriately.”Given the fact there are over 100 different XAI methods available to data scientists today and they often select the one that takes the least amount of effort/time, future regulation doesn’t specifically prescribe which explainability method should be used. The enterprise can elect to use local, global, or counterfactual explanations, but it “must be faithful to the model in the sense that they need to be an, at least approximately, correct reconstruction of the internal decision making parameters: explanation and explanandum need to match.”1While there is no nationally passed regulation in the U.S. regarding explainability, the National Institute of Standards and Technology (NIST) proposed in 2020 four principles for judging how explainable an artificial intelligence’s decisions are.And, the most recently released White House Blueprint for an AI Bill of Rights includes a Notice & Explanation principle, stating: “Automated systems should provide explanations that are technically valid, meaningful, and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context.”‍Explosion of XAI SolutionsAcademic R&D labs, open-source communities, and private software enterprises alike have seen legal compliance signals as a trigger to brainstorm XAI algorithms. The Partnership on AI (PAI) reports that “each year the number of available XAI tools (developed by both academics and industry practitioners) grows, resulting in more options than ever for those interested in using them. Here, we define ‘XAI tool’ broadly to mean any means for directly implementing an explainability algorithm. In our initial research, PAI identified more than 150 XAI-related tools published between 2015 and 2021.” The goal of the PAI project is to give enterprises tools to make more informed decisions about which XAI tool is best to deliver value to a business and help scale explanations. ‍OSS Concerns The vast majority of XAI tools are free-to-use open-source software (OSS). Public by nature, OSS offers a lot of benefits including crowdsourced examination for bugs or code evolution as well as enabling ethical conversations around ML applications. While OSS XAI libraries such as LIME or SHAP have done a lot to advance a broader understanding in the industry, they also pose performance doubts and security vulnerabilities. Some ML engineers are hesitant to apply OSS explainability methods into an application because they can slow down MLOps workflows and AI pipeline momentum. Additionally, cybersecurity experts are voicing concern that OSS explainable models are less secure given that when internal workings of model algorithms are publicized, bad actors can potentially manipulate the information via evasion, oracle, or poisoning attacks. Enterprises in competitive industries (where algorithms are treated as trade secrets or confidential IP), are worried that explainability may embolden industry competitors to reverse engineer ML models.  OSS caution is also echoed by the data science community. In a recent StackExchange post, it was acknowledged that commonly used and widely adopted open source ML packages are not regularly tested for reliability or de-bugged. One user posted, “Quite often those packages GitHub repos have existing unresolved issues and we may not go through them to identify any pitfalls. Business will be making critical actions based on the predictions/insights we, as a data scientist provide, which in turn could be based on those packages. How can we minimize the risk in such scenarios?” Custom Explainability TrendsGiven the combination of OSS security concerns and stakeholder resistance, companies are creating their own custom or proprietary explainability methods in-house or outsourcing the task to boutique consultants. As enterprises shift from OSS to XAI algorithmic IP, one would assume transparency would suffer—but that’s not necessarily true. Proprietary algorithmic IP may enable enterprises to fine-tune XAI methods to explain outcomes on an individual audience level, to provide more context around decision making rationale. Additionally, it gives the enterprise greater control over explanation content and verification. Ultimately, it may ensure there is sufficient domain knowledge expertise assigned to investigate models and dynamic datasets in order to fully comprehend the explanation.It’s predicted the trend for custom explainability will span across the model lifecycle, integrating into upstream and downstream ML team tasks. Given regulators—across GDPR, EU AI Act, or the AI Bill of Rights—are requiring easy-to-understand explanations for end customers and business stakeholders alike, there’s been recent work advancing natural language formatted explanations vs. technically dense feature importance scores based on LIME or SHAP.  As much as regulators are proponents of OSS, they also accept proprietary algorithms as long as there is sufficient evidentiary internal documentation and public disclosure to satisfy explainability laws. Divergence of Stakeholder Explainability GoalsRegardless of whether XAI methods are built on OSS or proprietary or a combination of both, the biggest challenge facing enterprises is that internal stakeholders don’t share the same explainability objectives. Each department has a distinct yet disparate goal of what they hope explainability will achieve. The Brookings Institute’s article, Explainability won’t save AI, broke down these fundamental differences. Typically, an explainability formula seeks to answer one perspective but fails to capture a broader context capturing angles from diverse, multi-stakeholders. Which is why the pursuit of explainability—either instigated by internal audit or external regulatory ask—in itself is not a panacea alone for risk management.  However, it is a starting point to shed light on the complex “black box” decision making that occurs between a machine learning system’s inputs and outputs. Discover Arthur’s explainability features across the pre-production and post-production MLOps lifecycle, including regional importance, global importance, and feature importance.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law
 4 Myths About the NYC AI Bias Law
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & Fairness4 Myths About the NYC AI Bias LawBy:Caryn LusinchiOctober 4, 2022Studies show that 99% of Fortune 500 companies rely on the aid of talent-sifting software, and 55% of human resources leaders in the U.S. use predictive algorithms to support hiring.1Given the widespread usage of predictive algorithms by human resources departments and the potential for it to go wrong, New York City is one of the first cities to pass legislation in an effort to prevent negative impacts resulting from automated decision employment tools (AEDT). Effective January 1, 2023, New York City Int. No. 1894-A Relating to Automated Decision Tools law will go into effect that restricts New York City employers from using automated employment decision tools unless it has been the subject of an independent bias audit no more than one year prior to its use.With under three months to go until this law is officially put into place, we wanted to dispel some myths about it. Keep reading to learn more.‍Myth #1: The entire end-to-end human resources lifecycle, spanning candidate screening to termination events, is covered by 1894-A.‍Fact: Nope. The law only covers hiring and internal promotional employment decisions that occur within New York City (not outside the city). It does not apply to demotions, firing, or downsizing actions. As we witness increased layoffs across industries due to mounting recessionary pressure, it’s regrettable that the law fails to cover “performance management” algorithms that are making automated decisions regarding which employees are on the chopping block.‍Myth #2: The law covers any automated process or system used in human resources.‍Fact: The law does not cover or materially impact employment decisions made by “a junk mail filter, firewall, antiviral software, calculator, spreadsheet, databases, data set, or other compilation of data.”2In legislation, AEDT or automated employment decision tools are defined as “computational process, derived from machine learning, statistical modeling, data analytics, or artificial intelligence, that issues simplified output, including a score, classification, or recommendation, that is used to substantially assist or replace discretionary decision making for making employment decisions that impact natural persons.”‍‍Myth #3: An individual can file a court complaint if they think they’ve been discriminated against by an enterprise’s AEDT. Fact: The law makes no mention of “private right of action.” If NYC finds an employer’s AEDT was discriminatory, there is a path for a federal court class action complaint. ‍Myth #4: Employers must provide notices to all employees and candidates that an AEDT tool will be used in connection with employee and/or candidate assessments/evaluations.Fact: Non-residents of New York City are not required to receive this notice, even when applying to a city-based position. The notice only needs to be sent to individuals who live in New York City. It must be made no less than 10 business days before use of AEDT, plus it must list the job qualifications/characteristics that the AEDT used for decision making. Employers have the option of publicly disclosing the type of data used, the data source, and data retention policy for the AEDT on their company website OR providing the above information to employees or candidates upon written request within 30 days of receiving the request. Upon notice, candidates can opt out and request an alternative selection process or accommodation. ‍After the law goes into effect on January 1, 2023, regulators know enterprises won’t be able to comply overnight—so naturally there will be an interim grace period to give companies enough time to put processes and tools in place to meet regulation fine print before fines are levied.Concerns around bias in the automated hiring process have pre-dated the passage of the NYC AI Bias law. Readers who are interested in taking a deeper dive into algorithmic hiring, equity, and bias should check out work by the non-profit group UpTurn. For additional reading, reference “We Need Fairness and Explainability in Algorithmic Hiring” co-authored by John Dickerson, Arthur’s Chief Scientist. Or, delve into the academic paper “Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices.” In summary, the NYC Bias Law requires an independent third-party audit to assess AEDT’s disparate impact on candidates or employees of a particular gender or race/ethnicity. But, how can you determine if your AEDT’s model outcomes result in disparate impact year-round and not just a point in time for employment decision making? Arthur’s Bias tab empowers human resources teams to view predictions (or outcomes) of your AEDT model, segmented by relevant subgroups in the population. An adjustable fairness threshold lets you quickly identify if your model is causing disparate impact for protected classes. With Arthur, you can proactively and continuously measure disparate impact with algorithmic bias model monitoring—and act on it to improve the outcomes for both future and current employees you serve.Interested in seeing the Arthur platform in action? Schedule a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/making-ai-work-for-even-more-people
 Making AI Work for Even More People
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesMaking AI Work for Even More PeopleBy: Adam WenchelSeptember 27, 2022At Arthur our mission is simple: make AI work for everyone. Make it work for our partners building their businesses on AI. Make it work for all of us who are impacted by AI every day through healthcare, social media, finances, and so many other areas of our lives. Make it work for the data scientists, ML engineers, product owners, and executives who need these systems to be reliable, ethical, and highly accurate.Simple does not mean easy. AI in the real world is in its infancy; it is just getting started. AI is fundamentally a step change in mankind’s ability to automate. Big leaps like this require relentless invention and reinvention.I am continually amazed by the way our team at Arthur embraces this challenge every day—literally building the future in the process.Today, I’m thrilled to announce the latest recognition of their great work.Arthur has raised $42M in Series B funding, which will accelerate our mission to make AI work for everyone. The investment is the industry’s largest-ever in a machine learning observability platform. Over the last year we have averaged 58% growth per quarter, a total of 445% over the whole period. More than anything, this is a significant recognition of the last 3+ years of category-defining work by our world-class group of researchers and engineers.We would be nowhere without the amazing partnerships we have with some of the largest, most forward-thinking companies in the world. They are the ones who are leaning in to the change and reaping the rewards that well-considered AI efforts bring. Thank you for sharing our vision and the journey.We are excited to work with Theresia Gouw and Asad Khaliq from Acrew Capital and Dylan Pearce and Bryan Subijano from Greycroft, our co-leads for the round. We’ve collaborated with Theresia, Asad, and the team at Acrew for the last 18 months and could not be more excited to take the next step in our partnership. Dylan, Bryan, and the Greycroft team join as new investors, bringing invaluable experience growing and operating successful software companies. We welcome new investor BAM Elevate, and all of our excellent existing investors who are doubling down including Index Ventures, Work-Bench, and Plexo Capital.What will we do with this funding? First and foremost, we have scaled up our investment in fundamental research. Our research team has produced a number of breakthroughs that not only advance the state of AI but also create significant value for our customers. It is this spirit of innovation, deeply rooted in Arthur’s DNA, that has driven our growth. We are also scaling up many client functions like customer success and our team of field data scientists to make sure that our quickly growing user base has the world-class support they deserve.Tomorrow, we will roll up our sleeves and be back at it—there is still much to be done. I feel lucky to be a part of this team and look forward to the journey to make AI work for everyone.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple
 Will AI Solve Climate Change? It’s Not That Simple
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationWill AI Solve Climate Change? It’s Not That SimpleBy:Arthur TeamAugust 23, 2022With the biggest commitment to mitigating climate change in U.S. history having recently been signed into law, climate is—even more so than usual—top of mind for many. From AI-based forecasts around extreme weather and CO₂ emissions to identification of climate-vulnerable regions, AI and machine learning technologies have been touted as the ultimate solution or silver bullet to overcoming climate change. Unsurprisingly, it’s not quite that simple. Keep reading to find some use cases for AI and climate change, as well as the ways in which AI is simultaneously solving and contributing to the phenomenon.‍The ProsIn so many ways, these technologies can be absolute game-changers when it comes to mitigating the effects of climate change. In fact, 87% of private and public sector CEOs with decision-making power in AI and climate believe AI is an essential tool in the fight against climate change. Below are just a few of the more common use cases.MitigationThis is one of the most crucial use cases for AI: the measurement, reduction, and removal of emissions and GHG effects.Measurement: Measuring emissions, both in the overall environment and at the level of individual products and actions, allows us to take stock of our current situation and forecast future trends. It also allows us to prioritize the abatement efforts with the highest potential to reduce emissions, at both the macro level and the micro level.Reduction: AI solutions can reduce emissions intensity by improving supply forecasts for intermittent renewable energy sources or creating more efficient electric-grid balancing. Additionally, they can reduce emissions-generating activities by optimizing supply chains, such as through improved demand forecasting or more efficient transportation of goods. Removal: AI-based solutions can be used to support and more accurately quantify the environmental removal process by analyzing satellite images to detect deforestation and estimate ecosystem carbon sequestration. They can also be used for technological removal efforts such as direct air capture (DAC) or carbon capture by assessing capture and storage locations, monitoring leakage, and optimizing the industrial processes and materials used for carbon capture.‍Adaptation & ResilienceAdapting to climate change is a critical undertaking for policymakers and the public, as it boosts resilience to the effects of long-term climate trends and extreme events. Solutions can apply AI to promote adaptation and resilience, particularly in hazard forecasting and in vulnerability and risk-exposure management. Hazard Forecasting: AI is well suited to help in forecasting and projecting climate-related hazards, whether in the form of improved long-term projections of regionalized events, such as sea-level rise, or in the form of early warning systems for extreme weather events such as hurricanes or droughts.Vulnerability & Risk-Exposure Management: AI can be used here to enable public- and private-sector leaders to make more informed decisions during crises, strengthen infrastructure through predictive maintenance of structures, protect populations through the prediction and monitoring of things like large-scale migration patterns or food insecurity risk, and preserve biodiversity through systems like intelligent irrigation.FundamentalsAI can be used to support research and education efforts on climate change, helping stakeholders understand the risks and implications and share what they learn. All such efforts reinforce and magnify progress toward mitigation and toward adaptation and resilience. AI can be used to bolster climate research and modeling; climate finance and analytics; and education, nudging, and behavior change (such as in the form of recommendations for environmentally friendly purchases).‍The ConsOn the other hand, the carbon footprint and computational costs of training large machine learning models can’t and shouldn’t be overlooked.In a recent publication, researchers from the University of Massachusetts at Amherst conducted a life cycle analysis for training several typical big AI models. They discovered that the process may produce over 626,000 pounds of CO₂ equivalent—which is 5x more than a U.S. car produces in its entire lifetime and a whopping 57x more than one human produces in a year.Artificial intelligence algorithms, which power some of technology’s most cutting-edge applications, such as producing logical stretches of text or creating visuals from descriptions, may need massive amounts of computational power to train—and their energy demands are rapidly increasing. The computing resources required to create a best-in-class AI model have doubled every 3 to 4 months in the deep learning era. This, in turn, necessitates a vast quantity of power, prompting many to fear that the carbon footprint of these increasingly popular ultra-large AI systems will render them environmentally unsustainable.Here are a few ways that the carbon footprint of AI can be reduced:Make use of computationally efficient machine learning algorithms. Don’t train a model from scratch if it is not necessary. When you can, use Automated ML. Since the model inference accounts for 80-90% of the model cost, use customized processors that increase the speed and efficiency of training and testing neural networks.AI is a tool that is already helping us and will continue to help us build a more resilient future—but, like all tools, it should be used wisely. Additionally, solving the climate change crisis requires not just technological innovation, but also the will of decision makers to take action and make the necessary changes toward a better future.‍Interested in learning how Arthur can help your organization monitor, measure, and improve AI efforts? Schedule a demo.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey
 What to Know as You Consider the Next Step in Your Tech Journey
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLife at ArthurWhat to Know as You Consider the Next Step in Your Tech JourneyBy:Reid ChamplinAugust 3, 2022After an unprecedented period of growth, the tech industry appears to be entering a stretch of profound uncertainty, leaving many professionals wondering what their next step should be.In just the first half of 2022, over 24,000 people were affected by layoffs at some of America’s top tech companies, including Netflix, Coinbase, Tesla, Twitter, and more. As fears of a potential recession grow, industry titans are pulling back on new hiring, with many saying the industry’s meteoric growth is being brought back to Earth.But for talent professionals and hiring managers throughout the industry, it’s clear the market remains red hot. Despite some high-profile companies rethinking their hiring strategies, tens of thousands of tech jobs remain unfilled as new opportunity emerges.Having your career shaken by factors beyond your control can be daunting, but your future is still bright. As you look forward to your next opportunity, here are three critical factors to keep in mind that’ll help you navigate the new job market and ensure your future success.1. Know the Industry As you consider new companies and organizations to further your career, ask yourself: What opportunities exist in this field, and how will those opportunities change and grow into the future? Finding life-changing opportunities often means being in the right industry at the right time. Not only are these opportunities less likely to be affected by uncertainty in the market, but they also offer the biggest reward as promising fields grow and occupy larger and larger niches within the tech world.Propelled by the limitless potential of AI, the machine learning industry is poised to explode in value to nearly $200 billion by 2030. The nascent field is already revolutionizing finance, healthcare, transportation, and countless other industries while employing more and more people every year—ML jobs have grown by 75% in the past four years alone. As the leader in the ML operations industry, Arthur has already experienced tremendous growth and will only continue to grow into the future.“As the AI industry continues to automate and mature the model development and deployment process, we’re going to witness the next 5-10 years intensely focused on bringing the same level of sophistication and ease to model performance management and optimization once these models go live in production,” says Victoria Vassileva, Arthur’s Commercial Accounts Lead. “This is where all of the value—and risk—is for any organization. Model performance and business performance, including regulatory and reputational risk, will be increasingly linked and so having the necessary oversight and controls will be the differentiator between success and failure, between positive impact and harm.”2. Know the CompanyAfter considering the broader industry outlook, ask yourself: Which company is best poised to take the lead in creating that new future? Knowing your prospective organizations inside and out is crucial to identifying which have the strengths needed to reach their goals—and to help you reach yours.Proven leadership is indispensable to any sound organization. The best indicator of future success is past success, and leaders who know what it takes to survive and realize their goals despite the odds are the most likely to do it again.No company can predict the future, and often the road to success is forged in difficult economic times. What separates the companies that float and the companies that sink is a focus on sustainable, responsible growth and commitment to take the steady route to success. Look into or ask about your prospective companies’ fundraising, annual recurring revenue (ARR), burn rate, and other key financial indicators. Are they prepared for whatever the future holds?Research has proven again and again that diverse organizations are higher-performing organizations and that fostering an inclusive company culture has a significant impact on a company’s fate. Are the companies you’re considering doing everything they can to foster a workplace where people of all backgrounds and identities are welcomed, included, and supported?At Arthur, we’ve built a culture-driven organization run by experienced leaders with an eye towards a more responsible future for the machine learning world. Our CEO Adam Wenchel has over 20 years of industry experience and found success both as CEO of Anax Security, a DC-based AI startup, and as VP of AI & Data Innovation at Capital One. We’re built by AI experts and backed by world-class, diverse investors, led by Mike Volpi at Index Ventures, Lo Toney, Theresia Gouw, Work-Bench, and Homebrew. And we’re even more proud to foster an environment where everyone feels welcome and invested in the Arthur mission.“It’s refreshing that we regularly and thoughtfully talk about how diversity and belonging are important to Arthur’s mission to make AI work for everyone,” says Genesis Whitlock, Arthur’s Head of Talent. “The time, energy, and focus it takes to find amazing talent from different backgrounds, experiences, and identities is so rewarding when you have leaders and colleagues who are actively working alongside you to create a diverse and inclusive environment.”‍3. Know Your ValuesWhat matters the most to you? Ultimately, only you can decide the right move for yourself. As you look forward to the next adventure in your career, take some time to think about what would make your next job the perfect one.Are you looking to grow and expand your skill set? In an industry that changes as rapidly as tech, staying up to date with the latest skills is critical to a long and impactful career. Do your prospective companies value mentoring and continuing education both in principle and in practice?Are you looking to take on leadership responsibilities and have your voice be heard? If so, it’s critical to know whether your new organization regularly encourages employees to take on new challenges and rewards them with opportunities to do so.Are you looking to work for a company with a mission you can believe in? People are far more likely to stay longer and achieve more if they believe in their organization’s mission. Consider what a truly good business looks like to you, and compare that vision against the companies you’re considering.Arthur’s values are at the heart of everything we do, and we work tirelessly to make sure we provide every opportunity our people could want. Even as we grow and scale, we remain a small team at heart, elevating employees to new opportunities and rewarding excellence with a platform to have your voice be heard. “With startups, building a great product doesn’t happen without a great culture as the foundation,” CEO Adam Wenchel says. “When I co-founded Arthur three years ago, I wanted to make sure that everyone on our team loved coming to work. We’ve spent a lot of time and energy fostering a company culture that reflects our core values of integrity, transparency, and inclusion.”‍We’re on a mission to make a more responsible AI-driven world—and we hope you’ll join us in making that vision a reality.‍Interested in learning more about a career at Arthur? Check out our job openings.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv
 Data Drift Detection Part II: Unstructured Data in NLP and CV
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringData Drift Detection Part II: Unstructured Data in NLP and CVBy:Karthik Rao and Rowan CheungAugust 2, 2022Note: This is the second part of a series where we take a deeper dive into the question of data drift detection. If you haven't yet, check out the first part where we discussed data drift in the context of tabular data!IntroductionData drift detection is a key component of a machine learning monitoring system. So far, we’ve discussed what data drift can look like in the context of tabular data, as well as some approaches to measuring drift. To recap, let’s revisit a simple example of data drift in a single feature:Comparing distributions. In this diagram, we examine a single input feature (Age) and look at the distribution of this variable at two time points. In the training data (green distribution) and in today’s production data (purple distribution).In this case, the distribution of age in the training dataset is different from its distribution in a production environment. Over time, the performance of a model using age as an input feature can decay in response to the change in the environment the model is deployed in. There are a variety of metrics we can use for measuring the difference in these two distributions, but how do we measure drift without structured features? Systems trained on unstructured data, like text or images, face the same risks when deployed in production. However, detecting drift in these scenarios is more subtle, as we cannot use common divergence metrics on the raw data. In this post, we’ll walk through a general framework for data drift detection with unstructured data and we’ll highlight the two example use cases of NLP and computer vision.
Specifically, we aim to identify datapoints that are anomalous, or belonging to
a distribution different than the training data. Formally, we would like to
surface incoming datapoints that are likely to have been drawn from a
distribution q(x) that is different from the training distribution
p(x). We’ll rely on two common use cases to illustrate the
out-of-distribution detection problem and evaluate our solution.
Our first example will be a computer vision use case, where the goal is to classify images based on the objects depicted in the image. For this setting, we used the STL-10 dataset from Stanford which provides high-resolution images from ten different possible classes including airplane, bird, dog, truck, and so on.‍Example images from the STL-10 dataset. We see images of cars, planes, trucks, dogs, etc. The STL-10 dataset contains 8,000 images.‍Our second use case will be in NLP and we used a News Headline dataset which contains news headlines along with their respective topics such as crime, entertainment, world news, comedy, etc. Here, our objective is to classify headline text to the correct category.Example datapoint from the News Headline dataset. We see that we receive information about the news category, news headline, authors, etc. The News Headline dataset contains 200,000+ records.Algorithm/ApproachAs with measuring multivariate drift in tabular data, the core motivation of the approach is to model the density, or distribution, of the reference dataset.OverviewThere are several different approaches for finding anomalies in unstructured data. For any given approach, the three main aspects to determine anomalies in unseen data require:
Vector Representation: Convert the unstructured data to a vector embedding.
Density Model: Define a density model for the reference dataset.
Scoring: Create a method for scoring new datapoints against the reference density model.
In this section, we will discuss the variety of different techniques used for each of these three different components. Further, we will highlight example results with NLP and computer vision datasets.Vector RepresentationWe must convert our image or text data into a meaningful vector representation in order to understand the underlying distribution of the reference dataset. These vector representations are a type of feature extraction that can capture a useful representation of our unstructured data. Transfer learning is one approach for creating these representations by extracting embeddings of each image or text sequence from a large pre-trained model. These large-scale models are generally trained on millions of different datapoints and use state of the art architectures (CNN’s for image data or Transformers for text data) that can take unseen datapoints and produce a meaningful vector representation. For images, pre-trained models such as ResNet, VGG, or similar will be appropriate. For NLP data, we need to extract document embeddings and turn to pre-trained (or fine-tuned) Large Language Models. While these are just a few examples of large-scale pre-trained models, there exist several others which are trained on different neural-network architectures and different datasets. This approach can be used with any type of vector embedding as long as it is meaningful for the context of your machine learning task. Density ModelOnce we have meaningful vector abstractions for every point in our reference dataset, we must now create a density model that can model the underlying distribution. We can train a flexible density model to these embedding vectors. This could be accomplished with many possible techniques such as an auto-encoder, a VAE, a Normalizing Flow, a GAN, etc. In each case, this density model learns the structure and distribution of the reference set images or text (as represented in the embedding space).Example of an auto-encoder architecture.As an example, auto-encoders are frequently used for unsupervised anomaly detection. Auto-encoders learn the latent representations of the reference set (consisting of vector embeddings) by encoding the vector to a lower dimensional vector and then decoding that representation back to its original dimension. We refer to the error measurement between the original input vector and the output vector as the reconstruction loss. Datapoints that are similar to points from the reference distribution will have a lower reconstruction error than points that are very different from the reference distribution. This property is useful for finding outliers as points that are outside the distribution of the reference set will have a high reconstruction error. Taking a look at our news headline example, we can inspect the space learned by our auto-encoder. We first train the model on news headlines categorized as CRIME, which we treat as our in-distribution data. Below is a visualization of held-out crime headlines, as well as entertainment headlines.UMAP visualization of in-distribution crime headlines (blue) and out-of-distribution entertainment headlines (red) as encoded by the auto-encoder.ScoringOnce we have trained our density model on our reference set, we must find a way to convert the reconstruction loss values from the model to actionable anomaly scores. Our approach is outlined below:
After training the model, we compute the reconstruction error of a holdout set (subset of the reference set) to use as a proxy distribution.
For every unseen datapoint, we compute the reconstruction error after being fed through our trained density model.
We find the percentile that our reconstruction error falls into relative to the reconstruction errors of the holdout set.
The motivation for our approach is twofold:
A lower reconstruction error means that the point is less likely to be anomalous (because the auto-encoder has seen many examples like it). Therefore, if an unseen datapoint yields a high reconstruction error (larger than anything from the holdout set), it is likely to be anomalous.
Because we rank in terms of percentiles, all our scores are normalized between 0 and 1. This makes it user-friendly and interpretable. Points that are close to 1 are more likely to be anomalous than points close to 0.
Evaluation/ResultsThere are very few open-source datasets that have labeled data to measure anomaly detection for unstructured data types. Therefore, we constructed a few different test cases with our example datasets introduced earlier in this paper to measure the efficacy of our anomaly detection algorithm for unstructured data. For each dataset (News Headlines and STL-10), we broke up our test cases as follows:
We segment our datapoints into in-distribution and out-of-distribution sets based on the labeled classes (e.g. all images as airplanes would be the reference set and all images as cars would be the out-of-distribution test set).
We furthermore segment the in-distribution dataset into 80% as the training set, and 20% as the holdout set. We use this to determine if our model would classify previously unseen datapoints as “anomalous” or “non-anomalous” based on the image object. For example, we would expect images from the in-class set should have low anomaly scores (near 0) and images from the out-of-class set should have high anomaly scores (near 1).
We run this test across different pairs of classes available in our dataset. We compute the AUC scores with relation to the “classification” of each datapoint as anomalous or non-anomalous (based on our scoring methodology).
We highlight two graphics below showcasing the results of our experiments.CV OOD: ROC Curves for STL-10 data where the in-distribution dataset is the class Planes, and the out-of-distribution class on the left is Ships and the out-of-distribution class on the left is Birds.The figures above are showcasing the ROC curves for two specific experiments we ran using the STL-10 dataset. The graph on the left is measuring the AUC when the in-distribution dataset (non-anomalous) was taken from a set of ship images while the out-of-distribution dataset (anomalous) was taken from a set of plane images. Similarly, the graph on the right shows the ROC curve where the in-distribution dataset was taken from a set of bird images and the out-of-distribution dataset was taken from a set of plane images. We notice that for both experiments, the anomaly detector does a very good job (AUC scores of 0.804 and 0.996) of being able to differentiate between in-distribution and out-of-distribution datapoints. NLP OOD: News Headlines dataset divided by each category as the reference set. Compares cross-class anomaly accuracies (where another class is all “anomalous”).The heatmap above is reporting the AUC scores for all possible pairwise experiments between possible classes in the news headline dataset (such as Crime, Entertainment, etc.). For any given cell in the heatmap, we are reporting the AUC score where the category on the x-axis is the in-distribution (non-anomalous) dataset while the category on the y-axis is the out-of-distribution (anomalous) dataset. We reported an average AUC score (across all crosswise pairs) to be 0.83, which is quite impressive given this task is difficult even for humans.ConclusionThis approach to out-of-distribution detection is especially powerful because it is completely unsupervised. In a production environment, we often don’t have prior knowledge of what kind of distribution shifts to expect or access to labeled data. Additionally, while we have considered two classification problems in this post, this technique can be applied to any type of machine learning task, as it only considers the input data and is therefore independent of the underlying ML task.Detection of out-of-distribution samples is only the first step in maintaining a robust machine learning system. Monitor your ML model for drift with Arthur. At Arthur, we’re helping data scientists and machine learning engineers detect, understand, and respond to unforeseen production environments.‍‍‍‍‍FAQ‍How do changes in the external environment, unrelated to the main features of the model, affect the process of data drift detection in NLP and CV applications within AI and ML frameworks, and how can one adjust the detection mechanisms to accommodate such changes?Changes in the external environment can significantly impact the effectiveness of AI applications, particularly in NLP and CV, by introducing new patterns or visual trends not present in the training data, leading to higher rates of misclassification or irrelevant results. This indicates a data drift. To adjust the detection mechanisms within AI and ML frameworks, one could incorporate adaptive learning strategies, allowing the model to periodically update its parameters based on new data. Additionally, implementing a robust anomaly detection framework capable of identifying and adapting to sudden shifts in data distribution without human intervention might help. Regularly updating the datasets with recent examples and employing domain adaptation techniques are also effective strategies to mitigate the effects of external changes on the performance of ML models.What are the specific computational costs associated with implementing the described data drift detection methods for unstructured data in real-world applications within AI and ML domains, and how do these costs compare with traditional data drift detection methods used for structured data?Implementing data drift detection methods for unstructured data in real-world AI and ML applications can be significantly more computationally intensive than for structured data. This is primarily due to the complexity of processing and analyzing unstructured data, such as images and text, which requires advanced algorithms and increased computational power typical of AI systems. Techniques like vector representation, density model training, and anomaly scoring, integral to ML workflows, are resource-intensive, especially when handling large datasets. In comparison, traditional data drift detection in structured data, often found in classical ML scenarios, relies on less computationally demanding statistical methods or simpler models. However, the exact computational costs can vary significantly depending on the specifics of each AI and ML implementation, the frequency of model updates, and the volume of data being analyzed.How does the data drift detection framework integrate with existing machine learning pipelines in AI systems, particularly in automated environments where continuous monitoring and instant decision-making are essential?The data drift detection framework can be integrated into existing ML pipelines within AI systems as a dedicated monitoring layer that functions in parallel with the main data processing workflow. In automated AI environments, this involves the continuous, real-time analysis of incoming data to assess its conformity to the model's initial training distribution, a cornerstone in ML operations. The framework should trigger alerts or initiate a retraining cycle if significant drift is detected, maintaining the ML model's accuracy and relevance. For effective integration, APIs could be developed to direct data from operational activities straight into the drift detection system and automate responses based on the outcomes, thereby enhancing the AI system's responsiveness and reliability. This ensures that the ML models remain accurate and relevant without disrupting the overall operational flow of AI-driven systems.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms
 Arthur Research: Equalizing Credit Opportunity in Algorithms
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationArthur Research: Equalizing Credit Opportunity in AlgorithmsBy:Lizzie KumarJuly 26, 2022For decades, financial institutions and lenders have used statistical models to make credit-related decisions. While these models improve efficiency and reduce variability, they have the potential to simultaneously perpetuate and accelerate historical patterns of discrimination. In the U.S., legislation like the Equal Credit Opportunity Act specifically bans discrimination in lending by forbidding credit score systems from using information like sex, race, marital status, national origin, and religion—and agencies are charged with enforcing this. However, credit invisibility and historical injustice mean that labeled credit data is limited on protected groups, which could negatively impact the accuracy of models trained on that data. Additionally, a vast body of research has demonstrated that even with sufficient training data, machine learning algorithms can encode many different versions of “unfairness.” This means that financial institutions could—potentially unwittingly—engage in illegal discrimination through the use of this technology.Two conversations exist in parallel here: one about U.S. discrimination law/policy, and the other about machine learning fairness research. Yet, policymakers and researchers in this space seem to talk past each other when it comes to data access, usage of input features, and the definition of “discrimination” (intent-based vs. outcome-based).Next Tuesday, at the AAAI/ACM conference on Artificial Intelligence, Ethics, and Society (AIES), we will be presenting a paper called Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation.The paper provides an overview of the following:The current landscape of credit-specific U.S. anti-discrimination law as it pertains to algorithms for fair lending researchersFair ML research results, contextualized to the realities of credit data to identify “discrimination risks” in the credit settingRegulatory opportunities to address those risksThe areas of lending regulation and of ML research are constantly evolving. We hope this paper is a useful tool for ML practitioners to understand the landscape and potential future directions.‍Interested in attending AIES? Check out the conference website. Also, learn more about Arthur’s R&D team here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data
 Data Drift Detection Part I: Multivariate Drift with Tabular Data
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringData Drift Detection Part I: Multivariate Drift with Tabular DataBy:Keegan Hines and Reese HydeJuly 12, 2022Note: This is the first in a series of posts where we take a deeper dive into the question of data drift detection. We explore not only why it is an important part of model monitoring, but we also discuss regimes and approaches to keep in mind. In the first part of the series, we discuss drift in the context of Tabular data and describe univariate and multivariate techniques for tackling these problems. In the follow-on posts, we’ll dive into unstructured data, such as images and documents, and discuss how we can build data drift detection systems in these more challenging regimes.Part I: Multivariate Data Drift with Tabular DataMonitoring the ongoing success of a machine learning model requires monitoring the data coming into that model. This means ensuring the data coming through today looks exactly how you expect it to look. Ultimately, you want to make sure the data looks typical: Does it look the same way it did when the model was first trained? If the data has changed significantly, your trained model is likely stale and resulting in inaccurate predictions. Whether you’re talking about Tabular numeric data, image data, or NLP data, the data monitoring problem remains the same. In all cases, we will have some sense of what the data ought to look like and then alert when things go astray. In technical terminology, this is often referred to as out-of-distribution detection: We want to find when the data no longer adheres to the shape and distribution that it used to (back when the model was trained). There are many ways of thinking about data drift detection, and in this post, we’ll describe the benefits of a high-dimensional and multivariate approach.A handy approach to begin thinking about data drift detection is to measure the distributional similarity between the data coming through a model now versus how the data is supposed to look, such as in the training set. A great starting approach is to separately look at each input variable to a model (and outputs as well). This so-called univariate drift approach can be tackled with many technical implementations. Common approaches include hypothesis tests, such as KS Test and ChiSquared test, and the so-called f-divergences, such as the KLDivergence, JSDivergence, or similar. Common to all of these approaches, we would typically apply them in a univariate way to each input feature to a model (see Figure 1).Figure 1: Comparing distributions. In this diagram, we examine a single input feature (Age) and look at the distribution of this variable at two time points: in the training data (green distribution), and in today’s production data (purple distribution). It is clear that the general shape of this distribution has changed quite a bit. This could lead to model inaccuracy.A higher-dimensional variant can be calculated in theory, but these methods are ineffective in high-dimensional applications due to data sparsity. Primarily, by using univariate measures for drift detection, we make an implicit assumption of feature independence. While this might be approximately true in some cases, most generally, our dataset likely has some complex interactions between features and other significant structures. Importantly, this can lead to missed events when we consider only one feature at a time. Therefore, we must consider the high-dimensional joint distribution of the data.In a multivariate approach, we fit a multi-dimensional ancillary model to the full joint distribution of the training set. This ancillary model will act as a density model and learn the patterns and structure in the dataset. This model can quantify how much any given datapoint is typical or atypical relative to the reference dataset. In implementation, there are many potential approaches for this density model—examples include things like a Variational Autoencoder, Normalizing Flow Models, Density Models, Isolation Forest, and so on. Any technique which is flexible should be able to work effectively. Then, we can use this learned density model to evaluate future datapoints on how similar they are to the training data.This approach is explained further in the sketch below, which shows a simplified view of the process. On the left, imagine we have a training dataset; in this case, it entails only a couple of continuous variables (X1 and X2). We have a small number of sample datapoints scattered around this space of X1 and X2. The dataset has some particular patterns and structure to it (the curved relationship between X1 and X2), perhaps unknown to us. In step two, we fit a density model to this dataset. For brevity, we omit implementation details here, but the overall goal is to quantify where in the X1-X2 space we saw lots of data and where we saw no data. That is indicated in this sketch by the shaded contours: the darkest shading suggests areas where we were very likely to see some data, and blank areas show where we didn’t see any data at all. In step 3, we can use this trained density model to score any new datapoints in terms of how likely they would have been seen, per the training data. Another way to think about this is to score a new datapoint based on whether it adheres to the typical shapes/patterns in the training set or if it is abnormal. As an example, one of the datapoints is green because it falls right in line with the “typical” regions of the density model. This datapoint is very similar to other data in the training set. In contrast, the red datapoint is found in a region where none of the training data was ever seen. In this way, this datapoint is an anomaly and is unlike anything in the training set. In technical terminology, this point is said to be “out of distribution” relative to the training set. In the example sketched here, note that the univariate drift measures would likely fail to notice the anomalous datapoint. When viewed in a univariate sense (against either X1 or X2), this anomalous datapoint is quite typical. However, because X1 and X2 have a complex structure, we find that the red datapoint is quite different from the training data. When we fail to consider the multivariate case, we can miss many subtle shifts where the production data falls off the data manifold.This form of out-of-distribution detection is an important part of monitoring the health of a machine learning system. It is becoming increasingly important that ML models can have understanding of their own uncertainty and confidence. In many cases, this amounts to uncertainty over its predictions, given an input. However, with out-of-distribution detection, we can understand what the model thinks the world looks like and we can flag when things are quite a bit different. This is useful because complex ML models are often overconfident in their predictions, especially for data that is unlike what they were trained on. By considering whether each input is in-sample or out-of-sample, we can better quantify when to trust a model’s prediction and when to be leerier. In this post, we introduced ideas about out-of-distribution detection for the context of Tabular data. But this problem is pervasive for all types of machine learning. In the next post, we dive into these ideas for computer vision models and NLP models.See how easy the Arthur platform enables you easily to detect and react to data drift in our product deep dive.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy
 What’s Missing from Your Model Governance Strategy?
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringWhat’s Missing from Your Model Governance Strategy?By:Arthur TeamJuly 8, 2022If you’re an enterprise with AI models in production across multiple lines of business, chances are you already have a model governance plan in place to comply with business, operational, and regulatory requirements. But governance goes beyond access controls, fancy arrow flow charts, policy PDFs, and checklists. How can you ensure your resulting model is accurate, robust, and reliable over time? Model MonitoringModel monitoring is a critical part of the AI lifecycle that enables data science teams to detect—and ultimately address—issues like data drift and algorithmic bias, while providing the necessary tools for correcting performance issues in the real world.According to Harvard Business Review, nearly 80% of AI projects won’t scale beyond proof of concept. Of the 20% remaining, it can take from 3 months to 2 years to roll a model into production. Given both the significant time and investment poured into ML model development, experimentation, training, and deployment, why wouldn’t you want to ensure that your model performs well post-production?  What Is Worth Monitoring in a Model? 1. PerformanceModel performance metrics can be a hybrid depending on model complexity. Regression: MAE, MSE, RMSE, Residual Histogram, Predicted vs. True, Forecast HorizonClassification: Accuracy, Confusion Matrix, Precision/Recall Tradeoff, F-1 Score, AU-ROC, Lift Curve, Cumulative Gains Curve, Calibration Curve, etc.These measures of accuracy are only useful, however, if you have access to the ground truth of your model. In a production environment, ground truth can sometimes take days, weeks, or even years to acquire (if it is available at all). In order to assess the performance of models without these prohibitive delays, proxies such as data drift can be used as a real-time barometer of your model’s activity. Drift metrics can be a meaningful leading indicator of the need for models to be retrained or restructured.2. Drift and DegradationData DriftOver time, data changes. Data drift is defined as a variation in the production data from the data that was used to train, test, and validate the model before putting it into production. The first two questions we often ask ourselves when we are faced with data drift are “When did this happen?” and “How did this happen?” Coincidentally, those are also some of the best ways to characterize different types of data drift. We can start to characterize how drift occurs by looking at the distributional makeup of the drift (i.e. what pieces of our data are drifting). We can then look at when the drift occurs by characterizing drift by intensity or timeline. ‍Covariate DriftCovariate drift is one of our most common types of real-world drift. It occurs when there is a change in the feature space of the model—or, in other words, when the distribution of one or more of the features has changed.‍Concept Drift Concept drift is a change in the relationship between input and output data variables over time.  This change can be gradual, reoccurring, or sudden. Oftentimes in production, we may not have access to the ground truth immediately. For example, if we are predicting whether or not a customer will default on a loan, we may not know if we are correct for months—or even years. In cases like these, it can be useful to also evaluate the relationship between features and your predicted values, instead of just the true (or ground truth) target variables. ‍Distribution Drift Common in real-world model scenarios is distribution drift. A key feature of distributional drifts in production is that they are silent. Arthur provides the ability to both monitor and set alerts to detect for different data distributional drifts, using both statistical and model-based methodologies. ‍Model degradation, or model decay, happens when a model’s performance becomes less reliable over time due to changes in the environment. When data drift occurs, the production-time assumption we made about the similarity between our training dataset and our production dataset is no longer true, which can cause the model’s decay.‍3. Explainability End users, data scientists, business leaders, and regulators need to understand how models make decisions. Improving model transparency can reduce model development and debugging time, highlight areas of concern for data drift and bias, and increase overall trust in the model. Explainability vs. Interpretability‍Model interpretability refers to models that are inherently understandable to humans. These models are simple enough that a human looking at the logic and internals of the model can understand how the model makes an inference given a particular input. In practice, few models are truly interpretable. Instead, there is often a tradeoff between interpretability and performance—especially for models performing complex tasks. High-performing models that do complex tasks are often the least interpretable models.  The goal of model explainability is to provide visibility into models that are too complex to be inherently interpretable. This often requires additional models and other techniques to generate explanations that are comprehensible to humans. Arthur’s platform offers powerful explainability techniques to provide prediction-level and whole model-level visibility into any model, including advanced “what if” analysis and feature importance ranking. ‍Local vs. Global Explainers‍Global explainers provide holistic model-level explanations. Global explanations are often presented as a summary of feature importance across the entire model. These explanations show which input features make the greatest impact on the output predictions of the model.  Since global explanations serve as simplified summaries of model behavior, they may not be accurate for specific data samples. However, they can help data scientists contextualize data drift to understand when a model needs to be retrained. This is especially useful when ground truth labels are unavailable. Global explainers are also useful to identify differences between groups for bias/fairness or debugging purposes. Local explainers provide a hypothesis of why a model made the prediction it did given a specific input sample. These explanations are useful in providing specific explanations to end users. They can also be helpful to data scientists when trying to identify and understand the cause of specific production issues. Local explanations can be aggregated across many samples to form global explanations.‍Types of ExplanationsFor explainers to be useful, they must present explanations in a way that is comprehensible and intuitive to humans. This is often presented in the form of data visualizations for feature importance. Explanations for tabular data can be intuitively represented as a bar plot of feature importances. Explanations for computer vision models and image data are provided by highlighting the most significant regions of an image, while natural language processing models can be explained by highlighting significant words and phrases.  Explainer AlgorithmsArthur leverages the industry standard LIME and SHAP algorithms to provide local and global explanations for tabular, computer vision, and natural language processing models. Both algorithms create simplified surrogate models to provide local explanations, which can be aggregated into global explanations. LIME and SHAP are model-agnostic explainers, meaning that they can generate explanations for any type of model, without accessing the internal logic and parameters of the model. Depending on the particular model and use case, a data scientist may favor one of these algorithms over the other. On the Arthur platform, LIME is used for image and text data, while SHAP is used for tabular data. 4. Bias / FairnessLocal and federal regulations around detecting and addressing bias are in the works (see NYC hiring law, Algorithmic Accountability Act of 2022). We have learned that traditional approaches which equate to “fairness through unawareness” simply do not work. In this case, ignorance is not bliss, and not enough to address existing/upcoming regulations. This unawareness-based approach may meet the requirements of avoiding the discriminatory practice known as disparate treatment, but does not address the possibility of the discriminatory practice known as disparate impact (this distinction is rooted in the Civil Rights Act of 1964). Essentially, a model that does not take into account membership of a protected class can still have adverse effects on members of that protected class.Detecting bias and discriminatory practices requires actively probing your data to see if groups are being treated unfairly. Arthur does this active probing for you and makes it easy to detect bias by making comparisons between subgroups, even if that group identity is not being used as an input to your model. There are a number of different metrics to quantify fairness. The three most common ones are demographic parity, equality of opportunity, and equalized odds. Arthur allows you to quickly identify, quantify, and visualize the degree of bias/fairness (using standard or custom fairness metrics) in your model’s outputs.If bias is identified, Arthur can help mitigate that bias based on post-processing techniques which do not require fundamentally changing your training data or model architecture. SummaryDon’t forget that an essential part of model governance is tracking model health post-production through model monitoring. Automated monitoring of performance, drift, model degradation, explainability, bias, and fairness as well as alerts/notifications of potential issues is an important aspect of ensuring responsible AI in your MLOps lifecycle.  Want to learn more about Arthur? Schedule a demo with one of our experts to see the AI monitoring, explainability, and bias analytics platform in action.‍Photo by Tingey Injury Law Firm on UnsplashPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance
 Arthur Recognized in 2022 Gartner® Hype Cycle™ for Data and Analytics Governance
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur Recognized in 2022 Gartner® Hype Cycle™ for Data and Analytics GovernanceBy:Arthur TeamJuly 1, 2022New York, NY –  Arthur, the AI Performance Company, was listed as a sample vendor for both AI Governance and Responsible AI in 2022 Gartner Hype Cycle Report for Data and Analytics Governance.This Gartner report highlights how "data and analytics leaders must understand the hype and progress of governance practice and technology innovations so their adoption delivers organizational value at the right time."“With AI now delivering value in practical enterprise application, data and analytics leaders see that scaling AI without governance is ineffective and dangerous,” the Gartner report states.As production AI continues to drive meaningful business value, Arthur is working with top enterprises to monitor, measure, and improve ML to drive better, more responsible results. Arthur is deployed at leading Fortune 100 enterprises across financial services, healthcare, retail, and technology, working together to accelerate MLOps and optimize for accuracy, explainability, and fairness.“Responsible AI means a deliberate approach in many directions at once,” the Gartner report states. “Data science’s responsibility to deliver unbiased, trusted and ethical AI is just the tip of the iceberg. Responsible AI helps AI participants develop, implement, utilize and resolve the dilemmas they face.”‍“We’re working with data scientists, product owners, and business leaders to ensure that their ML models are delivering accurate, transparent, and fair results—not only for the businesses themselves, but for the people they serve." - Adam Wenchel, Arthur Co-Founder and CEO‍The report underscores the importance of model monitoring, explainability, and fairness in building truly AI-native enterprises. “Arthur is proud to be deployed across business-critical use cases at leading enterprises at an especially essential time in the AI industry’s maturity,” said Adam Wenchel, Co-Founder and CEO of Arthur. “We’re working with data scientists, product owners, and business leaders to ensure that their ML models are delivering accurate, transparent, and fair results—not only for the businesses themselves, but for the people they serve.”As more and more businesses are turning to ML capabilities like Computer Vision and Natural Language Processing to solve key challenges, Arthur is the only AI Performance solution for CV and the industry leader in NLP performance management—in addition to exclusive support for multi-class, multi-label, and regression tabular models. Arthur is model-agnostic and platform-agnostic, providing a centralized dashboard and a flexible query API that allow enterprises to democratize ML performance and tie it directly to business KPIs.Earlier this year, Arthur was mentioned in 2022 Gartner Innovation Insight for Bias Detection/Mitigation, Explainable AI and Interpretable AI report [2], and in Infographic: Heat Index: Gartner’s 2021 Connecting Hot, Hype and Cool [3]. Arthur was named a 2021 Gartner Cool Vendor in AI Governance and Responsible AI [4].If you are a Gartner client you can access the 2022 Hype Cycle report here.Learn more about Arthur and sign up for a demo at www.arthur.ai. ‍References[1] Gartner, “Hype Cycle for Artificial Intelligence, 2021”, Shubhangi Vashisth, Svetlana Sicular, 29 July 2021[2] Gartner, “Innovation Insight for Bias Detection/Mitigation, Explainable AI and Interpretable AI”, Van Baker, Svetlana Sicular, Avivah Litan, Erick Brethenoux, 9 February 2022[3] Gartner, “Infographic: Gartner’s 2021 Heat Index: Connecting Hot, Hype and Cool”, Michael Woodbridge, Thomas Bittman, Sam Olyaei, Rita Sallam, 16 February 2022[4] Gartner, “Cool Vendors™ in AI Governance and Responsible AI”, Svetlana Sicular, Moutusi Sau, et al, 10 June 2021‍Disclaimer:GARTNER, COOL VENDORS and HYPE CYCLE are a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.‍Photo by Milad Fakurian on Unsplash‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity
 Why Only 12% of Companies Have Achieved ‘AI Maturity’
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationWhy Only 12% of Companies Have Achieved ‘AI Maturity’By:Arthur TeamJune 27, 2022A recent Accenture report called “The Art of AI Maturity: Advancing from Practice to Performance” went into detail about the journey some companies have taken from just testing the waters of AI to achieving a level of sophistication that is having massive positive impacts on their business. Precisely, “some companies” refers to the 12% of firms that, according to the report, “have advanced their AI maturity enough to achieve superior performance and growth.” The report also refers to these companies as “AI Achievers.” Another 25% of firms are “somewhat advanced in their level of AI maturity,” the report states, while the remaining 63% are still in the early stages.So, what exactly is AI maturity and why does it matter? Accenture defines AI maturity as “the degree to which organizations have mastered AI-related capabilities in the right combination to achieve high performance for customers, shareholders, and employees.” It’s become increasingly clear in recent years that harnessing the power of AI is crucial for businesses to have a competitive advantage. In fact, among executives of the world’s 2,000 largest companies, those who discussed AI on their earnings calls were 40% more likely to see their firms’ share prices increase.If most organizations are racing to embrace AI, why are some seeing more value than others? AI Achievers, according to the report, are not defined by the sophistication of any one capability, but by their ability to combine strengths across strategy, processes, and people. Here are five key success factors for Achievers:Their top leaders champion AI as a strategic priority for the entire organization.‍Companies can create strong AI strategies, but unless those strategies receive enthusiastic support from the CEO and the rest of the C-suite, they’re likely to flounder. When it comes to Achievers, 83% of these companies have CEO and senior sponsorship.They invest heavily in talent to get more from their AI investments.‍This senior sponsorship allows organizations to invest heavily in creating data and AI fluency across their workforces. 78% of Achievers have mandatory AI trainings for most employees, from product development engineers to C-suite executives.‍‍They industrialize AI tools and teams to create a strong AI core. ‍An AI core is an operational data and AI platform that taps into companies’ talent, technology, and data ecosystems, allowing firms to balance experimentation and execution.They design AI responsibly, from the start.‍As companies deploy AI for a growing range of tasks, adhering to laws, regulations, and ethical norms is critical to building a sound data and AI foundation. Achievers are 53% more likely than other companies to be responsible by design: designing, developing, and deploying AI with good intention to empower employees and businesses, and to fairly impact customers and society—allowing companies to engender trust and scale AI with confidence.‍‍They prioritize long- and short-term AI investments.‍One reason Achievers get more out of AI is simply because they invest more in it. Achievers also understand that their AI investment journey doesn’t have a finish line and that there is no “peak AI.” These companies know they have only scratched the surface of their AI transformations and that the quality of their investments matters just as much as the quantity.Echoing the report’s findings, Arthur’s customers self-identify as AI Experimenters (63%) or AI Innovators (13%), with longer-term aspirations of evolving into the AI Builder or Achiever categories. Advancing from practice to performance is a roadmap typically spanning a two-year time horizon for most enterprises. Maturity isn’t a one-size-fits-all path, either. While organizations may be farther along in ML/AI development and management maturity, we’ve discovered these same organizations are still in infancy for model monitoring and validation maturity. They’re still logging inferences, manually juggling Python notebooks, and facing problems from fragmented or restricted data stacks for day-to-day model data science workflows. The report says it best: “Advancing AI maturity is no longer a choice. It’s an opportunity facing every industry, every organization, and every leader.” The good news is that the share of AI Achievers will increase rapidly and significantly in the next few years, more than doubling from the current 12% to 27% by 2024. Will your organization be one of them?PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability
 How Arthur’s Tech Stack Is Built for Scalability
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct FeaturesHow Arthur’s Tech Stack Is Built for ScalabilityBy:Arthur TeamJune 13, 2022With the increasing number of enterprise ML models in production, the rising demand for localized models, and the focus on more robust model monitoring, scalability is more important than ever. Consequently, ML model monitoring platforms need to be built with a highly scalable architecture that can do everything your organization needs it to do at production grade.An ML model monitoring architecture that isn’t scalable can result in a number of technical challenges, including lack of responsiveness, increased infrastructure cost, and platform inelasticity. If your model monitoring system isn’t built to scale, your models—and ultimately, your business—will suffer the consequences.Unlike less resilient ML model monitoring and observability solutions, the Arthur platform is built for high performance and scalability from the ground up. Keep reading to find out why Arthur’s tech stack makes it the leading platform for enterprises that want to run high-performing ML models at scale.‍1. Database ManagementArthur leverages the strengths of both ClickHouse and Postgres to handle different types of workloads. ClickHouse, used for OLAP workloads, is a horizontally scalable database management system that allows for high insert rates and fast serving of complex queries against very large data for multi-dimensional analysis. Postgres, used for OLTP workloads, is one of the best relational databases. With its reliable transactional mechanism, it organizes the users and the models to support the recording and the querying of respective model metrics. ClickHouse and Postgres work together to provide the Arthur platform and its end users with an optimal level of flexibility.2. Auto-Scaler Mechanism & Streaming-First ArchitectureAs previously mentioned, components in Arthur’s platform are independently and horizontally scalable. The platform’s auto-scaler mechanism self-manages and optimizes resource utilization on a Kubernetes cluster, automatically scaling up and down based on platform activities as well as the lag observed in the data pipeline queue.Whether the data is coming from streaming or batch, Arthur’s streaming-first architecture allows a very large volume of data to be ingested reliably and efficiently in a non-blocking fashion. For queuing, Arthur uses Apache Kafka, which was built for streaming big data and is ideal for MLOps use cases such as high-throughput activity tracking, stream processing, event sourcing, and log aggregation.3. High-Performance Programming LanguageArthur’s platform core is written in the Go programming language, which was developed by Google and is used by leading enterprises like Uber and Dropbox. Go was chosen for a few reasons: it’s compiled to machine code, its runtime performance is up to 30x faster than languages that are interpreted or have virtual runtime, and it’s built for concurrency and parallelism.‍At the end of the day, scalability is far more than just a “nice to have.” As your organization grows and ML projects outpace their original deployment, your model monitoring system must be resilient and flexible to adjust to the ever-growing volume of data. The Arthur platform was not only built with all of this in mind, but it also continues to be optimized to ensure models are proactively scaling and ultimately maximizing value for your business. Read more about the importance of scalability and performance in our whitepaper here.‍Want to dig into Arthur’s tech stack even more? Check out our dev docs and resources.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent
 3 Strategies for Maintaining Your ML Talent
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedBest Practices3 Strategies for Maintaining Your ML TalentBy:Arthur TeamJune 16, 2022A 2021 365 Data Science Ltd study revealed that data science professionals remain with an employer for only 1.7 years on average. In fact, the sudden impulse to write a data science resignation letter doesn’t change depending on where you work in the world. The study data sampled data science professionals from the US (35%), the UK (25%), the EU (25%), and India (15%).So, when it comes to maintaining talent, what can employers do to combat data scientists’ desire to change jobs and keep them fulfilled in their roles?#1 Pay competitive salaries and incentivize with annual bonuses and equity refreshes.Currently, demand outstrips supply. It’s simple economics. According to the U.S. Bureau of Labor Statistics, the expected job market growth rate for data science and related fields is 31%, far greater than the overall average job growth rate of 4%. When there are more jobs than candidates, data scientists can easily jump jobs and get a pay bump because they have more salary bargaining power. #2 Carve out and create dedicated weekly innovation project time for team members.In a recent Medium blog post authored by Ian Xao, he noted that “data science teams at big companies are facing a serious retention problem” as scientists are quitting enterprise companies for new jobs at startups.Xao pointed out that many data scientists have an expectation-reality gap, leading to day-to-day job disillusion. Data scientists desire to work with advanced algorithms and troubleshoot models—but they end up spending up to 40% of their time on data collection, data cleaning, and tedious administrative manual data export-import activities to keep up with reporting demands.  Data scientist Natassha Selvaraj echoed Xao’s sentiment on a recent KDNuggets post:“At the end of the day, you’re no longer building complex algorithms and predictive models like you imagined. You now spend all your time brushing up on SQL and data preparation skills to pull data out of the system into different formats, and present this data to stakeholders so they can use it to make business decisions. Although your job title has the word ‘data science’ in it, you aren’t in a role you’ve always pictured yourself in. You’re unhappy being the company’s data janitor, and want to work on projects that will actually utilize the skills you’ve spent so long to gain.” Giving data scientists autonomy to work on model innovation or experimentation (i.e. passion projects) for a set percentage of time (10% to 20%) each week keeps your employees engaged and empowered because they feel a sense of ownership and control over their role. #3 Automate documentation workflows and reporting activities to reduce admin burdens.Documentation, documentation, documentation (cue Jaws theme). As AI/ML regulation matures and places greater pressure on higher up model risk managers to document rationale and explainability behind data sets and model decision making, repetitive reporting asks often trickle down to data scientists and become a dreaded chore. This can range from design docs to writing model cards for communicating to external or non-technical stakeholders deployed model context, performance evaluation criteria and other relevant information. Or it could be a more in-depth report to satisfy an DPIA (data protection impact assessment), internal, or third-party audit requirement.Investigate MLOps platforms that can automate a team’s manual, repeatable activities. By automating univariate and multivariate data drift detection, event logging, data exporting for monthly/quarterly reporting requirements and more, your team can break the chains from day-in, day-out data science “janitorial” maintenance and focus on complex model diagnosis and mitigation solutions.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest
 Learnings from TTC Summit & Good Tech Fest
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsLearnings from TTC Summit & Good Tech FestBy:Daniel NissaniMay 25, 2022Earlier this month, I attended the Trust, Transparency, and Control Labs (TTC Labs) Summit on Trustworthy AI experiences and Good Tech Fest. Each conference offered sessions that centered around one central theme: How do we make technology that is in service of humankind?They wrestled with tensions in regulatory practices that make it hard for governments to enforce AI protections, discussed the nuance and meaning necessary for good explanations in AI systems, and dissected the privilege present in AI ethics today and how we can change it by engaging communities that will be harmed at the onset. The conference left me thinking that transparent AI systems, standardized international regulatory mechanisms, and impact assessments can help AI be good for everyone. Read on to see how I arrived at these learnings.Tensions Around Regulatory PracticesDuring the Transparency & Explainability in AI Regulation panel, several tensions arose around the AI regulatory space. EU Laws like GDPR and the forthcoming AI Act are focused on citizen rights, whereas U.S. federal laws around data, such as HIPAA, are focused on sectoral regulation. This nuance in the intention of the laws makes it hard to come up with frameworks and guidance for organizations to follow, as more and more organizations operate internationally. And even if these frameworks provided good guidelines (as some do, such as the AI and data protection risk toolkit  ICO), creating tooling to automate and manage these regulations throughout an organization is still a challenging and cumbersome problem. I think my favorite part of the panel was an acknowledgement that until society writ large has an intuitive understanding of AI concepts, enforcing regulations is going to be extremely difficult.Even something as simple as defining an AI system is quite challenging. According to one of the speakers during the AI & Society: Demonstrating Accountability panel, the AI Act actually provides some guidance on what counts as an AI system. Via this guidance, a manually coded decision tree, with rules hard coded, is not considered an AI system—but a trained decision tree, that learns the same exact decision rules based on a training dataset, does. This implies that the former does not need to abide by the laws that would be enacted by the AI Act. Although definitions will always contain loopholes, acknowledging them is a first step to refining definitions over time.Going Past Explainable ModelsResearchers from Google walked us through some of their explainability case studies, a tool they use to help them think through the nuances behind explainability in AI systems. What fascinated me most was that the case study we did together made us think past the conventional idea of explaining why a model had a certain prediction. We discussed the amount of information presented, the way the information was framed, and how the urgency or necessity of such information should prompt different types of responses from AI systems. These are fine-grained ideas to grapple with, and start to move into the territory of design or HCI work around explainability. One of the panelists on the Where Control Happens panel emphasized on multiple occasions that explanations need to be meaningful. It isn’t just that an explanation needs to happen, but it needs to be useful for a user, possibly giving them agency that they otherwise would not have had.Tenets of AI EthicsWhen trying to design ethical AI systems, we need to think about those who are involved in creating the system and those who are impacted by it. At both conferences, this idea was brought up repeatedly as something that needs to be addressed. Specifically, we need to focus on communities that do not have a voice in the creation of AI systems because they are usually the ones most susceptible to harm. During the Explainable to All: Designing AI Experiences panel, several of the panelists discussed how bringing those voices that are traditionally underrepresented, if represented at all, will allow us to at least mitigate some of the harm that AI systems may impose on a community. But this is much easier said than done. As brought up on the panel, ideas of fairness, transparency, and consent in AI systems is primarily a notion in the Global North. Even using frameworks like participatory design to address these ideas proves challenging because the notion of participatory design itself comes from the Global North. Thus, those communities that are impacted in the Global South may have no good way to give consent or may not have the cultural understanding to participate effectively in a participatory design framework. Addressing the cultural issues behind our notions first will help us start to actively include these communities.This should also be considered when collecting data. As discussed at Good Tech Fest’s session on Missing Voices in Natural Language Processing, the majority of languages used in large language models comes from primarily privileged communities that have access to technology. Thus, we need to be conscious about our language data collection mechanisms and how we deploy and assess large language models. The presenter even cited one of our blog posts as a good guide to ethical natural language processing practices.Takeaways and ResourcesOverall, these conferences expanded my thinking on a lot of topics and reminded me of the nuance and complexity that fairness and explainability hold. Below are some takeaways coupled with some resources that can help you continue your own journey with these concepts.Transparency is key to solving the challenge of trustworthy AI. By explicitly showing how systems work, providing agency to individuals, and attempting to participate with communities being impacted, we can develop AI systems that will benefit humanity. Some of my favorite papers on transparency are documentation-based, such as Datasheets for Datasets or Model Cards.Creating standardized regulations for AI systems and having consistent auditing frameworks are also effective. Realizing these notions is easier said than done, but being able to enforce regulations, while also providing tools for organizations to be successful under this enforcement, will create more ethical uses of AI.Starting from impact assessments will help organizations realize who will be harmed most by AI systems. Once identified, one of the panelists on the Explainable to All: Designing AI Experiences panel expressed that inviting representatives from those communities and achieving a consensus with them will help address harms, while also gaining an effective proxy for consent.Lastly, part of my attendance at Good Tech Fest was as a DataKind volunteer. During DataKind’s session, we helped nonprofits start to think about their data scienceable problem. DataKind offers their playbook for how to design and scope out data sciencable problems ethically.‍ConclusionFairness and explainability are extremely nuanced and important fields because, if done right, we can make the world a better place. It’s part of the reason why I am so excited to be part of the Arthur team. The tools we are building help organizations start to operationalize the latest research in fairness and explainability, allowing them to realize their aspirations to be ethical. Looking forward to continuing the journey!‍Interested in learning more about what Arthur can do for you? Get in touch.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries
 Arthur Launches Custom RBAC to Strengthen Data Privacy & Reduce Compliance Risks for Enterprises in Highly Regulated Industries
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct FeaturesArthur Launches Custom RBAC to Strengthen Data Privacy & Reduce Compliance Risks for Enterprises in Highly Regulated IndustriesBy:Caryn LusinchiMay 11, 2022Arthur is the very first MLOps observability platform to launch custom RBAC (Role-Based Access Control) to support today’s enterprise security and data privacy demands.As organizational structures evolve and grow, companies need the flexibility to review and adjust permissions frequently to protect model data and MLOps lifecycle access management. Additionally, specific roles within MRM (model risk management) and third-party independent auditors require strict confidentiality, privacy, and access management controls. When businesses use highly sensitive personal identifiable information for privacy-sensitive data analysis in supervised learning environments, they face increased regulatory risk. To reduce privacy and compliance risks, segregation of duties is essential for different roles across the enterprise that are involved in ML monitoring and validation activities.To uphold the integrity of trust frontiers necessary in AI systems and provide transparency to higher-level stakeholders who are tasked with management/governance KPIs as data flows through the ML development lifecycle, Arthur now offers a custom Role-Based Access Control (RBAC) system.  Enterprise organizations cannot rely on offerings with inflexible preset/pre-defined roles and provisioning access. Arthur provides the ability to set up a fully customizable RBAC for on-premises customers using OIDC authentication, and the flexibility and freedom to review and adjust permissions associated with each role as their org structures evolve and grow. With this unique new capability, Arthur allows organizations to create groups and assign employees using any tool that speaks open ID connect language (OKTA, Microsoft Azure Active Directory, Duo, SecureAuth, AuthO, Ping Identity, and more). Organizations can leverage Arthur’s group mapping API for enterprise secure user authentication between data scientists, ML model engineers, systems integrators, domain practitioners, risk analysts, internal auditors, and external auditors.  Here’s an example managing users’ permissions in Arthur using a third-party identity provider, such as Okta. Each Okta Group corresponds to a role within an organization in Arthur’s platform. In the future, we plan to extend custom RBAC to our existing support of SAML 2.0-based SSO identity providers. When building trustworthy AI systems, it’s critical for enterprises to identify clear roles, requirements, and responsibilities for teams building, monitoring, and optimizing machine learning models.  Arthur’s RBAC-OIDC authentication solution was specifically built to support enterprises that are subject to EU, UK, US or state regulation that mandates algorithmic transparency and auditing. As such, our custom RBAC feature offers full-fledged flexibility across standard roles, plus organization-level, model-level, and alert-related permissioning. Fully Customizable Permissions Here’s an excerpt of some of the different permissions Arthur supports for use with custom roles. For a full list, you can check out our dev docs.Discover how Arthur’s custom RBAC fits with your enterprise OAuth. Schedule a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/learnings-from-odsc-east-2022
 Learnings from ODSC East 2022
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsLearnings from ODSC East 2022 By:Arthur TeamApril 27, 2022The Open Data Science Conference, or ODSC, is one of the largest gatherings of professional data scientists, and its attendees, presenters, and companies are shaping the present and future of the field. The goal of the conference is to bring together the global data science community in an effort to encourage the exchange of innovative ideas and the growth of open source software.Last week, three members of Arthur’s ML Engineering team headed up to ODSC East in Boston as both attendees and as speakers. Keep reading for some recaps of their favorite talks that they attended as well as the ones they presented themselves based on their own research.Unsolved ML Safety ProblemsDan Hendrycks, PhD Candidate at UC BerkeleyThough most business-critical machine learning systems today are relatively simple models, the research community has produced astounding results with large models such as GPT-3 and DALL-E, which often have surprising capabilities. In this talk, Dan Hendrycks, a PhD student at UC Berkeley, discussed some high-level approaches to the problem of ML safety. Hendrycks described three buckets for future research: Robustness, or the ability for ML systems to perform well even in unexpected or adversarial settings; Monitoring, or the ability to understand and anticipate new challenges; and Alignment, or the ability to coordinate the behavior of ML systems with human goals, values, and intentions. While the highest-stakes motivations for the talk, and the largest open questions, generally refer to large deep learning models, these challenges and goals are still relevant for simpler models. Operationalizing Fair ML: From Industry to Research and BackJessica Dai, Machine Learning Engineer at Arthur (co-presented with Laura Mariano, Lead Ethical AI Data Scientist at Humana)In both research and industry, discussion of “fair machine learning” has exploded in the past few years—yet there is often a gap between what is available in academia and the constraints and needs of a real-world organization. This talk was co-presented by Jessica Dai of Arthur and Laura Mariano of Humana, which has been an incredible partner of Arthur’s for years now. The two discussed Humana’s journey towards achieving informed, responsible use of machine learning to improve health outcomes. First, Humana implemented organizational and process-based tools for governance. Having set the stage for actively improving models, however, Humana’s data scientists then realized that none of the popular, published approaches to achieving fairness were applicable to their goals: the way Humana deployed and used machine learning violated assumptions made by many available “fair ML” methods. In the latter half of this talk, Jessica showed how these constraints motivated novel research questions and guided the development of an academic research project; explained and demonstrated the method we came up with; and discussed considerations as we folded this research work back into the product so that it was ultimately usable in a real-world production setting.Utilizing NLP in the Context of COMP360 Psilocybin Therapy for Treatment-Resistant DepressionGregory Ryslik, Executive Vice President, AI, Engineering, Digital Health Research & Technology at Compass PathwaysIn this talk, Gregory Ryslik of Compass Pathways discussed how NLP techniques have been used in the process of setting up clinical trials for using psilocybin as an intervention against treatment-resistant depression. The magnitude of the problem is, of course, enormous; the talk focused on how this specific application setting, as well as the constraints it brought—such as working with nontechnical clinicians, privacy and regulatory concerns, and following the clinical trial process—all shaped the development of NLP approaches on the technical side. Drift Detection in Structured and Unstructured DataKeegan Hines, Vice President of Machine Learning at ArthurMachine learning systems in production are subject to performance degradations due to many external factors and it is vital to actively monitor system stability and integrity. A common source of model degradation is due the inherent non-stationarity of the real-world environment, commonly referred to as data drift. In this presentation, Keegan described how to reliably quantify data drift in a variety of different data paradigms including Tabular data, Computer Vision data, and NLP data. Attendees of this talk came away with a conceptual toolkit for thinking about data stability monitoring in their own models, with example use cases in common settings as well as in more challenging regimes.Simplifying MLOps by Taking Storage Out of the EquationMiroslav Klivansky, Principal Data Architect, AI & Analytics at PureStorageOne of the biggest issues that enterprise companies face when trying to design and scale machine learning pipelines is worrying about the raw storage of the data. In this talk, speaker Miroslav Klivansky discussed the work he was doing at PureStorage, a flash storage company based in Palo Alto. He discussed how we need to think about handling large-scale data pipelines and having a data storage system that can handle the many different kinds of tasks from different machine learning models. The key takeaway from the talk was how we must differentiate between data and storage, and how they are not the same thing. He showcased a workflow (done at PureStorage) where we think about the different parts of the data pipeline abstracted away from the storage system, which yields cleaner and more efficient pipelines. FastCFE: A Distributed Deep Reinforcement Learning Counterfactual ExplainerKarthik Rao, Machine Learning Engineer at ArthurCounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps to move a data point from one side of a decision boundary to another. These explanations have a clear use case for several applications ranging from loan decisions to healthcare diagnosis, where they need to advise stakeholders about actions they can take to achieve a different outcome. Individuals not provided loans want steps they can take to achieve a loan, and similarly, patients want to know how they can achieve a better diagnosis.In this presentation, Karthik Rao showcased FastCFE, an algorithm and feature that uses reinforcement learning to provide real-time counterfactual explanations. The presentation was broken down as follows:Overview of Counterfactuals and Reinforcement Learning (RL)Deep distributed reinforcement learning using OpenAI Gym and Ray+RllibBenchmarks and ResultsA Unified View of Trustworthy AI with the 360 ToolkitsDr. Kush Varshney, Distinguished Research Staff Member and Manager at IBM ResearchThe final talk of the conference was given by Dr. Kush Varshney of IBM Research, discussing the open source initiatives that IBM has taken to develop trustworthy AI toolkits. This was a particularly enlightening talk and it is definitely beneficial to the AI community that large players such as IBM are committed to trustworthy initiatives and are even willing to open source these new technologies to help companies build trusted AI systems. If your organization has data scientists who can afford to invest the time and want to be truly hands-on, these IBM toolkits will work well for your team. Dr. Varshney explained three specific AI toolkits discussing:AI Fairness 360: Different methods to detect bias and use bias mitigation techniquesAI Explainability: A toolkit providing out of the box explainable tools to explain modelsAI Adversarial Robustness: How to detect and handle adversarial data attacked on modelsODSC East was an incredibly rewarding experience for the Arthur team, and we’re thrilled that our engineers were able to share the results of the hard work they’ve been doing to advance the field of machine learning. We’re feeling grateful to have finally attended a conference in person after so long—and especially to have been able to co-present with one of our long-time partners, Humana, to amplify the story of their leading work. See you at ODSC 2023!‍Interested in learning more about Arthur’s research-led approach to product development? Read about it here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems
 Mining for Proxies in Machine Learning Systems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessMining for Proxies in Machine Learning SystemsBy: Keegan HinesApril 18, 2022In deployed machine learning systems that affect people’s lives, it is important to ensure that your model is not resulting in disparate impact for some sub-populations of your data. The typical populations of concern might be the so-called protected basis variables such as race, age, or sex, but can also be any subgroup which is relevant to your industry. Creating ML systems that are fair can be tricky due to the complexities of data used for training.If you want to ensure that your model isn’t resulting in disparate impact across sex, it might be tempting to simply not use sex as an input variable into the model. This tactic, often referred to as fairness-through-unawareness, is insufficient. Even though the model doesn’t know about sex explicitly, it is highly likely that some other input feature (or combination of features) can act as a highly effective proxy for what you were trying to hide from the model.Many approaches to FairML have focused on mitigating unfairness either by augmenting the training data, the training algorithm, or the model’s outputs. While these approaches help to obtain a more fair model for any given set of inputs, they don’t illuminate for the data scientist what factors were resulting in the model bias. As the model developer, it is helpful to have full awareness of which features are acting as proxies for sensitive attributes. In this post, we describe a technique for effectively surfacing complex combinations of features which act as proxies for a sensitive attribute. ProxiesBeing able to identify proxies can aid the model developer in being judicious about feature selection and model development. At a first pass, identifying proxy variables is as simple as determining which variables have high correlation with a sensitive attribute, and then choosing whether or not to omit those variables. The downside of this approach is that measures of correlation (perhaps Pearson correlation or mutual information) will marginalize over the full domain, perhaps averaging out important effects only seen for some specific values. Further, it could be the case that pairwise combinations (or higher-order) of features are able to form highly effective proxies, more so than any univariate features.
As a fictitious example, suppose we have a dataset where one variable is a
binary sex (Female or Male), and another
variable is a boolean is_pregnant. While the is_pregnant
variable does provide some overall correlation with the sex variable,
the strongest proxying effects are seen only for some combinations of the two
variables. That is, if we know that is_pregnant = False for a particular
person, then we can’t necessarily conclude much about their sex, and likely
p(sex = Female) ~= p(sex = Male). However, if is_pregnant = True,
then we’ve likely gained significant certainty about sex and
p(sex = Female) >> p(sex = Male).
This example illustrates a desirable property when identifying proxies. It is of
only limited utility to know that is_pregnant is correlated with
our sensitive attribute sex. It is much more useful to know that
is_pregnant = True is a strong proxy for sex = Female,
whereas is_pregnant = False is uninformative. The difference here
is that we aim to identify small pockets in the data (and combinations of
features) that form highly predictive proxies.
To accomplish this, we’ll rely on the techniques of Frequent Pattern Mining and Association Rule Mining. This will allow us to surface exactly which values of a variable, and higher-order combinations thereof, are acting as strong proxies for any sensitive attributes. Association RulesAssociation Rule Mining is a rather classic technique in unsupervised pattern discovery, and it can be especially useful in the context of FairML and proxies. The ideas often stem from “market basket analysis”, which imagines we have a large database of purchases at a supermarket. For each shopper, we know what items they bought together, such as {milk, eggs, cheese} for one shopper and {diapers, beer} for another. From the co-occurrences of items in baskets, our goals are to (1) identify commonly co-occurring items and (2) identify directional correlations. For our first goal, we can use Frequent Pattern Mining techniques such as FPGrowth, which will result in a list of item combinations that were highly probable in the dataset. Going a step further, we can apply Association Rule Mining to identify not just correlated items, but items correlated with a particular directionality. For example, if a person bought sunglasses and a bathing suit, then maybe they also bought sunscreen. But if all we know is that they bought sunscreen, they’re not necessarily in the market for new shades. Association Rules are of the form “if A, then B”. The antecedent A can be one or more items, and the consequent B can be one or more items. Importantly, we’re not looking solely for relationships that are 100% fulfilled in our dataset. Instead, we’re looking for consequents that follow antecedents with higher probability than we might expect. Quantifying those base-level expectations means we have to introduce some new terminology and measures.MeasuresFor a particular itemset (whether it is an antecedent, consequent, or neither), the support for that itemset is a measure of how frequently the itemset occurs in the data. We can think of it as the overall probability of occurrence for an itemset. Note that for a particular itemset, we need to tabulate not just exact matches to that itemset, but also all other larger itemsets that include at least that one. An itemset with high support (like 0.80) would mean that 80% of the dataset is exactly this itemset. As you might imagine, itemsets with fewer items tend to have more support than itemsets with more items. For example, the itemset {milk} probably occurred in many of our shoppers’ baskets, whereas the combination of {milk, lettuce, hat} was probably more rare. For a particular rule “if A, then B”, the confidence quantifies how often that rule was true. Another way to think about it is: Whenever you saw A, what’s the probability that you would see B? Naturally, if a rule were a really strong correlation, then confidence in the consequent would be close to 100%. In contrast, for a weaker relationship, knowing about antecedent A perhaps doesn’t give us much certainty at all about B. Other useful concepts such as lift and conviction will help us quantify the extent to which A helps us predict B more than we’d be able to predict B otherwise.Mining for Proxies
Identifying association rules in a dataset can be a useful technique for
uncovering proxies for sensitive variables. In this case, we’ll be looking for
association rules of the form “if A, then B” where the consequent B is
one of the values of our sensitive attributes. For example, if we want to find
proxies for Female in our dataset, we can limit ourselves to
association rules where the consequent is the itemset
{Female} and nothing else. Then, the antecedent of such a
rule would be a feature value, or combination, that is highly predictive of a
person being Female. The confidence of the rule tells us how strong
the association is: a confidence near 50% means that the antecedent isn’t
actually a very effective proxy for Female, whereas a confidence
near 100% means we have found a combination of feature values that can identify
females with near certainty. Let’s try this idea out and see how it works.
Identifying Proxies for SexWe’ll use the UCI Adult dataset, which includes financial and demographic data about a group of 30,000 adults. While this dataset is often used for classification tasks (predicting if a person has high income), we’ll ignore for now the supervised learning aspect. Instead we’ll focus on some of the demographic variables (such as Sex or Race) and investigate whether these can be proxied by other variables (such as Education, Occupation, MaritalStatus and so on). We’ll focus on Sex and use Association Rule Mining to identify strong proxies. For generating candidate itemsets, we’ll focus first on categorical variables. Continuous variables can be easily incorporated by discretizing their domains, either manually (such as quartiles) or by dynamically identifying bin splits (in a similar spirit to decision tree algorithms). It is up to us to identify how many input features we might include for itemsets and what order of combinations we want to consider. For ease of interpretability, we might constrain ourselves to first-, second-, and third-order combinations. That is, in the antecedents of our rules, we’ll only want to consider itemsets with one, two, or three items. This choice is up to us, depending on what level of rule complexity we want to access. As we increase this complexity, the combinatorics of itemset and rule generation expands. Luckily, algorithms such as FPGrowth, which are based on tree structures, will allow us to apply these ideas to large datasets and complex itemsets, if we desire.
Let’s see whether the variables MaritalStatus, Occupation, and
Education can form proxies for Sex. Remember we’re not just
looking at the variables holistically, but instead identifying whether certain
values and combinations of those values can be highly predictive of certain
values of Sex. We’ll generate rules up to second-order and see which
rules can predict Male with high confidence. A few top examples are
below.
We see that some (but not all) occupations and educations are strong proxies for
males. For example, for all the rows in the data where Occupation was
Repair and MaritalStatus was Married, those
people were male 99% of the time. This combination is also fairly prevalent, as
the antecedent support is 8%, a sizable chunk of the data. So for this pocket of
the data, we have a near-perfect proxy for Sex. The same applies for
numerous other combinations, though they are less prevalent. There are many
association rules generated with confidence greater than 80% for predicting
Male. They typically entail small pockets of the data (support of
1% to 5%). The effect of each is small, but the aggregate is that for large
volumes of the data space, we can predict males with high confidence.
Similar results are returned if we look for proxies for Female.
Here, we see that MaritalStatus can be a strong proxy for
Female in some instances. Especially strong is the antecedent “if
MaritalStatus is Widowed”, an unsurprising rule since the
term “widowed” is fairly gendered to begin with. In combination with
Occupation, MaritalStatus can become an even more precise
predictor of Female.
Take for example the rule “if Occupation is Admin-Clerical,
then Sex is Female”. This rule’s confidence is 67%, so a bit
higher than the base-rate of Female in the dataset, but it does
have a support of over 10%. The higher order rule “if Occupation is
Admin-Clerical and Education is High School,
then Sex is Female”, has slightly higher confidence (at 70%)
and slightly lower support (at 5%). As the antecedent becomes more precise, we
can find smaller pockets and subsets of the data that yield more confident
proxies of the sensitive attribute.
Intuitively, these results are unsurprising: many jobs are highly correlated with males or females. But we now have a more precise way to understand this effect. The support of each rule tells us how prevalent a particular combination is in the data, and the confidence tells us how certain the proxying can be. This analysis is easy to apply to any sensitive attributes we want to explore. We could identify proxies for certain races, age groups, health conditions, and so on. Next StepsGiven how informative some of these combinations can be, it would be unsurprising for an ML model to pick up on these proxies even if it didn’t have direct access to sensitive attributes. Any sufficiently complex algorithm (even just a decision tree or tree ensemble) should easily be able to capture the simple combinations highlighted here, if they are helpful in predicting the target variable. Surfacing these relationships should be an early part of the model development process and should play a role in feature selection. Ultimately, our goal must be to mitigate disparate impact in the final system. With this clarity into proxies, we could choose to (i) omit certain variables from a model, (ii) omit/recode certain values of a variable from the model, or (iii) turn to training-time and post-hoc methods for bias mitigation. Want to further explore Arthur’s bias detection capabilities? Click here to learn more.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth
 Predicting the Future with Machine Learning
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Monitoring & PerformancePredicting the Future with Machine Learning By:Arthur TeamMarch 11, 2022The goal of machine learning is to predict the future, based on data from the past. It’s more important than ever to make predictions that match reality, but as the world changes around us, so does the data that is used to generate new predictions. Machine learning models fail silently, which means they will make predictions even if the incoming data looks nothing like the data they were trained against. They will make predictions for scenarios and situations they were never trained for. They will be inaccurate and incorrect; and worst of all, they will be confidently incorrect.And these incorrect predictions will influence business decisions, impacting both dollars and human lives.‍‍Using Ground Truth to Calculate Model Performance  The most straightforward way to measure model success in production is to compare how closely predictions match reality, using performance metrics like Accuracy, Precision, F1, Recall and more. Performance metrics provide insights into how the model is holding up in the production environment, and when the model needs re-training (or tuning). These metrics also help data scientists and model owners calculate the delta between predicted truth and ground truth. Machine learning teams rely on ground truth to test predictions that algorithms are making against the real world. No ML model guarantees 100% accuracy, but the goal is to get it as close as possible to this target. Each business area responsible for oversight of ML models in production sets their own tolerance when it comes to performance metrics. This threshold is carefully defined based on many aspects, including potential impact to revenue (both positive and negative).Depending on the use case and the nature of the data, model performance can start degrading as soon as the model is deployed to production without a monitoring solution in place. After weeks in production, if the accuracy of that model has dropped to 70%, it could be well below what’s acceptable by business leadership, causing a measurable impact to business KPIs, such as revenue. However, with an AI Performance solution like Arthur that provides continuous model monitoring, it’s easy to detect when the tolerance threshold set for model accuracy falls into an unacceptable range and correct for it. Measuring performance is straightforward if data is generated from a known model, the ground truth. However, all models trained are limited by the ground truth quality used to train and test them, and by the timing and availability of ground truth data in production.‍‍‍‍Ground Truth Challenges 1. Accounting for time series and seasonal variability‍Delayed ground truth is quite common when there’s a calendar delay between model prediction and when the ground truth information is ready. An example of this is in the financial services industry where customers have up to 3 months after a suspect transaction to flag it as fraudulent. There’s a 90 day lag; problems don’t manifest in real time when the original transaction occurs.‍2. Definitions for ground truth vary across the organization Supervised learning requires a large volume of diverse data with corresponding correct ground truth labels. Enterprise datasets are siloed in systems across the organization and often complex in nature. These systems are often not interoperable. Ground truth consistency suffers when there are missing, inconsistent or edge case annotations.     ‍‍‍3. Computer vision and NLP models require humans-in-the-loop labeling‍When you are working with model types like computer vision (CV) or natural language processing (NLP), ground truth labels are not readily available without manual annotation which requires labor-intensive processing. With NLP, you cannot always rely on the literal word meaning but someone must infer customer intent or satisfaction.  ‍‍‍‍Solving for Ground Truth ‍While some ground truth challenges must be addressed earlier in the ML lifecycle at the organizational level or during model development, accounting for delayed ground truth can be solved with Arthur’s performance technology.In some use cases, the ground truth will be available seconds after the prediction; in others, it could take months or years. Arthur’s platform allows for ground truth data to be updated at any moment, regardless of when the inference was recorded and with no data duplication. Performance metrics are then recalculated on the fly. Ground truth data can be updated individually (for every inference, as the data becomes available), or in bulk. With full support through the SDK and API, there are many different options to automate ground truth updates. Very simple scripts can be used to retrieve data from log files, databases or other sources and leverage the Arthur API to update ground truth data.‍‍Working with Models When Ground Truth Is Delayed‍In most production applications, there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. Leveraging labeling teams or services can help close this lag, but it will not completely remove it. Instead of monitoring metrics based on outputs, Arthur can alternatively monitor inputs based on data drift metrics with automating data drift thresholding. Arthur automatically creates relevant thresholds for detecting data drift, driving speed to value in optimizing ML models, while other ML observability and monitoring solutions rely on users to manually define thresholds for each attribute, which is slow and labor-intensive.  ‍“It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit fact.” - Sherlock HolmesRemember, without ground truth data, the value of your predictive algorithms can be called into question. Using Arthur with ground truth to calculate model performance or using automated data drift thresholding to combat delayed ground truth, will build greater trust in model predictions and drive better business outcomes for everyone. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense
 Arthur selected to provide critical AI performance capabilities for Department of Defense
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur selected to provide critical AI performance capabilities for Department of DefenseBy:Arthur TeamFebruary 16, 2022For Immediate ReleaseNew York, NY // February 16, 2022The Joint Artificial Intelligence Center (JAIC) has selected Arthur to deliver its industry-leading AI performance management solution as part of the Test and Evaluation (T&E) Blanket Purchase Agreement (BPA,) a $249M effort to accelerate production AI capabilities across the Department of Defense (DoD).This new vehicle will allow the DOD to leverage the Arthur platform for AI monitoring and optimization to deliver better results with accuracy, explainability, and fairness. Arthur’s world-class research team has pioneered exclusive capabilities in computer vision, natural language processing (NLP), bias mitigation, and other key functional areas. With the rollout of the DoD’s Ethical AI Principles — Responsible, Equitable, Traceable, Reliable, Governable — in February 2020, ensuring the appropriate foundation is in place for production AI capabilities is paramount.The Arthur team brings considerable expertise and commitment to the federal government, having successfully worked with DARPA, the IC, NSF, NIST, ARPA-E, Treasury, Air Force, Army, and Navy.Interested federal agencies should contact PublicSector@arthur.ai for more information and to schedule a briefing.				About ArthurArthur is the AI Performance Company. Our platform measures and improves machine learning models to deliver better results. We help data scientists, product owners, and business leaders accelerate model operations to optimize for accuracy, explainability, and fairness. Arthur’s research-led approach to product development drives exclusive capabilities in computer vision, NLP, bias mitigation, and other critical areas. At Arthur, we’re on a mission to make AI work for everyone, and we are deeply passionate about building ML technology to drive responsible results.	 	 ‍About the DoD Joint Artificial Intelligence Center‍The Department of Defense (DoD) Joint Artificial Intelligence Center (JAIC) was established in 2018 to be the focal point of the DoD Artificial Intelligence (AI) Strategy to accelerate scaling AI and its impact across the Department. The mission of the JAIC is “to accelerate the delivery of AI-enabled capabilities, scale the Department-wide impact of AI, and synchronize DoD AI activities to expand Joint Force advantages.” Working closely with the Services, Combatant Commands, and other Components, JAIC identifies appropriate use cases for AI across DoD, rapidly pilots solutions, and scales impact across its enterprise. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning
 Fast Counterfactual Explanations using Reinforcement Learning
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ExplainabilityFast Counterfactual Explanations using Reinforcement LearningBy:Karthik Rao and Sahil VermaMarch 18, 2022IntroductionCounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps to move a data point from one side of a decision boundary to another. These explanations have a clear use-case for several applications ranging from loan decisions (example shown below) to healthcare diagnosis.Problem: A binary classifier, being used by a financial services company, predicts if an individual should be approved (1) or denied for a loan (0). Individuals want to know how to get approved for a loan if the model predicts that they should be rejected.Counterfactual Explanation: We can now provide a series of steps (as changes in the input features) that can help an individual get approved for a loan, e.g. add $10,000 to your salary and gain 2 more years of education.In practice, we might have many more requirements of CFEs beyond just finding the other side of the decision boundary. For example, we might need the features to be constrained to only certain actionable sets, or we might need the resultant counterfactual to be realistic and similar to the training data. Recent work has summarized these various desiderata and the common research themes in the field. Additionally, we need to be able to compute counterfactuals in way that is computationally efficient for high-data-volume uses cases. In this article, we present a distributed and scalable reinforcement learning framework that can produce real-time counterfactual explanations for any binary classifier. We provide an overview of the algorithm used to find counterfactual explanations in real-time, and implementation details of how we've used Open AI Gym and Ray (RLLib) to put this work into practice.Counterfactuals OverviewWe define a counterfactual explanation (CFE) as follows: Definition. Given a model \(f\), and a data point \(x\), where \(f(x) = y\), we want to find a point \(x'\) where \(f(x') = 1 - y\) such that we minimize \(dist(x, x')\)
for any distance function \(dist\).This definition states that we want to find a point \((x')\) where our binary classifier returns the inverse of the original classification \((y)\).  This definition is also malleable, as we can model different causal relationships of features in order to produce more realistic counterfactual explanations. For example, we can define a set of immutable features which cannot be changed (such as gender and marital status), and a set of non-decreasing features (such as age and education level). We also want to find a counterfactual explanation \((x')\), such that \(x'\) is close to the original training data. Such additional constraints allow us to produce counterfactual explanations that are realistic and well-defined.One of the key shortcomings of most counterfactual approaches is that they are computed on each individual instance in a data set. This means that for any set of inferences, we must solve an optimization problem to find a counterfactual explanation that has the following properties:
Correct Output: We need the new data point \(x'\)  to have the desired output from the model (aka moving from a classification of 0 to 1).
Minimize Data Manifold Distance: We want to minimize the distance between our new counterfactual data-point \((x')\) and our original training dataset (through KNN distance).
Adhere to feature constraints: We have to respect the properties of features especially if they are immutable (cannot be changed) or non-decreasing (must only increase)
Solving a new optimization problem for each data point can be expensive, preventing us from creating real-time counterfactual explanations. Our approach creates a model that allows us to pay for this expensive training upfront for the entire dataset and produce fast inferences for any new data point that needs a counterfactual explanation.Reinforcement Learning Framework for CFEsBased on our goal of achieving near real-time explanations, we considered a reinforcement learning framework [Verma, Hines, Dickerson] that allows us to do a one-time training of the model and produce explanations in real-time. In this section, we will present a brief overview of reinforcement learning and how we applied it to counterfactual explanations.Reinforcement Learning OverviewReinforcement Learning is a machine learning framework that allows an agent to interactively "learn" a best set of actions for a desired state in a given environment. Reinforcement Learning has had the most success in robotics and game-playing scenarios (we have seen RL models beat the best players in the world at Poker and Go).A  few key terms will be used through this article as we model our counterfactuals problem:
Agent:
The agent operates in the entire state space (environment) for a problem. The agent aims to make a right series of actions through the state space to perform some task. It will sample the environment in order to learn the best actions in any state.
Environment: A generic set of states that produce a reward when an agent moves from state to state. The agent will only learn about the reward from the environment once it has arrived at a certain state.
Action: At any given state \(s_t\), an agent needs an action to move to state \(s_{t+1}\). The goal of any RL problem is to find a series of actions that move the agent from an undesirable state to a desirable state (by maximizing expected reward through all the states).
For the purposes of this article, we do not provide a more comprehensive overview of a reinforcement learning framework. I would highly recommend reading [this blog], which provides such an overview.Reinforcement Learning for CFEs
Our
recent work (appearing soon at AAAI) shows how to use a reinforcement learning framework to generate real-time counterfactual explanations. Let us consider a very simple dataset: \(x_1, x_2, y\). We have some binary classifier that creates a decision boundary shown in Figure 1. Our goal is to move any arbitrary point on the left side of the boundary, namely points in blue, across the boundary to be classified as the opposite class (orange points).
Figure 1: Decision Boundary for Arbitrary Dataset. Note the binary classifier is not perfect and mislabels a few points, namely the blue points on the right side of the graph.To frame this problem as a reinforcement learning problem, we define the following:
Agent: In our counterfactual scenario, our agent can be considered to be located on any point on the left side of the decision boundary (any of the blue points). The agent needs to take a series of actions (which we will define below), to move from the left side of the decision boundary to the right side.
Environment: Our environment (which represents our state-space) is defined as the entire grid (which is discretized into a finite number of states for every input in our feature space). Our environment will send a reward to agents at state \(s\). The reward function is defined as the sum of two different components:
Classification Reward:
We want to reward agents that are on the correct side of the decision boundary. Therefore, we give a high reward once an agent is across the decision boundary. We use the binary classifier’s predict function to determine if an agent is close to the boundary (if the class probability is close to the decision threshold). Therefore the closer to the decision boundary the agent is in, the higher reward it will receive and vice-versa. This value will always be between 0 and 1.
Dataset Manifold Reward: We want our final counterfactual point to be similar to the training dataset. Therefore, we give higher negative reward for points that are dissimilar to the training data. Computing this similar-ness of states can be implemented in several ways, such as auto-encoders, K-Nearest-Neighbors, etc. For this implementation we used KNN distance (normalized between 0 and 1) to measure the similar-ness of the final counterfactual point.
Action:
In our discretized environment, we allow our agent to move a small distance along any one of the feature dimensions. In our example above, an action \(a\) for an agent at state \(s\), will be a small \((±0.05)\) change for either feature \(x_1\) or \(x_2\). We opted for these small discretized movement to limit to number of movements an agent can perform at any given state. We also believe that an agent will incrementally learn the right movements in order to move towards the decision boundary.
Figure 2 shows a possible path that a point can take to find a CFE.Figure 2: Sample path that an agent can take in our environment.Open AI GymOpen AI Gym provides a framework that allows us to create an environment for an agent to interact with. This is the default standard for defining RL environments (and already comes with a set of pre-defined environments for different tasks), however you have the ability to define your own environment. In order for us to learn a model that achieves our RL task, we must fit it into the Open AI Gym framework.1. Observation Space: You must define what every state will look like for your RL environment. In Python, all observation spaces must be defined by one of the following types: Tuple, Discrete, Box, Dict, Multi-Binary, Multi-Discrete (view here).# We create an observation space as a Box (np array) with limits (-1, 1)
self.obeservation_space = gym.spaces.Box(low=np.ones(shape=len(num_features)) * -1,
high=np.ones(shape=len(num_features)))‍2. Action Space: You must also define what an action will look for any given agent. It is similarly defined by the same types as observation spaces (view here).# We create an action space as a tuple, where the first value is a feature
# index and the second value is binary (increasing or decreasing)
self.action_space = gym.spaces.Tuple((
gym.spaces.Discrete(num_features),
gym.spaces.Discrete(2)
))‍3. Step Function: We define a function that when given an action (from the action space) and a state, it will be able to return the reward produced by taking this action. It will also return the new state the agent moved because of this action (this may or may not be deterministic).def step(self, action: Tuple[int, int]) -> Tuple[tuple, float, bool, dict]:
"""
Step function for the Gym environment
:param: action: action in the gym environment (transformed_feature_index, increase/decrease)
:return: state: an observation space defined above
:return: reward: reward for moving to that state
:return: done: (bool) is the process complete
:return: info: dict info about the environment at every step (useful for debugging)
"""
# Get action and if we should increase or decrease
feature_index = action[0]
decrease = bool(action[1])
# Set default reward to be negative
reward = -10.0
done = False
constant_cost = 0.0
# Checks to make sure we are not changing an immutable feature
if feature_index in immutable_features:
return self.state, reward, done
# Check to make sure we are not decreasing a non-decreasing features
if feature_index in non_decreasing_features and decrease:
return self.state, reward, done
# Move the agent (X_train should be normalized between 0 and 1)
new_state = self.state
amount = -0.05 if decrease else 0.05
new_state[feature] += amount
self.state = new_state
# Compute classifier reward (if we crossed the decision boundary this number
# will be very large)
classifier_reward_, done = self.classifier_reward(self.state)
# Compute KNN reward
manifold_dist_loss =self.distance_to_data_manifold(self.state)
# Compute total reward
reward = classifier_reward - manifold_dist_loss
return self.state, reward, done, info‍4. Reset Function: Once the agent has reach a desired state (or we want to start over), we need to be able to reset to some starting state. We must define the policy for determining the starting state in the reset function.def reset(self, initial_state: pd.Series = None) -> tuple:
"""
Reset methods for Gym environment. Called after an episode is complete or called for starting evaluation with a
specified starting initial state
:param: initial_state: initial starting state (used for evaluation)
"""
if initial_state is not None:
# This is done for inference
self.state = initial_state
else:
# Randomly get a data point from our training dataset
self.state = self.X_train.sample()
return self.state‍We can now formally define our FastCFE (fast counterfactual explanations) class, which has all the above components defined in one python class.class FastCFE(gym.Env):
def __init__(self,
classifier_predict: Callable[[pd.Series], np.ndarray],
X_train: pd.DataFrame,
categorical_features: List[str],
immutable_features: List[str] = None,
non_decreasing_features: List[str] = None):
"""
Initializes the Gym Environment to train the RL Model for Counter-Factual-Explanations (CFE)
(https://arxiv.org/pdf/2106.03962.pdf)
"""
# Initialize all global class variables
self.classifier = classifier_predict
self.X_train = X_train
self.categorical_features = categorical_features
self.immutable_features = immutable_features
self.non_decreasing_features = non_decreasing_features
# Create the state and action space
# We create an observation space as a Box (np array) with limits (-1, 1)
self.obeservation_space = gym.spaces.Box(low=np.ones(shape=len(X_train.columns)) * -1,
high=np.ones(shape=len(X_train.columns)))
# We create an action space as a tuple, where the first value is a feature
# index and the second value is binary (increasing or decreasing)
self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(len(X_train.columns)),
gym.spaces.Discrete(2)))
def step(self, action: Tuple[int, int]) -> Tuple[tuple, float, bool, dict]:
"""
Step function for the Gym environment
:param: action: action in the gym environment (transformed_feature_index, increase/decrease)
:return: state: an observation space defined above
:return: reward: reward for moving to that state
:return: done: (bool) is the process complete
:return: info: dict info about the environment at every step (useful for debugging)
"""
# Get action and if we should increase or decrease
feature_index = action[0]
decrease = bool(action[1])
# Set default reward to be negative
reward = -10.0
done = False
constant_cost = 0.0
# Checks to make sure we are not changing an immutable feature
if feature_index in immutable_features:
return self.state, reward, done
# Check to make sure we are not decreasing a non-decreasing features
if feature_index in non_decreasing_features and decrease:
return self.state, reward, done
# Move the agent (X_train should be normalized between 0 and 1)
new_state = self.state
amount = -0.05 if decrease else 0.05
new_state[feature] += amount
self.state = new_state
# Compute classifier reward (if we crossed the decision boundary this number
# will be very large)
classifier_reward_, done = self.classifier_reward(self.state)
# Compute KNN reward
manifold_dist_loss =self.distance_to_data_manifold(self.state)
# Compute total reward
reward = classifier_reward - manifold_dist_loss
return self.state, reward, done, info
def reset(self, initial_state: pd.Series = None) -> tuple:
"""
Reset methods for Gym environment. Called after an episode is complete or called for starting evaluation with a
specified starting initial state
:param: initial_state: initial starting state (used for evaluation)
"""
if initial_state is not None:
# This is done for inference
self.state = initial_state
else:
# Randomly get a data point from our training dataset
self.state = self.X_train.sample()
return self.state‍Training the Model using Ray + Rllib
Now that we have created our environment, actions, reward, state-space, and have properly defined our OpenAI Gym environment, we must now produce a model that produces real-time counterfactual explanations. We must produce an optimal policy,
\({\pi}(s)\), which will produce an action for an agent in state \(s\) that will maximize future expected reward (reaching our desirable state across the decision boundary). In reinforcement learning, there are several ways to find the optimal policy
\({\pi}\), ranging from model-free to model-based optimization techniques. We recommend this link to review more about training reinforcement learning algorithms.
Ray and Rllib
We were focused on finding an optimization algorithm and framework that is fast, scale-able, and easy to use. Much of the recent work has been focused on Distributed Deep Reinforcement Learning, which uses neural networks to implicitly learn the optimal policy \({\pi}\). One such framework is [Ray + Rllib]:
Ray is a new distributed framework in python designed to distribute training tasks across any cluster. Rllib is a specific package within Ray that is designed to train different RL agents for different environments. Rllib has a variety of different optimization algorithms, and provides a configurable dictionary that allows us to distribute training and evaluation to different cores and machines very easily. Furthermore, it provides an easy API that allows use to modify the internal of the Deep RL Optimizer. Rllib is maintained by Anyscale (founded out of Berkeley RiseLab) and it one of the state-of-the art frameworks for distributed computing/machine learning.We opted to use the Proximal Policy Optimizer (a Deep RL Algorithm) because of its favorable balance of faster training times and simplicity. We needed an algorithm that would train relatively fast and that could distribute fairly simply, both of which PPO provides out of the box. We provide our pseudo-code that provides a FastCFE specific wrapper around a native Rllib PPO optimizer.For our implementation, we opted to use the Proximal Policy Optimizer (a Deep RL Algorithm), which outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time [Schulman et. al]. Rllib provides an out of the box way to use PPO and distribute it across a cluster and on your laptop. Below, we showcase some pseudo for how we wrapped our optimizer around Rllib to provide an easy package to use Rllib and our FastCFE model:class RayLib():
def __init__(self, env: FastCFE = None) -> None:
"""
Initializes a RayLib optimizer for PPO (can add functionally for other optimizers)
:param: env: the FastCFE class which contains our gym environment
"""
if env is not None:
super().__init__(env)
self.env = env
self.model: Optional[ppo.PPOTrainer] = None
self.config: Optional[Dict[str, Any]] = None
def train(self) -> None:
"""
Train the RL Agent for the CFE environment (with hyperparameters)
View more of the features here: (https://docs.ray.io/en/master/rllib-training.html)
"""
hyperparameters = ppo.DEFAULT_CONFIG.copy()
# Register the CFE environment to be trained in Ray
def env_creator(env: FastCFE):
return env
register_env("my_env", env_creator)
# Set the environment config variables
hyperparameters['env_config'] = self.env
# Create the PPO Trainer with the configuration and train
self.model = ppo.PPOTrainer(env="my_env", config=self.config)
print("Training RL Agent.....")
for i in range(10):
result = self.model.train()
print(f"Training Iteration: {i+1}/{num_epochs}")
print(f"Total number of datapoints sampled: {result['episodes_total']}")
print(f"Mean Reward (Max of 100): {result['episode_reward_mean']}")
print(f"Avg. steps to produce counterfactual (Min: 1, Max: 20): {result['episode_len_mean']}")
print("_______________________________________________________________________________________")
return None
def predict(self, initial_state: pd.Series, max_steps: int = None) -> Tuple[List[pd.Series], float, bool]:
"""
Make a counterfactual prediction
:param: initial_state: initial_state
:param: mode: scaled or unscaled path
"""
if self.model:
max_steps = 50
num = 1
next_state: Tuple = self.env.reset(initial_state=initial_state)
done: bool = False
reward: float = 0.0
steps = []
while num < max_steps:
action = self.model.compute_action(next_state)
next_state, reward, done, info = self.env.step(action)
steps.append(next_state)
if done:
break
num += 1
return steps, reward, done
else:
raise ValueError("Model not loaded")BenchmarksWe want to showcase two major benchmarks for this algorithm and implementation:Performance Metrics: We want to understand how our FastCFE approach using RL compares against other methods for performance metrics (described later).Training Time: We want to see how much savings we achieved when using a Distributed RL Framework like Ray against a single-threaded research implementation.These benchmarks for implemented for a variety of combinations of the following credit risk datasets. All of these datasets contained some from of credit risk data and the models are binary classifiers that predict a single applicant (one row) should be accepted or rejected for a loan. We want to find counterfactual explanations for all the rejected applicants. The sizes of the datasets are shown below (number of rows by number of columns):German Credit: 1,000 data-points x 21 featuresAdult Credit: 48,842 data-points x 14 featuresCredit Default: 30,000 data-points x 25 featuresSmall Encoded Credit Dataset: 332,000 data-points x 44 featuresLarge Encoded Credit Dataset: 332,000 data-points x 652 featuresThe first three datasets (German Credit, Adult Credit, Credit Default) are all open source datasets with the links provided above. The last two datasets are proprietary datasets with obfuscated column names. These datasets were larger and tested the scaleable of our implementation.Performance MetricsWe want to see how our FastCFE model compares to other well-known methods. Specifically, we are focusing on the following two metrics:
Validity: This is the total number of counterfactual explanations found divided by the total number of data points. This is represented as a percentage.
Mean Inference Time: This is the mean time it takes to calculate a batch of inferences. Namely is the time it takes to compute \(n\) inferences divided by \(n\).
The results shown below are FastCFE against a number of state of the art counterfactual explanation methods:Adult Credit
Validity(%)
Mean Inference Time (s)
Dice-Genetic
98.1
1.71
Dice-Random
100
0.17
MACE LR
100
38.45
MACE RF
100
101.29
FastCFE
97.3
0.07
German Credit
Validity(%)
Mean Inference Time (s)
Dice-Genetic
89.5
3.45
Dice-Random
100
0.22
Dice-Gradient
84
59.75
FastCFE
100
0.015
Credit Default
Validity(%)
Mean Inference Time (s)
Dice-Genetic
92.6
3.58
Dice-Random
100
0.39
Dice-Gradient
81.0
479.17
FastCFE
99.9
0.051
As we see here, we perform nearly as close as the best method (Dice-Random) across these three different datasets. Furthermore, we have inference times of up to 20x faster than Dice-Random.Training TimeThe first implementation of this project was done using a package called StablesBaseline3, and was naively computed trained on a single machine. This section wants to showcase the change in training time of our scaleable and distributed Rllib implementation against a naive implementation. The results are shown below:
Rllib Train Time (hrs)
Naive Train Time (hrs)
German Credit
.25
1.5
Credit Default
1
6
Small Encoded
1.5
8
Large Encoded
8
DNF
We achieve a nearly 6x savings in train time and can handle much larger datasets than we could through our naive implementation. This shows the promise and power of using a scaleable and distributed reinforcement learning framework — we can significantly reduce training times which is a major bottleneck for several reinforcement learning applications.ConclusionWe hope this article provided an overview into the following ideas/concepts:Counterfactual Explanations: What they are and how they are useful for industrial and explainability applicationsReinforcement Learning Implementation: How we implement a production level reinforcement learning model.Power of Distribution: How we can achieve tremendous savings by using scalable and distributed reinforcement learning frameworks such as Rllib.We hope that we provided some interesting ideas and some starter code to help you make your own Reinforcement Learning model. If you would like to learn more about this article, please reach out!‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but
 Two Commitments Every Employer Should Make in 2022
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLife at ArthurTwo Commitments Every Employer Should Make in 2022By: Adam WenchelJanuary 27, 2022“I really want to work at a startup. But, I care about my mental health.”If you want to see an animated reaction, make a comment like that to someone on our team. Yes, startups are hard work; audacious goals always are. But they don’t have to come at the expense of anybody’s well-being.The last couple of years have been challenging. People are reassessing their priorities, craving meaning in relationships, in free time pursuits, and importantly, in our work. At the same time, people want to feel supported – and employers need to step up and use their unique position to create community and care for people more deeply than they ever have before. At Arthur, we commit to both: fulfilling and challenging work at a company that provides community and care. People join Arthur because they are excited to solve complex problems at the intersection of technology and humanity, and to do it as part of a diverse, creative, and passionate team. The same team that is also raising children, caring for loved ones, taking classes, running marathons, exploring cities, and doing a myriad of other things that make us each happy and whole. It’s awesome to see both our work and our community get strengthened as our team grows and people bring new and unique passions with them!This belief that we need both fulfilling work and genuine community centers our culture at Arthur and we feel the impact of it every day. Here are 7 ways we are creating meaning, connection, and caring at Arthur:Work that has a huge impact on the world. We’re working to make AI more transparent and equitable, changing how technology affects real humans for the better.Flexible work locations. We have open offices for those that want to safely work and hang in the same room; and we support fully remote work across the country.A diverse team that cares & celebrates. We practice empathy, kindness, and inclusion in our work together. We shout out each other’s wins and take the time to help each other solve problems.Team offsites. We meet somewhere cool as a company twice a year for fresh air and time to be creative, think big-picture, and have some laughs together. A learning mindset and budget. Our work on the cutting edge of AI technology keeps us learning all the time, and we offer a generous education and professional development budget. True time away. Our flexible vacation policy includes mandatory minimums, and we fully close the office over the holidays. We also believe in “no weekend messages” and encourage “roam the earth” remote summer work to explore our awesome planet.  A big vision – and the chance to own a piece of it. There’s a ton of opportunity for ownership and growth along the road to achieving our big goals for Arthur. Every member of our team is critical in delivering on that vision.It’s neat to see word spread about the positive culture we are creating – we’ve recently been named one of Business Insider’s 2022 Enterprise-Tech Startups to Bet Your Career On and we made Built In NYC’s Best Places to Work.It’s an important formula that has an outsized impact – care about people, create connection, do meaningful work and build something really special in the process.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022
 Built In Honors Arthur in Its Esteemed 2022 Best Places To Work Awards
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesBuilt In Honors Arthur in Its Esteemed 2022 Best Places To Work AwardsBy:Arthur TeamJanuary 5, 2022Arthur Earns 15th Place on Built In’s List of 50 Best Small Companies in New York City‍For Immediate Release New York, NY // January 5, 2022 — Built In today announced that Arthur is a winner in their 2022 Best Places To Work Awards. Specifically, Arthur earned a place on the 50 Best Small Places to Work in NYC and 50 Best Paying Companies in NYC lists. The annual awards program includes companies of all sizes, from startups to enterprise, and honors both remote-first employers as well as companies in the eight largest tech markets across the U.S.“We’re thrilled to be recognized alongside the best companies in New York,” says Adam Wenchel, Arthur’s Co-Founder & CEO. “I’m constantly inspired by our team’s focus on our mission to make AI work for everyone. We’ve clearly seen and felt the challenges the last two years have presented for people – and so creating a caring culture built on integrity, equity, diversity, and compassion is the best thing we can do for all of us. 2022 will be a big year for us and we are excited to welcome many new voices!” Check out open roles at Arthur here. Built In determines the winners of Best Places to Work based on an algorithm, using company data about compensation, benefits and companywide programming. To reflect the benefits candidates are searching for more frequently on Built In, the program also weighs criteria like remote and flexible work opportunities, programs for DEI and other people-first cultural offerings. “It is my honor to extend congratulations to the 2022 Best Places to Work winners,” says Sheridan Orr, Chief Marketing Officer, Built In. “This year saw a record number of entrants — and the past two years fundamentally changed what tech professionals want from work. These honorees have risen to the challenge, evolving to deliver employee experiences that provide the meaning and purpose today’s tech professionals seek.”‍ABOUT BUILT INBuilt In is creating the largest platform for technology professionals globally. Monthly, more than three million of the industry’s most in-demand professionals visit the site from across the world. They rely on our platform to stay ahead of tech trends and news, develop their careers and find opportunities at companies whose values they share. Built In also serves 1,800 innovative companies of all sizes, ranging from startups to the Fortune 100. By putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard-to-reach tech professionals, locally, nationally or remotely. www.builtin.com  ABOUT BUILT IN’S BEST PLACES TO WORKBuilt In’s esteemed Best Places to Work Awards, now in its fourth year, honor companies across numerous categories: 100 Best Places to Work, 50 Best Small Places to Work, 100 Best Midsize Places to Work, 50 Companies with the Best Benefits and 50 Best Paying Companies, 100 Best Large Companies to Work For, and 100 Best Remote-First Places to Work. ABOUT ARTHURAt Arthur, we are deeply passionate about building technology to make AI work for everyone. Arthur is the AI performance company that delivers on the full potential of equitable AI for enterprise customers through performance monitoring and optimization, explainability, and bias detection. Learn more at www.arthur.ai Contacts:  Tory Marlin, Director of Marketing  tory@arthur.ai  PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners
 A Crash Course in Fair NLP for Practitioners
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessA Crash Course in Fair NLP for Practitioners By:Jessica DaiNovember 14, 2021The topic of fairness in Natural Language Processing (NLP) is exceptionally broad; in this post, we hope to distill some of the key points from academic literature for an audience of technical practitioners. In section 1, we outline a general framework for thinking about fairness; in section 2, we survey some notable academic work; and in section 3, we outline a set of questions that may be useful when considering specific applications.‍1. What does fairness in NLP even mean?The core idea we hope to illustrate in this post is that there is no panacea for magically achieving "fair NLP" — though many of the core problems are intuitive, the complexity of human language means that measuring, much less mitigating, "unfairness" is a difficult task. "Bias" and "fairness" are exceptionally broad terms that span wide range of possible behaviors. There is no single definition of desirable "fair" behavior; to the extent that NLP systems model the world (or a particular worldview), there is no single perfectly neutral, unbiased model. In other words, any NLP system involves some proposition about both what the world does look like and what the would should look like; for practitioners, it's critical to think deeply and with precision about what exactly desired behavior looks like, and why.That being said:The first component of any approach to fairness is defining who and what exactly we want to be fair with respect to. While social groups in the real world are fluid, in Artificial Intelligence (AI) and Machine Learning (ML) we typically define discrete groups along the axes of gender, race/ethnicity, and religion. (Fair ml, especially in language, unfortunately tends to treat gender as binary — both because it is mathematically convenient and because the data on which language models are trained often reflect a binary.)For fairness in a typical tabular data setting, we generally assume that each data point reflects information about a single person, and that the values of these demographic attributes are generally known or accessible for each datapoint. For fairness in NLP, however, there isn't always a clear mapping between text and demographic information. More specifically, social groups might be inferred, but demographic information may be labelled based on author demographic — that is, text generated by particular groups, which covers things like dialect, accent, or writing style — or subject demographic — that is, text about particular groups. Crucially, author and subject demographic are distinct approaches to defining fairness. Which method to use for demographic labelling is context-dependent and varies based on the task at hand.Similarly, there are many dimensions in NLP settings across which fairness can be measured. When the end-goal of the NLP model is something like classification or regression, we might be able to apply existing metrics for fairness in these applications by measuring the group-conditional performance (e.g. positivity rate, TPR, FPR, etc.). In language, particularly text generation, additional harms arise — most prominently, language models which propagate harmful societal stereotypes. Measuring stereotypes is a murkier task: existing (academic) approaches have focused on either investigating the trained model artifact itself (i.e. the word embeddings), or evaluating the model outputs on some specially-curated datasets. However, both of these approaches have known issues, and should not be considered to be any conclusive or concrete standards.‍2. A (non-exhaustive) survey of relevant workOrdered by year. Starred entries are worth reading in full!1. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (2016)‍One of the first works on "bias" in language models. Measures/illustrates bias by using the word embeddings to generate analogies via vector addition/substraction, showing that embeddings confirm stereotypes; demographic groups are therefore determined with respect to text content. Debiasing approach involves identifying the "gender subspace" and rotating the word embeddings such that they are orthogonal to the subspace.2. Gender and dialect bias in Youtube automatic captions (2017)‍This is an illustrative example of the many moving parts in what is casually referred to as "fair NLP". This is a speech-to-text task: i.e., one where the output itself is text. However, there is some notion of performance that summarizes the goodness of the text output in a single number. Demographic groups are determined with respect to speaker (author), not content. Bias is observed here because of differential model performance across groups.3. Social Bias in Elicited Natural Language Inferences (2017)This work analyzes a dataset that was popular at the time (the Stanford Natural Language Inference corpus; with the rise of larger language models trained on the web it's unclear the extent to which this is still used). The approach is similar to "Man is to Computer Programmer" — they use a mathematical measurement of similarity between words (in this case mutual information) to find (gendered) associations across words, with demographic groups determined by text content. In my opinion, this does exhibit some of the pitfalls outlined in "Language (Technology) is Power" — there is no explicit discussion of what comprises a harmful association.4. Neural User Factor Adaptation for Text Classification: Learning to Generalize Across Author Demographics (2019)Here, demographic groups are determined via authorship rather than text content — this work explores gender, age, country, and region; finds the existence of performance disparities across groups; and introduces a novel approach to learn text classifiers which reduce those performance disparities.5. Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings (2019)An extension of the 2016 "Man is to Computer Programmer" paper to the multiclass setting; the original work made use of "binary" gender in calculating a "gender direction/subspace".6. Mitigating Gender Bias in Natural Language Processing: Literature Review (2019)A lit review of approaches to gender bias x NLP (at this point a few years old); mostly useful for a high-level overview of many possible tasks and approaches.‍7. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them (2019)This work shows limitations of the approaches to debiasing word embeddings in "Man is to Computer Programmer" and "Black is to Criminal". In short, while the approaches enumerated in those papers do successfully debias with respect to their original definitions of "bias," they ultimately preserve most relationships between words in the corpus: "gendered" words still cluster together. As a result, it is possible to recover the original "biased" relationships, and they may persist in downstream applications of those embeddings even if not detected according to the original metric. This work is a clear example of why "debiasing" in general but especially in language must be evaluated with a critical/skeptical eye, and why specifications of desired "unbiased" behavior must be careful and precise.8. **Language (Technology) is Power: A Critical Survey of "Bias" in NLP (2020)This survey paper is worth reading (or at least skimming) in full. This paper is motivated by the idea that there is no single definition of "desirable behavior," and no such thing as a "completely unbiased" model or dataset; instead, any specification of desired behavior is inherently value laden. The survey conducted of work on "fairness/bias in NLP" finds that most such work does not state clearly what comprises "bias" and how to conceptualize algorithm behavior with respect to broader societal power structures — to whom the harm is done and how those groups are defined; whether the harm is primarily representational or allocational; what behavior is deemed harmful and what is not, and why — and more.9. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models (2020)Introduces a dataset for evaluating the performance of masked language models (models trained on data like This is a masked sentence; the [MASK] is to determine what word is behind the masked token. — the paper reports results on BERT and BERT+ models). Several axes of discrimination/bias are included — gender, race, sexual orientation, nationality, religion, age, dis/ability, appearance, and socioeconomic status.10. REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models (2020)This paper focuses on toxicity, specifically the generation of toxic (racist, sexist, etc) text by pre-trained language models. This is a slightly different paradigm than the typical "bias" approach — rather than considering harms against specific groups, this work groups all harmful/derogatory generated text as "toxic." The authors find that even surface-level innocuous prompts can trigger highly problematic output, and that existing methods are insufficient to prevent this; upon inspection of training corpora, they find high volumes of toxic content in the training data.11. **On the Dangers of Stochastic Parrots: Can Language Models be too Large? (2020)This is the infamous Stochastic Parrots paper that ultimately led to the ousting of Drs. Timnit Gebru & Margaret Mitchell from Google Research. This is a broader survey paper about the harms of large language models, including the centralization of power, cultural homongenization/flattening, environmental harms, among others. Worth a read for broad context in responsible NLP.12. REDDITBIAS: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models (2021)Introduces a dataset for evaluating the bias of conversational language tasks based on Reddit data, as well as evaluation frameworks for conversational model performance after debiasing — includes four axes (gender, race, religion, queerness). They benchmark DialoGPT and several debiasing approaches on this dataset and find evidence of bias with respect to religion that can be mitigated with some methods (though not all).13. Rooting Out Anti-Muslim Bias in Popular Language Model GPT-3 (2021)This is a blog post (with a link to the full paper) summarizing some explorations of anti-Muslim bias in GPT-3's text generation. In short, GPT-3 exhibits substantial anti-Muslim bias in its generated text, which is only slightly reduced in existing mitigation methods.14. **Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets (2021)This paper surveys existing benchmark datasets for evaluating "fairness" in NLP tasks. The authors — who also wrote the 2020 "Language (Technology) is Power" paper — apply a social science approach ("measurement modeling"), and find that the benchmark datasets themselves have unclear definitions and specifications behind both what constitutes "biased" or "stereotyping" behavior and what constitutes desirable model behavior. If evaluating models on the datasets covered in this paper, the results should be taken with a grain of salt. Worth a skim, especially the illustrative example on the first page, to understand the gist of the criticism.15. Challenges in Automated Debiasing for Toxic Language Detection (2021)This paper focuses on text classifiers, specifically toxicity detection. Demographic groups are explored both in terms of text content (swear words, slurs, identity mentions) and text authorship (AAVE dialectical markers). Bias here is defined by the unjustified flagging of toxic text (in conventional classification terms, high false positive rates). The authors find that existing methods are generally unsuccessful in debiasing toxicity detectors, and propose a proof of concept approach which synthetically relabels the training data; this approach (modifying the training data) is more effective than attempting to modify a pretrained model.'‍Additionally, some scholars who actively work in this area are Jieyu Zhao, Su Lin Blodgett, Amandalynne Paullada; their future work is likely to be relevant as well.‍3. A worksheet for practitioners1. Defining the language task and model setting:‍Assuming the model takes in some amount of text, does it generate a single output (e.g. a probability, a classification, or multiple classifications), or text output?2. Defining the sensitive attribute:Do you care about author demographic or subject demographic, or both?Are you able to come up with or access sensitive feature values for each data point? For example, can you come up with a vector that looks like [ <string input>, <demographic info> ]?3. Defining and measuring the harm:If the model generates a single output, we can check typical measures of fairness (disparate accuracies or TPRs or FPRs or positivity rates etc).If the model generates text output:‍— Is there any notion of performance that is used to measure the "goodness" of the text output? You may be able to measure the performance of the generated text and determine whether there are group-wise performance disparities if you already have a means for evaluating generated text.‍— What sorts of representational or stereotyping harms do you anticipate? In other words, what is the best-case expected output, and what does a "bad" output look like?4. Mitigating the harm:If the model generates a single output: existing classification/regression postprocessing approaches to fairness may be worth attempting, though they will be limited in that they cannot make use of the the text input. See annotated bibliography for some examples of bias work in text classification.If the model generates text output:— For concerns around representational harms (such as stereotyping), do you have a sense of what prompts might trigger "bad" output?— Most mitigation techniques for language models rely on adjusting model internals, and even then, have varied degrees of success (see annotated bibliography).‍Everything you need to know about model monitoring for natural language processing. Discover Arthur’s features for ensuring Fairness, including detecting and mitigating bias in NLP.‍Photo by Jason Leung on UnsplashPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems
 Hotspots: Automating Underperformance Regions Surfacing in Machine Learning Systems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringHotspots: Automating Underperformance Regions Surfacing in Machine Learning SystemsBy:Kenneth ChenNovember 1, 2021Isolating Problematic Data for Remediation and Retraining ML modelsWhen a system has high dimensional data, troubleshooting the right data input regions becomes a difficult problem. Hotspots automates identifying regions associated with poor ML performance to significantly reduce time and error of finding such regions.OverviewWe might have a ML model deployed in production and some monitoring in place. We might notice that performance is degrading from classic performance metrics or from drift monitoring combined with explainability techniques. We’ve identified that our model is failing, and the next step is to identify why our model is failing.This process would involve slicing and dicing our input data that caused model degradation. That is, we want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks or retraining our models on those regions.This basically boils down to a time-consuming task of finding needles in a haystack. What if we could reverse engineer the process and surface all of the needles, i.e. input regions associated with poor performance, directly to the user?We can! The steps we’ll take are(1) Train a decision tree on a proper partitioning objective.(2) Create and store hotspot tree artifact.(3) Retrieve hotspots from the hotspot tree at query time.Fig1: A general framework for training dataset and using it to create hotspot tree, which is then queried to retrieve hotspots. Each phase outputs the artifact that is used in part I of the following row.In the toy example below that we’ll use throughout this post, we have two ground truth regions separated by a parabolic function, with blue above and red below the parabola. The color of the datapoints represent the predictions. We want to isolate the hotspot regions where the prediction color does not match the region color, which we do so in the pale boxes for two different accuracy thresholds.Fig2: Example of hotspots retrieval on two inputs. The boxes are the hotspots, where blue points in red regions and red points in blue regions are incorrect classifications. As the threshold (accuracy, in this case) decreases, the algorithm targets node candidates with more incorrect classifications and the hotspot regions are narrower/purer.(1) Train a decision tree on a proper partitioning objectiveDecision TreeAs soon as we think about partitioning data into regions of interest, we should think about tree models, and specifically a decision tree. Remember that our task is ultimately an inference task and not a prediction task, so there is no need to use an ensemble of trees model like random forests or XGBoost because (a) we’re not trying to perform predictions and (b) ensembles introduce noise and non-deterministic decision paths for splitting our data.Recall that the premise of decision tree splits are based on selecting a feature and split value, among an enumeration over input features and their possible values, that minimizes impurity to create children that are more pure, based on the output labels.In simple speak, let’s say output was color and we have blue and red marbles. All of the marbles have varying diameters across both groups, but blue marbles are textured while red marbles are smooth. If we had to choose between diameter size or texture to partition our marbles, we’d choose to partition our marbles based on texture, i.e. textured or smooth, since that would perfectly separate blue marbles into one group and red marbles into another, effectively reducing impurity to 0 in each group in this case.In reality, a dataset would need multiple splits in order to reduce impurity to 0 in the leaf nodes.Partitioning ObjectiveSo what is exactly the equivalent of the the blue and red marbles example above? We ultimately want to separate the bad predictions from the good predictions and need some metric as the output, i.e. partitioning objective, in our decision tree.For classification, we can encode correct classifications as 1s and incorrect classifications as 0s. If we want more granularity while partitioning, we could also encode classifications as 1, 2, 3, or 4 for true positive, false positive, true negative, and false negative, respectively.For regression, we actually need to encode the regression outputs, i.e. RMSE between ground truths and predictions, as classification outputs, e.g. a datapoint is encoded as 1 if RMSE is greater than median RMSE + 2 median average deviation of RMSE and 0 otherwise. We could also use a percentile rule, e.g. datapoints with RMSE over the 80% percentile are 1s and 0s otherwise. The reason why we do not use mean and standard deviation is because those values are skewed by high RMSE outliers, and the entire point is to partition datapoints with high RMSE compared to the average. This mimics the behavior we want in the classification case, and we’ll dive more in the mathematics for why we cannot use regression outputs, after the methodology is explained for the classification case.(2) Create and store hotspot tree artifactIf we feed the 500 datapoints from our toy example into a decision tree, using our encoding method discussed above for four classes, the tree looks like this:Fig3: Example of [IV. Trained Decision Tree] on two inputs, X[0] and X[1]. The boxes are nodes, where each node has an associated set of metrics (i.e. accuracy, precision, recall, F1 score) that can be used as thresholds in querying for hotspots. Each node represents a collection of datapoints, which are filtered by the accumulated ranges on the input features on the path from the top root node to the particular node in question. A hotspot is a node that violates the threshold specified by a user when [III. Extract Hotspot Nodes] step is performed. Hotspots can be any node in the tree and not only the terminal leaf nodes at the bottom of the tree.Here, we only have two features: X[0] and X[1]. At each node in a decision tree, the data is split into two children nodes based on a feature and cutoff value. As an example, at the root node, the 500 datapoints are split into two groups, a left child group and right child group, where X[1] ≤ -2.258 and X[1] >-2.258, respectively. We can accumulate the rules along any particular path from the root node to any child node.We can also compute performance metrics like accuracy, precision, recall, and F1 on the data in the node.(3) Retrieve hotspots from the hotspot tree at query timeNow that we have our hotspot tree, let’s pick some hotspots! Notice in Fig2 that we have accuracy thresholds of .3 and .5. In the latter case, the hotspot regions are wider and accidentally capture more correctly classified points. As the threshold decreases, we are less tolerant with wider regions that accidentally capture correct classifications. Normally, we might think that we always want lower thresholds to capture only misclassified datapoints, but that does run the risk of (a) making the regions incredibly small and not interpretable and (b) isolating many regions that might not contain many datapoints, requiring lots of manual work to investigate.So how were those hotspots retrieved?When an accuracy threshold is sent in a query to extract hotspots from the hotspot tree, we traverse along all possible paths from the root node. At any node in the traversal, if a node violates the threshold, that node is defined as a hotspot and all of the information in that node is appended to a list of hotspots returned to the user.Specifically, in our example, accuracy is our metric. At any particular node, if that node’s accuracy is less than the threshold, we know that the datapoints in that node collectively have violated the threshold and that node is a hotspot. Whenever a node is identified as a hotspot, the traversal along that path stops, since downstream nodes would be more pure and the nodes with poor performance are in even more extreme violation of the user-provided metric and threshold.Of course, what’s great about defining the metric and threshold at query time is that a user can requery with different metrics and threshold combinations, depending on the question and tolerance level for the threshold.Hotspots Found!Using our example with the .5 threshold, our three hotspots areHotspots contain the filters on the input data regions that can be applied in fetching the entire dataset for further examination, research, and model development.‍Deep DiveAs promised, answers to the hanging question about regression above, plus some deeper aspects!Deep Dive: (1) Train a decision tree on a proper partitioning objectiveSo why not regression?The naive setup would be to take the RMSE between ground truths and predictions in our regression task and use them as our partitioning objective. However, the regression criterion to split the data in a node is based on some distance metric, e.g. MSE, to the mean output value of the data the node, which in this case are the RMSE values themselves. When we traverse the hotspot tree, we’d flag nodes with MSE above a certain threshold as hotspots.Let’s say we have a majority of RMSE values around . 05 and a few RMSE values around .10, where we want to flag and isolate the latter. It’s totally possible that a regression tree might group more of the .10 RMSE datapoints lower in the tree and we’d never be able to find them according to the stopping role once a hotspot is detected. For example, one path might be .06 MSE, .09 MSE, .04 MSE. Given a user threshold between .06 and .09 MSE, we would never reach the third node in this case since .09 > .04, and that third node could contain a lot of .10 RMSE datapoints (the low .04 value comes from the fact that there are many .10 RMSE datapoints and only a few .05 RMSE points, so the node is “pure” in the sense that the RMSE values are close together).We could fix this with a custom regression criterion that uses distances against 0, i.e. the raw RMSE values, instead of the MSE distance against the average RMSE in a node, but that introduces an issue of standardization across models and data rollups. What if a RMSE value of .10 really isn’t bad for one model but is for another? As such, it’s more robust to convert the regression setting into a classification setting.Data RollupIf we have a streaming (or batch) model, how much data do we put into creating a hotspot tree at one time? If we created a hotspot tree on last week’s (or a previous batch’s) data and now see incoming data, do we append those datapoints and retrain the previous tree or do we create a new hotspot tree for this week’s (or current batch’s) data?There’s certainly no wrong way to implement this, but at Arthur AI, we take the latter approach. If we have an orange tree and now have some apples that arrive in a box, we’ll probably be interested in the apple tree that generated those apples to figure out why we received some rotten apples, not the orange tree that we’ve already inspected last week (or batch).Metadata Galore!Notice how the inputs to the decision tree can really be anything, including metadata not used as inputs to the model that created the original predictions! This means surfacing insights via hotspots is not constrained only to model inputs, which can have benefits if we are tracking sensitive non-input attributes in a model, like race or gender.Deep Dive: (2) Create and store hotspot tree artifactWhy all the metrics?Accuracy is not king and users need different metrics depending on the task their ML model is solving. This is a good introduction to other performance metrics like precision and recall. After giving it a read, here are two practical examples that helped me when I first learned about precision and recall.(1) Consider an anomaly detection system for a bot that scrapes financial data from financial reports. It would be terrible if we have many false positives from a ML model, which means that the model would predict that wrong information (positives) exists for many documents that actually are correct (false), leading to a remediation team wasting time on tasks that are not incorrect (and it really wouldn’t be highly problematic if some false negatives slipped through the cracks). In this case, high precision is important.(2) Consider a cancer detection model. It would be terrible if we have many false negatives from a ML model, which means that the model would predict no cancer (negatives) for many people who indeed have cancer (false), leading to a lack of recommended treatment and further health complications for those individuals. In this case, high recall is important.Multiclassification and MicrometricsHow do we generalize to multiclassification tasks? Accuracy is the still the same, and we can use weighted precision, recall, and F1 Score, weighted by ground truth size, under global metrics in the toy example below. Notice how instead of just macrometrics, we also can now define micrometrics for precision, recall, and F1 Score, with respect to each ground truth class. This can be powerful, as now hotspot trees can be traversed on specific classes if, e.g. the bird class is causing a lot of model failures for an object detection model and we want to figure out what’s going on for bird images in particular.Fig4: A Multiclassification Example‍‍ConclusionThat’s it for today! Hope you gained some insights about how to implement hotspots surfacing for your particular use case! We implement these kinds of systems at Arthur, and automation is an important product category in ML monitoring for the customer experience.Happy monitoring!Photo by DESIGNECOLOGIST on UnsplashHeader Photo by Nareeta Martin on UnsplashPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems
 Automating Data Drift Thresholding in Machine Learning Systems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringAutomating Data Drift Thresholding in Machine Learning SystemsBy:Kenneth ChenNovember 1, 2021Monitoring Production ML Model Inputs Practically and Efficiently in the Absence of Ground Truth DataIn real life ML monitoring applications, we want to detect whether or not a ML model is performing well or is breaking. Not picking up on poor model performance can translate into poor or even biased predictions that can lead to lost revenue and even PR fires that we see year over year from even the large tech companies, from facial recognition systems that fail to pick up on certain minorities to hate speech in autocomplete for search engines.In this article, we’ll go over a brief overview of how data drift thresholding helps capture poor model performance, and the majority of the post will focus on two versions of implementing automated data drift detection in production level ML monitoring systems.OverviewWhy do we need data drift?The standard approach to monitoring models is based on performance metrics, i.e. accuracy/precision/recall/f-score, over some time period or from batch to batch. In order to produce these metrics, we need both predictions and ground truth labels for the datapoints, e.g. a credit risk model predicts that a person will pay their loan on time in 1 year and thus should be approved for a credit card, and we know whether or not that person paid their loan on time in 1 year. Already, we have an issue because we do not know ground truth until 1 year later.In most production applications, there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. Leveraging labeling teams or services can help close this lag, but it will not completely remove it. As such, instead of monitoring metrics based on outputs, we can instead monitor inputs based on data drift metrics.What is data drift?Data drift fundamentally measures the change in statistical distribution between two distributions, usually the same feature but at different points in time. As an example, in the univariate case where we’re looking at one input feature, we should reasonably expect that, if the shape of the feature shifts significantly between training time and prediction time, the model outputs will degrade in quality.As a toy example, if we train a ML model to solve math problems only on algebra questions, and all of a sudden geometry questions are fed into the model at prediction time, we’d expect the predictions to be pretty bad since the model hasn’t been trained on geometry questions.In essence, data drift is a proxy for our classical performance metrics in the absence of ground truth labels. The next natural question is how to formally quantify data drift.Overview of data drift metricsThere are many different kinds of metrics we could use for quantifying data drift. Here, we’ll focus on two popular families of metrics: f-divergence and hypothesis test metrics. For the former, we’ll look at KL Divergence and PSI. For the latter, we’ll look at Chi-Squared and KS test statistics.For any drift metrics, P is the training data (reference set) on which the ML model was trained and Q is the data on which the model is performing predictions (inference set), which can be defined on a rolling time window for streaming models or a batch basis for batch models.KL DivergenceIf you need a quick overview, I’ve found this introductory post very helpful.KL Divergence from P to Q is interpreted as the nats of information we expect to lose in using Q instead of P for modeling data X, discretized over probability space K. KL Divergence is not symmetrical, i.e. the value is different if P and Q are swapped, and should not be used as a distance metric.Discretized KL DivergencePopulation Stability Index (PSI)While KL Divergence is well-known, it’s usually used as a regularizing penalty term in generative models like VAEs. A more appropriate metric that can be used as a distance metric is Population Stability Index (PSI), which measures the roundtrip loss of nats of information we expect to lose from P to Q and then from Q returning back to P.Discretized PSIHypothesis TestsHypothesis testing uses different tests depending on whether a feature is categorical or continuous.For a categorical feature with K categories, i.e. K−1 are the degrees of freedom, where N_Pk and N_Qk are the count of occurrences of the feature being k, with 1≤k≤K, for P and Q respectively, then the Chi-Squared test statistic is the summation of the standardized squared differences of expected counts between P and Q.Chi-Squared Statistic with K-1 degrees of freedomFor a continuous features with F_P and F_Q being the empirical cumulative densities, for P and Q respectively, the Kolmogorov-Smirnov (KS) test is a nonparametric, i.e. distribution-free, test that compares the empirical cumulative density functions F_P and F_Q.Kolmogorov-Smirnov Test StatisticWhy Automated Drift Thresholds?Photo by Scott Rodgerson on UnsplashHaving humans manually look at drift metrics produced by a model over time or over batches is obviously tedious and not ideal, and the immediate use case for drift metrics would be to set alerts based on some threshold, e.g. PSI jumps over .3 for this batch and should create an alert for a person to examine. Remediation actions could be checking if a pipeline that’s feeding that feature into the model is broken, if there was a recent buggy code or calculation change, or if the feature really did drift and suggests that the model needs to be retrained.But where did that .3 come from? Setting arbitrary handpicked thresholds is not a good solution. If the threshold is too high, alerts that should be brought up are now ignored (more false negatives). If the threshold is too low, alerts that should not be brought up are now present (more false positives).A universal constant threshold is not robust because that threshold should depend on the shape of the the training data P. If P is uniform and we see a unimodal Q, that drift value is going to be significantly less than if P was highly unimodal and centered at some mean far away from that same Q, since the former P that is uniform is less certain if Q did not come from P. An example is shown below.In order to set robust thresholds that make sense for an alerting system, we’ll dive into how to automate calculation of such thresholds. That is, we want to recommend data drift thresholds to create alerts, so users (a) don’t have to manually set alerts and (b) don’t have to determine what a good threshold value is.‍Automated Drift Thresholds for Hypothesis Test MetricsFor hypothesis test metrics, the trivial solution would be setting thresholds at the the proper critical values for each test using the traditional α=.05, i.e. 95% confident that any hypothesis metric above the respective critical value suggests significant drift where Q ∼ P is likely false.Hypothesis tests, however, come with limitations, from sample sizes influencing significance for the Chi-Squared test to sensitivity in the center of the distribution rather than the tails for the KS test.For those reasons, it’s important to explore other classes of drift metrics such as f-divergence metrics, and we’ll now explore ways to automate f-divergence drift thresholds.Automated Drift Thresholds V1: MC SimulationOverviewSystem for MC Simulated Automated Drift Thresholds (Image by Author)(1) For a large number of simulations indexed by i, for a given input feature, MC (Monte Carlo) simulate m times, where m is sufficiently large, from the reference set P and construct an inference distribution Qᵢ*, which represents what we’d expect had our data truly come from P via simulation. Then we calculate and keep track of data drift f(P, Qᵢ*) for each simulation.(2) We set the data drift threshold to the conservative value of max_i(f(P, Qᵢ*)), which is essentially a critical value/cutoff where α → 0 in a traditional statistics setting, i.e. we are 99.999% confident that a data drift value from an actual Q, i.e. inference time slice or batch, that is above the threshold signifies that the inference data was not produced from the same underlying distribution as P. If incurring false positives is not expensive, then we could set the threshold instead to the 95% percentile of f(P, Qᵢ*) in the traditional α=.05 one-sided setting.Limitations of MC SimulationMC methods are attractive because they are simulation-based and distribution-free, but this method is not scalable due to two issues.(1) Running simulations for every feature per model is computationally expensive. Specifically, runtime on a per model basis would be:O(n_features*n_categories_per_feature*n_metrics*n_simulations)(2) The assumption of sampling stability is broken with lower sample sizes.As a toy example, let’s say we have red, white, blue, and orange marbles in P, distributed uniformly. Let’s now present Q of size 3; there is no way to tell if the Q came from P or not since there is no way for us to represent all four colors. Even if we had sampled 6 marbles, we could not closely approximate 1/4 for each category given those 6 marbles.Let m be the number of simulated datapoints on each simulation to create Qᵢ*. In the same toy example of marbles, sampling instability causes artificially high uncertainty(and therefore artificially high data drift thresholds) when m is low, as P has not been well MC-sampled and represented in Qᵢ*.KL Divergence MC Sampling Instability (Image by Author)The way to fix issue (2) is to create simulated threshold curves for a given feature by varying m, but now we have exacerbated issue (1) by multiplying runtime by m. We also now have issues with space complexity, since now we would have to store thresholds in a lookup (database or cache) for use in downstream applications, like an alert service creating alert rules or a query service cache. Forgoing the extra storage and simulating the threshold curve at query time is also not an option since the query would take too long for most user-facing applications.In short, the trade-off of using a flexible simulation setup is not worth the computational expenses when taking into account correction curves that would either (a) need to be stored, which would cost a lot in storage fees or blow up a cache, or (b) be simulated at query time, which is too slow for user-facing applications.Automated Drift Thresholds V2: Closed-Form StatisticsPhoto by Edge2Edge Media on UnsplashIt turns out that we can upper bound data drift thresholds in one shot using probability theory. We’ll make use of standard probability theory, the Dirichlet distribution, and first-order Taylor series expansions. Remember that we are still deriving the thresholds on a per feature basis.KL DivergenceFirst, letReference set P is known and inference set Q is unknown. Intuitively, this simply means that we have observed the training data used in creating our ML model and that we’re asking a hypothetical question of what we could expect data drift to be for some hypothetical inference set Q in the future. Formally,2nd line: Linearity of expectation and conditional probability.3rd line: Q ∼ Dirchlet(α), i.e. Q models a probability distribution and is a multivariate generalization of the beta distribution. As such, we can use the digamma ψ function in this case.α = (α_1, …, α_K) traditionally corresponds to the observed probabilities in the K categories for the feature from Q, offset by a small constant, i.e. prior counts. However, Q is unknown, so how do we derive the correct counts? We do so from P. After all, the entire point of this expectation statement is to quantify on average the drift we’d expect to see if Q ∼ P, and an amount above that value in an actual inference set signifies that we likely have significant drift, i.e. Q ∼ P is false.Back to α, we can use a Bayesian uniform prior Q ∼ Dirichlet(1_K) and update such that QP ∼ Dirichlet(1_K + N_q * p_K), where N_q is the number of datapoints in the inference set Q and p_K corresponds to the known K-dimensional probability vector for the feature from P. This is the multivariate generalization of the the beta-binomial conjugacy.So we thus have,Population Stability IndexPSI gets a bit more tricky and we need to upper-bound the expectation. LetThenHere are the steps:2nd line: Linearity of expectation and substituting the result for KL Divergence above.3rd line: Linearity of expectation and conditional probability.4th line: Expectation of a Dirichlet distribution.5th line: Delta method approximation (see appendix).6th line: Expectation of a Dirichlet distribution.7th line: Simplifying.V1 Issues Solved!Notice how we now have solved for the two issues from MC simulation.(1) We do not have to worry about runtime, as this form is a one shot solution that only involves counts and we can use an approximation to the digamma function.(2) We handle varying sample sizes by using a Bayesian plus pseudocount setup on the parameters of Q.‍ImplementationIn production, how do we implement this dynamic thresholding? Although the thresholds are dynamic depending on the metric and the incoming inference set, all of the expectations above, conditioned on P, can be expressed as SQL queries or custom but simple functions.Discretizing all continuous features, instead of smoothing discrete features, would be a reasonable trade-off: although we may not capture the full continuous nature of P and Q, we’ll avoid computationally expensive kernel density estimation in representing P and Q as continuous distributions.Wait, what about variance?We can calculate variances too and apply Delta method approximations, as we would run into variances and covariances on logarithmic terms that cannot be solved using standard probability theory. Those tools are below in the appendix. However, as it turns out, most metrics will not come out as cleanly as KL Divergence, so most of the conditional expectations and variances are upper-bounded via Cauchy-Schwarz. It’s certainly doable to include an additive upper-bounded variance term, or to even use second-order approximations, but the extra computation might not be worth the time gained in exchange for a stricter bound.For another time…We can also use Taylor series expansions to upper-bound other metrics, like JS Divergence and Hellinger Distance.ConclusionThat’s it for today! Hope you gained some insights about how to implement automated thresholds in a computationally and probabilistically sound manner! We implement these kinds of systems at Arthur, and a few equations can go a long way for the customer experience. Check out our Data Drift Detection Part II: Unstructured Data in NLP and CV blog post for more.Happy monitoring!Photo by Pablo Heimplatz on Unsplash‍Appendix: Delta Method ApproximationsNote that the first-order approximations for both sections below are upper bounds because the log function is concave.log(X) ApproximationBy a first-order Taylor series expansion around the mean,Using this approximation, we haveandwith inequality by Cauchy-Schwarz.Xlog(X)By a first-order Taylor series expansion around the mean,Using this approximation, we haveand lettingwe havewith inequality by Cauchy-Schwarz.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report
 Arthur—rapidly growing amidst surging interest in model monitoring—identified as a Sample Vendor in 2021 Gartner ® Hype Cycle ™ for AI report
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur—rapidly growing amidst surging interest in model monitoring—identified as a Sample Vendor in 2021 Gartner ® Hype Cycle ™ for AI reportBy:Arthur TeamSeptember 15, 2021New York, NY –  Arthur, the leading machine learning (ML) monitoring platform, is identified as a Sample Vendor for the AI Governance category. Gartner released its annual “Hype Cycle for Artificial Intelligence 2021” report, highlighting the growing importance of AI governance practices in AI-enabled companies as it states, “Data and analytics leaders must leverage this research to successfully navigate AI-specific innovations that are in various phases of maturation, adoption and hype.” [1]“With AI now delivering value in practical enterprise application, data and analytics leaders see that scaling AI without governance is dangerous. When each AI output is replicated millions of times, they ask how to balance the business value promised by AI against the need for appropriate oversight, risk management, and investment decisions.”As digital transformation continues to be a driving force in operations across industries, Gartner highlights the necessity to invest in technology to support AI governance, Responsible AI, and ModelOps within an organization. The Gartner report states: “With AI now delivering value in practical enterprise application, data and analytics leaders see that scaling AI without governance is dangerous. When each AI output is replicated millions of times, they ask how to balance the business value promised by AI against the need for appropriate oversight, risk management, and investment decisions.” [1]“Over the past three years, we’ve successfully enabled our customers to put in place robust AI monitoring and governance at companies of all sizes. We believe that this Gartner recognition of the importance of AI governance is further proof of its foundational importance in the enterprise. Our team is uniquely positioned to help customers put in place this governance and maximize their AI investments.”Arthur believes the report underpins the transformative impact that AI has on all enterprises. As a result, the need for explainable AI and fairness is paramount. “As enterprises become AI-enabled, the performance of their models drives the performance of their business. We are seeing an increased focus on AI governance all the way up to the board level,” said Adam Wenchel, Founder and CEO of Arthur. "Over the past three years, we’ve successfully enabled our customers to put in place robust AI monitoring and governance at companies of all sizes. We believe that this Gartner recognition of the importance of AI governance is further proof of its foundational importance in the enterprise. Our team is uniquely positioned to help customers put in place this governance and maximize their AI investments.”Since its inception in 2018, Arthur has enabled customers like Humana, Zesty.ai, and Truebill to accelerate their business transformation through AI. Customers use Arthur to monitor their ML models, mitigate, analyze and mitigate unwanted biases, and explain how the models are making decisions.The platform continues to innovate—Arthur has been the first to market in enabling the adoption of computer vision (CV) and natural language processing (NLP) models. In April, it was named one of CB Insight’s AI 100 list, recognizing promising private AI companies globally for the second consecutive year. In June, it was named a Gartner ‘Cool Vendor’ in AI Governance and Responsible AI [2].If you’re a Gartner client, you can access the full report here: Hype Cycle for Artificial Intelligence 2021Learn more about Arthur at arthur.ai/product. To schedule a demo of Arthur, please contact info@arthur.ai.[1] Gartner, “Hype Cycle for Artificial Intelligence, 2021”, Shubhangi Vashisth, Svetlana Sicular, 29 July 2021
[2] Gartner, “Cool Vendors ™ in AI Governance and Responsible AI”, Svetlana Sicular, Moutusi Sau, et al, 10 June 2021
Required Disclaimer: GARTNER, COOL VENDORS and HYPE CYCLE are a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.NOTE: Additional information may be included in the press release, but must be in compliance with the Gartner Copyright and Quote Policy: http://www.gartner.com/technology/about/policies/copyright.jsp  Industry-general excerpts from the report are permitted (e.g., evaluation criteria);No company-specific excerpts permitted as they may appear endorsing;Vendor company information may be included provided it is clearly differentiated from the Gartner report.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthurs-nist-response
 Arthur's Response to NIST Guidance on Bias Risk in AI
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur's Response to NIST Guidance on Bias Risk in AIBy:Lizzie KumarAugust 4, 2021In June, the National Institute of Standards and Technology (NIST) released a draft of a document outlining an approach for identifying and managing bias in artificial intelligence. The team driving responsible AI research at NIST has requested feedback on the document "from people whom AI affects, both from those who create AI systems and also those who are not directly involved in its creation." At Arthur, we are excited to see NIST embrace and elevate the importance of these issues. In our response, we express our support for NIST's efforts as well as address potential gaps in their framework. Read it here.Figure 1 from NIST’s proposal. Note the arrow from “Deployment” back to “Pre-Design.”PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology
 Google’s Dermatology App Announcement Highlights Promises and Potential Perils of Computer Vision Technology
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessGoogle’s Dermatology App Announcement Highlights Promises and Potential Perils of Computer Vision TechnologyBy:Connor ToupsJune 28, 2021Last week, Google announced a new dermatology app that will use Computer Vision to automatically classify skin conditions from images and provide “dermatologist-reviewed information and answers to commonly asked questions, along with similar matching images from the web” for the matching skin condition. Why it’s important: Google first published their model in Nature in 2020. Moving to embed that technology into a public-facing app highlights the continued impact of computer vision in the healthcare industry.‍Concerns around bias: Google’s announcement prompted concerns that the model would be less accurate for darker-skinned people. According to the Nature paper Google published, Fitzpatrick Type V and VI skin, which broadly correlates to brown and dark brown skin, accounted for under 3.5% of the train set. That class imbalance could cause the model to underperform on darker-skinned people -- raising concerns about disparate impact bias. Things we noticed in Google’s validation set numbers‍As others have noted, just 2.7% of the validation set examples from the Nature publication used Type V or VI skin, so it’s difficult to assess how well the model generalizes to dark skin.We also noticed that the topline accuracy numbers Google reported for ethnicity-level accuracy might obscure racial disparities in the validation set. In slides shared with Motherboard (Vice’s tech publication), Google reported that its model had an 87.9 percent accuracy rate for Black patients -- the highest of any ethnicity group. The Nature publication also reported a relatively high accuracy rate for Black patients (though we couldn’t find the 87.9 percent figure) that were on par with accuracy rates for White patients.Just looking at the accuracy rate, however, can obscure potential racial disparities. In the supplementary figures, Google’s Nature publication reported its average sensitivity, which they calculate by averaging sensitivity across the 26 conditions the model considers. This has the advantage of weighting each condition’s performance equally, so common conditions don't skew the accuracy rates as much.Looking at average sensitivity, Google’s model performed worst on Black patients. The difference between accuracy and sensitivity rates for Black patients could suggest that the Black patients in their validation set were more likely to have a common condition (eg. Acne) that the model can perform very well on, but when it comes time to detect a more serious but less common condition like skin cancer, the model underperforms for Black patients. Non skin-cancer conditions could be overrepresented in the Black sample because the prevalence of skin cancer in non-White people is lower than the prevalence for White people (however, skin cancer diagnoses in non-White people often come at a more advanced stage -- increasing the lethality rate). Imagine we randomly sample a group of Black patients with some type of skin condition (not exclusively skin cancer) and an equally sized group of White patients with some type of skin condition: if the prevalence of skin cancer is higher in the White population than the Black population, a random sampling process will result in the White sample having more patients with skin cancer and the Black sample having more patients with some non-cancerous skin condition. The code below demonstrates this phenomenon. Whatever the underlying cause for the disparities, low sensitivity rates for Black patients could exacerbate existing racial inequalities in healthcare. A Brief History of Bias in Computer Vision ModelsAs computer vision research has exploded, there’s been an increased interest in auditing these models for potential racial and gender bias.  In 2018, Gebru and Buolamwini found three major commercial facial recognition systems performed significantly worse on darker-skinned people and women than lighter-skinned people and men. That landmark study inspired further research into auditing other computer vision models that might exhibit these pernicious biases.Studies have also examined CV models in a healthcare context: last year, a study looked at chest X-ray diagnosis computer vision models and found that they exhibited TPR (true positive rate) disparities with respect to gender, race, and age. TPR disparities with respect to race were especially prominent with the models performing best on White patients and worst on Hispanic patients. Difficulties in auditing dermatology CV modelsAfter Google’s announcement, our team looked into conducting an audit of a dermatology computer vision model using an external test set to look for possible racial or gender bias. However, as we explored the feasibility of such a study, it became clear that there were no open source dermatology datasets that included metadata on each patient’s race or skin type. External audits are important methods of ensuring that models in production are equitable and inclusive: balanced datasets that include patient skin type and race metadata are critical to enabling these audits, and we at Arthur hope that the healthcare and tech industries work together to create these datasets and conduct these audits.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/2021-gartner-cool-vendor
 Arthur named a 2021 Gartner Cool Vendor in AI Governance and Responsible AI
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur named a 2021 Gartner Cool Vendor in AI Governance and Responsible AIBy:Arthur TeamJune 29, 2021NEW YORK, NY—Arthur, the AI monitoring and governance company, is announcing today that it has been named a “Cool Vendor” as featured in the report titled “Cool Vendors in AI Governance and Responsible AI” by expert analysts at Gartner, Inc.The report “is designed to highlight interesting, new and innovative vendors, products and services”, and highlights the “adoption of artificial intelligence is pushing organizations from declarations of their responsible and ethical AI principles to actions.”According to Gartner, “Responsible artificial intelligence (AI) is in the spotlight because of the increased organizational and societal exposure to AI bias, distrust and lack of fairness that stand in the way of AI adoption. The more ubiquitous AI becomes, the more AI practitioners recognize in their everyday work that data and algorithms amplify and perpetuate human biases.”According to the same report, Gartner predicts that “Through 2025, 80% of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance.”Through 2025, 80% of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance. (Gartner)Since its inception in 2018, Arthur has cemented its position as the leading model monitoring and governance solution in the market with many industry-exclusive features such as comprehensive support for natural language processing (NLP) and computer vision models. In April, it was named one of CB Insight’s AI 100 list, which recognizes promising private AI companies globally, for the second consecutive year.“We consider it a great honor to be named a Gartner ‘Cool Vendor,’” says Adam Wenchel, CEO of Arthur. “We strongly believe in the importance of AI governance and responsibility and are glad to see Gartner highlighting trends in this industry. We believe this designation recognizes our commitment to creating the most comprehensive set of model monitoring tools possible to enable any company to operationalize AI governance seamlessly.”If you’re a Gartner client, you can access the full report here: Cool Vendors in AI Governance and Responsible AILearn more about Arthur at arthur.ai/product. To schedule a demo of Arthur, please contact info@arthur.ai.Gartner DisclaimerGartner does not endorse any vendor, product or service depicted in our research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.About ArthurArthur is a proactive machine learning model monitoring platform that gives you the confidence that your AI deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. With performance monitoring, bias detection, and customizable alerts, Arthur makes sure that you never miss an issue, and Arthur’s explainability engine makes runtime debugging effortless.Arthur’s customers, including Fortune 100 companies like Humana and AI-driven startups like Expel and Truebill, are using the platform to ensure that they can catch and fix any issues with their production AI systems before they become billion-dollar problems.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models
 Introducing Monitoring for Computer Vision Models
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringIntroducing Monitoring for Computer Vision ModelsBy:Arthur TeamJune 17, 2021As computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. The adoption of computer vision into industry applications promises enormous potential upside; however, computer vision models, like any ML model, must be carefully monitored. A promising model that has gone off the rails can quickly become a dangerous liability. Today, Arthur is excited to provide the first model monitoring support for computer vision (CV) models.  With Arthur, you can launch CV models into production, and rest assured that you’ll be immediately notified when something warrants your attention. Arthur supports both image classification and object detection, providing monitoring for performance and data drift, as well as in-depth explainability. In this post, we describe the key components for successful monitoring of CV models and how Arthur helps ensure your models are performing as expected – maximizing success for your organization while mitigating risk.Understanding Data Drift in Computer Vision ApplicationsA critical aspect of monitoring any ML model is to ensure that the data coming into the model continues to look as expected. In computer vision models, this means ensuring that the images we see today are similar to those used to train the model. In technical terminology, we refer to this as out-of-distribution detection or anomaly detection, and it is a field of burgeoning research in the ML scientific community. Using a reference dataset of images, we can perform ongoing monitoring of all new images to understand which ones are similar to the training data and which ones seem like anomalies.It is essential to know exactly when your model won’t generalize to new settings. For example, if your object detection model was trained primarily on images from outdoor locations in good weather and good lighting, it will likely underperform in rainy and dark conditions. Arthur’s data drift detection tool can automatically detect if these dissimilar images start coming in so that your data scientists can get ahead of any model issues.In another example, let’s say you’ve trained a computer vision model that examines chest x-rays to diagnose tuberculosis. Despite your best efforts, you’re only able to collect training examples from a small number of x-ray machines, each of which tends to have its own set of artifacts and nuances. Once you deploy your model into the wider world, you’re nervous that these artifacts will prevent your model from generalizing well to image data drawn from a much larger set of machines.In this case, it would be helpful to quantify how much each production image adheres to the attributes of the training dataset. Luckily, Arthur’s data drift detection tool can identify if your model fails to generalize and prompts you to take action before your model derails and causes adverse impacts.The diagram above demonstrates how anomaly detection works. Here, we trained our computer vision model using images that consist of typical street scenes. Most of the images involve transportation, outdoor lighting, people, buildings, and so on. At production time, some of the incoming data is visually (and semantically) similar to the training data, such as the top image of a train platform. However, the bottom image of the toy robot and cat is quite different from anything in the training set. Therefore, our tool would flag this image as an anomaly. This kind of drift detection for image data will ensure that your model’s output predictions remain trustworthy and reliable. Furthermore, we allow for sorting and searching through images based on their anomaly score: how dissimilar are they to the training data. This classification model was trained on aerial imagery primarily of green landscapes. However, the model occasionally sees dense urban images and we are able to quickly identify that these images are likely going to lead to misclassifications. This dynamic filtering gives data scientists the tool to quickly find representative examples of anomalous images.Whatever your computer vision model does, it’s easy to imagine how data drift could cause issues. It’s critical to monitor data drift to prevent unwanted surprises and wasted time debugging your model.Explainability in CV modelsOne concern with complex ML models, especially computer vision models, is that they can be “right for the wrong reasons.” When models are in production, we want to ensure that they’re looking at the ‘right things’ to make decisions. For example, if we have a model for identifying cancerous cells in micrographs, we would want to ensure that the model picks up on medically important aspects of cells instead of some artifacts that happened to be present in the training data. Using Arthur’s local explainability techniques, you can visualize saliency maps over images to reveal which image components were particularly important for the model’s decision. The importance scores are associated with a class, so a positive score indicates that a region strongly contributed to a positive class prediction. A negative score indicates that a region was negatively associated with a target class.The Arthur platform makes image explanations easy to use - it shades regions of the original image with green or red to indicate the importance of that region. Green shading indicates a region that positively contributed to the selected class, while red shading indicates a region that negatively contributed to the selected class. A user can drag a slider bar interactively, indicating how many regions are shown, sorted by overall importance. In this example, data scientists, researchers, and business analysts alike can utilize the Arthur platform to guarantee peak model performance. Arthur’s computer vision explainability tool is simple, easy to understand, and provides cross-functional teams with key insights. Monitoring your Computer Vision Model for Algorithmic BiasAs with any machine learning model, it’s necessary to ensure that algorithmic bias hasn’t seeped into your computer vision model. Unfortunately, a growing body of research demonstrates that some of the most popular computer vision models are biased. In 2018, researchers Gebru and Buolamwini found three major commercial facial recognition systems performed significantly worse on darker-skinned people and women than lighter-skinned people and men. A year later, a National Institute of Standards and Technology study that evaluated 189 facial recognition models found pervasive bias across facial recognition models.That same NIST study, however, found that some models performed equitably across all demographic groups. While the study didn’t evaluate causal reasons for this outcome, it suggests that the model you use—and the data you train on—affects the degree of bias in computer vision models. This study further emphasizes the importance of continuously monitoring and evaluating each of your models to see where it falls on the fairness spectrum.As computer vision continues to offer new opportunities for innovation and growth, we must ensure that its applications are equitable and inclusive to avoid encoding dangerous systemic biases. Arthur has built-in bias monitoring so you can easily compare equity across various groups, and maintain high standards of fairness.Monitor your CV Models with ArthurThe Arthur platform has recently released extensive monitoring support for CV models, including performance monitoring, data drift and bias detection, and explainability features. Unlike other model monitoring solutions, Arthur has you covered for Tabular data, NLP data, and Image data, so you can be assured that your monitoring platform can grow easily with your ambitious AI agenda.If you’re deploying CV models into production and are looking for a solution for monitoring those models over time, we’d love to connect and show you how Arthur can help. Request a demo today.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise
 Arthur releases the first computer vision model monitoring solution for enterprise
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur releases the first computer vision model monitoring solution for enterpriseBy:Arthur TeamJune 17, 2021‍NEW YORK, NY—Today, Arthur, a machine learning model monitoring company, is releasing a suite of new tools and features that enable organizations to monitor the performance of their computer vision (CV) models, including image classification and object detection models. Arthur’s new CV feature set allows companies to:Monitor CV model pipelines for data anomaliesReview potential biases in image modelsUnderstand model predictions and decisionsExplore, investigate, and triage model predictions and errorsView model predictions and important regions for determining a class output in a particular imagePerformance monitoring: Arthur now offers performance monitoring of CV models, including tracking data drift and anomaly detection (out-of-distribution detection for images). This helps users identify any images being sent to the model where the model is likely to be underperforming. In addition to supporting all tabular and NLP models, Arthur’s CV functionality allows companies to monitor all of their machine learning models on one centralized dashboard, regardless of model types.Bias detection: The new CV feature set enables companies to identify biases in their CV models by evaluating image classification outputs for particular classes that users are interested in evaluating. By examining outputs for a specific class, users can better understand where their model misclassifies and perpetuates biases.Explainability: Arthur’s CV feature set offers insights into how CV models are making decisions. This capability gives users a clear visualization of which regions of an image are impactful for a model’s decision, making it easy to identify when models perform unexpectedly. Additionally, users can explore the results of their object detection models with an interactive interface that displays the model outputs for pipeline images.The release of the CV feature set comes just a few weeks after Arthur released the first natural language processing (NLP) model monitoring solution, furthering Arthur’s position as the most comprehensive machine learning model monitoring solution in the market.View object detection model outputs on Arthur's interactive interfaceThe growing need for CV model monitoringAs computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. According to Stanford HAI’s 2021 AI Index Report, companies are investing a larger amount of computational resources in training computer vision algorithms than ever before.The adoption of computer vision into industry applications promises enormous potential upside; however, computer vision models, like any ML model, must be carefully monitored. A promising model that has gone awry can quickly become a dangerous liability. In 2018, researchers Gebru and Buolamwini found three major commercial facial recognition systems performed significantly worse on darker-skinned people and women than lighter-skinned people and men. A year later, a National Institute of Standards and Technology study that evaluated 189 facial recognition models found pervasive bias across facial recognition models. Without the proper tools to audit CV models, it’s difficult to pinpoint the issues they may cause.Armed with Arthur’s CV model monitoring functionality, companies can maximize the performance of their CV models and minimize any risks associated with them.---To schedule a demo of Arthur, visit arthur.ai/demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations
 Reinforcement Learning for Counterfactual Explanations
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedExplainable AIReinforcement Learning for Counterfactual ExplanationsBy:Sahil VermaJune 10, 2021Authors: Sahil Verma, Keegan Hines, John DickersonIn the field of Explainable AI, a recent area of exciting and rapid development has been counterfactual explanations. In this approach, we aim to understand the decisions of a black-box machine learning model by quantifying what would have needed to have been different in order to get a different decision. A common example is that of credit lending: if I am denied for a loan (by an algorithm), then a counterfactual explanation of that decision could inform me that if my income were $10K higher and my credit score were 30 points higher, then I would have been approved. This basic premise is intriguing, but it also comes along with several subtle constraints and desirable add-ons. These might include things like, causal constraints, realism constraints, actionability, sparsity, computational efficiency, and so on. If you’re interested in learning more about these overall areas of research, feel free to read our recent review paper from the NeurIPS workshop on ML Retrospectives, which won a Best Paper Award.We’ve recently posted a new approach to these problems which allows us to solve for many of these constraints. By framing the problem of generating counterfactual explanations as a Markov Decision Problem, we can associate many of the desiderata to various components of an agent and environment (such as the transition function or reward function). Then, using common techniques from reinforcement learning, we can train an agent to calculate counterfactuals for a given model. Once this agent is trained, we have amortized the computation of new counterfactuals, since any new counterfactual is calculated by simply evaluating the agent’s policy for any starting point. This makes the method extremely computationally efficient for calculating new counterfactuals. As you can see in the table below, our approach, which we termed FastCFE, is able to accommodate all of the major desiderata/constraints that have been brought forth recently in the counterfactual explainability community. This is an exciting development in operationalizing counterfactual explainability for real-world and high-volume use cases. We hope you enjoy the paper. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur
 Serving, hosting and monitoring of an xgboost model: UbiOps and Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesServing, hosting and monitoring of an xgboost model: UbiOps and Arthur By:Arthur TeamMay 25, 2021Making ML Ops observable and explainable: serving, hosting and monitoring combined. Maximizing performance and minimizing risk are at the heart of model monitoring. In mission-critical AI models, real-time visibility is crucial. Respond quickly by retraining your model, replacing the current version or tweaking the pipelines. Additionally, setting up your serving and hosting infrastructure in a scalable way can be time consuming and complex. For these reasons, Arthur and UbiOps are pleased to announce an exciting integration that brings best-in-class tooling for model serving and model monitoring.UbiOps is the easy-to-use serving and hosting layer for data science code. UbiOps makes it easier than ever to use a top-notch deployment, serving, and management layer on top of your preferred infrastructure. Accessible via the UI, client library, or CLI, it’s suitable for every type of data scientist, without the need for in-depth engineering knowledge.Arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their AI deployments are performing at peak. Arthur provides a layer of performance monitoring, algorithmic bias detection, and explainability, even for black box models, so data science teams can detect, diagnose, and fix any issues in production.Together, Arthur and UbiOps provide a powerful set of tools that give you complete control over your production AI. This blog post will demonstrate just how easy it is to get started with the Arthur and Ubiops integration, so you can complete your AI stack and stay in control.1. Use case descriptionWith the recent increases in house prices across the world, a model to predict what the price of a specific house would be could come in handy. But how can we keep track of the model’s performance, and how can we explain why one house may be priced higher than another?Using a publicly available dataset from King County, USA, we trained a xgboost model to predict the house prices in that region. Using the UbiOps CLI, the model is easily containerised, deployed on a Kubernetes pod, and served behind an automatic API endpoint. Via the API, Arthur picks up the model’s inputs and predictions and calculates data drift, model stability, and explanations.2. Notebook walkthrough and instructionsThis Notebook is based on the Xgboost recipe from the UbiOps cookbook. Please check out the full notebook for everything you need to set this up. Compared to the Xgboost recipe there are 2 big differences.The first adjustment is the following: we need to register the (pre-trained) model to the Arthur platform. We do that using the following code snippets:
from arthurai import ArthurAI
from arthurai.common.constants import Stage, InputType, OutputType, ValueType
ARTHUR_URL = "https://app.arthur.ai"
ARTHUR_ACCESS_KEY = "" # Fill this in
connection = ArthurAI(url=ARTHUR_URL, access_key=ARTHUR_ACCESS_KEY)
# Define the model schema
arthur_model = connection.model(partner_model_id="UbiOps House Prices",
input_type=InputType.Tabular,
output_type=OutputType.Regression,
is_batch=True)
arthur_model.from_dataframe(train_data[input_columns], Stage.ModelPipelineInput)
arthur_model.add_regression_output_attributes({"price": "price_gt"}, value_type=ValueType.Float)
arthur_model.review()
The dataframe above represents how the model will look to Arthur, and the format of the data it will expect. Notice how it detected some columns as categorical (such as Waterfront and View).Now we can save the model to Arthur, and store the Arthur Model ID to be used by our deployment
arthur_model_id = arthur_model.save()
with open("xgboost-deployment/arthur-model-id.txt", 'w') as f:
f.write(arthur_model_id)
Then, we’ll upload the data we used to train the model as a reference set. Future data sent to the model will be compared to this reference set, to measure how much it has drifted from the types of inputs the model was built from.
ref_df = train_data[['price'] + input_columns].rename(columns={'price': 'price_gt'})
ref_df['price'] = xgb.predict(ref_df[input_columns].to_numpy())
ref_df
arthur_model.set_reference_data(data=ref_df)
The second addition is sending the inference data from within a deployment to the Arthur platform. You can see it in a snippet from the actual deployment file below (again look at our full notebook for the complete code). It is as simple as using the `send_inference` method of the Arthur client library.
def request(self, data):
"""
Method for deployment requests, called separately for each individual request.
:param dict/str data: request input data. In case of deployments with structured data, a Python dictionary
with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment
with plain input, it is a string.
:return dict/str: request output. In case of deployments with structured output data, a Python dictionary
with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment
with plain output, it is a string. In this example, a dictionary with the key: output.
"""
print('Loading data')
batch = pd.read_csv(data['data']).astype({'id': str})
batch_id = str(uuid4()).split('-')[-1]
print("Predictions being made")
batch['price'] = self.model.predict(batch.drop(columns=['id']).to_numpy())
print("Sending batch to Arthur")
inference_data = [{'inference_timestamp': datetime.datetime.now(pytz.utc),
'partner_inference_id': row['id'],
'batch_id': batch_id,
'inference_data': {k: row[k] for k in row.keys() if k != 'id'}}
for row in batch.to_dict(orient='records')]
self.arthur_model.send_inferences(inference_data)
Now the Arthur platform can compare training data to actual inferences. You can explore the data in Data Explorer and view drift metrics for the input and output variables, as well as accuracy metrics such as Root Mean Square Error (RMSE).3. Wrap upNow that you’ve seen how UbiOps and Arthur.ai integrate to make serving, hosting and monitoring easily accessible, we hope that this helps you in your day-to-day projects. For any questions or suggestions and the latest product updates please join the UbiOps community slack channel and the Arthur slack community.Download the full notebook here. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/cb-insights-ai-100-2021
 CB Insights recognizes Arthur as one of the most innovative AI startups in the world
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesCB Insights recognizes Arthur as one of the most innovative AI startups in the worldBy:Arthur TeamApril 7, 2021Today, CB Insights released their “AI 100” ranking for 2021, highlighting the 100 most promising and innovative private AI companies from around the world—and we are thrilled to be included in the list for the second year in a row.Arthur is one of just two companies that CB Insights selected in the model monitoring space, and we are the only model monitoring company to make the list two years in a row. Last year’s AI 100 companies went on to raise a total of $5.2B in additional funding (including us!), and many companies on the list last year went on to exit via M&A, SPAC, or IPO. We’re honored that CB Insights has recognized Arthur alongside some of our favorite companies in the space, including our partners at Algorithmia.With so many notable startups lined up next to us on this year’s AI 100 list, the future looks bright. We believe in AI’s amazing potential to transform the world and are excited to be doing our part to ensure this transformation is safe, reliable, and fair. CB Insights' AI 100 is a ranking of the 100 most innovative and promising AI startups around the world—and we're thrilled to be included for the second year in a row.‍About ArthurArthur is a proactive machine learning model monitoring platform that gives you the confidence that your AI deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. With performance monitoring, bias detection, and customizable alerts, Arthur makes sure that you never miss an issue, and Arthur’s explainability engine makes runtime debugging effortless.Arthur’s customers, including Fortune 100 companies like Humana and AI-driven startups like Expel and Truebill, are using the platform to ensure that they can catch and fix any issues with their production AI systems before they become billion-dollar problems.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data
 Interactive analysis with petabytes of model data
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringInteractive analysis with petabytes of model dataBy: Keegan HinesApril 6, 2021Machine learning models in production can degrade in lots of unexpected ways. There are certain questions, concepts, and metrics that we might know ahead of time that we’ll want to check in on week over week. And then, of course, there are unknown unknowns - problems, questions, and calculations that we are unable to anticipate but will become critical in 6-12 months from now. For this reason, the Arthur platform has been built to be as configurable as possible, to not be prescriptive, and to allow our users full and flexible interfaces for interrogating and understanding model performance. In this post, we’ll describe some of the architectural choices that underlie this design philosophy, and share a couple of the powerful modes of interaction that are made possible by these choices.Optimized StorageAs your model data is ingested into the Arthur platform, it is indexed into a distributed columnar datastore. Not only is this highly scalable (can easily handle petabytes of model data), but it allows for rapid aggregations and queries, thus allowing exploratory and ad-hoc access to all the information about historical model performance. You don’t have to make the difficult choice to subsample or summarize your data, and you don’t need to decide your favorite metrics ahead of time. You’ll have at your fingertips all the historical data about your model’s inputs, predictions, performance, explanations, and other insights. This enables a powerful ability to slice-and-dice model performance across any facets or subpopulations that are relevant to you. For Arthur users, this capability results in two particularly powerful ways of monitoring data and models.Data ExplorerThe Arthur platform provides an interactive data visualization suite that allows you to explore and understand the data pertaining to your model. Our backend architecture allows us to compute the necessary aggregations, groupings, and filters in milliseconds, even over hundreds of millions of data points. In the examples below, we can visualize the distributions and correlations amongst model inputs, outputs, and even explanations. We can quickly navigate through different time slices, facet the data by groupings of different variables, and understand your model’s predictions and data landscape.Query EngineIn addition to the rich set of visualizations and metrics available in the Arthur UI, you can also fetch any and all of this underlying data (and computation) through our API. Our API-first approach means that data scientists can quickly check-in on model performance using a familiar tool, such as a Jupyter notebook. Our Query Engine exposes a SQL-like language that will be quickly familiar to data scientists, so that they can compute and visualize ad-hoc summaries and aggregations on large sets of data. As an example, one day we might be curious to know how our model is performing for Males versus Females, and if that has been changing over time. We construct a query with familiar group-by’s and filters and the Arthur backend computes aggregations over millions of inferences in just a few milliseconds. The result of this query is easy to drop into a pandas DataFrame for quick visualization.You can use Arthur's Query Engine to explore and visualize your monitoring data—for example, plotting average prediction value by sex over time.In addition to model evaluation metrics, it is a snap to get a quick view of the distribution(s) of a model’s input or outputs, to understand how they may be shifting over time.Arthur's Query Engine makes it easy to visualize how your model input and output distributions are shifting over time.From a notebook, we can quickly and easily dive into any subpopulations and assess model performance and data stability. Once we are alerted to issues with model performance or data drift, having this data at our fingertips empowers us to conduct an investigation and drill down to its root cause.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur
 Everything you need to know about model monitoring for natural language processing
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringEverything you need to know about model monitoring for natural language processingBy: Keegan HinesMarch 29, 2021From simple chatbots to document classifiers to generative models like GPT-3, natural language processing models are seemingly everywhere these days. NLP models are powerful tools for processing unstructured text data—but with great power comes great responsibility. If you’re not monitoring your NLP models just as you would your tabular models, you can overlook a number of sticky issues that could quickly become billion-dollar problems.Here are a few things that any organization deploying NLP models into production should be doing to ensure that those models continue to perform as expected.Monitor for data drift to prevent drops in performance.An NLP model, just like any other machine learning model, is trained on a training dataset that represents the world at a point in time—and as the world changes and the input data coming into your language model starts to change, your model will start to run into performance issues. Over time, your NLP model performance will degrade if you’re not proactively monitoring for and correcting data drift as it occurs.Monitoring NLP models for data drift involves comparing the statistical similarity of new input documents to your training set documents.  As input documents shift in their typical word-use, you might need to update your model to account for the linguistic patterns. Understanding when and where drift is occurring is essential to maintaining the integrity of your NLP models over time.The Arthur platform monitors NLP models for data drift and automatically alerts you when your input documents start drifting beyond acceptable bounds. Monitor for bias to protect against discrimination.Algorithmic bias is a critical issue in any machine learning system, but issues of bias can be especially pernicious and difficult to solve in language models.There are several methods of detecting and mitigating bias, which we cover extensively in blog posts, Making Models More Fair and A Crash Course in Fair NLP for Practioners. In addition to the techniques laid out in those blog posts, there are a few additional ways to track potential bias in your machine learning models.First, you can track the performance of your NLP models on input documents partitioned by sensitive attributes that you may know. For example, you can track whether your medical document classifier is more or less accurate for documents from medical visits of men versus women, or Black patients versus white patients. Uncovering differences in accuracy and other performance measures across different subgroups can help you identify—and fix—unfair model bias. Check out our Data Drift Detection Part II: Unstructured Data in NLP and CV blog post. The Arthur platform offers this type of performance-bias analysis for natural language and tabular models, as well as the ability to partition by multiple attributes at a time to provide you more granular insights into potential biasesAnother step is to take proactive steps to debias language outputs from your models. We at Arthur are working on building out more features to do this in our platform—stay tuned for more!Use token-level explanations to understand black box NLP model behavior.Whether your NLP model is a “bag of words” model (the position of the words doesn’t matter) or a sequence-based model (the context and position of words do matter), getting a token-level explanation of your model output is incredibly useful and important for understanding why your model might be getting something wrong, and understanding how to take the right steps to fix it.For example, if we are using a medical document classifier to predict different document types in a given set of medical records, we can use this technique to understand the specific words that resulted in a given document being classified as a document about cardiovascular and pulmonary issues.On the far right-hand side, we see the top words contributing to the predicted class in green and words contributing against it in red. We see this illustrated in the document itself. This is useful for a domain expert to understand in human context the “real world” value of the highlighted words. For example, we see the word “procedure” appears at the top of the list of explanations, meaning it most significantly impacted the (correctly) predicted classification of Cardiovascular/Pulmonary.If we click through, on the left-hand side, some of the other incorrect predictions, we begin to notice this word is also a top influencer in those predictions as well. See Urology, below:Arthur's NLP explainability tools provide token-level explanations for model outputs.Above, we see that “procedure” was the second most influential word in this classification prediction.This is where we see the importance of domain expertise and human involvement. Given the context of a healthcare system, it is easy for us to imagine how the word “procedure” might appear frequently across medical specialties. Perhaps it’s worth exploring a different model that accounts for the positioning of particular words, or perhaps we simply need to retrain how it treats certain words based on what we know is most influencing predictions. In any case, we see how providing this analysis for each prediction and word is crucial information.Monitor your NLP Models with Arthur.The Arthur platform has recently released extensive monitoring support for NLP models, including NLP data drift detection, token-level explainability that provides insight into the key drivers in NLP classification model predictions, and bias detection for NLP models.If you’re deploying NLP models into production and are looking for a solution for monitoring those models over time, we’d love to connect and show you how Arthur can help. Request a demo today.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/ai-bias-mitigation-101
 Making models more fair: everything you need to know about algorithmic bias mitigation
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessMaking models more fair: everything you need to know about algorithmic bias mitigationBy:Jessica DaiMarch 23, 2021As ML models affect our lives more and more, machine learning practitioners need to ensure that our models are not creating harm for end-users. It is especially important to make sure that our models are not unfairly harming any subgroups of our population. The first step is identifying and quantifying any potential bias in a model, and many different definitions of group fairness have been proposed. The Arthur platform provides proactive bias monitoring and alerting, so you know exactly when, where, and how algorithmic bias is occurring in your models. At Arthur, we’re also interested in the question of what we can do to help make your models more fair. In this post, we briefly describe families of techniques for bias mitigation: ways to improve an unfair model. While this is an active area of research, current mitigation techniques target specific parts of the model development lifecycle: preprocessing, or adjustments on the training data; in-processing, or algorithms specifically intended to be fair; and post-processing, or adjusting the outputs of the model. The right solution might depend on the use case, the industry, and the model deployment; in this post, we outline several families of approaches to fair ML. Why do we observe unfair classifiers?Intuitively, training classifiers on historical datasets “bakes in” bias into the model: if your hiring algorithm uses data from the 1960s, it’s likely to think that women are great at being secretaries, while men should be CEOs. However, actually characterizing how “biased data” might be related to a “biased model” is a more complicated task.One commonly discussed cause of bias in a learned model is an imbalance between subgroups in the data. By definition, if we have a subgroup that is a minority represented in our training data, then that means we have fewer observations of them compared to majority groups. When the classifier is training, it is optimizing a loss function over the whole dataset. If the major class is truly dominant, then it is possible that the best way to achieve high overall accuracy on the training data is to be as accurate as possible on the majority group while incurring errors on the minority group. Therefore, if the majority group and minority group have any differences in their properties and relationship to the target variable, the model is likely adhering primarily to the patterns of the major data and potentially ignoring the contributions of the minority. This could mean the model is fairly accurate for the majority groups, but much less accurate for the smaller subgroups. Of course, it’s worth mentioning that not all data biases result from undersampling. In many cases where the data in question represents human behaviors, as in the cases of loan performance, hiring, or crime data, historical biases from human personalities can also show up in the information. Hiring data from the 1960s made predictively, for instance, might suggest that women are best suited as secretaries rather than executives. No matter what technique you choose to ameliorate bias among these options, close attention should be paid to the historical context through which the data collection practices and societal influence can be better understood.Pre-Processing Bias MitigationPre-processing techniques for bias mitigation tend to be all about the data. As described in the previous section, particular characteristics of the training data may directly cause the problematic performance of a learned model. For this reason, many techniques for pre-processing focus on modifying the training set to overcome versions of dataset imbalance. This could be achieved in many ways including resampling rows of the data, reweighting rows of the data, flipping the class labels across groups, and omitting sensitive variables or proxies. Other techniques consider learning direct modifications and transformation functions that achieve desired fairness constraints.  In all cases, the strategy is to change the underlying training data, and then proceed with training using any classification algorithm desired. By modifying the training data in these specific ways, the outputs of the learned classifier will be less biased.In-Processing Bias MitigationWith in-processing techniques, we want to create a classifier that is explicitly aware of our fairness goals. That is, in training the classifier, it is not enough to simply optimize for accuracy on the training data. Instead, we modify the loss function to account simultaneously for our two goals: our model should be both accurate and fair. This modification can be achieved in many ways such as using adversarial techniques, ensuring underlying representations are fair, or by framing constraints and regularization. In each case, the goal is that the underlying classifier is directly taking fairness into consideration. As a result, the outcomes of that trained classifier will be less biased as compared to a classifier that knew nothing about fairness. Post-Processing Bias MitigationFinally, there is a family of techniques that aim to only adjust the outputs of a model and leave the underlying classifier and data untouched. The benefit here is appealing in its simplicity - in using post-processing methods, we allow the model development team to use any modeling algorithms they wish, and they don’t need to modify their algorithm or retrain a new model to make it more fair. Instead, post-processing methods center on the idea of adjusting the outputs of an unfair model such that the final outputs become fair. As an example, early works in this area have focused on modifying outcomes and thresholds in a group-specific manner. Suppose we build a classification model to assist in credit risk decisions. After much hyperparameter tuning, we arrive at a model that is accurate and generalizes well, but we notice that it tends to favor older loan applicants over younger applicants. With post-processing techniques, we would keep the classifier as is, but adjust the outcomes so that the overall acceptance rates are more equitable. We would pick a definition of fairness (say, Demographic Parity), and adjust the treatments across the groups such that the final outcomes are as desired. This means we might have group-specific thresholds instead of a single threshold for the classifier. Detecting unintended bias is the first step to mitigating it. Arthur gives you the power to detect and analyze bias in your ML models.It’s important to note that in this scenario, there remains a lot of legal ambiguity around bias mitigation. With so much unknown about how courts will handle algorithmic discrimination, many organizations are leaning heavily on their legal teams for how to navigate this complexity!Many post-processing techniques have this basic structure in common: they leave the classifier and the data alone, and only adjust the outcomes in a group-dependent way. And while binary classification has been a focus in the past, recent work has sought to extend these ideas to regression models as well. The overall framework can be effective for achieving fairness in an ML system, though in some use cases, treating groups differently could be an uncomfortable proposition, or even an illegal one. Accuracy/Fairness TradeoffWhen we embark on deploying ML models that are more fair, we have to acknowledge that this fairness does not come for free; in fact, in many cases, it may conflict with model accuracy. Consider one extreme: a model that is as accurate as possible (relative to available ground truth) is potentially quite unfair and discriminates against at least one subpopulation. Consider the other extreme: a model which is perfectly fair and is equitable across all populations, this model must be less accurate than a model that did not consider fairness as a constraint. (Though some recent work suggests the tradeoff may not necessarily always occur, the behavior of "fair algorithms" when deployed in the real world will not always match the results demonstrated in theory; as a result, understanding the relationship between fairness and accuracy is critical to being confident in the models we choose to use. )Between these two extremes live a broad family of possible models that balance the concerns of accuracy and fairness. This set of models forms a Pareto frontier (efficient frontier) in the space of Accuracy vs Fairness. The figure below, from a 2019 survey paper characterizing the performance of many popular fair ML algorithms, illustrates this tradeoff empirically: on the x-axis is Disparate Impact, a measure of fairness, while the y-axis is accuracy. (The entire paper is worth reading; it is an excellent introduction to many common considerations for the performance of fair ML models.) As practitioners and stakeholders, we must confront questions about this tradeoff: for each use case, we must weigh the costs of potential harm through unfairness against costs of potential harm through reduced accuracy.The following charts illustrate the trade-off between fairness and accuracy among some of the more popular fair ML algorithms.These are challenging questions that have no single right answer. Instead, ML practitioners must work together with stakeholders such as business leaders, humanities experts, compliance, and legal teams and formulate a program for how to best treat your population. The Arthur platform brings together performance monitoring and algorithmic bias monitoring into a unified view for all of your stakeholders, so you can make informed decisions about how to make your models more fair and effective. If you’d like to learn more about how Arthur can help you combat bias in your AI systems, please reach out to schedule a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/algorithmia-partnership
 Deploy, serve, monitor, and maintain AI at scale with Arthur and Algorithmia
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesDeploy, serve, monitor, and maintain AI at scale with Arthur and AlgorithmiaBy:Arthur TeamFebruary 22, 2021At Arthur, we are obsessed with making sure that our customers have complete AI observability for all of their machine learning models, no matter where they are deployed. But we know that production AI systems need more than just monitoring—and that’s why today, we are excited to announce our partnership with Algorithmia, the industry leader in machine learning operations (MLOps) and model management.Algorithmia’s enterprise MLOps platform manages all stages of the machine learning lifecycle within existing operational processes, enabling you to put models into production quickly, securely, and cost-effectively. Unlike inefficient and expensive do-it-yourself MLOps management solutions that lock users into specific technology stacks, Algorithmia automates ML deployment, optimizes collaboration between operations and development, leverages existing SDLC and CI/CD systems, and provides enterprise-grade security and governance across the lifecycle.Arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their AI deployments are performing at peak. Arthur provides a layer of performance monitoring, algorithmic bias detection, and explainability, even for black box models, so data science teams can detect, diagnose, and fix any issues in production.We are working with the Algorithmia team to make it easier than ever to use their top-notch deployment, serving, and management tools and our leading monitoring capabilities. Together, Arthur and Algorithmia provide a powerful set of tools that give you complete control over your production AI. This blog post will demonstrate just how easy it is to get started with the Arthur and Algorithmia integration, so you can complete your AI stack.Arthur and Algorithmia: Helping you reach AI maturity fasterAlgorithmia’s 2021 enterprise trends in machine learning report revealed that 56% of organizations struggle with governance, security, and auditability issues—making it the #1 challenge for model deployment and management. Arthur extends Algorithmia’s monitoring capabilities to give you rich insights into model performance, data integrity, and AI fairness, helping you manage AI governance more completely. If you’re already using Algorithmia for MLOps, it takes just minutes to add Arthur monitoring. With our integration, Algorithmia customers can take full advantage of advanced ML lifecycle management workflows enabled by Arthur’s runtime monitoring capabilities—think automated refits based on data drift, horse racing (A/B testing) different versions of a model, and more.Let’s walk through what it looks like to use Arthur and Algorithmia together to monitor and maintain AI at scale.Deploy your model behind an endpoint with AlgorithmiaWe’ll use Algorithmia to serve our trained model behind an Algorithmia endpoint. Here is an example Algorithmia project where we’re doing just that: We have a saved model (pkl file), and we’re loading this model into memory. Our implementation of Algorithmia’s apply() function is to invoke the predict_proba() method for any new inputs to the endpoint.Deploying your machine learning model behind an endpoint with Algorithmia's apply() function.This model is now deployed behind an endpoint that we can hit with REST, or with any of Algorithmia’s supported SDKs in several languages.Log your model inputs and outputs with ArthurNext, let’s integrate this project with Arthur so we can monitor this model. Let’s assume we’ve already registered a model with Arthur, and will fetch this object by its model ID. We’ll import the Arthur AI Python library and set up a connection with the platform. We will use that connection to fetch our model by its ID.The last step here is that we’ll want to wrap our model’s predict_proba() method using the loginferences decorator provided in the arthurai library. To do this, we’ll define a simple function called **modelpredict()** for this wrapping, and we’ll decorate that method as shown below.Finally, our implementation of Algorithmia’s apply() function is simply to call our model_predict() wrapper for any inputs to the endpoint. Each time the endpoint is invoked, the model’s input and output are logged with Arthur for automated analytics and alerting.Just a few lines of Python, and you're all set to monitor your Algorithmia-deployed model with Arthur's ML monitoring platform.With a couple of extra lines of Python, we can easily add Arthur’s monitoring tools to any models we have deployed with Algorithmia.Unlock model optimization, versioning, and fairness with ArthurAI and AlgorithmiaThe Arthur-Algorithmia integration enables you to deploy, serve, manage, and maintain your AI at scale with advanced monitoring and governance. Ready to get started today?If you’re already using Algorithmia and Arthur, it takes minutes to implement the integration as demonstrated above. If you’re not already using Algorithmia, sign up to watch a demo today and learn how enterprise MLOps can accelerate your time-to-value for ML.And if you’re not already using Arthur, get in touch with us to schedule a demo and learn more about how we can help you get started with proactive model monitoring.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/neurips-responsible-ml
 Our top takeaways from NeurIPS 2020 on Responsible Machine Learning
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsOur top takeaways from NeurIPS 2020 on Responsible Machine LearningBy:Jessica DaiJanuary 8, 2021No longer the niche sub-field it was several years ago, "fair" ML and related topics have now become a core part of the field of machine learning. At NeurIPS 2020, the broader theme of responsibility (even beyond fairness) in ML has a much larger footprint at the conference; on a more granular level, fair ML as a field overall has also developed substantially, asking a broader range of questions critical to operationalizing fairness.From the start of the conference, NeurIPS 2020 emphasized the importance of thinking deeply about the implications of work in machine learning research: the conference kicked off with a keynote from Georgia Tech's Charles Isbell, titled "You Can't Escape Hyperparameters and Latent Variables: Machine Learning as a Software Engineering Enterprise." Despite its somewhat dry title, the talk is anything but. Isbell makes the case for machine learning researchers to approach their work in the same way as software engineers must, considering how the usage of software must shape its development. Isbell dispells the myth of bias being "only due to biased datasets" and therefore something that individual scientists need not be concerned about; this keynote set the tone for how the rest of the conference unfolded. (You can watch the full video of the keynote; this is nothing like any pre-recorded talk I've ever seen, and I cannot recommend it more strongly.)The growing community around fair/responsible ML is clear not only in the papers accepted to the main conference, but also in the workshops, several of which were dedicated to discussing concerns related to developing fair ML—Resistance AI, Dataset Curation and Security, Fair AI in Finance, Algorithmic Fairness through the Lens of Causality and Interpretability, Consequential Decisions in Dynamic Environments—and several more where Retrospectives, Surveys, and Meta-Analyses, ML for Economic Policy, Broader Impacts of AI Research, Human and Machine-in-the-Loop Evaluation and Training Strategies. In the rest of this piece, I will be highlighting contributions from both workshops and the main conference.At Arthur, we’re really excited about the development of these research directions:Fairness for a broader range of algorithms and applications.Most early work in fair ML focused on binary classifiers with a single protected attribute; today, the vast majority of available open-source implementations are also only available for binary classifiers with a single protected attribute. Of course, there are many other problem settings where fairness may be a concern, and this year's NeurIPS saw the introduction of many new fair ML algorithms for many more scenarios: clustering, streaming, online learning, regression models, overlapping group membership, multiple classes, and scenarios without access to information about demographics.More nuanced ways of measuring, understanding, and communicating fairness.While metrics of fairness based on output distributions and group-conditional error rates have been common for some time now, some exciting lines of work have been developing around more nuanced approaches to measurement. Can I Trust My Fairness Metric? emphasizes the high variance of fairness metrics when batch or dataset sizes are small, and introduces a way to generate more accurate and lower-variance estimates of metric values; meanwhile, Measuring Bias with Wasserstein Distance, from the Dataset Curation and Security workshop, proposes an alternate metric for fairness that captures inequity that may be missed by current standard metrics. Opportunities for a More Interdisciplinary Approach to Perceptions of Fairness in ML, from the ML Retrospectives, Surveys, and Analyses workshop (ML-RSA), draws insight from the field of psychology to discuss how common fairness metrics are interpreted by human end-users.Analyzing the qualities of fair algorithms, such as robustness and downstream implications.Complementary to the development of fair algorithms themselves, a substantial amount of new work discusses how fair algorithms might perform under a variety of conditions and situating the algorithms in the broader context of deployment. For example, How Do Fair Decisions Fare in Long-term Qualification? considers the impact of static fairness constraints on long-term well-being; the Workshop on Consequential Decision Making in Dynamic Environments was dedicated to work in this area. Similarly, Fair Multiple Decisionmaking Through Soft Interventions considers the scenario where there are multiple, potentially-interacting algorithms. In a parallel thread, Ensuring Fairness Beyond the Training Data proposes the first (to my knowledge) algorithm to train a fair classifier that is provably robust to a set of possible distribution shifts. More broadly, the Workshop on Algorithmic Fairness through the Lens of Causality and Interpretability has work contextualizing fair algorithms in many ways, such as Fairness and Robustness in Invariant Learning, which connects topics from causal inference and domain generalization to fairness. The Fair AI in Finance workshop also touches on related issues, particularly with regards to the application.Concerns of fairness and bias complement concerns of model robustness writ large. Indeed, discussion of the “robustness” of a machine-learning-based system necessarily includes discussions of dataset and model security, policy and privacy, dataset and model bias, data ingest via scraping and labeling, amongst numerous other considerations. Toward that end, joint with colleagues at CMU, IBM, Illinois, Maryland, and TTIC, our Chief Scientist John Dickerson hosted the Workshop on Dataset Curation and Security, which brought together researchers from the adversarial ML and fairness in ML communities as well as policy wonks from the Brookings Institute and other “tech-adjacent” bodies for a full day of discussion of what it means to claim, and what it might take to improve, “robustness” of machine learning models. In short, it is hard to make statements about model behavior (vis-à-vis, for example, fairness) without also deeply considering other dimensions such as security and the legal landscape. Our VP of Responsible AI, Liz O’Sullivan, also gave an invited talk at this workshop on some of the dangers of scraping data from the Internet, focusing primarily on how this can take agency from unknowing humans and may result in otherwise unexpected or undefined behavior. Similar concerns and sentiments were raised at other NeurIPS workshops, in papers, and in the aforementioned invited talk by Charles Isbell; we expect this trend to continue in the coming months and years.Beyond fairness: bigger-picture views of algorithmic (in)justice.It's been clear for a while now that it is not enough for algorithms to simply be "fair" — there are several related topics of technical interest, as well as broader considerations that must be made when analyzing the impact of algorithms in the world. The ML-RSA workshop has several works in this category, such as Arthur's own Counterfactual Explanations for Machine Learning, as well as A Survey of Algorithmic Recourse. ML-RSA also has some more critical work, such as Data and its (dis)contents: a survey of dataset development and use in ML research. Finally, it behooves everyone working in fair ML to take a look at the work from the Resistance AI workshop, which is full of thought-provoking work, both technical and non-technical, that questions the way power is arranged and rearranged by ML systems.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/counterfactual-explainability-neurips
 An Overview of Counterfactual Explainability
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedExplainable AIAn Overview of Counterfactual ExplainabilityBy: Keegan HinesDecember 10, 2020Within the field of explainable AI (XAI), the technique of counterfactual explainability has progressed rapidly, with many exciting developments just in the past couple years. To help crystallize and understand the major development areas, we’ll be presenting a new paper at the NeurIPS Workshop on ML Retrospectives, Surveys, and Meta-Analyses. This post will provide a high-level summary, but if you’re interested in getting started in this area, we encourage you to check out the full paper.What are counterfactual explanations?Counterfactual explanations (CFEs) are an emerging technique for local, example-based post-hoc explanations methods. Given a datapoint A and its prediction P from a model, a counterfactual is a datapoint close to A, such that the model predicts it to be in a different class Q (P ≠ Q). A close datapoint is considered a minimal change that needs to be made to get a different prediction. This can help in scenarios like rejection of a loan or credit card request, where the applicant is willing to know about the smallest change in the feature set that can lead to acceptance of the request.Explicitly, consider a person who applies for a loan and is rejected. In our simple example, say that the person is represented by a length-3 feature vector x: liquid assets of $10K; outstanding debt of $50K; and annual income of $45K. So, x = ($10K, $50K, $45K). Our fictitious loan agency uses a pre-trained binary “loan acceptance” classifier, f, that takes as input length-3 feature representations of applications and returns one of two labels: y = 0 (reject), or y = 1 (accept). Here, then, the applicant is rejected because f( x = ($10K, $50K, $45K) ) = reject. Roughly speaking, a counterfactual explanation for this decision (reject) would describe changes, call them x’, to the applicant’s initial feature vector x such that f( x’ ) = accept. There may be many possible counterfactual explanations: for example, lowering outstanding debt from $50K to $25K to form x’ = ($10K, $25K, $45K); or, increasing liquid assets from $10K to $20K to form x’’ = ($20K, $50K, $45K). Some may be easier or harder to attain for the applicant -- and some may be completely impossible to achieve -- motivating research into the creation of “the best” counterfactual explanations for a particular use case, which we discuss in greater depth below.Fig1: Illustrative diagram counterfactual explanations. The datapoint labeled X (blue) got classified in the negative class. CF1 (red) and CF2 (green) are two counterfactuals for X, which the model classifies in the positive class. Several counterfactuals can be generated for a datapoint, which differ in closeness to the original datapoint and other desirable properties.Themes of research in CFEsMuch of the literature in counterfactual explanations have proposed algorithms to address additional aspects of the problem. We categorize recent research into the following major themes:Actionability: A CFE is only useful if it prescribes changes to features that can actually change. It would be unhelpful if I were told to change my birthplace in order to receive a loan.Sparsity: A useful CFE should modify only a few features in order to be simple and easy to use.Proximity: A useful CFE should be the smallest possible change that achieves the desired outcome.Causality: A useful CFE must be able to adhere to any causal constraints that a domain expert specifies. For example, I should not have to decrease my age in order to get a loan.Data Manifold: A useful CFE should result in a datapoint that is similar to other datapoints seen in the training data. It would be less trustworthy if the resulting datapoint is utterly unlike anything the classifier has ever seen.Speed: CFEs should be generated quickly for new, incoming datapoints.Model Access: Some CFE approaches require detailed knowledge of model internals and gradients. Others can work in a black-box fashion and are model-agnostic.In our survey paper, we collect, review, and categorize 39 recent papers that propose algorithms to solve the counterfactual explanation problem. We design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. This provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.ConclusionsCFEs present a compelling form of XAI, providing users with understandable and actionable feedback. The additional constraints and desiderata explored in recent years seek to ensure that these explanations are always reasonable and useful. Many exciting open questions remain, and we close our paper by proposing research challenges for the community to tackle in the coming years. We firmly believe that CFEs will form a long-lasting part of the ML explainability toolkit.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/recommender-systems-need-fairness-too
 Recommendation Engines Need Fairness Too!
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessRecommendation Engines Need Fairness Too!By: Keegan HinesJuly 16, 2020As we turn to digital sources for news and entertainment, recommendation engines are increasingly influencing the daily experience of life, especially in a world where folks are encouraged to stay indoors. These systems are not just responsible for suggesting what we read or watch for fun, but also for doling out news and political content, and for surfacing potential connections with other people online. When we talk about bias in AI systems, we often read about unintentional discrimination in ways that apply only to simple binary classifiers (e.g. in the question “Should we let this prisoner out on parole?”, there are only two potential predictions yes, or no). Thinking about mitigating bias in recommendation engines is much more complex. In this post, we’ll briefly describe how these systems work, then surface some examples of how they can go wrong, before offering suggestions on how to detect bias and improve your users’ experience online, in a fair and thoughtful way.Part 1 The Anatomy of a Recommender SystemIf you’re coming to this article as someone who regularly builds or works on recommender systems, feel free to skip this part. For those of you needing a refresher or primer on the topic, read on!Recommender engines help companies predict what they think you’ll like to see. For Netflix, YouTube and other content providers, this might happen in the format of choosing which video cues next in auto-play. For a retailer like Amazon, it could be picking which items to suggest in a promotional email. At their core, recommender systems take as input two “sides” of a problem -- users and items. In the case of Netflix, each user is an account, and each item is a movie. For Amazon, users are shoppers, and items are things you can buy. For YouTube, users are viewers, items are videos, and a third component are the users that create the content. You can imagine analogues with newspapers and other media sources such as the New York Times and the Wall Street Journal, music streaming services such as Spotify and Pandora, as well as social networking services such as Twitter and Facebook.Users rate some items, but not all of them. For example, even if you binge watch shows on Netflix, it’s unlikely that you have rated even a small fraction of Netflix’s vast content catalogue, much less so when it comes to YouTube’s library, where over 300 hours of content are uploaded every minute. A recommender system’s goal is, given a user, find the items or items that will be of greatest interest to that user, under the assumption that most items have not been rated by most users. How is this done? By learning from other, similar items, similar users, and combinations of the two.Recommender systems recommend content based on inductive biases. One common inductive bias is that users who seem similar in the past will continue to seem similar in the present and future. In the context of recommender systems, this means that users who have, for example, rated videos similarly on YouTube in the past will probably rate videos similarly moving forward. Recommendations based on this intuition might try to find similar users to a particular user, and similar pieces of content to a particular piece of content, and then combine learnings from those two neighborhoods into an individual score for that particular pairing of a user and item. By doing this for every user-content pair, the recommender system can “fill in all the blanks”, that is, predict a rating for each combination of user and piece of content. After that, it is simply a matter of picking the most highly-rated pieces of content for that customer, and serving those up as you might see in a sidebar on YouTube or a “view next” carousel on Amazon Shopping.Part 2 What Could Go Wrong?As we’ve discussed above, recommender engines attempt to “fill in the blanks” for a particular user by guessing at their level of interest in other topics when we only know how they feel about things they’ve already seen or read. Most recommender engines are a blend of “nearest neighbor” calculations and active rating elicitation, using a combination of supervised and unsupervised learning alongside deterministic rules that modify the selection process among the content that you could potentially recommend. To discuss some of the issues that often arise in recommender engine bias, we’ll look at a couple of examples from industry that illustrate the nuance and complexity involved.One of the more common issues we see in industry can be illustrated by YouTube’s spectacularly named “Gangnam Style Problem”. The problem is this no matter what content you recommend to your user, when one looks at the potential pathways they could take from one recommendation to the next, they all lead back to whatever happens to be the most popular video that day. While this may be good news for PSY and K-pop stans worldwide, gaining traction within a recommender engine can make or break the experience for someone creating content on these platforms, where they need their content to be seen in order to survive.More so every day, we hear about complaints from within the YouTube creator community, claiming that their channels suffer from this disparity, and that YouTube is biased against emerging artists. Thinking this through from a business perspective, it’s easy to see why this might be the case YouTube wants to keep users on the page, and they’re more likely to do that if they can show you content that they know you’ll actually like. In fact, the less YouTube knows about how users will interact with your particular brand of content, the more risky it becomes to promote it.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap
 ArthurAI Fintech Innovation Lab: Class of 2020 Recap
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsArthurAI Fintech Innovation Lab: Class of 2020 RecapBy:Victoria VassilevaJuly 1, 2020The LabEvery year since 2010, the Fintech Innovation Lab - a joint program between the Partnership Fund for New York City and Accenture - chooses just 10 emerging enterprise tech startups developing innovative solutions for financial institutions. This year, over 250 early- and growth-stage companies applied. Over 40 participating financial institutions and VC firms across the globe choose each class. Needless to say, we’re proud to have been selected alongside the rest of our 2020 class.The ExperienceThe Lab provides access to top executives at the world’s leading financial institutions who, throughout our experience, routinely extended exceptionally generous and open feedback. The added exposure to Procurement and Security leadership for insight into their requirements and timelines is a priceless check on our enterprise-readiness. Passing these checks is crucial to success and reflects how thoroughly the Lab works to prepare us .Arthur was born out of the pain points that our founders personally experienced within the financial industry. The opportunity to quickly and repeatedly validate our product and market fit directly with key decision makers and core users alike was invaluable. This is table stakes for producing a best-of-breed, turnkey observability platform.Model performance, explainability, and fairness are board-level, company-wide concerns. We built Arthur to serve as a single pane of glass for the entire organization - serving data scientists and engineers as well as operations, risk teams, and business stakeholders. This made the Lab the perfect opportunity to refine our messaging across dozens of high-leverage use cases.The Challenge - 2020 and Beyond2020 has presented no shortage of tough challenges. For this reason we feel particularly privileged for the chance to join forces with leading financial institutions on the joint mission of not only triaging the immediate and ongoing impact of COVID-19, but also investing in proactive production AI monitoring fit for the advanced technologies driving their customers’ financial health.The profound changes to our society immediately showed up in data and financial models. The impact proved to flow both ways. Model performance, explainability, and bias required a renewed urgency, with the pandemic underscoring how they are connected. We have been inspired by the cultures of care and innovation we witnessed in so many of our partners to address these needs.The regulators and governance & compliance leaders were no different. At Arthur some of our relationships with regulators go back decades and we remain active in the policy space. While we regularly participate as advisors in this arena, we were nonetheless very motivated by the enthusiasm from FINRA, the NYS DFS, the OCC, and the CFTC for open dialogue as partners toward industry model fairness. We are excited by their interest to collaborate on solutions.COVID-19 will certainly carry a lasting impact - it has been promising to engage with our peers on the financial and customer challenges at hand not only today, but ensuring they are also prepared for tomorrow.Going VirtualAs a result of the ongoing pandemic, the Lab, like many, pivoted to a first-time-ever, entirely virtual experience - we’re happy to report that this came with its own advantages. Meeting with senior executives is typically a fairly formal event in a corporate office, but now we were entering each other’s homes - admiring background art, scoping home libraries, and grinning as a few kids made surprise appearances. It was a wonderfully leveling experience that enabled genuine relationships when it mattered most. We’ll see you over drinks in 2021.The FiguresWe mutually paired with 6 premier Financial Institutions40+ meetings with executives and counting7 meetings with regulatory bodies such as FINRA, NYS DFS, the OCC, and the CFTCDozens of meetings with Fintech VCs, Insurance leaders, entrepreneurs, and media and marketing advisorsWhat’s NextThough the program has come to a formal close, the opportunities and support have not. Having tailored and proven our enterprise-grade, production model observability platform and its value with some of the most advanced financial technology providers, we are excited about the discussions of pilot programs and upcoming members of our select Customer Advisory Board. Stay tuned for more on that!We know we have made lasting and ongoing partnerships with the institutions and organizers alike - for that alone we’d advise anyone to pursue a spot in the Lab.Special thanks to Maria Gotsch, Sunny Parikh, and Jeff Kalski for their ever-present positivity and support!Your TurnGet in touch to see for yourself.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise
 How to Build a Production-Ready Model Monitoring System for your Enterprise
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringHow to Build a Production-Ready Model Monitoring System for your EnterpriseBy: Keegan HinesJune 15, 2020Whether your team has recently deployed their first or their 100th ML model to production, you likely appreciate the importance of proactively monitoring your models’ inputs and outputs. In an ever-changing world, the model you trained in the lab is almost certainly performing differently out in the wild. In order to ensure that all stakeholders can trust your ML systems, you need to ensure that your models continue to perform well, that they can be understood, and that they don’t discriminate unfairly. These ideas form the major pillars for AI Observability: Performance, Explainability, and Fairness. In this post, we’ll walk through the components of building an enterprise-wide system for model monitoring, so that you can provide an always-on visibility to stakeholders across your company, including data scientists as well as risk officers, legal advisers, and business stakeholders. This guide will help you understand what you’d need to build for your enterprise, or, you can reach out to us at ArthurAI to learn more about our turnkey system for enterprise model monitoring.Platform Agnostic IntegrationAt a large organization, you might have many different data science teams, all of whom use their favorite tools for model development and deployment. Perhaps some corners of your org are big fans of Tensorflow, others love SageMaker, others love H20, and yet some others have in-house tooling built with open source stacks. We see this pattern all the time and we think it’s great: let data scientists use the tools that they love so they can be as effective as possible. When it comes to model development and deployment, this heterogeneity can be a sign of a diverse and creative data science community. However, from the perspective of technology risk management, all these disparate systems might make you (and your legal team) a little uneasy. To answer a simple question like “Are all the models doing ok today?”, you’d have to go check many different systems and hope they all offer comparable information. What you’d really want is a single go-to place for seeing how all your models are performing, and a way to be notified as soon as something goes wrong. This is why it is important that across your ML ecosystem you need to create a centralized and standardized layer for model monitoring and governance. With this goal in mind, your monitoring system needs to be platform agnostic and allow drop-in integration across all those different stacks.Fortunately, much of model monitoring can be achieved without tight coupling to the specific environments for model development and deployment. As we’ll describe below, monitoring the stability of a model’s outputs is a process that can be completely ignorant of the model’s development process or deployment environment. The same is true for monitoring a model’s inputs and for ensuring a model is fair. And with many methods for model explainability, the specifics of the model training procedure are irrelevant once we have a trained model to probe. Therefore, it is possible to build out a platform-agnostic monitoring system that is accessed through REST APIs so that all the models in your ecosystem are consistently monitored in real-time, no matter how they were built or deployed. Even better, it’ll be important to provide data scientists and engineers with easy-to-use client libraries in their favorite languages (mostly Python, but possibly also Java, Scala, or R).Performance and Data MonitoringModel OutputsThe most salient question you want to answer is this: Is my model performing as well today as I expect it to? Notions of “good” performance will vary depending on different business cases, but there are many general themes you can bring to bear. In the ideal setting, your models will have access to a timely ground-truth so that you can effortlessly compute things like accuracy, confusion matrices, ROC curves, and so on. Monitoring a model’s accuracy can make you aware of a performance issue as soon as it happens. In fact, it would be great if you could make any such metrics extensible, because surely someday you’ll find a data scientist who wants something slightly different than the metrics you are calculating in your system. Allowing users to onboard custom metrics calculations will ensure that no data scientists are left out. In a use case such as product recommendation or targeted advertising, you’ll know instantly whether your model’s outputs resulted in a click or not. However, there are many cases where ground-truth is hard to come by, and what can we do then?An example is issuing credit cards - if you approve someone for a credit card, it will likely be months or years before you decide that was a bad decision. In these cases, you’ll need to generate proxy metrics for accuracy by monitoring the stability of a model’s inputs and outputs. The stability of a model’s output predictions is a useful proxy in the absence of ground truth. Since a model, once fitted and deployed, never changes it’s view of the world (its decision surface), any notable changes to its output can be attributed to significant changes in its inputs - something your data scientists will want to know about. You should consider monitoring the distribution of output predictions from a model, whether a regression model, classification model, multilabel model, multimodal model, or anything else. There are many ways to quantify similarity/stability of distributions, depending on the type of model and dimensionality (more on that below). When it comes to monitoring the stability of distributions, you might quantify changes through time, or changes relative to baseline (such as the training set), or both.Data DriftJust as important as monitoring a model’s outputs, it is vital to monitor the stability of a model’s inputs: often referred to as Data Drift or Concept Drift. Your model is trained on a fixed snapshot of the world - the training data. Once your model is deployed, the world will inevitably change and depending on how severe that change is, the learned decision boundary of your model might become irrelevant. By regularly quantifying the similarity of the data coming through the model today to the data the model was trained on, you’ll be able to quickly identify when things start to go off the rails.There are many ways to quantify distributional similarity and to build a system for data drift detection. Your first approach might be to monitor each of the incoming features to a model and quantify similar to the training set. For example, if a model has Age as an input feature, then you’d want to look at the training set and develop a statistical profile of the data in that training set. Then going forward, you can calculate the similarity of the data coming through, as compared to the training data. Measuring the similarity of two distributions can be achieved in many ways including non-parametric hypothesis tests, KL Divergence, Jensen-Shannon Divergence, Population Stability Index, Wasserstein distance, and many more. What these have in common is that they will take in two empirical distributions as input and will result in a similarity score as output. When that similarity score starts to have big changes, it is an indication that the data today is quite different than the training set, and maybe it’s time for a retrain and redeploy.Computing distributional similarity for each input feature independently will generally tackle a large proportion of the problem, but is not without limitations. An early thing to consider is that we’re implicitly looking for drift only in the marginal distributions of each feature and not in the higher-dimensional joint distribution over the data. For high dimensional datasets, and especially for unstructured data such as imagery and text, it will be important to consider multivariate methods for quantifying distributional similarity. While there is generally not an easily-computed high-dimensional analog for KL Divergence, we can take a model-based approach to quantify data drift. In this approach, we will train a model of some kind on the training set. This model isn’t a classification or regression model, but instead is some kind of density model (or similar) which is trying to understand how the data is distributed in the high dimensional space. Once we have fitted such a model to the training set, we have a snapshot of what “normal” data looks like. Going forward, we can collect new inferences and query this model to quantify how well the new inferences adhere to the distribution of the training dataset. For each single inference, this gives us a powerful mechanism for anomaly detection, since we can now identify incoming points that the model has scored but yet don’t really look anything like the training data. Further, as we aggregate over larger groups of inferences, we have an holistic view into multivariate data drift in a high dimensional space.That general framework can be accomplished with many different modeling techniques. One approach might be using an Isolation Forest or KDTree to fit to your dataset. More recent techniques show promise for fitting properly-normalized probability models to high dimensional datasets, including Variational Autoencoders and Normalizing Flows. Additionally, preprocessing methods such as dimensionality reduction have been shown to be helpful for high-scale problems. In all cases, you’d need to also build a system for training, storing, containerizing, and serving each one of these density models so that streaming inferences can be scored as they come in.Finally, it is worth noting that not all data drift is created equal. This idea is sometimes referred to as ‘virtual’ concept drift and denotes instances where the data has drifted but in a direction that doesn’t materially affect the model’s outputs. Thus, it is very helpful to combine data drift monitoring for each feature with a simultaneous quantification of how important each feature is for model decisioning. Combining data drift modeling with model explainability (more on that below) is a powerful way to prioritize your teams’ time and attention.Explainability as a ServiceUnderstanding a model’s decisions is an important part of building trust for the adoption of ML across your organization. With increasingly-complex ML techniques, their flexibility is often accompanied by a difficulty in understanding why they make their predictions. The field of Explainable AI has put forth many valuable techniques for calculating explanations of model predictions. In your model-monitoring system, if you are able to provide these explanations for every prediction your models make, this can go a long way toward building trust and comfort among a broad class of stakeholders.There are many great techniques for calculating local explanations of ML models. These methods could be model agnostic (like LIME), model-based (like DeepLift), or both (like Shap). In all cases, you need to access a model’s predict function in order to probe the relationships between a model’s inputs and outputs. For your monitoring platform, this means you don’t need to be tightly coupled to the model training environment. Instead, you only need access to the final trained model and ability to probe it. You might hook into existing model deployment APIs, or you might build and replicate a model microservice solely for computing explanations on the fly. In this case, you’ll need to think about containerization for replicating the model’s dependencies and environment. And if your model requires high throughput or high dimensional data (or both) you’ll want to think about ways for autoscaling the computation of explanations in order to keep up with the inference load. Additionally, you might put some thought into refactoring some of those favorite explainability methods to make them more performant for your use cases.Once you’ve computed explanations for every inference coming through your models, it opens many exciting possibilities for helping your organization. First, it provides your data scientists with a granular view in a model’s decision surface, allowing them to identify areas where a model might be underperforming and helping to debug models in production. Second, these explanations form a useful audit trail for your modeling system, ensuring that every decision that a model makes is logged and can be understood at a later time. And finally, considering local and global feature importance will help you understand and prioritize data drift and the emergence of new patterns in your data.Algorithmic FairnessIt is important to not only ensure your models are making the “right” decision from a statistical standpoint, but also the “right” decisions from an ethical standpoint. It has become clear across many industries that systems for automated decision making can exacerbate disparate impact and discrimination against specific groups of people. Ensuring that your models are fair is tantamount to ensuring that they result in similar predictions/outputs for all relevant subpopulations of your customer base. Traditionally, fairness analysis is conducted over protected classes as defined by race, age, and sex. But for your business, there could be many other dimensions and factors over which you want to ensure your model is resulting in comparable outcomes. For example, you might want to understand model disparities by geography, income, spending level, or any business-driven segmentation of your customer base.Many researchers have attempted to quantify notions of algorithmic fairness, with many such definitions proposed in recent years. For your business, you should decide which definitions and fairness metrics are most aligned with your company’s goals for Responsible AI. The next step is to build a Fairness Audit framework into all modeling done in your organization. In this pursuit, you would examine all models’ inputs and outputs per any sensitive/protected classes and arrive at a quantification of disparate impact. Naturally, this kind of fairness audit would not be a static process but would be an ongoing analysis that is continually conducted as new inferences go through your models. Ideally, you could provide an easy-to-use dashboard for identifying, exploring, and mitigating disparate impact in your models. This dashboard would make these fairness metrics accessible to all important stakeholders in your organization, and ensure that this information is not just residing with data scientists.APIs, Storage, and ComputeWith inferences stream into your monitoring platform, and real-time metrics being computed for drift, accuracy, and more, you’ll need to put some careful thought into data storage, access, and streaming analytics. Some data science uses cases operate in large batches while some might operate in a streaming fashion, so a combination architecture with Kafka and Spark might prove useful. Many of the metrics and analytics we’ve described can be computed in a streaming context and autoscaled to meet load requirements. Once these metrics, analytics, and explanations have been computed over the inferences, it would nice to make all of this data available to data science teams to explore on their own. You might consider storing the metrics in a datastore appropriate for fast access and large scale, so that your data scientists can quickly slice and dice this data. You could even connect this backend store to an interactive data visualization dashboard, allowing your teams to explore a model’s decision space and better understand areas to improve and debug.Real-Time AlertingOnce you’re calculating all the previously described metrics (and housing them in a scalable data store), you’ve got everything you need for real-time alerting. You can let data scientists know the moment a model’s accuracy starts to drop too much or an important feature seems to have drifted significantly away from the training set. You can build alert integrations directly to where people are spending their time, including email, Slack, or ServiceNow. Apologies in advance when you get a 2am wakeup call about your model’s performance plummeting.User InterfaceThe great thing about instrumenting a platform-agnostic model monitoring tool, is that it suddenly enables disparate stakeholders to have effortless access to a model’s outputs and behaviors. Concepts that might have typically been stuck inside a data scientist’s notebooks (things like model explanations) are now readily available for a broader audience to consume. You now have a few different personas to think about when designing interfaces. The first might be data science practitioners who are hands-on with model development and will want a very specific and technical view into the data surrounding each model. The second might be data science leaders and risk management leaders, who primarily will be concerned with ensuring that everything is healthy and nothing needs escalation. Finally, you’ll want to think about less-technical business stakeholders who are using these models to accomplish their business goals (and accepting these operational risks). You’ll want to make sure they have easy-to-understand access to major insights around model performance and risk.The Other Important StuffLet’s not forget about things like role-based access control (RBAC), single sign on (SSO) integration with enterprise user directories, end-to-end encryption and other policy compliance. You’ve got to get this stuff right the first try, so be sure to move very carefully and thoughtfully through these topics.Putting It All TogetherWith these pieces in place, your data science teams should be able to effortlessly onboard new (and old) models into your monitoring system and begin sending inferences and telemetry. Your monitoring system will aggregate this information and compute real-time metrics for stability, performance, fairness, and anything else important to your organization. Alerting will provide real-time awareness so that data scientists can begin solving problems before it’s too late. The dashboarding you’ve built will provide access to these concepts to a whole new suite of stakeholders across your business - not just for data scientists anymore. Good luck!Monitor your Models with ArthurOf course, you can build! Or you can trust Arthur. Arthur is a proactive machine learning model monitoring platform that gives you the confidence that your AI deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. With performance monitoring, bias detection, and customizable alerts, Arthur makes sure that you never miss an issue, and Arthur’s explainability engine makes runtime debugging effortless.If you’re deploying any (i.e., Tabular, NLP, Computer Vision, etc.) models into production and are looking for a solution for monitoring those models over time, we’d love to connect and show you how Arthur can help. Request a demo today.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/ai-during-black-swan-events
 AI During Black Swan Events
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringAI During Black Swan EventsBy:Liz O'SullivanJune 8, 2020The world has been turned upside down by COVID-19 and civil demonstrations during the last few months. The effects on our society, the overall economy, and just about every industry are unprecedented. April and May look absolutely nothing like January or February, and with the situation still unfolding, June and July will be completely different as well. One of the less obvious impacts of this period of rapid change is to the behavior of AI models that play critical roles in our society.At Arthur, we’ve seen models across many industries affected. Trading algorithms are one clear example, as recently covered in the Wall Street Journal.“Hedge funds that use artificial intelligence models to suggest trades and stock picks declined when stock markets unraveled in late February... market experts warn the unpredictable nature of the economy these days could trip up some algorithms that continue to rely on data gathered during better times.”-WSJ, AI Funds Decline—Then Recover—During Market Turmoil, May 1, 2020Let’s examine why these models are so vulnerable.The Shortcomings of Machine LearningMachine Learning (ML) is the most prevalent form of AI today, to the point that the two terms are often used synonymously. ML works by learning patterns in existing data (aka the training set) and then using those learned patterns to reason about future data. It performs quite well when that future data is similar to the training set. As that similarity disappears, the lessons the ML model has learned become less relevant and the model becomes less able to “reason” correctly about the new data it is seeing.In a recent MIT Technology Review interview, Gary Marcus, a vocal critic of deep learning-based approaches to AI said “I think that we’re living in a weird moment in history where we are giving a lot of trust to software that doesn’t deserve that trust.” So what are these shortcomings and how do they relate to our current situation?Consider the ancient game of Go. We all watched as AlphaGo proved that algorithms had finally surpassed humans on this extraordinarily complex challenge. But what if we were to replay those games, adding in a little twist. In this new game, every few moves the rules get altered slightly -- additional rows of dots added, the ability to place more than one stone per turn, etc. This time around, the human would best the algorithm because it would never have seen any data like this in the past. The human, while faced with a more difficult task than a routine game of go, would still be able to play relatively effectively.Simply stated, current approaches to AI do not adapt well to changing situations.AI in the age of COVID-19Right now, the automated systems affecting our finances, healthcare, travel, purchasing intents, and many other aspects of our life are making decisions based on patterns they learned prior to COVID-19. So are models that make business decisions in capital markets, high frequency trading, supply chain management, and manufacturing. But those patterns are much less applicable now; the world has shifted.These shifts and models’ inability to adapt will result in companies losing money at a time when they are already being strained. Meanwhile, consumers and end users who interact with these systems will be on the receiving end of less reliable decisions.Is there a solution?There are avenues of research that seek to solve this problem. Reinforcement and online learning focus on models that continue to learn as new data flows in, so they aren’t limited to patterns learned from a “point in time” snapshot training set. But these approaches are often hard or impossible to apply to real world scenarios.Symbolic AI is an alternative to deep learning that seeks to explicitly model truths about the real world and then allow machines to programmatically apply formal logic to these truths to draw conclusions. It has the huge advantage of being able to reason about scenarios it has never seen before. While it has been out of fashion for many years due to reliability concerns, it has had a resurgence in the last couple of years as researchers grapple with the shortcomings of deep learning.These types of scientific advances are a ways off and will require time to mature to the point they can be utilized in the real world.The immediate, pragmatic solution is to make your AI observable. Systematic model monitoring allows you to immediately assess the shifts in the data feeding your models, and understand the degree of impact it is having on model decisioning. With this knowledge you can respond immediately, smartly prioritizing model performance improvements to ensure your business maintains maximum continuity.Going forwardPeople who have worked in data science long enough have battle scars from model performance loss. Sometimes it’s quick, such as when you encounter a black swan event. More frequently, it’s gradual. Often the gradual degradation is worse because it flies under the radar for a long time.A lot will change as a result of COVID-19. People are already questioning social norms that go back centuries -- handshakes, sporting events, working in an office, etc. For those of us in the ML community this is a wake up call to build systems that are resilient. That starts with building AI that is observable. Knowing when your AI is struggling to adapt to rapid change and which models are the most impacted is critical for real world systems. And longer term, continued research into more adaptable approaches to AI such as symbolic AI and reinforcement learning will be paramount. Automated systems are creating extraordinary value -- making them more observable will ensure that continues even when the world shifts.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected
 How Explainable AI and Bias are Interconnected
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessHow Explainable AI and Bias are InterconnectedBy: Keegan HinesApril 24, 2020In the world of artificial intelligence, explainability has become a contentious topic. One view among machine learning experts is that the less a model can be interpreted, the more accurate it will be. Some fear that adoption of explainable AI will slow down the adoption of machine learning. However, explainability doesn’t seek to slow down advancements in AI - it seeks to make that advancement fairer and safer for both everyday people and businesses implementing the AI. Explainability also goes hand-in-hand with decreasing bias in AI.How important is the interpretability of your taxi driver?Yann Lecun, VP & Chief AI Strategist at Facebook, once asked the question, “How important is the interpretability of your taxi driver?” On the surface, this thought experiment might seem to be an explainability killer. As long as you get from point A to point B safely, who cares how and why the taxi driver got you there? But this is a red herring -- of course you want to understand your taxi driver’s choices! Otherwise, what’s to stop someone from taking a longer route that makes you late or costs your more money? Also, the taxi driver needs to be able to explain their actions to protect themselves against any legal issues.Explainable AI aims to address the lack of information around how AI decisions are made. It is important because when an uninterpreted model fails, those in vulnerable groups are most likely to be negatively impacted, as we saw in the Gender Shades study. In the study, it was revealed that three top AI companies performed better on men than women and especially performed the worst on women with dark skin tones. Without explainability, discriminatory decisions go unchecked.There are also growing legal issues surrounding AI that explainability seeks to address. The European Union, for example, has expanded guidelines around automated decision making rights under GDPR. Meanwhile, in the US, certain decisions like those based on creditworthiness are also subject to right to explanation laws. As regulations catch up with technologies, the need to be able to explain AI decisions will become increasingly important.Plus, when you do get anomalous decisions from a model, that often does reflect something in the world that wasn’t picked up on during model development. It’s important that when this happens data scientists know what data or new features are needed to have a properly trained model.A lack of clarity around AI decisions can lead to frustrations from users who aren’t able to understand why a decision was made and even legal implications if black box decisions further discrimination of protected groups. This underscores the importance of understanding AI predictions better to develop AI that are more fair and helpful for society.Ignorance is not bliss.There’s a real and very valid concern around the legality of implementing black box models in sensitive areas. If a police officer catches someone based on an algorithm, how do they explain that that wasn’t based on prior bias ? If an algorithm tells a doctor a patient is dying, but the AI is wrong, who’s liable for the incorrect diagnosis -- the doctor using the algorithm? The company that sold the software? or the architects of the original algorithm?For example, look at the now-scrapped HR system Amazon built:Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry. In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter. They did not specify the names of the schools. (Reuters)Even though Amazon edited the programs to make them neutral to these particular terms, there was still no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory. This led to Amazon ultimately disbanding the team with recruiters not able to trust the system enough to rely on it solely, leading executives to lose hope for the project. And herein is one of biggest arguments for explainability: it allows for the safe, responsible adoption of AI. Explainability could’ve helped these developers catch this disparity prior to deployment, and consequently, a lot more women would have had opportunities to be screened.Explainability will likely continue to be a push and pull in the AI world. However, we’re already seeing regulations like GDPR push explainable AI forward. Being able to interpret AI remains key to addressing the lack of trust around black box decisions, avoiding vulnerabilities in models, and decreasing the amount of human bias in machine learning.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance
 3 Reasons Model Monitoring is Vital for Strong AI Performance
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model Monitoring3 Reasons Model Monitoring is Vital for Strong AI PerformanceBy: Keegan HinesApril 16, 2020Model monitoring is key for having continually high-performing artificial intelligence models in place. As we know, AI models make predictions based on historical data. However, as the world changes, that historical data becomes less and less relevant, which can lead to incorrect predictions and business outcomes . It’s important to understand the ways in which models fail, so you can become aware the moment it happens, and update your models to reflect the new conditions in production . To help with understanding how model monitoring can be put into use, we’ve outlined 3 of the top ways data issues can cause AI performance loss below.1. A Changing WorldWith a few exceptions, machine learning models do not learn on their own. Businesses rely on humans and automated systems to deploy updates. And while the world naturally changes and evolves, your model can fall more and more out of tune with reality . This is data drift, and it can happen in a few different ways .First, a models may be inadvertently used in ways that data scientists did not intend. For instance, suppose your business sells skincare products targeted towards a more mature clientele, and you use AI to manage your ad spend. Your data is likely built up using demographic information about your original age target. However, if you launch a new product targeted towards teens and begin using those models without updating the training data, your advertising will likely underperform.Second, the world isn’t static -- it evolves, constantly. This means past data points aren’t always good indicators for future predictions. For example, let’s say you run an app with a built-in messaging feature. You may implement a model to predict and suppress spam in order to improve the user experience. However, over time, spammers will likely figure out what your model looks for and change their behavior to subvert your models.2. Extreme Novel EventsGood and bad novel moments happen all the time - we’re living through one right now with COVID19. And these events can have a large impact on businesses. As a lighter example, there’s the event of sudden, and unexpected popularity .Take, for example, dalgona coffee. Thanks to this viral TikTok coffee trend, searches for instant coffee on Google grew by more than 700% in about a week.Retailers online and offline often have models that use historical data to predict how much stock they should carry of any given product, but that data doesn’t take into account sudden trends - meaning lots of customers found themselves frustrated at the lack of stock available for purchase and tons of missed revenue for the business. Model monitoring, here, could have given these retailers advance notice of the incoming surge in purchases.3. Data Ecosystem ChangesUsually, you don’t just have one model feeding into one outcome. Instead, models feed into more models to make much more complex predictions and decisions. This means if you change even something seemingly innocuous like going from nulls to 0s in one model, it can have a big impact in the performance of your whole system. In many cases, changes may not be immediately catastrophic. However, these quieter inaccuracies are often what keep data scientists up at night because they’re much harder to spot and can cause huge monetary losses.Here, model monitoring is vital to catch those subtle (yet powerful) changes that would be otherwise overlooked. Those small changes and inconsistencies are easy to miss, but having a model monitoring system in place provides protection from data changes leading to model failures and lost revenue.Today, AI is vital for growing and scaling businesses. But we must be cognisant that AI is still quite brittle . Understanding the ways in which AI models can lose accuracy enables your business to monitor and address those issues before they cause real problems for your team. These guardrails ensure you will deploy a safe and reliable AI system within your business and get the most out of the models you’ve spent so much effort to deploy into production.Have questions about model monitoring? Shoot us an email at info@arthur.ai‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/fairness-in-ml
 Fairness in Machine Learning is Tricky
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessFairness in Machine Learning is TrickyBy:John DickersonApril 8, 2020Non-experts and experts alike have trouble even understanding popular definitions of fairness in machine learning — let alone agreeing on which definitions, if any, should be used in practice.Human decision-making processes are known to be biased. Look at the promotions process at a typical large company. In the words of Tomas Chamorro-Premuzic, the Chief Talent Scientist at one of the world’s largest staffing firms, “… most companies focus on the wrong traits, hiring on confidence rather than competence, charisma rather than humility, and narcissistic tendencies rather than integrity.”This observation isn’t new, and myriad other examples exist. In Australia, if the name on your resume “sounds” Middle Eastern or Chinese, you are less likely to be hired; in the US, replacing a male-sounding name with a female-sounding one on an otherwise identical CV can result in a lower chance of offer, as well as a lower starting salary. When giving a negative medical evaluation, doctors exhibit similar levels of implicit bias based on race and gender as the general public might — which is to say, quite a bit. And, while open and explicit redlining for banking and insurance in the US is no longer legal, there is evidence that its impacts are still implicitly felt today.Can we just take humans out of the loop and rely on cold, hard data? Initially, some proponents of automated decisioning techniques stemming from the data mining and machine learning (ML) communities pushed for this. But algorithms take data as input, along with whatever biases come along with how those data were — or weren’t — sampled. This includes: which features were stored, which humans made those decisions, and disparities in sample sizes of different subgroups of inputs, among others. Worse still, often any resultant discrimination is an emergent, i.e. learned, property of the system (rather than a hand-coded rule written by an algorithm designer), making identification and any partial mitigation of the issue more difficult.Remember our earlier examples of bias in human decision-making systems for promotions, hiring, healthcare, and banking? Their counterparts have all been found in the automated versions of those systems as well. So, bias in ML-based decisioning processes is ubiquitous, just like bias in human decision-making processes. Can we do anything about it? Yes, and we should — but it’s important to set our goals realistically, and keep both stakeholders and domain experts heavily and continually involved.One approach taken by the machine learning community is explicitly “defining fairness” — that is, proposing different metrics of fairness as well as approaches to encoding that into machine learning pipelines.Some of these definitions are explicitly or implicitly based on existing legal doctrine. For example, channeling Title VII of the Civil Rights Act of 1964, an algorithm is said to result in disparate impact if it adversely affects one group of people of a protected characteristic (aka “sensitive attribute”) over another. Similarly, that algorithm is said to result in disparate treatment if its decisioning is performed in part based on membership in a group. Then, one goal that a fairness in machine learning practitioner might have is to mathematically certify that an algorithm does not suffer from disparate treatment or disparate impact, perhaps given some expected use case or input distribution.Toward that end, let’s dive a bit deeper into disparate treatment — specifically, what does it mean to make a decision based on membership in a group? Well, the algorithm could formally discriminate, that is, take as input explicit membership in a group, and then use that in some way to determine its output. This is often illegal to do, so many systems already do not do this. Yet, given a rich enough dataset, membership in a protected group is almost surely redundantly encoded, at least to some extent, by other features. Indeed, unless the target is completely uncorrelated with membership in a protected group, given enough data a well-trained model will completely recapture the protected group membership’s impact on the target — without ever having explicit access to that particular (protected) feature!A standard example treats “race” as a protected group in a dataset including features such as “zip code.” Here, observing “zip code” alone often provides strong signal about “race” — even without explicit access to the “race” feature. So, do we remove “zip code” from the input as well? Maybe not, because it’s quite likely some other set of features correlate with “zip code,” too, and we’re back to the drawing board. Indeed, and in general, it’s not immediately clear how to write down a formal set of rules to enforce accepted legal definitions of fairness in decisioning systems.It’s not the point of this article to completely overview the state-of-the-art in definitions of fairness in machine learning; many definitions have been proposed,¹ and many people have written² about them. My point so far is that it’s tough or impossible to write down agreed-upon legal rules and definitions using formal mathematics — even for “simple” systems performing binary classification on relatively small, well-defined inputs. For the sake of discussion, though, let’s say we have decided on a definition of fairness,³ and we have been able to write it down using formal mathematics, ready to be put into our ML pipeline. Now what?Given a well-defined definition of fairness implemented in a machine-learning-based system, it is natural to ask what the people impacted by that system (i) understand about the system itself and (ii) think about the rules under which it is operating.Ditto with the operators of the system, as well as other stakeholders (e.g., policymakers, lawyers, domain experts). And, when different classes of stakeholder have different opinions about what “fairness” means, how should we manage that?Let’s start with a simpler setting: asking one class of stakeholder if they comprehend well-known definitions of fairness. In joint work⁴ with researchers at Maryland and Berkeley ICSI, we recently did just this: we created a metric to measure comprehension of three common definitions: demographic parity, equal opportunity, and equalized odds, and then evaluated it using an online survey with the goal of investigating relationships between demographics, comprehension, and sentiment.In our study, fairness definitions were presented in multiple real-world scenarios (e.g., in our vignette on hiring, demographic fairness was described as “[t]he fraction of applicants who receive job offers that are female should equal the fraction of applicants that are female. Similarly, the fraction of applicants who receive job offers that are male should equal the fraction of applicants that are male”). Then, comprehension and sentiment questions were asked. Some takeaways:Education is a strong predictor of comprehension, at least for the accessible explanations of fairness used in our study. The negative impacts of ML-based systems are expected to disproportionately impact some segments of society, for example by displacing employment opportunities for those with the least education. Thus, that already-at-risk group’s ability to effectively advocate for its members may be adversely impacted by lower comprehension.Weaker comprehension correlates with less negative sentiment toward fairness rules. One way to interpret this is that those with the lowest comprehension of fairness concerns in ML systems would also be the least likely to protest against it.One promising direction is to learn stakeholders’ views about fairness via simulation or observation of actions over time. Some research has been done in this space. for example, researchers at ETH Zürich fit functions to users’ preferences over a finite and pre-determined feature space using pairwise comparisons of simulated profiles. They found that the well-known notion of demographic parity aligned reasonably well with their human subjects’ responses. One outstanding issue in this study, and in most preference and moral value judgment aggregation studies in this space, is the lack of consideration of different classes of stakeholder. For example, how can we combine the input of a layperson with that of a (domain expert) doctor in a healthcare setting — especially when people’s judgments often disagree?So what do stakeholders of the same type want, in general? Perhaps unsurprisingly, leading technology firms such as Microsoft and Google have taken steps in this direction with respect to the applied production settings that their engineers encounter. For example:Researchers at Microsoft Research surveyed practitioners from 25 ML product teams in 10 major technology firms, and found (broadly) that the “fair ML” research literature focuses too specifically on methods to assess biases, and would benefit from focusing more broadly on the full machine learning pipeline. A striking example from their paper involved an image labeler in a computer vision application systematically labeling female doctors as “nurses,”⁵ which would then serve as “gold standard” input to any downstream algorithms.Researchers from Google recently published a case study of a fielded classification system where adverse actions are taken against examples predicted to be in the positive class (e.g., “if you are predicted to be a spammer, then I will deactivate your account”). They found that it is difficult to even measure a direct-from-the-literature definition (equality of opportunity), and then give a series of steps they took to build a more applicable tweak to that definition into their models.Research suggests that: (i) laypeople largely do not understand the accepted definitions of fairness in machine learning; (ii) those who do understand those definitions do not like them; (iii) those who do not understand them could be further marginalized; and (iv) practitioners are not being served well by the current focus of the fairness in ML community. It sounds negative, but there are explicit next steps to take to help mitigate these issues. Read on!Earlier, I asked if we could “do anything about it” when it comes to the difficulties — and often, the impossibilities — of deciding on, and enforcing, fairness in decisioning systems. The answer is, still, that we can — but responsibly, with input from all appropriate parties, and with an understanding that there is no panacea.Below, I give a non-exhaustive list of action items for researchers and practitioners in the “fair ML” space.We need to understand what (lay)people perceive to be fair decision making. Given an explicit definition of fairness, is it understood, and is it acceptable to a wide audience? If particular subgroups of the general population do not comprehend parts of automated systems that impact them, then they will be more easily disadvantaged.Dovetailing with the above, we need to understand what specialists perceive to be fair decision making as well — and what tools they would need to help them do their jobs. This could mean developing tools to help audit systems, or to better curate high-quality and well-sampled input datasets, or to permit faster exploratory data analysis (EDA) to help find holes in the input and output of prototype or deployed systems.We need techniques that, given a definition of fairness or of bias, can measure at enterprise-scale whether or not an ML-based system is adhering to that definition or those definitions — and, if not, (i) describe by how much and (ii) alert humans, when appropriate, if the system deviates beyond an acceptable level.Additionally, effective UI/UX will be required to allow stakeholders of all walks to comprehend the state-of-the-art in various fielded automated systems. Fielded systems ingest (incomplete) high-dimensional data and output high-dimensional data, over time. Communicating the state of a system vis à vis particular definitions of fairness and bias in a human-understandable way is paramount.Quoting directly from the Microsoft Research study discussed earlier, “[a]nother rich area for future research is the development of processes and tools for fairness-focused debugging.” Debugging tools with a fairness focus would help practitioners identify, e.g., under-sampled portions of an input dataset, or previously overlooked subgroups being adversely impacted by new decisioning rules.Finally, we need to develop shared languages between all involved parties, but particularly engineers, laypeople, and policymakers. Engineers implement, policymakers make society-wide rules — and laypeople are impacted by the interaction between the two. All three need to understand the wants, incentives, and limitations of the others through open and continuous communication.Throughout, it is important to balance prescriptive and descriptive approaches to understanding, measuring, and implementing fairness in machine-learning-based systems. Prescriptive approaches necessarily assume some consensus around what “should” occur under a (societally decided-upon) definition of fairness, whereas descriptive approaches focus more on uncovering what that consensus might be in the first place.Researchers and practitioners interested in fairness in machine learning — myself included! — have focused too much on the former, largely due to its amenability to mathematical characterization. Yet, the latter is nowhere close to understood, and is an absolutely necessary complement if not precedent to more formal prescriptive approaches. That will, of course, require in-depth discussions with stakeholders of all walks — laypeople, policymakers, politicians, ethicists, lawyers, and domain experts. Yet, these discussions will need to be complemented with accurate and scalable techniques that measure and communicate real-world systems’ adherence to various definitions of bias and fairness in machine learning — so that they can provide human feedback to further improve automated decision systems performance in practice.Thanks to Michelle Mazurek, Liz O’Sullivan, and Monica Watson for comments on earlier versions of this piece.[1] I’d urge you to check out the (free, ever-updating) Fairness and Machine Learning book by field experts Barocas, Hardt, & Narayanan, a formally published overview by to-be-ArthurAI researcher Verma & Rubin, or the proceedings of area-specific conferences such as FAccT and AIES.[2] I’d typically also recommend Wikipedia, but at the time of writing, the Wikipedia page for Fairness (machine learning) is a bit of a mess, with fourteen different binary-classification-centric fairness criteria roughly defined amongst a mess of mathematics and essentially nothing else. This one sentence in the third paragraph of the introduction really sums it up, though: “The algorithms used for assuring fairness are still being improved.” Still a ways to go![3] This is a pretty strong assumption! Indeed, it’s almost always impossible to create a system that ensures three “reasonable” definitions of fairness, even in binary classification: calibration, a form of proportional treatment based on relative group size; balance for the negative class, which roughly states that people in the (true) “zero” class should be scored the same; and, balance for the positive class, which is its complement for the (true) “one” class.[4] Our study is ongoing. The working paper is available as “Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,” with authors Debjani Saha, Candice Schumann, Duncan McElfresh, John Dickerson, Michelle Mazurek, and Michael Tschantz. An initial report on our study, titled “Human Comprehension of Fairness in Machine Learning,” appeared at the 2020 ACM/AAAI Conference on AI, Ethics, & Society (AIES-20).[5] Here, the practitioners used a “failsoft” solution to mitigate this input bias and combined the “nurse” and “doctor” labels in their input dataset. Yet, without explicit monitoring, this systematic labeling error likely would’ve gone undiscovered.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/cbinsights
 CB Insights AI 100
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesCB Insights AI 100By:Arthur TeamMarch 5, 2020Today we are honored to announce that we've been included in the CB Insights AI 100. We’re thrilled that CB Insights recognizes the value we are bringing to the leading companies in healthcare, finance, technology, and other industries who trust us with their AI monitoring. CB Insights has long been a signal of the best companies to follow in every category, with incredible companies filling in the top 100 list each year. We believe in the amazing potential of AI to transform the world, and are excited to be doing our part to ensure this transformation is done in a way that is safe, reliable, and fair. With so many incredible companies lined up next to us on the top 100 list, the future of AI looks bright. We're excited to be a part of it.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
Content type: arthur_blog
Source: https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective
 Team Arthur at NeurIPS-19: A Retrospective
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsTeam Arthur at NeurIPS-19: A RetrospectiveBy:John DickersonDecember 17, 2019Arthur is fresh off the plane returning from NeurIPS, AI’s largest — and somewhat infamous — research conference. While there, Arthur announced its seed round and hosted a 50-person model monitoring meetup next to the convention center. Beyond that, the full NeurIPS was seven packed days of new advances and directional changes in the machine learning community. Here’s what the experience was like for us, written by Arthur Chief Scientist, John Dickerson.I’ll start with a broad takeaway: NeurIPS this year was welcoming.In previous years, the conference was known for being somewhat rowdy, coming to a head in 2017/18 with controversies that led, in part, to the conference being renamed. With 13,000 attendees this year, my worry was that things would feel even more chaotic. But, a largely inclusive and constructive tone was set via community-wide norm changes led by leaders in the field, community-building events such as Black in AI and Queer in AI, and workshops such as the ever-growing Women in Machine Learning. This progress is great to see, as it is increasingly evident that AI algorithms are influenced not just by the raw data fed into them, but also the traits and experiences of those who build them.Along those lines, bias, fairness, and explainability were front and center in many talks, posters, and workshops. One of my favorite talks, given by Aaron Roth from Penn, blended ideas from individual and statistical (aka “group”) notions of fairness. The former constrains treatment across pairs of inputs (e.g., “similar individuals are treated similarly” due to Dwork et al.), while the latter partitions inputs into groups (e.g., based on age, race, or sex) and enforces equalization of various metrics across those groups. Both of these approaches have well-known pros and cons. Group-level fairness is relatively easy to define and enforce on general input distributions, but may result in unexpected behavior as one looks at sub-groups (e.g., inputs with a specific race and sex). Individual notions of fairness, unsurprisingly, come with attractive guarantees at the individual input level, but may be hard to define and incorporate into models. Aaron and colleagues’ work provides a natural middle ground, where explicit group selection is no longer needed and some level of guarantee (in expectation) is given to individual inputs; check out the paper!Explainability was also center stage across the board, ranging from workshops such as Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness, and Privacy with panel participants affirming the financial services industry’s need for trustworthy and explainable systems, to industry expo days like Fairness and Explainability: From Ideation to Implementation to domain-specific papers and demos focusing on computer vision and natural language processing tasks. Understanding why models make particular inferences is important for everything from debugging systems to adhering to regulatory requirements, and it was great to see the research community stepping up to provide new tools that will, hopefully, find their way into industry.This was a big year for NLP and, specifically, all things BERT (Bidirectional Encoder Representations from Transformers, from Google) — and all kinds of visualizations of and attacks on BERT and friends.¹Over the last couple of years especially, NLP has seen explosive progress, and that was showcased at this year’s NeurIPS as well.Panel at the NeurIPS-19 “Minding the Gap: Between Fairness and Ethics” workshop.Probably the biggest NLP paper presented XLNet (also from Google), which dominates BERT on a number of tasks. It’s exciting to see NLP move forward so quickly, of course, but many — including big names in the field — are starting to get grouchy² about the field’s seeming obsession with making already big models even bigger. This Twitter thread by Yoav Goldberg is a nice place to start for under-loved holes in NLP research, ranging from theory to explainability to generalization to incorporating linguistic theory back into deep-learned-based techniques.The generalization capabilities of deep networks are poorly understood — they famously don’t align with the traditional statistical view that, after a point, bigger models are worse. Rather, they tend to exhibit a “double descent”, where as a model grows in size test error decreases, then increases, then decreases again. At least two nice papers came out in this “understanding deep learning generalization” space: the winner of the Outstanding New Directions Paper Award from researchers at CMU argues against a particular class of theoretical bounding technique, and another nice paper from Princeton and CMU advanced the state of understanding what larger (namely, infinitely larger) networks can represent.At the NeurIPS Retrospectives Workshop on Friday, leading researchers asked about their past work, “What should readers of this paper know now, that is not in the original publication?”I love this idea and hope it takes off — researchers, especially younger researchers (when many do some of their “most famous” work!), often write papers from a single perspective. As their research gains prominence, folks from other fields inevitably chime in with references to earlier related work or methods for better framing. This feels personal to me: before heading off to do my PhD, I submitted my first research paper and was informed a few weeks later that a nearly-identical problem had been solved by researchers in the USSR and at RAND in the US in the 1970s, available online in poorly-scanned PDFs — in both Russian and English!It turns out Michael Littman, a reinforcement learning luminary, had a similar experience. In the 1990s, he published seminal and extremely highly-cited work (pdf) introducing “Markov games” to the CS community, which helped spur thinking in multi-agent reinforcement learning. In his retrospective on that paper, he mentioned wishing he’d known about “stochastic games” — effectively the same idea, but introduced in the 1950s (!!), and with more theoretical rigor. Still, Mike states he is happy he wrote the paper — as is the community, because this pushed forward interest in multi-agent reinforcement learning, a topic of increasing importance today in application areas ranging from simple game playing to energy production and disaster relief operations. AI is such an interdisciplinary field that this type of cross-pollination is more a blessing than a curse; indeed, we’d all benefit from looking — and talking! — beyond traditional technology fields.Also on Friday, our co-founder Liz spoke on a panel at the Minding the Gap: Between Fairness and Ethics workshop, alongside a variety of research scientists and engineers from Google focusing on ethics, fairness, and AI. The workshop featured practitioners and, in general, participants from outside the traditional AI/ML communities, which led to in-depth conversations about the gulf between what “standard” ML-based definitions of fairness offer and what practitioners might want or at least find practical to use in their day-to-day jobs.While the AI/ML community (including myself) has started to address this divide, there’s clearly a ton to be done when it comes to properly incorporating the wants and needs of stakeholders into modern AI systems.I spent Saturday at the CausalML [b1] aka Do the Right Thing: Machine Learning and Causal Inference for Improved Decision Making workshop. Invited speakers such as Susan Athey and Susan Murphy connected techniques for uncertainty management from the AI/ML world with application areas in business, including labor markets and advertising, and healthcare. I found the poster sessions in this workshop particularly enlightening: causal and counterfactual reasoning are two intertwined topics that are forming the basis for both fair and robust (e.g., to adversarial manipulations, or to simple noise in data) automated systems, and I look forward to these concerns and ideas melting more into the greater ML community.Finally, what would a large AI/ML conference be without a little drama?Bengio and Schmidhuber, two deep learning visionaries from different continents, stay at loggerheads about who should give whom credit about what. I won’t dignify linking to the /r/machinelearning and Blind threads discussing this, but suffice it to say that the “who did it first in deep learning” rabbit hole is still getting, ah, deeper.All in all, NeurIPS this year captured the largely positive zeitgeist of the machine learning and AI community. After a period of extreme growth, the conference — and, by proxy, the community — feels like it’s growing, if not grown, up. Yet, NeurIPS’ workshops, events, and keynotes captured another, more subtle thread: as the impact of AI continues to spread — and the community continues to learn and understand how that spread impacts society — researchers and practitioners alike are desperate for tools and guidance about how to integrate models safely and responsibly. There’s so much left to be done regarding not just the scalability and generalizability of modern ML methods, but also the ability to define and incorporate notions of fairness, bias, trustworthiness, and accountability into models and pipelines in ways that are interpretable at both train/test time as well as in deployment. I’d love to see more research and development time spent on&$58;Crossing boundaries and discussing exactly what industry practitioners and other stakeholders want and need. The human-computer interaction (HCI) and AI/ML communities are starting to build this knowledge out, but what we need is a feedback loop between stakeholders and researchers formalizing the communication pipelines between both parties. This is the only way we will settle on meaningful definitions of what it means to approach “fairness” in different domainsTracking and analyzing the impact of implementing various objectives or constraints (e.g, promoting combinations of fairness, diversity, and economic efficiency) on truly dynamic systems, that is, systems where the input data distribution drifts over time, metrics change, and so on.Simply put, measuring things. A discussion we have constantly at Arthur revolves around which metrics we should (i) show by default, (ii) pre-compute and allow to be toggled on or off, (iii) don’t pre-compute but allow to be computed, and (iv) leave to the user to define and pass to our system as a custom metric. We need to help industry understand what it needs, then develop scalable methods to measure that, and then integrate those metrics and measurements in live systems to track model performance. That means not just the so-called efficiency metrics such as accuracy, precision, and f-score, but also measures for fairness, bias, diversity, explainability, and anything else stakeholders might want.That last point is a multidisciplinary one, and — frankly — there’s no way to escape that. As AI practitioners, we aim to build systems for users in myriad fields, and we build them so they (i) work and (ii) work in a way that users understand. The more progress the field makes toward understanding and then building scalable and general methods, the better. Those methods need to consider the competing and sometimes contradictory wants of stakeholders: efficiency, fairness, robustness, explainability, and even justice. It’s not a simple problem, and it’s not one that can be solved by the AI/ML community alone. I look forward to the work and discussion that will come from future AI/ML conferences — such as AIES and AAAI in NYC, Arthur’s home city, in early 2020![1] Or, in this case, attacks on BERT by friends. I’m a fan of bad jokes, and approve of the team from AI2 and UW building Grover, a fake news generator.[2] There’s an Oscar the Grouch joke in here somewhere.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy
=====================
