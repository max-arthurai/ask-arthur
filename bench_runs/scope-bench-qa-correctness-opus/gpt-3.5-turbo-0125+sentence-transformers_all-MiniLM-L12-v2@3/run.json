{"id": "7325edd1-07ca-40ea-8254-db7dc596b104", "name": "gpt-3.5-turbo-0125+sentence-transformers_all-MiniLM-L12-v2@3", "test_suite_id": "4a810683-5bd4-44aa-b42a-784620f41c0d", "test_cases": [{"id": "f879070d-a94a-44c2-9f67-ff60b802b749", "output": "Enrichments available in Arthur include Anomaly Detection, Bias Mitigation, Explainability, and Hot Spots.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "6fc74721-9cba-4844-8f15-f4469422d124", "output": "All model types allow explainability in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "b63703c2-2fad-47af-9aa3-00b595aeff22", "output": "The enrichments available for object detection models are:\n1. Anomaly Detection\n2. Hot Spots\n3. Explainability\n4. Bias Mitigation", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "f680f682-5b0f-4b7a-8e48-bccb6fd7ee51", "output": "Anomaly Detection, Hot Spots, Explainability, Bias Mitigation", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "d9f912c8-a449-4745-beec-26e562416d08", "output": "The two alert levels available in Arthur are streaming and batch models.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "a7abaee1-a0e3-4d8c-bf88-83b546521882", "output": "For on-prem deployment in airgapped mode, the additional requirements include downloading all artifacts required for installing the Arthur platform from a customer-specific password-protected portal accessible within the corporate network, ensuring the Kubernetes cluster has no internet access, and having access to a private/public container registry and Pypi/Conda registry for connectivity.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "e64b4d2b-3e58-4e39-9b23-a639f42f4cd2", "output": "To enable explainability for your model in Arthur, you need to provide access to your model's predict function. This allows Arthur to capture the inputs to the model and the predictions it makes.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "b1c20735-345d-4998-9198-1b64b34239e8", "output": "KMS is not explicitly mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "64c42b24-cb61-4870-9dfe-679466bf1344", "output": "You can use the exact_match scorer with optional configurations like case_sensitive=False for evaluating LLMs at writing summaries.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "22cd57d3-a9a9-4c66-bc14-2e82b0c50449", "output": "You can use the ExactMatch scorer from the Bench package to evaluate LLMs at answering questions. This scorer allows you to specify optional configurations such as case sensitivity.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "ffdb3450-ebb2-49bc-ab2c-c367cc89c4a0", "output": "You can use the \"exact_match\" scorer from Bench for evaluating LLMs at coding. This scorer has optional configurations like case sensitivity that can be adjusted to match your use case.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "a72ef6a5-a67c-47c9-8c66-eb4482504f53", "output": "To view your Bench test results, you can launch the Bench UI locally by setting the BENCH_FILE_DIR environment variable to point to the location of your test suite data and running the \"bench\" command from the command line. This will provide you with a URL for a local server that you can copy and paste into your browser to navigate the UI.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "0af305b2-29b4-476f-a3af-84ee88452b96", "output": "Test suites in Bench are the main touch point for creating and running tests, while test runs are the actual execution of those tests within a suite. Test suites define the data and parameters for the tests, while test runs execute those tests with specific inputs and outputs.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}], "updated_at": "2024-03-27T11:41:50.970341", "created_at": "2024-03-27T11:41:50.970341", "page": null, "page_size": null, "total_pages": null, "total_count": null}