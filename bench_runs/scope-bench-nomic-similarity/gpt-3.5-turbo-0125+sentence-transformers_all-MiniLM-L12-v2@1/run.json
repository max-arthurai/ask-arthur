{"id": "e6b6ecab-0eac-4fb5-974f-a7e827bf1d58", "name": "gpt-3.5-turbo-0125+sentence-transformers_all-MiniLM-L12-v2@1", "test_suite_id": "1c594684-5fd1-4bba-b209-d6c321bc01ac", "test_cases": [{"id": "6631fb8f-c16e-4b51-9bdb-e46dc35dbe1f", "output": "Enrichments available in Arthur include Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.952106773853302, "input": null, "reference_output": null, "score_result": {"score": 0.952106773853302, "category": null}}, {"id": "12245a16-8cc3-4cad-a0df-8d65b2ccdbb4", "output": "Black box models like deep neural networks and ensemble methods such as random forests and gradient boosting machines do not allow explainability due to their complex and non-linear nature.", "score": 0.7434698343276978, "input": null, "reference_output": null, "score_result": {"score": 0.7434698343276978, "category": null}}, {"id": "907e435b-f3d1-4201-bdf3-a514a8975b2d", "output": "Enrichments available for object detection models in Arthur include Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.8715211749076843, "input": null, "reference_output": null, "score_result": {"score": 0.8715211749076843, "category": null}}, {"id": "22d619b4-c43b-4d17-9dd5-1c3d9e81b44a", "output": "The enrichments available for tabular binary classification models are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 0.8009505271911621, "input": null, "reference_output": null, "score_result": {"score": 0.8009505271911621, "category": null}}, {"id": "c30d8faf-8e01-4e50-b93c-e633c9da7fd1", "output": "Yes, you can restore a deleted Arthur model by following the steps outlined in the platform documentation for backing up and restoring the Arthur platform.", "score": 0.8861050605773926, "input": null, "reference_output": null, "score_result": {"score": 0.8861050605773926, "category": null}}, {"id": "e9bc0fdf-b920-48eb-bce7-9d10b050a664", "output": "The two alert levels available in Arthur are:\n1. Critical Alerts\n2. Warning Alerts", "score": 0.9729884266853333, "input": null, "reference_output": null, "score_result": {"score": 0.9729884266853333, "category": null}}, {"id": "4040ca43-f1db-4111-9df4-124ef94eea08", "output": "For on-prem deployment in airgapped mode, additional requirements include having all required files downloaded and available locally, as well as potentially needing to whitelist specific URLs for efficient install and upgrade experiences.", "score": 0.8115866780281067, "input": null, "reference_output": null, "score_result": {"score": 0.8115866780281067, "category": null}}, {"id": "498e6ba4-c53d-405d-88bd-88b27aa1106f", "output": "To enable explainability for your model in Arthur, you will need to provide access to your model's predict function.", "score": 0.8831324577331543, "input": null, "reference_output": null, "score_result": {"score": 0.8831324577331543, "category": null}}, {"id": "3c216b5e-f6ea-41c6-933a-a20d4bf59e08", "output": "No, KMS (Key Management Service) is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 0.7521085739135742, "input": null, "reference_output": null, "score_result": {"score": 0.7521085739135742, "category": null}}, {"id": "9e8c3e89-6a0c-4ae3-85d6-7a141c9bdcfe", "output": "You can use Bench scorers such as BLEU, ROUGE, and METEOR for evaluating LLMs at writing summaries.", "score": 0.8523798584938049, "input": null, "reference_output": null, "score_result": {"score": 0.8523798584938049, "category": null}}, {"id": "1925c2f0-86e8-4e5c-8c5c-572775db7beb", "output": "You can use Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.", "score": 0.682470440864563, "input": null, "reference_output": null, "score_result": {"score": 0.682470440864563, "category": null}}, {"id": "93e6a397-63d3-4678-bb58-42e192ed38c5", "output": "You can use the ExactMatch scorer from the Bench package for evaluating LLMs at coding.", "score": 0.8075540065765381, "input": null, "reference_output": null, "score_result": {"score": 0.8075540065765381, "category": null}}, {"id": "67293353-3c84-4967-810d-ee179a1ae002", "output": "To view your Bench test results, you can launch the Bench UI locally by setting the BENCH_FILE_DIR environment variable to point to the location of your test runs and running the 'bench' command from the command line.", "score": 0.9390919208526611, "input": null, "reference_output": null, "score_result": {"score": 0.9390919208526611, "category": null}}, {"id": "bb8aa841-074e-4b8f-aa62-ea33d91280bd", "output": "Test suites in Bench are used to define the data and parameters for testing, while test runs actually execute the tests using the specified test suite configurations.", "score": 0.7527045011520386, "input": null, "reference_output": null, "score_result": {"score": 0.7527045011520386, "category": null}}], "updated_at": "2024-03-26T16:07:20.083640", "created_at": "2024-03-26T16:07:20.083640", "page": null, "page_size": null, "total_pages": null, "total_count": null}