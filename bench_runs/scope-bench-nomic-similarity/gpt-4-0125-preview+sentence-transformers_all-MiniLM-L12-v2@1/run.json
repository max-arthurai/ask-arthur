{"id": "82c4c1c4-ec20-4ed8-9400-d22f4f26d75d", "name": "gpt-4-0125-preview+sentence-transformers_all-MiniLM-L12-v2@1", "test_suite_id": "1c594684-5fd1-4bba-b209-d6c321bc01ac", "test_cases": [{"id": "6631fb8f-c16e-4b51-9bdb-e46dc35dbe1f", "output": "The available enrichments in Arthur are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.9530126452445984, "input": null, "reference_output": null, "score_result": {"score": 0.9530126452445984, "category": null}}, {"id": "12245a16-8cc3-4cad-a0df-8d65b2ccdbb4", "output": "The document does not provide information on model types that do not allow explainability.", "score": 0.8206563591957092, "input": null, "reference_output": null, "score_result": {"score": 0.8206563591957092, "category": null}}, {"id": "907e435b-f3d1-4201-bdf3-a514a8975b2d", "output": "The document does not specify the enrichments available for object detection models.", "score": 0.8057407140731812, "input": null, "reference_output": null, "score_result": {"score": 0.8057407140731812, "category": null}}, {"id": "22d619b4-c43b-4d17-9dd5-1c3d9e81b44a", "output": "The available enrichments for tabular binary classification models are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 0.7985771894454956, "input": null, "reference_output": null, "score_result": {"score": 0.7985771894454956, "category": null}}, {"id": "c30d8faf-8e01-4e50-b93c-e633c9da7fd1", "output": "The provided context does not contain information about restoring deleted Arthur models.", "score": 0.8137456178665161, "input": null, "reference_output": null, "score_result": {"score": 0.8137456178665161, "category": null}}, {"id": "e9bc0fdf-b920-48eb-bce7-9d10b050a664", "output": "The document does not provide information on specific alert levels available in Arthur.", "score": 0.8419811725616455, "input": null, "reference_output": null, "score_result": {"score": 0.8419811725616455, "category": null}}, {"id": "4040ca43-f1db-4111-9df4-124ef94eea08", "output": "For an airgap Kubernetes cluster (K8s) install, all required files must be downloaded and available locally since there won't be Internet access.", "score": 0.6671960353851318, "input": null, "reference_output": null, "score_result": {"score": 0.6671960353851318, "category": null}}, {"id": "498e6ba4-c53d-405d-88bd-88b27aa1106f", "output": "To enable explainability, you will need to provide access to your model's `predict` function.", "score": 0.788661003112793, "input": null, "reference_output": null, "score_result": {"score": 0.788661003112793, "category": null}}, {"id": "3c216b5e-f6ea-41c6-933a-a20d4bf59e08", "output": "No, KMS is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 0.763473629951477, "input": null, "reference_output": null, "score_result": {"score": 0.763473629951477, "category": null}}, {"id": "9e8c3e89-6a0c-4ae3-85d6-7a141c9bdcfe", "output": "The document does not mention any specific Bench scorers for evaluating LLMs at writing summaries.", "score": 0.8304141163825989, "input": null, "reference_output": null, "score_result": {"score": 0.8304141163825989, "category": null}}, {"id": "1925c2f0-86e8-4e5c-8c5c-572775db7beb", "output": "You can use Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models, which includes a scoring workflow for manually evaluating the LLM responses in datasets.", "score": 0.7345701456069946, "input": null, "reference_output": null, "score_result": {"score": 0.7345701456069946, "category": null}}, {"id": "93e6a397-63d3-4678-bb58-42e192ed38c5", "output": "The document does not list specific Bench scorers for evaluating LLMs at coding.", "score": 0.7392427921295166, "input": null, "reference_output": null, "score_result": {"score": 0.7392427921295166, "category": null}}, {"id": "67293353-3c84-4967-810d-ee179a1ae002", "output": "To view your Bench test results locally, set the `BENCH_FILE_DIR` environment variable to point to the location of your new test runs and run `bench` from the command line.", "score": 0.9200448393821716, "input": null, "reference_output": null, "score_result": {"score": 0.9200448393821716, "category": null}}, {"id": "bb8aa841-074e-4b8f-aa62-ea33d91280bd", "output": "Test suites are created to define the data and evaluation metrics for testing, while test runs execute these test suites against specific model outputs to generate performance comparisons.", "score": 0.8402427434921265, "input": null, "reference_output": null, "score_result": {"score": 0.8402427434921265, "category": null}}], "updated_at": "2024-03-26T16:07:26.306687", "created_at": "2024-03-26T16:07:26.306687", "page": null, "page_size": null, "total_pages": null, "total_count": null}