{"id": "1c594684-5fd1-4bba-b209-d6c321bc01ac", "name": "scope-bench-nomic-similarity", "scoring_method": {"name": "embedding_similarity", "type": "custom", "config": {}, "output_type": "continuous", "categories": null}, "test_cases": [{"id": "6631fb8f-c16e-4b51-9bdb-e46dc35dbe1f", "input": "What enrichments are available in Arthur?", "reference_output": "The enrichments available in Arthur are:\\n\\n- anomaly detection\\n- hotspots\\n- explainability\\n- bias mitigation"}, {"id": "12245a16-8cc3-4cad-a0df-8d65b2ccdbb4", "input": "What model types do not allow explainability?", "reference_output": "The model types that do not allow for explainability are Object detection & token sequence (LLM) models."}, {"id": "907e435b-f3d1-4201-bdf3-a514a8975b2d", "input": "What enrichments are available for object detection models?", "reference_output": "The enrichments available for object detection models are:\\n\\n- anomaly detection"}, {"id": "22d619b4-c43b-4d17-9dd5-1c3d9e81b44a", "input": "What enrichments are available for tabular binary classification models?", "reference_output": "The enrichments available for object detection models are:\\n\\n- anomaly detection\\n- hotspots\\n- explainability\\n- bias mitigation"}, {"id": "c30d8faf-8e01-4e50-b93c-e633c9da7fd1", "input": "Can I restore a deleted Arthur model? ", "reference_output": "If you delete an Arthur model you cannot restore it. You can re-register a new model with the same schema, but the history of your model\u2019s inferences are deleted and cannot be restored."}, {"id": "e9bc0fdf-b920-48eb-bce7-9d10b050a664", "input": "What are the two alert levels available in Arthur? ", "reference_output": "The two levels of alerts available in Arthur are Warning and Critical."}, {"id": "4040ca43-f1db-4111-9df4-124ef94eea08", "input": "What are the additional requirements for on-prem deployment specific to the airgapped mode?", "reference_output": "The requirements for an airgapped on-prem deployment, in addition to the online on-prem deployment requirements, are:\\n\\n- An existing private container registry\\n- Existing private Python registries (PyPI, Anaconda) when the model explanation feature is enabled\\n- Access to your private container and Python registries"}, {"id": "498e6ba4-c53d-405d-88bd-88b27aa1106f", "input": "What artifacts do I need to enable explainability for my model? ", "reference_output": "To enable explainability for your Arthur model you will need:\\n\\n- A python script that wraps your models predict function\\n- your serialized model file\\n- A requirements.txt with the dependencies to support the above"}, {"id": "3c216b5e-f6ea-41c6-933a-a20d4bf59e08", "input": "Is KMS required for Arthur's backup and restore capability?", "reference_output": "No, but Arthur highly recommends that your EBS volumes are encrypted with KMS."}, {"id": "9e8c3e89-6a0c-4ae3-85d6-7a141c9bdcfe", "input": "What Bench scorers can I use for evaluating LLMs at writing summaries?", "reference_output": "The Bench scorers that can evaluate LLMs at writing summaries are:\\n\\n- Summary Quality (summary_quality)\\n- BERT Score (bert_score)"}, {"id": "1925c2f0-86e8-4e5c-8c5c-572775db7beb", "input": "What Bench scorers can I use for evaluating LLMs at answering questions?", "reference_output": "The Bench scorers that can evaluate LLMs at answering questions are:\\n\\n- Exact Match (exact_match)\\n- QA Correctness (qa_correctness)\\n- BERT Score (bert_score)"}, {"id": "93e6a397-63d3-4678-bb58-42e192ed38c5", "input": "What Bench scorers can I use for evaluating LLMs at coding?", "reference_output": "The Bench scorers that can evaluate LLMs at coding are:\\n\\n- Python Unit Testing (python_unit_testing)"}, {"id": "67293353-3c84-4967-810d-ee179a1ae002", "input": "How do I view my Bench test results?", "reference_output": "To view your Bench test results in the local UI, run the command `bench` from the same local directory as your bench file directory"}, {"id": "bb8aa841-074e-4b8f-aa62-ea33d91280bd", "input": "What is the difference between Bench test suites and test runs?", "reference_output": "A test suite stores the input & reference output data for your testing use case along with a scorer. A test run evaluates the candidate outputs provided in the run and assigns a score to each test case."}], "created_at": "2024-03-26T16:07:18.208518", "updated_at": "2024-03-26T16:07:18.208518", "description": null, "last_run_time": "2024-03-26T16:07:47.542967", "num_runs": 32, "page": null, "page_size": null, "total_pages": null, "total_count": null}