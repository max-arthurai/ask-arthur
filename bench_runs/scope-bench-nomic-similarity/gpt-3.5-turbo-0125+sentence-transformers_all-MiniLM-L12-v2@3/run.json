{"id": "453dd00c-ec0a-4808-8d12-fe344d93e45e", "name": "gpt-3.5-turbo-0125+sentence-transformers_all-MiniLM-L12-v2@3", "test_suite_id": "1c594684-5fd1-4bba-b209-d6c321bc01ac", "test_cases": [{"id": "6631fb8f-c16e-4b51-9bdb-e46dc35dbe1f", "output": "Enrichments available in Arthur include Anomaly Detection, Bias Mitigation, Explainability, and Hot Spots.", "score": 0.9624549746513367, "input": null, "reference_output": null, "score_result": {"score": 0.9624549746513367, "category": null}}, {"id": "12245a16-8cc3-4cad-a0df-8d65b2ccdbb4", "output": "All model types allow explainability in Arthur.", "score": 0.757067084312439, "input": null, "reference_output": null, "score_result": {"score": 0.757067084312439, "category": null}}, {"id": "907e435b-f3d1-4201-bdf3-a514a8975b2d", "output": "The enrichments available for object detection models are:\n1. Anomaly Detection\n2. Hot Spots\n3. Explainability\n4. Bias Mitigation", "score": 0.9399377703666687, "input": null, "reference_output": null, "score_result": {"score": 0.9399377703666687, "category": null}}, {"id": "22d619b4-c43b-4d17-9dd5-1c3d9e81b44a", "output": "Anomaly Detection, Hot Spots, Explainability, Bias Mitigation", "score": 0.8033602833747864, "input": null, "reference_output": null, "score_result": {"score": 0.8033602833747864, "category": null}}, {"id": "c30d8faf-8e01-4e50-b93c-e633c9da7fd1", "output": "Yes, you can restore a deleted Arthur model by following the steps to restore the Arthur platform components from existing backups. This involves restoring the database, installing the Arthur platform, restoring ClickHouse data, messaging infrastructure, enrichments, and running smoke tests for validation.", "score": 0.8842507600784302, "input": null, "reference_output": null, "score_result": {"score": 0.8842507600784302, "category": null}}, {"id": "e9bc0fdf-b920-48eb-bce7-9d10b050a664", "output": "The two alert levels available in Arthur are streaming and batch models.", "score": 0.8724740743637085, "input": null, "reference_output": null, "score_result": {"score": 0.8724740743637085, "category": null}}, {"id": "4040ca43-f1db-4111-9df4-124ef94eea08", "output": "For on-prem deployment in airgapped mode, the additional requirements include downloading all artifacts required for installing the Arthur platform from a customer-specific password-protected portal accessible within the corporate network, ensuring the Kubernetes cluster has no internet access, and having access to a private/public container registry and Pypi/Conda registry for connectivity.", "score": 0.7976635694503784, "input": null, "reference_output": null, "score_result": {"score": 0.7976635694503784, "category": null}}, {"id": "498e6ba4-c53d-405d-88bd-88b27aa1106f", "output": "To enable explainability for your model in Arthur, you need to provide access to your model's predict function. This allows Arthur to capture the inputs to the model and the predictions it makes.", "score": 0.8730559349060059, "input": null, "reference_output": null, "score_result": {"score": 0.8730559349060059, "category": null}}, {"id": "3c216b5e-f6ea-41c6-933a-a20d4bf59e08", "output": "KMS is not explicitly mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 0.7294731140136719, "input": null, "reference_output": null, "score_result": {"score": 0.7294731140136719, "category": null}}, {"id": "9e8c3e89-6a0c-4ae3-85d6-7a141c9bdcfe", "output": "You can use the exact_match scorer with optional configurations like case_sensitive=False for evaluating LLMs at writing summaries.", "score": 0.7361025810241699, "input": null, "reference_output": null, "score_result": {"score": 0.7361025810241699, "category": null}}, {"id": "1925c2f0-86e8-4e5c-8c5c-572775db7beb", "output": "You can use the ExactMatch scorer from the Bench package to evaluate LLMs at answering questions. This scorer allows you to specify optional configurations such as case sensitivity.", "score": 0.8790748119354248, "input": null, "reference_output": null, "score_result": {"score": 0.8790748119354248, "category": null}}, {"id": "93e6a397-63d3-4678-bb58-42e192ed38c5", "output": "You can use the \"exact_match\" scorer from Bench for evaluating LLMs at coding. This scorer has optional configurations like case sensitivity that can be adjusted to match your use case.", "score": 0.7864477634429932, "input": null, "reference_output": null, "score_result": {"score": 0.7864477634429932, "category": null}}, {"id": "67293353-3c84-4967-810d-ee179a1ae002", "output": "To view your Bench test results, you can launch the Bench UI locally by setting the BENCH_FILE_DIR environment variable to point to the location of your test suite data and running the \"bench\" command from the command line. This will provide you with a URL for a local server that you can copy and paste into your browser to navigate the UI.", "score": 0.9323213696479797, "input": null, "reference_output": null, "score_result": {"score": 0.9323213696479797, "category": null}}, {"id": "bb8aa841-074e-4b8f-aa62-ea33d91280bd", "output": "Test suites in Bench are the main touch point for creating and running tests, while test runs are the actual execution of those tests within a suite. Test suites define the data and parameters for the tests, while test runs execute those tests with specific inputs and outputs.", "score": 0.7769424319267273, "input": null, "reference_output": null, "score_result": {"score": 0.7769424319267273, "category": null}}], "updated_at": "2024-03-26T16:07:21.195868", "created_at": "2024-03-26T16:07:21.195868", "page": null, "page_size": null, "total_pages": null, "total_count": null}