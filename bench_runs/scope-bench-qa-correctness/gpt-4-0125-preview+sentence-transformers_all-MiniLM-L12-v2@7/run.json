{"id": "ce65e0cd-c37f-448d-af87-2ec4bedd61ec", "name": "gpt-4-0125-preview+sentence-transformers_all-MiniLM-L12-v2@7", "test_suite_id": "5d0d8642-7bbf-4d7d-a744-cfdee7647a3a", "test_cases": [{"id": "9302872f-61dc-4092-add4-0b928fc05679", "output": "The enrichments available in Arthur are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "1a3cfeeb-c6a0-41de-9444-c609ec78bc71", "output": "The document does not provide information on model types that do not allow explainability.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "2b285103-b671-43c0-8485-3253fadcc23e", "output": "The available enrichments for object detection models are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "640965fd-e43b-4794-9719-de37084d9221", "output": "The available enrichments for tabular binary classification models are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "16e9a74b-faca-45ac-aea5-5fbf28ea0c85", "output": "The document does not mention specific alert levels available in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "45550df3-ab06-4bde-b929-4e84c7796b80", "output": "For an airgap Kubernetes cluster (K8s) installation, the additional requirements specific to the airgapped mode include having all the required files downloaded and available locally, as the installation can proceed without Internet access.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "c8c0a65e-7374-4abe-9d68-458a7a0d48c0", "output": "To enable explainability, you will need to provide access to your model's `predict` function.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "15379cb5-bba6-43d1-8b2f-80b2a00d0227", "output": "No, KMS is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "242d60dd-516d-4724-b515-fc5acd86f83c", "output": "The document does not mention any specific Bench scorers for evaluating LLMs at writing summaries.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "bde29bd5-7fc5-4b40-811f-f03f6a8eeaca", "output": "You can use Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "7c3bdff6-414e-49b6-ac52-ad6a7d168f13", "output": "The document does not specify which Bench scorers can be used for evaluating LLMs at coding.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "8d479d1b-5ed7-4926-8d58-5282a8248eee", "output": "To view your Bench test results locally, set the `BENCH_FILE_DIR` environment variable to point to the location of your new test runs and run `bench` from the command line. This will launch a local UI where you can navigate and view your test results.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "a88c1ebb-490c-4669-91c3-ca56bc83861e", "output": "Test suites are created to define the data and evaluation metrics for testing, while test runs execute these test suites against specific model outputs to assess performance.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}], "updated_at": "2024-03-27T11:31:45.899856", "created_at": "2024-03-27T11:31:45.899856", "page": null, "page_size": null, "total_pages": null, "total_count": null}