{"id": "3dfeabe9-ced8-4627-a812-f96cba40cbfd", "name": "gpt-3.5-turbo-0125+nomic-ai_nomic-embed-text-v1.5@7", "test_suite_id": "5d0d8642-7bbf-4d7d-a744-cfdee7647a3a", "test_cases": [{"id": "9302872f-61dc-4092-add4-0b928fc05679", "output": "Enrichments available in Arthur include Anomaly Detection, Bias Mitigation, and Explainability.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "1a3cfeeb-c6a0-41de-9444-c609ec78bc71", "output": "Black box models like deep neural networks and ensemble methods such as random forests and gradient boosting machines do not allow explainability due to their complex and non-linear nature.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "2b285103-b671-43c0-8485-3253fadcc23e", "output": "Enrichments available for object detection models include Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "640965fd-e43b-4794-9719-de37084d9221", "output": "Anomaly Detection, Hot Spots, Explainability, Bias Mitigation", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "16e9a74b-faca-45ac-aea5-5fbf28ea0c85", "output": "The two alert levels available in Arthur are Critical Alerts and Warning Alerts.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "45550df3-ab06-4bde-b929-4e84c7796b80", "output": "For on-prem deployment in airgapped mode, additional requirements include having all required files downloaded and available locally, as well as potentially needing to whitelist specific URLs for efficient install and upgrade experiences.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "c8c0a65e-7374-4abe-9d68-458a7a0d48c0", "output": "To enable explainability for your model in Arthur, you will need to provide access to your model's predict function.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "15379cb5-bba6-43d1-8b2f-80b2a00d0227", "output": "No, KMS (Key Management Service) is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "242d60dd-516d-4724-b515-fc5acd86f83c", "output": "You can use Bench scorers such as BLEU, ROUGE, and METEOR for evaluating LLMs at writing summaries.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "bde29bd5-7fc5-4b40-811f-f03f6a8eeaca", "output": "You can use Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "7c3bdff6-414e-49b6-ac52-ad6a7d168f13", "output": "You can use the ExactMatch scorer from the Bench package for evaluating LLMs at coding. This scorer allows you to configure options such as case sensitivity to match scorers with your specific use case.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "8d479d1b-5ed7-4926-8d58-5282a8248eee", "output": "To view your Bench test results, you can run the bench UI locally by setting the BENCH_FILE_DIR environment variable to point to the location of your test runs and then running the 'bench' command from the command line. This will launch a local server with a URL that you can copy and paste into your browser to navigate the UI.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "a88c1ebb-490c-4669-91c3-ca56bc83861e", "output": "Test suites in Bench are the main touch point for creating and running tests, while test runs are the actual execution of those tests within the test suite. Test suites define the data and parameters for the tests, while test runs execute those tests with specific inputs and outputs.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}], "updated_at": "2024-03-27T11:31:00.126180", "created_at": "2024-03-27T11:31:00.126180", "page": null, "page_size": null, "total_pages": null, "total_count": null}