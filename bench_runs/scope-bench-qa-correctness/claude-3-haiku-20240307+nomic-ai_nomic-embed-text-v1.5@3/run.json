{"id": "1712d7f9-3be1-482f-b925-b1c7389c5efc", "name": "claude-3-haiku-20240307+nomic-ai_nomic-embed-text-v1.5@3", "test_suite_id": "5d0d8642-7bbf-4d7d-a744-cfdee7647a3a", "test_cases": [{"id": "9302872f-61dc-4092-add4-0b928fc05679", "output": "Based on the provided context, the following enrichments are available in Arthur:\n\n1. Anomaly Detection: Arthur can detect anomalies in your model's performance and data.\n2. Bias Mitigation: Arthur can help mitigate biases in your model's predictions.\n3. Explainability: Arthur can provide explanations for your model's predictions.\n\nThe context does not provide detailed information about other enrichments that may be available in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "1a3cfeeb-c6a0-41de-9444-c609ec78bc71", "output": "Based on the context provided, the document does not mention any specific model types that do not allow explainability. The focus of the document is on model schemas within the MLOps ecosystem, and it does not discuss the explainability of different model types. The context does not contain information to answer the question \"What model types do not allow explainability?\".", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}, {"id": "2b285103-b671-43c0-8485-3253fadcc23e", "output": "Based on the provided context, the Arthur platform supports the following enrichments for object detection models:\n\n1. Anomaly Detection: Arthur can detect anomalies in the object detection outputs, such as unexpected objects or changes in the detected objects over time.\n2. Bias Mitigation: Arthur can help identify and mitigate biases in the object detection model, such as demographic biases in the detected objects.\n3. Explainability: Arthur can provide explanations for the object detection model's predictions, helping users understand the model's decision-making process.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "640965fd-e43b-4794-9719-de37084d9221", "output": "Based on the provided context, the following enrichments are available for tabular multiclass classification models in Arthur:\n\n- Anomaly Detection\n- Hot Spots \n- Explainability\n- Bias Mitigation\n\nThe context mentions that these enrichments can be enabled for multiclass classification model types in Arthur.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "16e9a74b-faca-45ac-aea5-5fbf28ea0c85", "output": "Based on the provided context, the Arthur Scope FAQ does not mention anything about alert levels. The FAQ covers topics such as using Arthur without the Python SDK, model access, data security, ground truth labels, SDK errors, credentials, and data drift metrics, but does not discuss alert levels. Therefore, I cannot provide a brief answer to the question \"What are the two alert levels available in Arthur?\".", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "45550df3-ab06-4bde-b929-4e84c7796b80", "output": "Based on the provided context, the additional requirements for on-prem deployment in airgapped mode are:\n\n1. The Kubernetes cluster must have access to a container registry and a PyPI/Conda registry, as the Arthur platform requires connectivity to these registries.\n2. All required artifacts for installing the Arthur platform must be downloaded and made available locally, as the cluster will not have internet access.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "c8c0a65e-7374-4abe-9d68-458a7a0d48c0", "output": "To enable explainability for your model in Arthur, you will need to provide access to your model's `predict()` function. Specifically, you need to provide the following assets:\n\n1. The model's `predict()` function - This allows Arthur to access the model and generate explanations for its predictions.\n2. Any necessary dependencies or libraries required by the model's `predict()` function.\n\nWith these assets, Arthur can integrate with your model and provide explainability features, such as generating feature importance explanations for the model's predictions.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "15379cb5-bba6-43d1-8b2f-80b2a00d0227", "output": "Based on the provided context, there is no mention of KMS (Key Management Service) being required for Arthur's backup and restore capability. The document focuses on the steps to restore various components of the Arthur platform, such as the RDS Postgres database, ClickHouse data, messaging infrastructure, and enrichments, without any specific references to KMS.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "242d60dd-516d-4724-b515-fc5acd86f83c", "output": "Unfortunately, the provided context does not contain any information about benchmarking scorers for evaluating LLMs at writing summaries. The context is focused on explaining the concept of token likelihood and how it can be used to monitor the performance of LLM models in the Arthur platform. There is no mention of specific benchmarking tools or metrics for evaluating LLM summarization capabilities.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "bde29bd5-7fc5-4b40-811f-f03f6a8eeaca", "output": "Based on the context provided, the key information is:\n\nThe article mentions that the procedure used to manually evaluate the LLM responses in the dataset will be included as a scoring workflow in Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.\n\nTherefore, the Bench scorer that can be used for evaluating LLMs at answering questions is the scoring workflow that was used in the manual evaluation described in the article.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "7c3bdff6-414e-49b6-ac52-ad6a7d168f13", "output": "The Bench documentation mentions that you can use the `ExactMatch` scorer to evaluate LLM outputs, and you can configure it to be case-insensitive. Additionally, the documentation states that Bench supports creating custom scorers, which can provide even more flexibility for evaluating LLM performance on coding tasks. To learn more about creating custom scorers, you can refer to the \"Custom Scoring\" guide in the Bench documentation.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "8d479d1b-5ed7-4926-8d58-5282a8248eee", "output": "To view your Bench test results, you have two options:\n\n1. Local setup:\n   - Ensure the `BENCH_FILE_DIR` environment variable is set to the location of your test run data.\n   - Run the `bench` command from the command line to launch the local Bench UI.\n   - The local UI will provide a visual interface to view and explore your test results.\n\n2. SaaS (Coming Soon):\n   - Set the environment variables `ARTHUR_BENCH_AUTOLOG`, `ARTHUR_API_URL`, and `ARTHUR_API_KEY` to connect to the Arthur platform.\n   - Your test results will be automatically logged to the Arthur platform, which will provide a web-based interface to view and analyze your test data.\n\nThe documentation provides examples for both the local and SaaS setup options to help you get started with viewing your Bench test results.", "score": 0.0, "input": null, "reference_output": null, "score_result": {"score": 0.0, "category": {"name": "incorrect", "description": "model output is incorrect given the context"}}}, {"id": "a88c1ebb-490c-4669-91c3-ca56bc83861e", "output": "The main difference between Bench test suites and test runs is:\n\n1. Test Suites: A test suite is a collection of test cases that are designed to evaluate the performance and behavior of an AI model or application. It defines the input data, reference outputs, and the scoring metrics to be used.\n\n2. Test Runs: A test run is the execution of a test suite against a specific candidate model or application. It generates the candidate outputs and compares them to the reference outputs using the specified scoring metrics.\n\nIn summary, a test suite defines the test cases and evaluation criteria, while a test run executes the test suite against a candidate model and produces the performance results. The test suite is the blueprint, and the test run is the actual assessment of the model's performance.", "score": 1.0, "input": null, "reference_output": null, "score_result": {"score": 1.0, "category": {"name": "correct", "description": "model output is correct given the context"}}}], "updated_at": "2024-03-27T11:33:29.222161", "created_at": "2024-03-27T11:33:29.222161", "page": null, "page_size": null, "total_pages": null, "total_count": null}