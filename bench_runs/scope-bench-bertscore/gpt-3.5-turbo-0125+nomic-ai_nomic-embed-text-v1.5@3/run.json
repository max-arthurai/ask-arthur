{"id": "b73017bb-bc95-44ed-9795-8446d45535d7", "name": "gpt-3.5-turbo-0125+nomic-ai_nomic-embed-text-v1.5@3", "test_suite_id": "4feac22d-31c1-42a0-b252-1adca2000ebf", "test_cases": [{"id": "dfdd97cc-9df7-4d98-a306-0f48214d7d27", "output": "Enrichments available in Arthur include Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.5202502012252808, "input": null, "reference_output": null, "score_result": {"score": 0.5202502012252808, "category": null}}, {"id": "51d72f8a-c329-469e-a00d-7ab12ec66087", "output": "Black box models like deep neural networks and ensemble methods such as random forests and gradient boosting machines do not allow explainability due to their complex and non-linear nature.", "score": 0.620424747467041, "input": null, "reference_output": null, "score_result": {"score": 0.620424747467041, "category": null}}, {"id": "5aba4ddc-2962-412b-9dd0-3f14954786a2", "output": "Enrichments available for object detection models include Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.6232226490974426, "input": null, "reference_output": null, "score_result": {"score": 0.6232226490974426, "category": null}}, {"id": "c5142157-54c1-4f56-a2e5-21202a2e20fb", "output": "For tabular binary classification models, the available enrichments are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 0.5591233372688293, "input": null, "reference_output": null, "score_result": {"score": 0.5591233372688293, "category": null}}, {"id": "7fa1360f-9632-44d7-8868-6d5b45d1f1d1", "output": "Unfortunately, once a model is deleted in Arthur, it cannot be restored. It is important to make sure you have backups or copies of your models to prevent data loss.", "score": 0.6226241588592529, "input": null, "reference_output": null, "score_result": {"score": 0.6226241588592529, "category": null}}, {"id": "a3688edd-c062-4d5d-8542-c526075e8785", "output": "The two alert levels available in Arthur are Critical Alerts and Warning Alerts.", "score": 0.8985893726348877, "input": null, "reference_output": null, "score_result": {"score": 0.8985893726348877, "category": null}}, {"id": "0c2c9c1a-7769-4c81-b695-7388ddcbbf4e", "output": "For on-prem deployment in airgapped mode, additional requirements include having all required files downloaded and available locally, as well as potentially needing a private container registry and PyPI/Conda registry accessible to the Kubernetes cluster.", "score": 0.572711169719696, "input": null, "reference_output": null, "score_result": {"score": 0.572711169719696, "category": null}}, {"id": "c265c857-9311-49d2-9b89-24603aaab946", "output": "To enable explainability for your model, you will need to provide access to your model's predict function. This means you need to have the necessary assets required for explainability, as outlined in the Arthur documentation.", "score": 0.5354589223861694, "input": null, "reference_output": null, "score_result": {"score": 0.5354589223861694, "category": null}}, {"id": "9d51a3ac-a7cb-4b13-92f6-a137c59d409a", "output": "No, KMS (Key Management Service) is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 0.5488037467002869, "input": null, "reference_output": null, "score_result": {"score": 0.5488037467002869, "category": null}}, {"id": "b32f4653-bad2-4180-8afc-b04bc0b53323", "output": "You can use Bench scorers such as BLEU, ROUGE, and METEOR for evaluating LLMs at writing summaries.", "score": 0.5020933151245117, "input": null, "reference_output": null, "score_result": {"score": 0.5020933151245117, "category": null}}, {"id": "5e742e10-1dcd-43b0-9d5a-b0227a8bfc3d", "output": "You can use the Arthur Bench scoring workflow, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.", "score": 0.3916974663734436, "input": null, "reference_output": null, "score_result": {"score": 0.3916974663734436, "category": null}}, {"id": "0ba7dcb4-e439-418f-b09d-24a120339ddc", "output": "You can use the ExactMatch scorer from the Bench package for evaluating LLMs at coding.", "score": 0.49024006724357605, "input": null, "reference_output": null, "score_result": {"score": 0.49024006724357605, "category": null}}, {"id": "05adad2b-eecc-422e-8877-1868229f1565", "output": "To view your Bench test results, you can run the command \"bench\" from the command line after setting the BENCH_FILE_DIR environment variable to point to the location of your test runs. This will launch a local UI that you can access through a URL in your browser.", "score": 0.7652438879013062, "input": null, "reference_output": null, "score_result": {"score": 0.7652438879013062, "category": null}}, {"id": "e0af19dd-7279-4fbe-8a91-e87d36c31a69", "output": "Test suites in Bench are the main touch point for creating and running tests, while test runs are the actual execution of those tests within the test suite. Test suites define the data and parameters for the tests, while test runs involve running those tests with specific inputs and outputs.", "score": 0.5910260677337646, "input": null, "reference_output": null, "score_result": {"score": 0.5910260677337646, "category": null}}], "updated_at": "2024-03-26T15:31:40.116297", "created_at": "2024-03-26T15:31:40.116297", "page": null, "page_size": null, "total_pages": null, "total_count": null}