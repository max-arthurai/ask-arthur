{"id": "2d2fa4a4-42e7-48d5-93ef-256855276e41", "name": "gpt-4-0125-preview+sentence-transformers_all-MiniLM-L12-v2@1", "test_suite_id": "4feac22d-31c1-42a0-b252-1adca2000ebf", "test_cases": [{"id": "dfdd97cc-9df7-4d98-a306-0f48214d7d27", "output": "The available enrichments in Arthur are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.5311673283576965, "input": null, "reference_output": null, "score_result": {"score": 0.5311673283576965, "category": null}}, {"id": "51d72f8a-c329-469e-a00d-7ab12ec66087", "output": "The document does not provide information on model types that do not allow explainability.", "score": 0.6466416716575623, "input": null, "reference_output": null, "score_result": {"score": 0.6466416716575623, "category": null}}, {"id": "5aba4ddc-2962-412b-9dd0-3f14954786a2", "output": "The document does not specify the enrichments available for object detection models.", "score": 0.5988750457763672, "input": null, "reference_output": null, "score_result": {"score": 0.5988750457763672, "category": null}}, {"id": "c5142157-54c1-4f56-a2e5-21202a2e20fb", "output": "The available enrichments for tabular binary classification models are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 0.5702216029167175, "input": null, "reference_output": null, "score_result": {"score": 0.5702216029167175, "category": null}}, {"id": "7fa1360f-9632-44d7-8868-6d5b45d1f1d1", "output": "The provided context does not contain information about restoring deleted Arthur models.", "score": 0.47069570422172546, "input": null, "reference_output": null, "score_result": {"score": 0.47069570422172546, "category": null}}, {"id": "a3688edd-c062-4d5d-8542-c526075e8785", "output": "The document does not provide information on specific alert levels available in Arthur.", "score": 0.5787457823753357, "input": null, "reference_output": null, "score_result": {"score": 0.5787457823753357, "category": null}}, {"id": "0c2c9c1a-7769-4c81-b695-7388ddcbbf4e", "output": "For an airgap Kubernetes cluster (K8s) install, all required files must be downloaded and available locally since there won't be Internet access.", "score": 0.4394932687282562, "input": null, "reference_output": null, "score_result": {"score": 0.4394932687282562, "category": null}}, {"id": "c265c857-9311-49d2-9b89-24603aaab946", "output": "To enable explainability, you will need to provide access to your model's `predict` function.", "score": 0.48940151929855347, "input": null, "reference_output": null, "score_result": {"score": 0.48940151929855347, "category": null}}, {"id": "9d51a3ac-a7cb-4b13-92f6-a137c59d409a", "output": "No, KMS is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 0.5428208112716675, "input": null, "reference_output": null, "score_result": {"score": 0.5428208112716675, "category": null}}, {"id": "b32f4653-bad2-4180-8afc-b04bc0b53323", "output": "The document does not mention any specific Bench scorers for evaluating LLMs at writing summaries.", "score": 0.46995997428894043, "input": null, "reference_output": null, "score_result": {"score": 0.46995997428894043, "category": null}}, {"id": "5e742e10-1dcd-43b0-9d5a-b0227a8bfc3d", "output": "You can use Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models, which includes a scoring workflow for manually evaluating the LLM responses in datasets.", "score": 0.39284321665763855, "input": null, "reference_output": null, "score_result": {"score": 0.39284321665763855, "category": null}}, {"id": "0ba7dcb4-e439-418f-b09d-24a120339ddc", "output": "The document does not list specific Bench scorers for evaluating LLMs at coding.", "score": 0.48407965898513794, "input": null, "reference_output": null, "score_result": {"score": 0.48407965898513794, "category": null}}, {"id": "05adad2b-eecc-422e-8877-1868229f1565", "output": "To view your Bench test results locally, set the `BENCH_FILE_DIR` environment variable to point to the location of your new test runs and run `bench` from the command line.", "score": 0.720098614692688, "input": null, "reference_output": null, "score_result": {"score": 0.720098614692688, "category": null}}, {"id": "e0af19dd-7279-4fbe-8a91-e87d36c31a69", "output": "Test suites are created to define the data and evaluation metrics for testing, while test runs execute these test suites against specific model outputs to generate performance comparisons.", "score": 0.595986545085907, "input": null, "reference_output": null, "score_result": {"score": 0.595986545085907, "category": null}}], "updated_at": "2024-03-26T15:31:41.894243", "created_at": "2024-03-26T15:31:41.894243", "page": null, "page_size": null, "total_pages": null, "total_count": null}