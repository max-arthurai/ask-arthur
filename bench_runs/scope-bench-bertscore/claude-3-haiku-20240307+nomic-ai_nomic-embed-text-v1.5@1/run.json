{"id": "8f0bdc1b-8a66-4e4b-b971-353d5128b624", "name": "claude-3-haiku-20240307+nomic-ai_nomic-embed-text-v1.5@1", "test_suite_id": "4feac22d-31c1-42a0-b252-1adca2000ebf", "test_cases": [{"id": "dfdd97cc-9df7-4d98-a306-0f48214d7d27", "output": "Based on the provided context, the following enrichments are available in Arthur:\n\n1. Anomaly Detection: Arthur can detect anomalies in your model's performance and data.\n2. Bias Mitigation: Arthur can help mitigate biases in your model's predictions.\n3. Explainability: Arthur can provide explanations for your model's predictions.\n\nThe context does not provide detailed information about other enrichments that may be available in Arthur.", "score": 0.4946339726448059, "input": null, "reference_output": null, "score_result": {"score": 0.4946339726448059, "category": null}}, {"id": "51d72f8a-c329-469e-a00d-7ab12ec66087", "output": "Based on the context provided, the document does not mention any specific model types that do not allow explainability. The focus of the document is on model schemas within the MLOps ecosystem, and it does not discuss the explainability of different model types. The context does not contain information to answer the question \"What model types do not allow explainability?\".", "score": 0.5861067175865173, "input": null, "reference_output": null, "score_result": {"score": 0.5861067175865173, "category": null}}, {"id": "5aba4ddc-2962-412b-9dd0-3f14954786a2", "output": "Based on the provided context, the Arthur platform supports the following enrichments for object detection models:\n\n1. Anomaly Detection: Arthur can detect anomalies in the object detection outputs, such as unexpected objects or changes in the detected objects over time.\n2. Bias Mitigation: Arthur can help identify and mitigate biases in the object detection model outputs.\n3. Explainability: Arthur provides explainability features to help understand the model's decision-making process for object detection.\n\nThe context does not provide any example Python code for these enrichments, as the question is asking about the available enrichments rather than requesting specific code examples.", "score": 0.5338831543922424, "input": null, "reference_output": null, "score_result": {"score": 0.5338831543922424, "category": null}}, {"id": "c5142157-54c1-4f56-a2e5-21202a2e20fb", "output": "Based on the provided context, the following enrichments are available for tabular multiclass classification models in Arthur:\n\n- Anomaly Detection\n- Hot Spots \n- Explainability\n- Bias Mitigation\n\nThe context mentions that these enrichments can be enabled for multiclass classification model types in Arthur.", "score": 0.5876333713531494, "input": null, "reference_output": null, "score_result": {"score": 0.5876333713531494, "category": null}}, {"id": "7fa1360f-9632-44d7-8868-6d5b45d1f1d1", "output": "Unfortunately, the provided context does not contain any information about restoring deleted Arthur models. The Arthur Scope FAQ document covers various topics related to using the Arthur platform, but does not mention anything about restoring deleted models. Without additional information or documentation on the Arthur platform's model management capabilities, I cannot provide a definitive answer to this question.", "score": 0.5297735333442688, "input": null, "reference_output": null, "score_result": {"score": 0.5297735333442688, "category": null}}, {"id": "a3688edd-c062-4d5d-8542-c526075e8785", "output": "Based on the provided context, the Arthur Scope FAQ does not mention anything about alert levels. The FAQ covers topics such as using Arthur without the Python SDK, model access requirements, data security and privacy, handling of ground truth labels, error handling, and data drift metrics. There is no information about the specific alert levels available in Arthur.", "score": 0.5545108318328857, "input": null, "reference_output": null, "score_result": {"score": 0.5545108318328857, "category": null}}, {"id": "0c2c9c1a-7769-4c81-b695-7388ddcbbf4e", "output": "Based on the provided context, the key additional requirements for on-prem deployment in airgapped mode are:\n\n1. The Kubernetes cluster must have access to a local container registry, as the cluster will not have internet access to pull images from public registries.\n\n2. The Kubernetes cluster must have access to a local PyPI/Conda registry to ensure optimal utilization of platform features.\n\n3. All required artifacts, including the KOTS CLI installer, plugins, and updates, must be downloaded and made available locally, as the cluster will not have internet access.", "score": 0.6170526742935181, "input": null, "reference_output": null, "score_result": {"score": 0.6170526742935181, "category": null}}, {"id": "c265c857-9311-49d2-9b89-24603aaab946", "output": "To enable explainability for your model in Arthur, you will need to provide access to your model's `predict()` function. Specifically, you will need to provide the following assets:\n\n1. The model's `predict()` function - This allows Arthur to access the internal workings of your model and generate explanations for its predictions.\n2. Any necessary dependencies or libraries required by your model's `predict()` function.\n\nWith these assets, Arthur can integrate with your model and generate detailed explanations for the model's predictions, enabling you to better understand and debug your AI applications.", "score": 0.5617845058441162, "input": null, "reference_output": null, "score_result": {"score": 0.5617845058441162, "category": null}}, {"id": "9d51a3ac-a7cb-4b13-92f6-a137c59d409a", "output": "Based on the provided context, there is no mention of KMS (Key Management Service) being required for Arthur's backup and restore capability. The document focuses on the steps to restore various components of the Arthur platform, such as the RDS Postgres database, ClickHouse data, messaging infrastructure, and enrichments, but does not discuss any KMS requirements.", "score": 0.5027071833610535, "input": null, "reference_output": null, "score_result": {"score": 0.5027071833610535, "category": null}}, {"id": "b32f4653-bad2-4180-8afc-b04bc0b53323", "output": "Unfortunately, the provided context does not contain any information about benchmarking scorers for evaluating LLMs at writing summaries. The context is focused on explaining token likelihood, which is a metric for understanding the confidence of an LLM's predictions, but it does not mention any specific benchmarking tools or metrics for evaluating LLM performance on summarization tasks. Without additional context about LLM evaluation for summarization, I cannot provide a meaningful answer to the question.", "score": 0.44398772716522217, "input": null, "reference_output": null, "score_result": {"score": 0.44398772716522217, "category": null}}, {"id": "5e742e10-1dcd-43b0-9d5a-b0227a8bfc3d", "output": "Based on the context provided, Arthur will be including a scoring workflow in their open-source evaluation tool, Arthur Bench, to evaluate LLM responses to questions. The workflow they used to manually evaluate the LLM responses in their dataset will be included as a scorer in Arthur Bench. This scorer can be used to compare the performance of different LLMs, prompts, and hyperparameters on question-answering tasks.", "score": 0.4000261425971985, "input": null, "reference_output": null, "score_result": {"score": 0.4000261425971985, "category": null}}, {"id": "0ba7dcb4-e439-418f-b09d-24a120339ddc", "output": "The Bench documentation does not explicitly mention scorers for evaluating LLMs at coding tasks. However, the Bench package provides a few scorers that could be useful for this purpose:\n\n1. **ExactMatch**: This scorer can be used to check if the candidate output exactly matches the reference output, ignoring capitalization differences if needed.\n\n2. **Custom Scorers**: The Bench documentation mentions that you can create custom scorers to provide even more flexibility. This could allow you to implement scorers specifically designed for evaluating LLM performance on coding tasks, such as checking for correct syntax, logical correctness, or adherence to best practices.\n\nTo use these scorers for evaluating LLMs on coding tasks, you would need to create a test suite with the appropriate input text (coding prompts) and reference outputs (expected solutions). You can then run the test suite and analyze the scores to assess the LLM's performance.", "score": 0.5089207887649536, "input": null, "reference_output": null, "score_result": {"score": 0.5089207887649536, "category": null}}, {"id": "05adad2b-eecc-422e-8877-1868229f1565", "output": "To view your Bench test results, you have two options:\n\n1. Local setup:\n   - Ensure the `BENCH_FILE_DIR` environment variable is set to the location of your test run data.\n   - Run the `bench` command from the command line to launch the local Bench UI.\n   - The local UI will provide a visual interface to view and explore your test results.\n\n2. SaaS (Coming Soon!):\n   - Set the environment variables `ARTHUR_BENCH_AUTOLOG`, `ARTHUR_API_URL`, and `ARTHUR_API_KEY` to connect to the Arthur platform.\n   - Your test results will be automatically logged to the Arthur platform, which will provide a web-based interface to view and analyze your test data.\n\nThe documentation provides examples for both the local and SaaS setup options to help you get started with viewing your Bench test results.", "score": 0.6949473023414612, "input": null, "reference_output": null, "score_result": {"score": 0.6949473023414612, "category": null}}, {"id": "e0af19dd-7279-4fbe-8a91-e87d36c31a69", "output": "The main difference between Bench test suites and test runs is:\n\n1. Test Suites: A test suite is a collection of test cases that are designed to test a specific functionality or feature of an AI application. It defines the input data, reference outputs, and the scoring metric to be used for evaluating the performance of the AI model.\n\n2. Test Runs: A test run is the execution of a test suite against a specific AI model or candidate. It takes the input data from the test suite, generates outputs using the candidate model, and evaluates the performance using the specified scoring metric.\n\nIn summary, a test suite defines the test cases and evaluation criteria, while a test run executes the test suite against a candidate model and reports the results.", "score": 0.6392365097999573, "input": null, "reference_output": null, "score_result": {"score": 0.6392365097999573, "category": null}}], "updated_at": "2024-03-26T15:31:50.944576", "created_at": "2024-03-26T15:31:50.944576", "page": null, "page_size": null, "total_pages": null, "total_count": null}