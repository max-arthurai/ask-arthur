{"id": "d142b1c4-da8b-4029-93bd-2012644728ca", "name": "gpt-4-0125-preview+nomic-ai_nomic-embed-text-v1.5@5", "test_suite_id": "4feac22d-31c1-42a0-b252-1adca2000ebf", "test_cases": [{"id": "dfdd97cc-9df7-4d98-a306-0f48214d7d27", "output": "The enrichments available in Arthur are Anomaly Detection, Bias Mitigation, and Explainability.", "score": 0.5425406098365784, "input": null, "reference_output": null, "score_result": {"score": 0.5425406098365784, "category": null}}, {"id": "51d72f8a-c329-469e-a00d-7ab12ec66087", "output": "The document does not provide information on model types that do not allow explainability.", "score": 0.6466416716575623, "input": null, "reference_output": null, "score_result": {"score": 0.6466416716575623, "category": null}}, {"id": "5aba4ddc-2962-412b-9dd0-3f14954786a2", "output": "Anomaly Detection, Bias Mitigation, Explainability", "score": 0.38479864597320557, "input": null, "reference_output": null, "score_result": {"score": 0.38479864597320557, "category": null}}, {"id": "c5142157-54c1-4f56-a2e5-21202a2e20fb", "output": "The available enrichments for tabular binary classification models are Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation.", "score": 0.5702216029167175, "input": null, "reference_output": null, "score_result": {"score": 0.5702216029167175, "category": null}}, {"id": "7fa1360f-9632-44d7-8868-6d5b45d1f1d1", "output": "The provided context does not contain information about restoring deleted Arthur models.", "score": 0.47069570422172546, "input": null, "reference_output": null, "score_result": {"score": 0.47069570422172546, "category": null}}, {"id": "a3688edd-c062-4d5d-8542-c526075e8785", "output": "The document does not mention specific alert levels available in Arthur.", "score": 0.5984597206115723, "input": null, "reference_output": null, "score_result": {"score": 0.5984597206115723, "category": null}}, {"id": "0c2c9c1a-7769-4c81-b695-7388ddcbbf4e", "output": "For an airgap Kubernetes cluster (K8s) install, the additional requirements include having all the required files downloaded and available locally, as the installation can proceed without Internet access.", "score": 0.47456109523773193, "input": null, "reference_output": null, "score_result": {"score": 0.47456109523773193, "category": null}}, {"id": "c265c857-9311-49d2-9b89-24603aaab946", "output": "To enable explainability, you will need to provide access to your model's `predict` function.", "score": 0.48940151929855347, "input": null, "reference_output": null, "score_result": {"score": 0.48940151929855347, "category": null}}, {"id": "9d51a3ac-a7cb-4b13-92f6-a137c59d409a", "output": "No, KMS is not mentioned as a requirement for Arthur's backup and restore capability in the provided context.", "score": 0.5428208112716675, "input": null, "reference_output": null, "score_result": {"score": 0.5428208112716675, "category": null}}, {"id": "b32f4653-bad2-4180-8afc-b04bc0b53323", "output": "The document does not mention any specific Bench scorers for evaluating LLMs at writing summaries.", "score": 0.46995997428894043, "input": null, "reference_output": null, "score_result": {"score": 0.46995997428894043, "category": null}}, {"id": "5e742e10-1dcd-43b0-9d5a-b0227a8bfc3d", "output": "You can use Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.", "score": 0.38434138894081116, "input": null, "reference_output": null, "score_result": {"score": 0.38434138894081116, "category": null}}, {"id": "0ba7dcb4-e439-418f-b09d-24a120339ddc", "output": "The document does not list specific Bench scorers for evaluating LLMs at coding.", "score": 0.48407965898513794, "input": null, "reference_output": null, "score_result": {"score": 0.48407965898513794, "category": null}}, {"id": "05adad2b-eecc-422e-8877-1868229f1565", "output": "To view your Bench test results locally, set the `BENCH_FILE_DIR` environment variable to point to the location of your test runs and execute the `bench` command from the command line. This will launch a local UI where you can view your test results.", "score": 0.750830352306366, "input": null, "reference_output": null, "score_result": {"score": 0.750830352306366, "category": null}}, {"id": "e0af19dd-7279-4fbe-8a91-e87d36c31a69", "output": "Bench test suites are created to define the data and evaluation metrics for testing, while test runs execute these test suites against specific model outputs to assess performance.", "score": 0.5907567739486694, "input": null, "reference_output": null, "score_result": {"score": 0.5907567739486694, "category": null}}], "updated_at": "2024-03-26T15:31:45.473815", "created_at": "2024-03-26T15:31:45.473815", "page": null, "page_size": null, "total_pages": null, "total_count": null}