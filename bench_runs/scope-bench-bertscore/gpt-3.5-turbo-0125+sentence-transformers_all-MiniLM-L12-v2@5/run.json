{"id": "349a9d93-8e4e-427a-be5f-da64bf9105e3", "name": "gpt-3.5-turbo-0125+sentence-transformers_all-MiniLM-L12-v2@5", "test_suite_id": "4feac22d-31c1-42a0-b252-1adca2000ebf", "test_cases": [{"id": "dfdd97cc-9df7-4d98-a306-0f48214d7d27", "output": "Anomaly Detection, Hot Spots, Explainability, and Bias Mitigation are the enrichments available in Arthur.", "score": 0.5403957962989807, "input": null, "reference_output": null, "score_result": {"score": 0.5403957962989807, "category": null}}, {"id": "51d72f8a-c329-469e-a00d-7ab12ec66087", "output": "Generative text models like Large Language Models (LLMs) do not allow for traditional model explainability due to their complexity and the nature of their outputs.", "score": 0.7150177955627441, "input": null, "reference_output": null, "score_result": {"score": 0.7150177955627441, "category": null}}, {"id": "5aba4ddc-2962-412b-9dd0-3f14954786a2", "output": "The enrichments available for object detection models are:\n1. Anomaly Detection\n2. Hot Spots\n3. Explainability\n4. Bias Mitigation", "score": 0.7244204878807068, "input": null, "reference_output": null, "score_result": {"score": 0.7244204878807068, "category": null}}, {"id": "c5142157-54c1-4f56-a2e5-21202a2e20fb", "output": "Anomaly Detection, Hot Spots, Explainability, Bias Mitigation", "score": 0.4480181336402893, "input": null, "reference_output": null, "score_result": {"score": 0.4480181336402893, "category": null}}, {"id": "7fa1360f-9632-44d7-8868-6d5b45d1f1d1", "output": "Yes, you can restore a deleted Arthur model by following the backup and restore process for the Arthur Platform. This process involves restoring the necessary components such as ClickHouse data, Kubernetes deployment state, enrichments infrastructure, workflows, and more. By following the steps outlined in the documentation, you can recover deleted models along with their associated data.", "score": 0.5695503354072571, "input": null, "reference_output": null, "score_result": {"score": 0.5695503354072571, "category": null}}, {"id": "a3688edd-c062-4d5d-8542-c526075e8785", "output": "The two alert levels available in Arthur are daily and weekly.", "score": 0.7798938751220703, "input": null, "reference_output": null, "score_result": {"score": 0.7798938751220703, "category": null}}, {"id": "0c2c9c1a-7769-4c81-b695-7388ddcbbf4e", "output": "For on-prem deployment in airgapped mode, the additional requirements include:\n1. An existing private container registry\n2. Existing private Python registries (PyPI, Anaconda) - only required for the model explanation feature\n3. Access to container images and deployment manifest files from specific public registries\n4. Ensure that your private container and Python registries are accessible\n5. Make sure your firewall configurations allow necessary egress and ingress for the platform\n\nFor more detailed information, refer to the \"On-Prem Deployment Requirements\" document.", "score": 0.7454208731651306, "input": null, "reference_output": null, "score_result": {"score": 0.7454208731651306, "category": null}}, {"id": "c265c857-9311-49d2-9b89-24603aaab946", "output": "To enable explainability for your model in Arthur, you will need to provide access to your model's predict function. This can be done by creating a model schema that outlines the relationship between your dataset and the model, including information such as model inputs, outputs, data columns, and data access details. Additionally, you will need to provide the necessary artifacts for installing the Arthur platform, which are available on a customer-specific password-protected portal.", "score": 0.5664807558059692, "input": null, "reference_output": null, "score_result": {"score": 0.5664807558059692, "category": null}}, {"id": "9d51a3ac-a7cb-4b13-92f6-a137c59d409a", "output": "Yes, KMS is recommended for encrypting data at rest when using Arthur's backup and restore capability.", "score": 0.618254542350769, "input": null, "reference_output": null, "score_result": {"score": 0.618254542350769, "category": null}}, {"id": "b32f4653-bad2-4180-8afc-b04bc0b53323", "output": "You can use the \"ExactMatch\" scorer from Bench to evaluate LLMs at writing summaries. This scorer allows you to compare candidate outputs with reference outputs and can be configured to ignore capitalization differences.", "score": 0.489370733499527, "input": null, "reference_output": null, "score_result": {"score": 0.489370733499527, "category": null}}, {"id": "5e742e10-1dcd-43b0-9d5a-b0227a8bfc3d", "output": "You can use the ExactMatch scorer for evaluating LLMs at answering questions.", "score": 0.42628422379493713, "input": null, "reference_output": null, "score_result": {"score": 0.42628422379493713, "category": null}}, {"id": "0ba7dcb4-e439-418f-b09d-24a120339ddc", "output": "You can use the custom scorer feature in Bench to evaluate LLM performance. One example is the TrigramRepetition scorer, which evaluates responses based on repeated trigrams. You can define custom scorers like TrigramRepetition to suit your specific evaluation needs.", "score": 0.46969008445739746, "input": null, "reference_output": null, "score_result": {"score": 0.46969008445739746, "category": null}}, {"id": "05adad2b-eecc-422e-8877-1868229f1565", "output": "To view your Bench test results, you can run the Bench UI locally by using the following command: `bench`. This will launch the local UI where you can explore and analyze your test results.", "score": 0.7115365266799927, "input": null, "reference_output": null, "score_result": {"score": 0.7115365266799927, "category": null}}, {"id": "e0af19dd-7279-4fbe-8a91-e87d36c31a69", "output": "Test Suites store input & reference output data with a scorer, while Test Runs evaluate candidate outputs against the Test Suite's criteria and assign scores to each test case.", "score": 0.742101788520813, "input": null, "reference_output": null, "score_result": {"score": 0.742101788520813, "category": null}}], "updated_at": "2024-03-26T15:31:38.238804", "created_at": "2024-03-26T15:31:38.238804", "page": null, "page_size": null, "total_pages": null, "total_count": null}