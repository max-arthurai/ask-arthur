{"page_content": ": 0 text : arthur now available : recommender system support in arthur scoperead moresolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##observabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedthe ai delivery enginelaunch, secure, and optimize ai at scale. request demobuildarthur \u2019 s turnkey, plug - and - play solutions allow companies to build on top of their internal knowledge base and make informed, data -", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ildarthur \u2019 s turnkey, plug - and - play solutions allow companies to build on top of their internal knowledge base and make informed, data - driven decisions when integrating the latest generative ai technologies into their operations. learn moreprotectdeploy ai confidently and safely by using arthur to protect your organization from the biggest llm threats including data leakage, hallucinations, toxic language generation, and prompt injection. learn moreoptimizedrive key business results for your enterprise by using the arthur platform to optimize model operations and performance at scale across tabular, cv, nlp, and large", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "key business results for your enterprise by using the arthur platform to optimize model operations and performance at scale across tabular, cv, nlp, and large language models. learn moreevaluatearthur bench is an open - source evaluation product for comparing llms, allowing you to make informed, data - driven decisions when integrating the latest ai technologies into your operations. learn morethe # 1 ai delivery platformmodelsthe arthur platform is model - and platform - agnostic, providing the # 1 monitoring platform for models ranging from classic tabular and computer vision, to robust llms. platformsthe arthur platform scales up and down", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##nostic, providing the # 1 monitoring platform for models ranging from classic tabular and computer vision, to robust llms. platformsthe arthur platform scales up and down with complex enterprise needs \u2014 it can ingest up to 1mm transactions per second and deliver insights quickly. learn moredeploymentarthur works seamlessly with all leading data science and mlops tools, including databricks, amazon sagemaker, tensorflow, pytorch, singlestore, and salesforce. governancearthur \u2019 s platform offers model risk management capabilities across validation, monitoring, and reporting, helping organizations to avoid adverse consequences from decisions made based on", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##force. governancearthur \u2019 s platform offers model risk management capabilities across validation, monitoring, and reporting, helping organizations to avoid adverse consequences from decisions made based on model errors. learn morecollaborationour platform allows for quick, seamless communication across teams via a centralized performance dashboard, real - time metrics, optimization alerts, and fully customizable permissions across organizations. securityhaving achieved soc 2 type ii compliance, our robust model monitoring solution adheres to best - in - class security and data privacy controls. we are committed to meeting the industry \u2019 s most rigorous data security, availability, and confidentiality standards. learn", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "- in - class security and data privacy controls. we are committed to meeting the industry \u2019 s most rigorous data security, availability, and confidentiality standards. learn more \u201c arthur bench helped us develop an internal framework to scale and standardize llm evaluation across features, and to describe performance to the product team with meaningful and interpretable metrics. \u201d priyanka oberoistaff data scientist, axios hqfrom the blogview moreaaai 2024 recap : future visions of recommendation ecosystems read morewhat \u2019 s going on with llm leaderboards? read morenow available : recommender system support in arthur scoperea", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "future visions of recommendation ecosystems read morewhat \u2019 s going on with llm leaderboards? read morenow available : recommender system support in arthur scoperead morefrom the studioview moreground truth episode 5 : ai crashes the science partywatchllms & generative ai in editorial content creation & mediawatcharthur llm product demo & research on generative ai challengeswatchsee what arthur can do for you. get started we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dr", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai /", "metadata": {"source": "https://www.arthur.ai/", "row": 0, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 1 text : evaluation solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresour", "metadata": {"source": "https://www.arthur.ai/solution/evaluation", "row": 1, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedevaluationthe solution to help teams evaluate llm options quickly, easily, and consistently. as the llm landscape rapidly evolves, companies must continually ensure their llm choice remains the best fit for the organization \u2019 s specific needs. arthur bench, our open source evaluation product, helps businesses with : model selection & validationbudget", "metadata": {"source": "https://www.arthur.ai/solution/evaluation", "row": 1, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "choice remains the best fit for the organization \u2019 s specific needs. arthur bench, our open source evaluation product, helps businesses with : model selection & validationbudget & privacy optimizationstranslation of academic benchmarks to real - world performance \u201c understanding the differences in performance between llms can have an incredible amount of nuance. with bench, we \u2019 ve created an open source tool to help teams deeply understand the differences between llm providers, different prompting and augmentation strategies, and custom training regimes. \u201d adam wenchelco - founder & ceothe most robust way to evaluate llmsbench is our solution to help teams evaluate different", "metadata": {"source": "https://www.arthur.ai/solution/evaluation", "row": 1, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "and custom training regimes. \u201d adam wenchelco - founder & ceothe most robust way to evaluate llmsbench is our solution to help teams evaluate different llm options in a quick, easy, and consistent way. model selection & validationcompare llm options using a consistent metric to determine the best fit for your application. budget & privacy optimizationnot all applications require the most advanced or expensive llms \u2014 in some cases, a less expensive ai model can perform just as well. translating academic benchmarks to real - world performancetest and compare the performance of different models quantitatively with a set of standard metrics to ensure accuracy", "metadata": {"source": "https://www.arthur.ai/solution/evaluation", "row": 1, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "as well. translating academic benchmarks to real - world performancetest and compare the performance of different models quantitatively with a set of standard metrics to ensure accuracy and consistency. try benchrelated articleswhat \u2019 s going on with llm leaderboards? arthur teamread moreintroducing arthur chat : fast, safe, custom ai for businessarthur teamread morellm - guided evaluation : using llms to evaluate llmsarthur teamread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatl", "metadata": {"source": "https://www.arthur.ai/solution/evaluation", "row": 1, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / solution / evaluation", "metadata": {"source": "https://www.arthur.ai/solution/evaluation", "row": 1, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 2 text : protection solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresour", "metadata": {"source": "https://www.arthur.ai/solution/firewall", "row": 2, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcomplete protection with the world's first firewall for llmsdeploying and utilizing llms has numerous and well - documented risks which, if not mitigated and monitored, can lead to negative user experiences and significant reputational impact. these risks include : pii or sensitive data leakageprompt injectionsha", "metadata": {"source": "https://www.arthur.ai/solution/firewall", "row": 2, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##igated and monitored, can lead to negative user experiences and significant reputational impact. these risks include : pii or sensitive data leakageprompt injectionshallucinationstoxic, offensive, or problematic language generationcomplete protection with the world's first firewall for llmsdeploying and utilizing llms has numerous and well - documented risks which, if not mitigated and monitored, can lead to negative user experiences and significant reputational impact. these risks include : pii or sensitive data leakageprompt injectionshallucinationstoxic, offensive, or problematic language generation \u201c llms", "metadata": {"source": "https://www.arthur.ai/solution/firewall", "row": 2, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ". these risks include : pii or sensitive data leakageprompt injectionshallucinationstoxic, offensive, or problematic language generation \u201c llms are one of the most disruptive technologies since the advent of the internet. arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk. \u201d adam wenchelco - founder & ceothe first firewall for llmsshield is our solution to help companies deploy their llms confidently and safely. fits into the llm architecturesits between the application layer and the deployment", "metadata": {"source": "https://www.arthur.ai/solution/firewall", "row": 2, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##msshield is our solution to help companies deploy their llms confidently and safely. fits into the llm architecturesits between the application layer and the deployment layer to validate user prompts and model responses on two endpoints. works with any llmwhether you \u2019 re using openai or another large language model, shield will be able to be integrated into the workflow. provides real - time protectionour inference deep dive capabilities allow us to detect and intercept any prompts that may potentially be considered harmful or elicit a potentially dangerous output. try shieldsee what arthur can do for you. get started we make ai better", "metadata": {"source": "https://www.arthur.ai/solution/firewall", "row": 2, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "prompts that may potentially be considered harmful or elicit a potentially dangerous output. try shieldsee what arthur can do for you. get started we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / solution / firewall", "metadata": {"source": "https://www.arthur.ai/solution/firewall", "row": 2, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 3 text : observability solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvta", "metadata": {"source": "https://www.arthur.ai/solution/observability", "row": 3, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedobservabilitymonitor, measure, and improve ml models to deliver better results. arthur helps enterprise teams optimize model operations and performance at scale. across llm, tabular, cv, and nlp models, our platform tracks and improves : model accuracy & data driftexplainability & transparencyfa", "metadata": {"source": "https://www.arthur.ai/solution/observability", "row": 3, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "across llm, tabular, cv, and nlp models, our platform tracks and improves : model accuracy & data driftexplainability & transparencyfairness & bias detection \u201c thanks to arthur, we know that our preventative care models are fair, and that we can catch any potential issues before they impact our members \u2026 and the arthur platform allows us to detect and fix data drift before it becomes a real problem. \u201d heather carroll coxchief analytics officer, humanathe complete ai performance solutionuse arthur observability across llm, tabular, cv, and nlp models to monitor, measure, and improve", "metadata": {"source": "https://www.arthur.ai/solution/observability", "row": 3, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##athe complete ai performance solutionuse arthur observability across llm, tabular, cv, and nlp models to monitor, measure, and improve model metrics and achieve business results. accuracy & data drifttrack model performance to detect and react to data drift, improving model accuracy for better business outcomes. explainability & transparencybuild trust, ensure compliance, and drive more actionable ml outcomes with arthur \u2019 s explainability features. fairness & bias detectionget actionable insights into how your model treats different population groups, set fairness thresholds that make sense for your business, and get notified instantly if there are any problems", "metadata": {"source": "https://www.arthur.ai/solution/observability", "row": 3, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "actionable insights into how your model treats different population groups, set fairness thresholds that make sense for your business, and get notified instantly if there are any problems. try observabilityrelated articlesdetecting unexpected drift in time series featuresakash khannaread moremodel schemas within the mlops ecosystemsarah ostermeierread morekeep the lights on : making deployed ai / ml better for everyonejohn dickersonread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnl", "metadata": {"source": "https://www.arthur.ai/solution/observability", "row": 3, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / solution / observability", "metadata": {"source": "https://www.arthur.ai/solution/observability", "row": 3, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 4 text : arthur bench solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedthe most robust way to evaluate llmsbench is our solution to help teams evaluate the different llm options out there in a quick, easy and consistent way. try bench \u201c llms are one of the most disruptive technologies since the advent of the internet. arthur has created the tools needed to deploy this technology more quickly and", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "bench \u201c llms are one of the most disruptive technologies since the advent of the internet. arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk. \u201d adam wenchelco - founder & ceomodel selection & validationarthur bench helps companies compare the different llm options available using consistent metrics so they can determine the best fit for their application in a rapidly evolving ai landscape. budget & privacy optimizationnot all applications require the most advanced or expensive llms \u2014 in some cases, a less expensive ai model can perform tasks", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "evolving ai landscape. budget & privacy optimizationnot all applications require the most advanced or expensive llms \u2014 in some cases, a less expensive ai model can perform tasks equally as well. additionally, bringing models in - house can offer greater controls around data privacy. translating academic benchmarks to real - world performancebench helps companies test and compare the performance of different models quantitatively with a set of standard metrics to ensure accuracy and consistency. additionally, companies can add and configure customized benchmarks, enabling them to focus on what matters most to their specific business and customers. try bencharthur bench is the key to fast, data", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##e customized benchmarks, enabling them to focus on what matters most to their specific business and customers. try bencharthur bench is the key to fast, data - driven llm evaluationfull suite of scoring metricsfrom summarization quality to hallucinations, bench comes complete with a full suite of scoring metrics, ready to leverage. additionally, you can create and add your own scoring metrics. intuitive user interfaceleverage the arthur user interface to quickly and easily conduct and compare your test runs and visualize the different performance of the llms. local and cloud - based versionsgain access via our github", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "easily conduct and compare your test runs and visualize the different performance of the llms. local and cloud - based versionsgain access via our github repo and run it locally or sign up for our cloud - based saas offering. we offer both versions for greatest flexibility. completely open sourcethe best part is that bench is completely open source, so new metrics and other valuable features will continue to be added as the project and community grows. visit our github repothe generative assessment projecta research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like openai, anthropic", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ub repothe generative assessment projecta research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like openai, anthropic, and meta as well as other open source models. learn morethe generative assessment programa research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like openai, anthropic, and meta as well as other open source models. learn morerelated articleswhat \u2019 s going on with llm leaderboards? february 19, 2024read moreintroducing arthur chat : fast, safe, custom ai for businessdecember", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "with llm leaderboards? february 19, 2024read moreintroducing arthur chat : fast, safe, custom ai for businessdecember 5, 2023read morellm - guided evaluation : using llms to evaluate llmsseptember 29, 2023read more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##pcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / product / bench", "metadata": {"source": "https://www.arthur.ai/product/bench", "row": 4, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 5 text : arthur shield solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/product/shield", "row": 5, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedthe first firewall for llmsshield is our solution to help companies deploy their llms confidently and safely. learn morefits into the llm architecturesits between the application layer and the deployment layer to validate user prompts and model responses on two endpoints. works with any llmwhether you \u2019", "metadata": {"source": "https://www.arthur.ai/product/shield", "row": 5, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "between the application layer and the deployment layer to validate user prompts and model responses on two endpoints. works with any llmwhether you \u2019 re using openai or another large language model, shield will be able to be integrated into the workflow. provides real - time protectionour inference deep dive capabilities allow us to detect and intercept any prompts that may potentially be considered harmful or elicit a potentially dangerous output. learn morearthur shield is the key to deploying llms quickly and safelysensitive data leakageprotect your user \u2019 s data as well as your company \u2019 s proprietary data from being", "metadata": {"source": "https://www.arthur.ai/product/shield", "row": 5, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "deploying llms quickly and safelysensitive data leakageprotect your user \u2019 s data as well as your company \u2019 s proprietary data from being unintentionally leaked. toxicityblock llm responses that are not value - aligned with your organization. prompt injectionsidentify and block attempts to override the intended behavior of an llm by malicious users. hallucinationsdetect likely incorrect or unsubstantiated responses from an llm before they can cause harm to the end user. related articleswhat \u2019 s going on with llm leaderboards? arthur teamread moreintroduc", "metadata": {"source": "https://www.arthur.ai/product/shield", "row": 5, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "before they can cause harm to the end user. related articleswhat \u2019 s going on with llm leaderboards? arthur teamread moreintroducing arthur chat : fast, safe, custom ai for businessarthur teamread morethe real - world harms of llms, part 2 : when llms do work as expectedsarah ostermeierread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentation", "metadata": {"source": "https://www.arthur.ai/product/shield", "row": 5, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / product / shield", "metadata": {"source": "https://www.arthur.ai/product/shield", "row": 5, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 6 text : scope solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresour", "metadata": {"source": "https://www.arthur.ai/product/scope", "row": 6, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedthe complete ai performance solutionwith scope, enterprise teams can optimize ml operations and performance, delivering better results across llm, tabular, cv, and nlp models. learn moremonitors all model typesarthur scope is the leading performance solution for all model types, including nlp, cv, tabular, and ll", "metadata": {"source": "https://www.arthur.ai/product/scope", "row": 6, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ". learn moremonitors all model typesarthur scope is the leading performance solution for all model types, including nlp, cv, tabular, and llm. highly scalable with complex enterprise needsthe arthur platform \u2019 s highly scalable microservices architecture makes it the leading platform for enterprises that want to run high - performing ml models at scale. learn morearthur scope is the key to accelerating model operations to drive business resultsaccuracy & data drifttrack model performance to detect and react to data drift, improving model accuracy for better business outcomes. fairness & bias detectionget actionable insights into how your model", "metadata": {"source": "https://www.arthur.ai/product/scope", "row": 6, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##track model performance to detect and react to data drift, improving model accuracy for better business outcomes. fairness & bias detectionget actionable insights into how your model treats different population groups, set fairness thresholds that make sense for your business, and get notified instantly if there are any problems. explainability & transparencybuild trust, ensure compliance, and drive more actionable ml outcomes with arthur \u2019 s explainability features. real - time alertsmaintain peace of mind knowing you will proactively receive alerts on any metric after poor model performance is detected. related articleswhat \u2019 s going on with llm leaderboards?", "metadata": {"source": "https://www.arthur.ai/product/scope", "row": 6, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "you will proactively receive alerts on any metric after poor model performance is detected. related articleswhat \u2019 s going on with llm leaderboards? arthur teamread moreintroducing arthur chat : fast, safe, custom ai for businessarthur teamread morethe real - world harms of llms, part 2 : when llms do work as expectedsarah ostermeierread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresour", "metadata": {"source": "https://www.arthur.ai/product/scope", "row": 6, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / product / scope", "metadata": {"source": "https://www.arthur.ai/product/scope", "row": 6, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 7 text : arthur chat solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedfast, safe, custom ai for businessas a completely turnkey ai chat platform built on top of your enterprise documents and data, arthur chat is the fastest way to unlock the value of your llm. try chatcustom, completely turnkey chat experiencellm applications are hard to build. they require resources, knowledge, and", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "the value of your llm. try chatcustom, completely turnkey chat experiencellm applications are hard to build. they require resources, knowledge, and time for your team to ramp up on new concepts. with arthur chat, you can focus more on delivering value, rather than delivering code. powered by your datathe arthur chat platform is built on top of your internal knowledge base, composed of many documents and data \u2014 both structured and unstructured \u2014 so you can provide the best and most accurate responses to your users. just bring your own data and we \u2019 ll handle the rest. protected by arthur shieldarthur shield, the", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "the best and most accurate responses to your users. just bring your own data and we \u2019 ll handle the rest. protected by arthur shieldarthur shield, the world \u2019 s first firewall for llms, is embedded within the arthur chat platform, enabling real time checks and protection against pii / sensitive data leakage, toxic language generation, hallucinations, and prompt injection. try chatarthur shield is the key to deploying llms quickly and safelysensitive data leakageprotect your user \u2019 s data as well as your company \u2019 s proprietary data from being unintentionally leaked. toxicityblock ll", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "data leakageprotect your user \u2019 s data as well as your company \u2019 s proprietary data from being unintentionally leaked. toxicityblock llm responses that are not value - aligned with your organization. prompt injectionsidentify and block attempts to override the intended behavior of an llm by malicious users. hallucinationsdetect likely incorrect or unsubstantiated responses from an llm before they can cause harm to the end user. related articleswhat \u2019 s going on with llm leaderboards? arthur teamread moreintroducing arthur chat : fast, safe, custom ai", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "related articleswhat \u2019 s going on with llm leaderboards? arthur teamread moreintroducing arthur chat : fast, safe, custom ai for businessarthur teamread morethe real - world harms of llms, part 2 : when llms do work as expectedsarah ostermeierread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiries", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##copechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / product / chat", "metadata": {"source": "https://www.arthur.ai/product/chat", "row": 7, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 8 text : llm solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedthe control plane for llmseverything you need to quickly and safely deploy llms into mission - critical applications. arthur is the all - in - one solution for deploying and running llms, trusted by the most important companies in the world with mission - critical applications. from evaluation and validation to firewall protection and monitoring", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "deploying and running llms, trusted by the most important companies in the world with mission - critical applications. from evaluation and validation to firewall protection and monitoring, we \u2019 ve developed a state - of - the - art llm product suite that makes generative ai simple, useful, and safe. firewallevaluationobservabilitychatcompanies across industries are rapidly integrating large language models into their operations, but they don \u2019 t have a way to ensure deployment that \u2019 s both fast and safe. arthur shield, the world \u2019 s first firewall for llms, protects organizations against the most serious risks and safety", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "deployment that \u2019 s both fast and safe. arthur shield, the world \u2019 s first firewall for llms, protects organizations against the most serious risks and safety issues with llms in production. mitigate risks like : pii or sensitive data leakagehallucinationstoxic, offensive, or problematic language generationprompt injectionslearn moreas the llm landscape rapidly evolves, it \u2019 s crucial for companies to keep abreast of advancements and continually ensure their llm choice remains the best fit for the organization \u2019 s specific needs. with arthur bench, our open source evaluation product, companies can make informed,", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "ensure their llm choice remains the best fit for the organization \u2019 s specific needs. with arthur bench, our open source evaluation product, companies can make informed, data - driven decisions by comparing different llm options. bench helps businesses with : model selection & validationbudget & privacy optimizationtranslation of academic benchmarks to real - world performancelearn morearthur helps enterprise teams optimize model operations and performance at scale. our platform tracks and improves key metrics for not only your llms in production, but for tabular, cv, and nlp models as well. with arthur scope, you can : detect model and", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "not only your llms in production, but for tabular, cv, and nlp models as well. with arthur scope, you can : detect model and data issues immediatelysurface actionable insights to improve performanceoptimize model portfolio managementreduce risk with comprehensive ml governancelearn morellm applications are hard to build \u2014 they require resources, knowledge, and time for your team to ramp up on new concepts. arthur chat is a highly configurable, plug - and - play, llm - powered chat experience that allows you to focus more on delivering value, rather than delivering code. chat provides organizations with :", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "plug - and - play, llm - powered chat experience that allows you to focus more on delivering value, rather than delivering code. chat provides organizations with : a completely turnkey chat experience, ready to deploy in under an hourthe ability to customize and build on top of your internal knowledge baseprotection from arthur shield, the world \u2019 s first firewall for llmslearn morefirewallevaluationobservabilitychatcompanies across industries are rapidly integrating large language models into their operations, but they don \u2019 t have a way to ensure deployment that \u2019 s both fast and safe. arthur shield, the", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "rapidly integrating large language models into their operations, but they don \u2019 t have a way to ensure deployment that \u2019 s both fast and safe. arthur shield, the world \u2019 s first firewall for llms, protects organizations against the most serious risks and safety issues with llms in production. mitigate risks like : pii or sensitive data leakagehallucinationstoxic, offensive, or problematic language generationprompt injectionslearn moreas the llm landscape rapidly evolves, it \u2019 s crucial for companies to keep abreast of advancements and continually ensure their llm choice remains the best fit for the organization \u2019", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "evolves, it \u2019 s crucial for companies to keep abreast of advancements and continually ensure their llm choice remains the best fit for the organization \u2019 s specific needs. with arthur bench, our open source evaluation product, companies can make informed, data - driven decisions by comparing different llm options. bench helps businesses with : model selection & validationbudget & privacy optimizationtranslation of academic benchmarks to real - world performancelearn morearthur helps enterprise teams optimize model operations and performance at scale. our platform tracks and improves key metrics for not only your llms in production, but for tabular,", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "optimize model operations and performance at scale. our platform tracks and improves key metrics for not only your llms in production, but for tabular, cv, and nlp models as well. with arthur scope, you can : detect model and data issues immediatelysurface actionable insights to improve performanceoptimize model portfolio managementreduce risk with comprehensive ml governancelearn morellm applications are hard to build \u2014 they require resources, knowledge, and time for your team to ramp up on new concepts. arthur chat is a highly configurable, plug - and - play, llm - powered chat experience that", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "your team to ramp up on new concepts. arthur chat is a highly configurable, plug - and - play, llm - powered chat experience that allows you to focus more on delivering value, rather than delivering code. chat provides organizations with : a completely turnkey chat experience, ready to deploy in under an hourthe ability to customize and build on top of your internal knowledge baseprotection from arthur shield, the world \u2019 s first firewall for llmslearn more \u201c thanks to arthur, we know that our preventative care models are fair, and that we can catch any potential issues before they impact our members", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##learn more \u201c thanks to arthur, we know that our preventative care models are fair, and that we can catch any potential issues before they impact our members \u2026 and the arthur platform allows us to detect and fix data drift before it becomes a real problem. \u201d heather carroll coxchief analytics officer, humanallm solutionsarthur is the all - in - one solution for deploying and running llms, trusted by the most important companies in the world with mission - critical applications. from evaluation and validation to firewall protection and monitoring, we \u2019 ve developed a state - of - the - art llm product suite that makes genera", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "applications. from evaluation and validation to firewall protection and monitoring, we \u2019 ve developed a state - of - the - art llm product suite that makes generative ai simple, useful, and safe. firewallevaluationobservabilitycompanies across industries are rapidly integrating large language models into their operations, but they don \u2019 t have a way to ensure deployment that \u2019 s both fast and safe. arthur shield, the world \u2019 s first firewall for llms, protects organizations against the most serious risks and safety issues with llms in production. mitigate risks like : pii or sensitive data leakagehalluc", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms, protects organizations against the most serious risks and safety issues with llms in production. mitigate risks like : pii or sensitive data leakagehallucinationstoxic, offensive, or problematic language generationprompt injectionslearn moreas the llm landscape rapidly evolves, it \u2019 s crucial for companies to keep abreast of advancements and continually ensure their llm choice remains the best fit for the organization \u2019 s specific needs. with arthur bench, our open source evaluation product, companies can make informed, data - driven decisions by comparing different llm options. bench helps businesses with : model selection & validationbu", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "our open source evaluation product, companies can make informed, data - driven decisions by comparing different llm options. bench helps businesses with : model selection & validationbudget & privacy optimizationtranslation of academic benchmarks to real - world performancelearn morearthur helps enterprise teams optimize model operations and performance at scale. our platform tracks and improves key metrics for not only your llms in production, but for tabular, cv, and nlp models as well. with arthur scope, you can : take action to improve performanceincrease speed & efficiencydemocratize ml performancereduce risk through ml governancelea", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ". with arthur scope, you can : take action to improve performanceincrease speed & efficiencydemocratize ml performancereduce risk through ml governancelearn morefirewallevaluationobservabilitycompanies across industries are rapidly integrating large language models into their operations, but they don \u2019 t have a way to ensure deployment that \u2019 s both fast and safe. arthur shield, the world \u2019 s first firewall for llms, protects organizations against the most serious risks and safety issues with llms in production. mitigate risks like : pii or sensitive data leakagehallucinationstoxic, offensive,", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "serious risks and safety issues with llms in production. mitigate risks like : pii or sensitive data leakagehallucinationstoxic, offensive, or problematic language generationprompt injectionslearn moreas the llm landscape rapidly evolves, it \u2019 s crucial for companies to keep abreast of advancements and continually ensure their llm choice remains the best fit for the organization \u2019 s specific needs. with arthur bench, our open source evaluation product, companies can make informed, data - driven decisions by comparing different llm options. bench helps businesses with : model selection & validationbudget & privacy optimizationtranslation", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "can make informed, data - driven decisions by comparing different llm options. bench helps businesses with : model selection & validationbudget & privacy optimizationtranslation of academic benchmarks to real - world performancelearn morearthur helps enterprise teams optimize model operations and performance at scale. our platform tracks and improves key metrics for not only your llms in production, but for tabular, cv, and nlp models as well. with arthur scope, you can : detect model and data issues immediatelysurface actionable insights to improve performanceoptimize model portfolio managementreduce risk with comprehensive ml governancelearn more", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": detect model and data issues immediatelysurface actionable insights to improve performanceoptimize model portfolio managementreduce risk with comprehensive ml governancelearn moresee what arthur can do for you. get startedsee what arthur can do for you. get started we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##oggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / model - type / llm", "metadata": {"source": "https://www.arthur.ai/model-type/llm", "row": 8, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 9 text : nlp solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedunderstand your nlp models like never before. try the first - ever complete nlp monitoring & explainability solution today. request demoensure consistent pipelineensure consistency in information extraction pipelines and monitor for data drift. explore key insightseasily filter, search, and explore key insights in your nlp models such as an", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "in information extraction pipelines and monitor for data drift. explore key insightseasily filter, search, and explore key insights in your nlp models such as anomalous inferences or specified attributes about each document. use explainability techniquesuse explainability techniques to identify the most important features in determining the predictions of your nlp models. explainability and monitoring for your nlp modelsrequest democompare the similarity of new input documents to the documents used to train your nlp models. detect biases in your nlp models by uncovering differences in accuracy and other performance metrics across different subgroups. identify the specific words within", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##p models. detect biases in your nlp models by uncovering differences in accuracy and other performance metrics across different subgroups. identify the specific words within a document that contributed the most to a given prediction. download our nlp whitepaperfrom simple chatbots to document classifiers to generative models like gpt - 4, natural language processing models are seemingly everywhere these days. nlp models are powerful tools for processing unstructured text data \u2014 but with great power comes great responsibility. if you \u2019 re not monitoring your nlp models just as you would your tabular models, you can overlook many sticky issues that could quickly", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "comes great responsibility. if you \u2019 re not monitoring your nlp models just as you would your tabular models, you can overlook many sticky issues that could quickly become billion - dollar problems. arthur \u2019 s \u201c increase ml model visibility with nlp monitoring \u201d whitepaper is everything you need to know about model monitoring for natural language processing whitepaper covers what any organization deploying nlp models into production should be doing to ensure that those models continue to perform as expected. learn more about how model monitoring can help you improve your nlp model performance with the help of arthur. download whitepaperlearn more about monitoring nlp models with arthurda", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "how model monitoring can help you improve your nlp model performance with the help of arthur. download whitepaperlearn more about monitoring nlp models with arthurdata drift detection part ii : unstructured data in nlp and cvkarthik rao and rowan cheungread morearthur releases the first computer vision model monitoring solution for enterprisearthur teamread moreintroducing monitoring for computer vision modelsarthur teamread moresee what arthur can do for you. get started we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchs", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "you. get started we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / model - type / nlp", "metadata": {"source": "https://www.arthur.ai/model-type/nlp", "row": 9, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 10 text : cv models solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedsee your cv models more clearly. try the first - ever computer vision monitoring solution today. request demoidentify anomalies & data driftautomatic out - of - distribution detection lets you identify where your model is likely making mistakes. improve your models and explore your datasetsexplore the results of your vision", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "of - distribution detection lets you identify where your model is likely making mistakes. improve your models and explore your datasetsexplore the results of your vision models ( classification and object detection ) with an interactive interface that makes it easy to identify issues. understand your models bettervisualize important image regions that are impactful for model predictions. explainability and monitoring for your cv modelsmonitor cv model pipelines for data anomalies using built - in out - of - distribution detection and track the accuracy of bounding box models. detect biases in your cv models by evaluating image classification outputs using an interactive interface and locating where your", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "distribution detection and track the accuracy of bounding box models. detect biases in your cv models by evaluating image classification outputs using an interactive interface and locating where your models misclassify and perpetuate biases. visualize which regions of an image are impactful for an image classification model \u2019 s decision or how your object detection models are performing on pipeline images. download our cv whitepaperas computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. the adoption of computer vision into industry applications promises enormous potential upside ; however, computer vision models", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "increasingly adopted computer vision models to augment and automate critical processes. the adoption of computer vision into industry applications promises enormous potential upside ; however, computer vision models, like any ml model, must be carefully monitored. a promising model that has gone off the rails can quickly become a dangerous liability. in this whitepaper, we lay out several aspects of computer vision models that are important for users to understand and demonstrate how arthur \u2019 s product offers simple solutions to these pressing problems. download whitepaperrelated articlesdata drift detection part ii : unstructured data in nlp and cvkarthik rao and rowan cheungread morearth", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "whitepaperrelated articlesdata drift detection part ii : unstructured data in nlp and cvkarthik rao and rowan cheungread morearthur releases the first computer vision model monitoring solution for enterprisearthur teamread moreintroducing monitoring for computer vision modelsarthur teamread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##nlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / model - type / cv", "metadata": {"source": "https://www.arthur.ai/model-type/cv", "row": 10, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 11 text : tabular solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularres", "metadata": {"source": "https://www.arthur.ai/model-type/tabular", "row": 11, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##ms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedmonitor, measure, and improve your tabular models. achieve better results across accuracy, explainability, and fairness. request demoidentify anomalies & data driftautomatic out - of - distribution detection lets you identify where your model is likely making mistakes. use explainability techniquesuse explainability techniques to identify the", "metadata": {"source": "https://www.arthur.ai/model-type/tabular", "row": 11, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "driftautomatic out - of - distribution detection lets you identify where your model is likely making mistakes. use explainability techniquesuse explainability techniques to identify the most important features in determining the predictions of your models. quickly mitigate biasuse arthur \u2019 s proprietary techniques, or customize with your own metrics, to reduce business risk and prevent discrimination. learn about monitoring your tabular models with arthurdetecting unexpected drift in time series featuresakash khannaread moremodel schemas within the mlops ecosystemsarah ostermeierread morekeep the lights on : making deployed ai / ml better for everyonejo", "metadata": {"source": "https://www.arthur.ai/model-type/tabular", "row": 11, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##del schemas within the mlops ecosystemsarah ostermeierread morekeep the lights on : making deployed ai / ml better for everyonejohn dickersonread more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / model - type", "metadata": {"source": "https://www.arthur.ai/model-type/tabular", "row": 11, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": "##press inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / model - type / tabular", "metadata": {"source": "https://www.arthur.ai/model-type/tabular", "row": 11, "content_type": "arthur_site"}, "type": "Document"}
{"page_content": ": 12 text : welcome to arthur scope! jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by welcome to arthur scope! as your team's data science operations center, arthur helps enterprise teams monitor, measure and optimize ai performance at scale. suggest editslooking to operationalize your machine learning systems in production? arthur's observability platform helps enterprise teams monitor, measure, and improve machine learning at scale. what happens when ai meets the real world? as the ai performance company, arthur helps data scientists, product", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "monitor, measure, and improve machine learning at scale. what happens when ai meets the real world? as the ai performance company, arthur helps data scientists, product owners, and business leaders accelerate model operations at scale. our platform monitors, measures, and improves machine learning models for better results across three core areas : accuracy, explainability, and fairness. video overview got 4 minutes? check out a video overview of our product : get started today jump into the quickstart guide or start learning about the arthur platform. updated 3 months ago what \u2019 s nextquickstarttable of contents what happens when ai meets the real world? video", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "start learning about the arthur platform. updated 3 months ago what \u2019 s nextquickstarttable of contents what happens when ai meets the real world? video overview get started today source : https : / / docs. arthur. ai / docs", "metadata": {"source": "https://docs.arthur.ai/docs", "row": 12, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 13 text : binary classification jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by binary classificationbinary classification for text models in arthursuggest editsbinary classification models predict a binary outcome ( ie. one of two potential classes ). in arthur, these models fall into the category of classification and are represented by the multiclass model type. some common examples of text binary classification are : is this email spam or not? was this review written by a human or a robot? frequently, these models output not only a yes /", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "classification are : is this email spam or not? was this review written by a human or a robot? frequently, these models output not only a yes / no answer but also a probability for each ( i. e. prob _ yes and prob _ no ). these probabilities are then categorized into yes / no based on a threshold. in these cases, during onboarding teams will supply their classification threshold and continuously track the class probabilities ( i. e. prob _ yes, prob _ no ). formatted data in arthur text binary classification models require three things to be specified in their schema", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "e. prob _ yes, prob _ no ). formatted data in arthur text binary classification models require three things to be specified in their schema : the text input, predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( text input ) probability of prediction aprobability of prediction bground truthnon - input attribute ( numeric or categorical ) ubi amor, ibi dolor. 95.", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of prediction aprobability of prediction bground truthnon - input attribute ( numeric or categorical ) ubi amor, ibi dolor. 95. 05amalelupus in fabula. 86. 14bfemale predict function and mapping these are some examples of common values teams need to onboard for their binary classification models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. there are 3 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "3 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below are common examples of the required runnable predict function ( that outputs two values, the probability of each potential class ). prediction to ground truth mappingexample prediction functionexample prediction function with transformations # # option 1 : single prediction column, single ground truth column # map predictedvalue column to its corresponding groundtruth value. # this tells arthur that the ` pred _ proba _ credit _ default ` column represents # the probability that the", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to its corresponding groundtruth value. # this tells arthur that the ` pred _ proba _ credit _ default ` column represents # the probability that the ground truth column has the value 1 pred _ to _ ground _ truth _ map _ 1 = {'pred _ proba _ credit _ default': 1 } # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 1, ) # # option 2 : multiple prediction", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 1, ) # # option 2 : multiple prediction columns, single ground truth column # map each predictedvalue attribute to its corresponding groundtruth value. pred _ to _ ground _ truth _ map _ 2 = {'pred _ 0': 0,'pred _ 1': 1 } # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 2, positive _ predicted _ attr ='pred _ 1') # # option 3 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. pred _ to _ ground _ truth _ map _ 3 = {'pred _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'} # building the model with this", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' pred _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'} # building the model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 3, positive _ predicted _ attr ='pred _ 1') # example _ entrypoint. py sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( x )", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( x ) # example _ entrypoint. py from utils import pipeline _ transformations sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( pipeline _ transformations ( x ) ) available metrics when onboarding text classification models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ing text classification models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) when teams onboard a binary classification model. find out more about these metrics in the performance metrics section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##formancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. of", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case text ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftpredict", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##kl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. for text models, however, the", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. for text models, however, the only attribute required to onboard a model is the text attribute. so, it is only possible to monitor non - input attributes for fairness in text models. metricmetric typeaccuracy ratefairnesstrue positive rate ( equal opportunity ) fairnesstrue negative ratefairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##nessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxx ( non - input attributes ) updated 3 months ago what \u2019 s nextlearn more about the model onboarding process or jump right into an", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##igationxxx ( non - input attributes ) updated 3 months ago what \u2019 s nextlearn more about the model onboarding process or jump right into an nlp onboarding quickstartmodel onboardingnlp onboardingtable of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / text - binary - classification - 1", "metadata": {"source": "https://docs.arthur.ai/docs/text-binary-classification-1", "row": 13, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 14 text : binary classification jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by binary classificationbinary classification for image models in arthur scopesuggest editsbinary classification models predict a binary outcome ( i. e., one of two potential classes ). in arthur scope, these models fall into the classification category and are represented by the multiclass model type. some common examples of image binary classification are : does this ct scan contain a tumor? is a roof in a satellite image showing signs of damage? frequently, these models output", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "examples of image binary classification are : does this ct scan contain a tumor? is a roof in a satellite image showing signs of damage? frequently, these models output both a yes / no answer and a probability for each ( i. e., prob _ yes and prob _ no ). these probabilities are then categorized into yes / no based on a threshold. in these cases, during onboarding, teams will supply their classification threshold and continuously track the class probabilities ( i. e., prob _ yes, prob _ no ). formatted data in arthur image binary classification models require three things to", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##babilities ( i. e., prob _ yes, prob _ no ). formatted data in arthur image binary classification models require three things to be specified in their schema : the image input, the predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( image input ) probability of prediction aprobability of prediction bground truthnon - input attribute ( numeric or categorical ) image _ 1.", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". attribute ( image input ) probability of prediction aprobability of prediction bground truthnon - input attribute ( numeric or categorical ) image _ 1. jpg. 95. 05amaleimage _ 2. jpg. 86. 14bfemale predict function and mapping these are some examples of common values teams need to onboard for their binary classification models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. there are 3 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "performance metrics. there are 3 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below are common examples of the required runnable predict function ( that outputs two values, the probability of each potential class ). prediction to ground truth mappingexample prediction functionexample prediction function with transformations # # option 1 : single prediction column, single ground truth column # map predictedvalue column to its corresponding groundtruth value. # this tells arthur that the ` pred _ proba _ credit _ default ` column", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# map predictedvalue column to its corresponding groundtruth value. # this tells arthur that the ` pred _ proba _ credit _ default ` column represents # the probability that the ground truth column has the value 1 pred _ to _ ground _ truth _ map _ 1 = {'pred _ proba _ credit _ default': 1 } # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 1, ) #", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 1, ) # # option 2 : multiple prediction columns, single ground truth column # map each predictedvalue attribute to its corresponding groundtruth value. pred _ to _ ground _ truth _ map _ 2 = {'pred _ 0': 0,'pred _ 1': 1 } # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 2, positive _ predicted _ attr ='pred _ 1') # # option 3 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. pred _ to _ ground _ truth _ map _ 3 = {'pred _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'}", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ map _ 3 = {'pred _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'} # building the model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 3, positive _ predicted _ attr ='pred _ 1') # example _ entrypoint. py sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( x ) # example _ entrypoint. py from utils import pipeline _ transformations sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( pipeline _ transformations ( x ) ) available metrics when onboarding tabular classification models, several default metrics are available to you within the ui. you can learn more about each specific metric", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") available metrics when onboarding tabular classification models, several default metrics are available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics when teams onboard a binary classification model, the following metrics are automatically available in the ui ( out - of - the - box ). learn more about these metrics in the performance metrics section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecifi", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##racy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. learn more about these metrics in the drift and anomaly", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "dataset is onboarded for your model, these metrics are available out of the box for comparison. learn more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case, text ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeat", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##feature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams can evaluate drift for inference data at different intervals with our python sdk and query service ( for example, data coming into the model now compared to a month ago ). fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. for text models, however", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. for text models, however, the only attribute required to onboard a model is the text attribute. so, monitoring non - input attributes for fairness in image models is only possible. metricmetric typeaccuracy ratefairnesstrue positive rate ( equal opportunity ) fairnesstrue negative ratefairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##nessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxx ( non - input attributes ) updated 3 months ago what \u2019 s nextmodel onboardingtable of contents formatted data in arthur predict function and", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tionxxx ( non - input attributes ) updated 3 months ago what \u2019 s nextmodel onboardingtable of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / image - binary - classification - 3", "metadata": {"source": "https://docs.arthur.ai/docs/image-binary-classification-3", "row": 14, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 15 text : spark ml jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by spark mlsuggest editsthis guide provides an example of integrating with the arthurai platform to monitor a sparkml model. we'll use an example dataset to train a sparkml model from scratch, but you could also use an existing spark pipeline. pythonfrom pyspark. sql import sparksession from pyspark. ml import pipeline from pyspark. ml. feature import vectorassembler import pyspark. sql", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sql import sparksession from pyspark. ml import pipeline from pyspark. ml. feature import vectorassembler import pyspark. sql. functions as f from pyspark. ml. classification import logisticregression import pandas as pd import numpy as np from arthurai import arthurai from arthurai. client. apiv3 import inputtype, outputtype, stage train and save sparkml model first, we'll instantiate a spark session and load in a sample dataset. in this example, we'll use a dataset derived from the famous boston housing dataset to", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##iate a spark session and load in a sample dataset. in this example, we'll use a dataset derived from the famous boston housing dataset to build a simple model. pythonspark = sparksession. builder. appname ('app'). getorcreate ( ) data = spark. read. csv ( '. / data / boston _ housing. csv ', header = true, inferschema = true ) train, test = data. randomsplit ( [ 0. 7, 0. 3 ] ) we'll use a lasso classification model to try to predict the", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "test = data. randomsplit ( [ 0. 7, 0. 3 ] ) we'll use a lasso classification model to try to predict the is _ expensive column from all the others. this column encodes whether or not a property value was above or below the local average. as preprocessing, we'll use the vectorassembler class to pull together the input columns into a single numeric feature vector. pythonfeature _ columns = data. columns [ : - 1 ] # here we omit the final column assembler = vectorassembler ( inputcols = feature _ columns,", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data. columns [ : - 1 ] # here we omit the final column assembler = vectorassembler ( inputcols = feature _ columns, outputcol = \" features \" ) lasso _ classifier = logisticregression ( featurescol = \" features \", labelcol = \" is _ expensive \", maxiter = 10, regparam = 0. 3, elasticnetparam = 1. 0 ) using a pipeline, we'll combine our preprocessing steps and our ml model, and we'll fit to the training data and save. if you have an existing spark pipeline,", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "combine our preprocessing steps and our ml model, and we'll fit to the training data and save. if you have an existing spark pipeline, you can load from disk. pythonpipeline = pipeline ( stages = [ assembler, lasso _ classifier ] ) fitted _ pipeline = pipeline. fit ( train ) fitted _ pipeline. write ( ). overwrite ( ). save ( '. / data / models / boston _ housing _ spark _ model _ pipeline') onboard to arthur pythonarthur = arthurai ( url ='https : / / app. arthur. ai ', login =", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ pipeline') onboard to arthur pythonarthur = arthurai ( url ='https : / / app. arthur. ai ', login = \" < your _ username _ or _ email > \", password = \" < your _ password > \" ) to onboard our model with arthur, we'll register the schema of the data coming into and out of the model. for simplicity, you can use a pandas dataframe for this step. we will take a sample of the sparkdf to the driver, and use this to register the model to arthur. pythonsample _ df = train. take", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will take a sample of the sparkdf to the driver, and use this to register the model to arthur. pythonsample _ df = train. take ( 5000 ). topandas ( ) sample _ y = sample _ df. loc [ ['is _ expensive'] ] sample _ x = sample _ df. drop ('is _ expensive ', axis = 1 ) python # instantiate basic model arthur _ model = arthur. model ( { \" partner _ model _ id \" : \" boston housing \", \" input _ type \" : inputtype. tabular, \" output _ type \" :", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" partner _ model _ id \" : \" boston housing \", \" input _ type \" : inputtype. tabular, \" output _ type \" : outputtype. multiclass, \" is _ batch \" : true } ) # use pandas dataframes to register data schema arthur _ model. from _ dataframe ( sample _ x, stage. modelpipelineinput ) arthur _ model. add _ binary _ classifier _ output _ attributes ( positive _ predicted _ attr ='expensive ', pred _ to _ ground _ truth _ map = {'prediction _ expensive':'ground _ truth", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ predicted _ attr ='expensive ', pred _ to _ ground _ truth _ map = {'prediction _ expensive':'ground _ truth _ expensive ','prediction _ cheap':'ground _ truth _ cheap'}, threshold = 0. 75 ) the from _ dataframe ( ) method will inspect your dataset and infer the input schema, datatypes, and sample statistics. you can review the model structure and see if any fixes are needed. pythonarthur _ model. review ( ) python # chas and rad were inferred as categorical, lets change those", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "fixes are needed. pythonarthur _ model. review ( ) python # chas and rad were inferred as categorical, lets change those to be continuous arthur _ model. get _ attribute ('chas ', stage. modelpipelineinput ). set ( categorical = false ) arthur _ model. get _ attribute ('rad ', stage. modelpipelineinput ). set ( categorical = false ) arthur _ model. review ( ) monitoring for bias for any attributes that you want to monitor for bias, you set the monitor _ for _ bias boolean. in fact,", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". review ( ) monitoring for bias for any attributes that you want to monitor for bias, you set the monitor _ for _ bias boolean. in fact, these don't have to be model inputs, they can also be of stage noninputdata. pythonsensitive _ attributes = [ \" gender \", \" race \", \" income _ bracket \" ] for attribute _ name in sensitive _ attributes : arthur _ model. get _ attribute ( attribute _ name, stage. modelpipelineinput ). monitor _ for _ bias = true save now you're ready to save your model and finish onboarding.", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", stage. modelpipelineinput ). monitor _ for _ bias = true save now you're ready to save your model and finish onboarding. pythonarthur _ model. save ( ) set reference data you can set a baseline dataset in order to speed up the calculation of data drift and inference anomaly scoring. this reference set is typically the training set the model was fitted to, or a subsample. you can use either a pandas dataframe or a directory of parquet files. the reference data can include model input features, ground truth features or model predictions on training sets. however, it is recommended that", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a directory of parquet files. the reference data can include model input features, ground truth features or model predictions on training sets. however, it is recommended that only model input features are provided. pythonarthur _ model. set _ reference _ data ( directory _ path = \". / data / august _ training _ data / \" ) ( sparkml _ explainability ) = enabling explainability for sparkml to enable explainability, you'll supply a python file that implements a predict ( ) function for a single observation ( a numpy array ). this predict function can contain anything you need, including loading a serialized model,", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a predict ( ) function for a single observation ( a numpy array ). this predict function can contain anything you need, including loading a serialized model, preprocessing / transformations, and making a final prediction. the returned result should be a numpy array. you'll also supply a requirements file for all the dependencies for running an inference through your model. for more details around enabling explainability, see the { doc } / user - guide / walkthroughs / explainability guide. below we provide a spark specific example. the first step is to save your sparkml model and pipeline so it can", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##throughs / explainability guide. below we provide a spark specific example. the first step is to save your sparkml model and pipeline so it can be imported for use in the predict ( ) function pythonfitted _ pipeline. write ( ). overwrite ( ). save ( '. / data / models / boston _ housing _ spark _ model _ pipeline') next is to create your predict ( ) function. python # entrypoint. py import pandas as pd import numpy as np from pyspark. sql import sparksession from pyspark. ml import pipelinemodel # to start", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s as pd import numpy as np from pyspark. sql import sparksession from pyspark. ml import pipelinemodel # to start the spark session on the model server specify the master url as local. # by default this will run spark using 1 thread, to increase threads you can specify # local [ x ] where x is the number of threads. when allocating more compute and memory to the spark # session be sure to increase the amount allocated to the model server when calling arthurmodel. enable _ explainability ( ) # in the sdk ( by default 1 cpu and 1gb of memory is", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to the model server when calling arthurmodel. enable _ explainability ( ) # in the sdk ( by default 1 cpu and 1gb of memory is allocated to the model server ). spark = sparksession. builder. master ('local'). appname ('app'). getorcreate ( ) loaded _ pipeline = pipelinemodel. load ( \". / data / models / boston _ housing _ spark _ model _ pipeline \" ) def predict ( input _ data ) : col _ names = ['crim ','zn ','indus ','chas ',", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "def predict ( input _ data ) : col _ names = ['crim ','zn ','indus ','chas ','nox ','rm ','age ','dis ','rad ','tax ','ptratio ','b ','lstat'] input _ df = pd. dataframe ( input _ data, columns = col _ names ) spark _ df = spark. createdataframe ( input _ df ) predictions = loaded _ pipeline. transform ( spark _ df ) return np. array", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ df = spark. createdataframe ( input _ df ) predictions = loaded _ pipeline. transform ( spark _ df ) return np. array ( [ float ( x. prediction ) for x in predictions. select ('prediction'). collect ( ) ] ) you are then ready to { ref } enable explainability < enabling _ explainability > pythonarthur _ model. enable _ explainability ( df = sample _ df, project _ directory = '. ', user _ predict _ function _ import _ path ='entrypoint ', requirements _ file ='requirements. txt') send", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= '. ', user _ predict _ function _ import _ path ='entrypoint ', requirements _ file ='requirements. txt') send batch of inferences once your model has been onboarded, it is ready to receive inferences and model telemetry. there are some standard inputs needed to identify inferences and batches. first, each inference needs a unique identifier so that it can later be joined with ground truth. include a column named partner _ inference _ id and ensure these ids are unique across batches. for example, if you run predictions across your customer base on a daily - batch", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "partner _ inference _ id and ensure these ids are unique across batches. for example, if you run predictions across your customer base on a daily - batch cadence, then a unique identfier could be composed of your customer _ id plus the date. second, each inference needs to be associated with a batch _ id, but this id will be shared among one or more inferences. finally, each inference needs an inference _ timestamp and these don't have to be unique. additionally, the predictions / scores from your model should match the column names in the registered schema. if we take a look above at", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "have to be unique. additionally, the predictions / scores from your model should match the column names in the registered schema. if we take a look above at arthur _ model. review ( ) we'll recall that columns we created correspond to the clasiffier's output probabilities over the classes ( \" prediction _ cheap \" and \" prediction _ expensive \" ) and the corresponding ground truth over the possible classes in one - hot form ( \" ground _ truth _ cheap \" and \" ground _ truth _ expensive \" ). we will process a batch of datapoints through the pipeline and save the inputs ( and predictions ) to", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cheap \" and \" ground _ truth _ expensive \" ). we will process a batch of datapoints through the pipeline and save the inputs ( and predictions ) to parquet. we will do the same for the ground truths. pythonloaded _ pipeline = pipelinemodel. load ( \". / data / models / boston _ housing _ spark _ model _ pipeline \" ) inferencesdf = loaded _ pipeline. transform ( test ). withcolumnrenamed ( \" probability \", \" prediction _ expensive \" ) uuidudf = udf ( lambda : str ( uuid. uuid4 ( ) ), string", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" prediction _ expensive \" ) uuidudf = udf ( lambda : str ( uuid. uuid4 ( ) ), stringtype ( ) ) inferencesdf = inferencesdf. withcolumn ('partner _ inference _ id ', uuidudf ( ) ) # add required columns inferencesdf [ \" inference _ timestamp \" ] = datetime. utcnow ( ) inferencesdf [ \" batch _ id \" ] = \" inferences _ batch _ 001 \" inference _ df [ \" partner _ inference _ id \" ] =... # write inferences", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" ] = \" inferences _ batch _ 001 \" inference _ df [ \" partner _ inference _ id \" ] =... # write inferences inferencesdf. write. parquet ( \". / data / inference _ files / inferences. parquet \" ) # write ground truths ground _ truth _ df = test. select ( [ \" ground _ truth _ cheap \", \" ground _ truth _ expensive \" ] ) ground _ truth _ df [ \" partner _ inference _ id \" ] =... ground _ truth _ df [ \" ground _ truth _ timestamp \" ] = date", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ \" partner _ inference _ id \" ] =... ground _ truth _ df [ \" ground _ truth _ timestamp \" ] = datetime. utcnow ( ) ground _ truth _ df [ \" batch _ id \" ] = \" gt _ batch _ 001 \" ground _ truth _ batch. write. parquet ( \". / data / ground _ truth _ files / ground _ truth. parquet \" ) with our model's inputs and outputs save as parquet, we upload a batch by pointing to the directory containing one or more parquet files. the directory will be traversed and all par", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "outputs save as parquet, we upload a batch by pointing to the directory containing one or more parquet files. the directory will be traversed and all parquet files will be joined into the corresponding batch. note, the model inputs and predictions will be uploaded separately from the ground truth. pythonarthur _ model. send _ bulk _ inferences ( directory _ path = '. / data / inference _ files /') you can separately upload ground truths for each inference. every row in the ground truth file ( s ) should have an external _ id column that matches any ids you create for the inferences. pythonarth", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". every row in the ground truth file ( s ) should have an external _ id column that matches any ids you create for the inferences. pythonarthur _ model. send _ bulk _ ground _ truths ( directory _ path = '. / data / ground _ truth _ files /') updated 3 months ago table of contents train and save sparkml model onboard to arthur set reference data enabling explainability for sparkml send batch of inferences source : https : / / docs. arthur. ai / docs / spark - ml", "metadata": {"source": "https://docs.arthur.ai/docs/spark-ml", "row": 15, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 16 text : appendix jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by appendixsuggest editsrunning the velero cli velero provides a command - line interface ( cli ) for taking backups and performing restores. the cli can be installed locally, or it can be invoked by kubectl exec on the velero backup controller pod. local installation refer to the velero documentation for installing velero on your platform. velero uses your kubeconfig", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backup controller pod. local installation refer to the velero documentation for installing velero on your platform. velero uses your kubeconfig file to connect to the cluster. $ velero - - help velero is a tool for managing disaster recovery, specifically for kubernetes cluster resources. it provides a simple, configurable, and operationally robust way to back up your application state and associated data. if you're familiar with kubectl, velero supports a similar model, allowing you to execute commands such as'velero get backup'and'veler", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with kubectl, velero supports a similar model, allowing you to execute commands such as'velero get backup'and'velero create schedule '. the same operations can also be performed as'velero backup get'and'velero schedule create '. usage : velero [ command ] available commands : backup work with backups backup - location work with backup storage locations bug report a velero bug client velero client related commands completion generate completion script create create velero resources debug generate debug bundle delete delete velero resources describe describe ve", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "client related commands completion generate completion script create create velero resources debug generate debug bundle delete delete velero resources describe describe velero resources get get velero resources help help about any command install install velero plugin work with plugins restic work with restic restore work with restores schedule work with schedules snapshot - location work with snapshot locations uninstall uninstall velero version print the velero version and associated image flags : - - add _ dir _ header if true, adds the file directory to the header - - alsologtostder", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lero version and associated image flags : - - add _ dir _ header if true, adds the file directory to the header - - alsologtostderr log to standard error as well as files - - colorized optionalbool show colored output in tty. overrides'colorized'value from $ home /. config / velero / config. json if present. enabled by default - - features stringarray comma - separated list of features to enable for this velero process. combines with values from $ home /. config / velero / confi", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- separated list of features to enable for this velero process. combines with values from $ home /. config / velero / config. json if present - h, - - help help for velero - - kubeconfig string path to the kubeconfig file to use to talk to the kubernetes apiserver. if unset, try the environment variable kubeconfig, as well as in - cluster configuration - - kubecontext string the context to use to talk to the kubernetes apiserver. if unset default", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "as in - cluster configuration - - kubecontext string the context to use to talk to the kubernetes apiserver. if unset defaults to whatever your current - context is ( kubectl config current - context ) - - log _ backtrace _ at tracelocation when logging hits line file : n, emit a stack trace ( default : 0 ) - - log _ dir string if non - empty, write log files in this directory - - log _ file string if non - empty, use this log file - - log _ file _ max _ size uint defines the maximum", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in this directory - - log _ file string if non - empty, use this log file - - log _ file _ max _ size uint defines the maximum size a log file can grow to. unit is megabytes. if the value is 0, the maximum file size is unlimited. ( default 1800 ) - - logtostderr log to standard error instead of files ( default true ) - n, - - namespace string the namespace in which velero should operate ( default \" velero \" ) - - skip _ headers if true, avoid header prefixes in the log messages - - skip _", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##o should operate ( default \" velero \" ) - - skip _ headers if true, avoid header prefixes in the log messages - - skip _ log _ headers if true, avoid headers when opening log files - - stderrthreshold severity logs at or above this threshold go to stderr ( default 2 ) - v, - - v level number for the log level verbosity - - vmodule modulespec comma - separated list of pattern = n settings for file - filtered logging use \" velero [ command ] - - help \" for more information about a command. executing on", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list of pattern = n settings for file - filtered logging use \" velero [ command ] - - help \" for more information about a command. executing on the velero backup controller pod if it is not possible to install the velero cli on the local workstation, you can still run velero commands directly on the velero pod as follows : shell $ velero _ namespace = \" put your velero namespace here \" $ kubectl exec velero - 699dc869d4 - r24bh - n $ velero _ namespace - c", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "kubectl exec velero - 699dc869d4 - r24bh - n $ velero _ namespace - c velero - - / velero help velero is a tool for managing disaster recovery, specifically for kubernetes cluster resources. it provides a simple, configurable, and operationally robust way to back up your application state and associated data. < < < output - truncated - for - brevity > > > backups using velero creating a backup to take a backup of arthur, you would invoke the cli as follows", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "brevity > > > backups using velero creating a backup to take a backup of arthur, you would invoke the cli as follows. shell $ arthur _ namespace = \" put your arthur namespace here \" $ velero _ namespace = \" put your velero namespace here \" $ velero backup create < some - unique - name > \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - storage - location = docs - demo - backup - location - velero listing all backup", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- namespaces = $ arthur _ namespace \\ - - storage - location = docs - demo - backup - location - velero listing all backups you can list all backups using the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup get - n $ velero _ namespace describing a backup you can get an overview of the backup using the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup describe < insert -", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##o cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup describe < insert - backup - name > - n $ velero _ namespace debugging a backup for debugging a backup, you can access the backup's logs using the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup logs < insert - backup - name > - n $ velero _ namespace head restores using velero similar to backup, restore happens using", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backup logs < insert - backup - name > - n $ velero _ namespace head restores using velero similar to backup, restore happens using the velero cli. a restore takes a backup object and then executes the restore procedure. attempting a restore you can execute a restore with the following velero cli command : shell $ velero _ namespace = \" put your velero namespace here \" $ velero restore create \\ - - from - backup < insert - backup - name > \\ - - namespace $ velero _ namespace \\ - - restore - volumes =", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\\ - - from - backup < insert - backup - name > \\ - - namespace $ velero _ namespace \\ - - restore - volumes = true listing all restore attempts just like with the backup, velero will create a restore velero resource, which you can inspect with the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero restore get - n $ velero _ namespace describing a restore attempt you can get an overview of the restore attempt using the velero cli : shell $ velero _", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lero _ namespace describing a restore attempt you can get an overview of the restore attempt using the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero restore describe < insert - restore - name > - n $ velero _ namespace debugging a restore attempt for debugging a restore attempt, you can access the logs using the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero restore logs < insert - restore - name > - n", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "velero _ namespace = \" put your velero namespace here \" $ velero restore logs < insert - restore - name > - n $ velero _ namespace head running backups on a schedule there are two ways the arthur platform can be backed up on a schedule : scripting the entire backup process { ref } ( see example ) < scripted _ solution >, and executing it on a fixed schedule from a job runner ( jenkins, gitlab - ci etc. ) leveraging native schedulers to back up the individual components of the platform : clickhouse is backed up at midnight ( by default )", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ab - ci etc. ) leveraging native schedulers to back up the individual components of the platform : clickhouse is backed up at midnight ( by default ) using kubernetes cronjobs out - of - the - box. use velero schedules to create velero backups : messaging infrastructure shell # you need to configure this by getting the name of the backup storage location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` $ storage _ location = \" put your storage", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or ` kubectl get backupstoragelocation - n < velero - namespace > ` $ storage _ location = \" put your storage location here \" $ arthur _ namespace = \" put your arthur namespace here \" $ velero _ namespace = \" put your velero namespace here \" $ velero schedule create messaging - infra - backup - nightly \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - selector ='app in ( cp - zookeeper, cp - kafka )", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- include - namespaces = $ arthur _ namespace \\ - - selector ='app in ( cp - zookeeper, cp - kafka )'\\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, services, endpoints, configmaps, poddisruptionbudgets -", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##itions. apiextensions. k8s. io, services, endpoints, configmaps, poddisruptionbudgets - - storage - location = $ storage _ location \\ - - schedule \" 0 0 * * * \" \\ - - ttl 720h0m0s enrichments ( infrastructure and workflows ) shell # you need to configure this by getting the name of the backup storage location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` $ storage", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` $ storage _ location = \" put your storage location here \" $ arthur _ namespace = \" put your arthur namespace here \" $ velero _ namespace = \" put your velero namespace here \" $ velero schedule create enrichments - workflows - backup - nightly \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - include - resources = workflows \\", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "$ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - include - resources = workflows \\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ -", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ - - schedule \" 0 0 * * * \" \\ - - ttl 720h0m0s $ velero schedule create qa - enrichments - infra - backup - nightly \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - selector ='component in ( kafka - mover - init - connector, model _ server )'\\ - - include - resources", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\\ - - selector ='component in ( kafka - mover - init - connector, model _ server )'\\ - - include - resources = deployments, services \\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location =", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##resourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ - - schedule \" 0 0 * * * \" \\ - - ttl 720h0m0s rds databases can be automatically backed up on a schedule, not at a specific point in time but within a 30 - minute window. and during this window, the database is snapshotted at a random time. due to this limitation from aws, ensure there are no operations ( like model crud, etc. ) on the arthur platform during the", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a random time. due to this limitation from aws, ensure there are no operations ( like model crud, etc. ) on the arthur platform during the backup window. shell $ aws rds modify - db - instance \\ - - db - instance - identifier rds _ db _ name \\ - - backup - retention - period 14 \\ - - preferred - backup - window 23 : 45 - 00 : 15 \\ - - profile aws _ profile _ name \\ - - region aws _ region \\ - - apply - immediately sample backup script ( manual ) the following script can be used to run all the backup steps", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\\ - - region aws _ region \\ - - apply - immediately sample backup script ( manual ) the following script can be used to run all the backup steps together : shell #! / bin / bash set - euo pipefail ifs = $'\\ n \\ t'# you need to configure this by getting the name of the backup storage location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` storage _ location = \" put your storage location here \" arthur _ namespace =", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backupstoragelocation - n < velero - namespace > ` storage _ location = \" put your storage location here \" arthur _ namespace = \" put your arthur namespace here \" velero _ namespace = \" put your velero namespace here \" backup _ date = $ ( date + % y - % m - % d - % h - % m - % s ) ; name = arthur - backup - $ backup _ date echo \" creating a new backup with name $ name \" echo \" taking a backup of ch data \" kubectl create job $ name - clickhouse - backup \\", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "creating a new backup with name $ name \" echo \" taking a backup of ch data \" kubectl create job $ name - clickhouse - backup \\ - - namespace = $ arthur _ namespace \\ - - from = cronjob / clickhouse - backup - cronjob ch _ backup _ jobname = $ ( kubectl get jobs - o name - n \" $ arthur _ namespace \" grep \" $ name - clickhouse - backup \" ) kubectl wait $ ch _ backup _ jobname \\ - - namespace = $ arthur _ namespace \\ - - for =", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- backup \" ) kubectl wait $ ch _ backup _ jobname \\ - - namespace = $ arthur _ namespace \\ - - for = condition = complete \\ - - timeout = 30m echo \" taking a backup of the enrichments infrastructure \" velero backup create $ name - enrichments \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - selector ='component in ( kafka - mover - init - connector, model _ server )'\\ - - include - resources = deployments, services", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' component in ( kafka - mover - init - connector, model _ server )'\\ - - include - resources = deployments, services \\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ - - wait echo \" taking a backup of workflows \" velero backup create $ name - workflows \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - include - resources = workflows \\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ - - wait echo \" taking a backup of kafka / kafka - zk statefulsets, their ebs volumes, and related components \" veler", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "echo \" taking a backup of kafka / kafka - zk statefulsets, their ebs volumes, and related components \" velero backup create $ name - messaging \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - selector ='app in ( cp - zookeeper, cp - kafka )'\\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerre", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, services, endpoints, configmaps, poddisruptionbudgets \\ - - storage - location = $ storage _ location \\ - - wait echo \" taking a backup of the rds database \" aws rds create - db - cluster - snapshot \\ - - db -", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\\ - - wait echo \" taking a backup of the rds database \" aws rds create - db - cluster - snapshot \\ - - db - cluster - snapshot - identifier $ name - snapshot \\ - - db - cluster - identifier rds _ db _ name \\ - - profile aws _ profile _ name \\ - - region aws _ region updated 3 months ago table of contents running the velero cli local installation executing on the velero backup controller pod backups using velero creating a backup listing all backups describing a backup debugging a backup restores using", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "on the velero backup controller pod backups using velero creating a backup listing all backups describing a backup debugging a backup restores using velero attempting a restore listing all restore attempts describing a restore attempt debugging a restore attempt running backups on a schedule sample backup script ( manual ) source : https : / / docs. arthur. ai / docs / appendix", "metadata": {"source": "https://docs.arthur.ai/docs/appendix", "row": 16, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 17 text : installation overview jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by installation overviewsuggest editsarthur scope can be installed to existing kubernetes clusters ( k8s ) or virtual machines ( vm ). for both kubernetes and virtual machine deployments, arthur supports two types of installation methods, online and airgapped. the online method allows fast and continuous differential upgrades. the airgapped install supports environments where the cluster has no outbound internet connectivity. updated 3 months ago source :", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "online method allows fast and continuous differential upgrades. the airgapped install supports environments where the cluster has no outbound internet connectivity. updated 3 months ago source : https : / / docs. arthur. ai / docs / on - prem - deployment", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment", "row": 17, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 18 text : installing kubernetes jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by installing kubernetessuggest editsthis section covers the steps required for installing arthur on a kubernetes cluster. there are separate steps required for online and airgapped installations. an additional set of instructions are also available for installing arthur that \u2019 s scoped within a k8s namespace. updated 3 months ago source : https : / / docs. arthur. ai / docs / installing - kubernet", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a k8s namespace. updated 3 months ago source : https : / / docs. arthur. ai / docs / installing - kubernetes", "metadata": {"source": "https://docs.arthur.ai/docs/installing-kubernetes", "row": 18, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 19 text : backing up the arthur platform jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by backing up the arthur platformsuggest editsonce all the pre - requisites have been met, the various arthur platform components can be backed up. the process to manually backup individual components is detailed below, which may also be { ref } scripted < scripted _ solution >. backing up clickhouse data by default, the arthur platform ships with a kubernetes cronjob, which backs up clickhouse daily at midnight.", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backing up clickhouse data by default, the arthur platform ships with a kubernetes cronjob, which backs up clickhouse daily at midnight. to manually back up clickhouse data, you can run the following commands : shellarthur _ namespace = \" put your arthur namespace here \" $ kubectl get cronjobs - n $ arthur _ namespace grep - i clickhouse name schedule suspend active last schedule age clickhouse - backup - cronjob 0 0 * * * false 0 14h 2d18h $ kubectl create job clickhouse - backup - - from =", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- cronjob 0 0 * * * false 0 14h 2d18h $ kubectl create job clickhouse - backup - - from = cronjob / clickhouse - backup - cronjob - n $ arthur _ namespace job. batch / clickhouse - backup created $ kubectl get jobs - n $ arthur _ namespace name completions duration age clickhouse - backup - cronjob - 27735840 1 / 1 8m35s 14m backing up enrichments the arthur platform uses velero to take a backup of the enrichments infrastructure and the enrichments", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ 1 8m35s 14m backing up enrichments the arthur platform uses velero to take a backup of the enrichments infrastructure and the enrichments workflows. the enrichments infrastructure and enrichment workflows are orchestrated as separate backups and will require running 2 separate commands. backing up enrichments infrastructure to manually back up the enrichments infrastructure, run the following commands : shell # you need to configure this by getting the name of the backup storage location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` storage _ location = \" put your storage location here \" arthur _ namespace = \" put your arthur namespace here \" velero _ namespace = \" put your velero namespace here \" $ backup _ date = $ ( date + % y - % m - % d - % h - % m - % s ) ; $ name = arthur - backup - $ backup _ date arthur _ namespace = < insert - arthur - names", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "% h - % m - % s ) ; $ name = arthur - backup - $ backup _ date arthur _ namespace = < insert - arthur - namespace - here > velero _ namespace = < insert - velero - namespace - here > $ velero backup create $ name - enrichments \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - selector ='component in ( kafka - mover - init - connector, model _ server )'\\ - - include - resources = deployments", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "selector ='component in ( kafka - mover - init - connector, model _ server )'\\ - - include - resources = deployments, services \\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##finitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ - - wait backing up enrichments workflows to manually back up the enrichments workflows, run the following commands : shell # you need to configure this by getting the name of the backup storage location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` storage _ location = \" put your storage location here \" arthur _", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ctl get backupstoragelocation - n < velero - namespace > ` storage _ location = \" put your storage location here \" arthur _ namespace = \" put your arthur namespace here \" velero _ namespace = \" put your velero namespace here \" $ backup _ date = $ ( date + % y - % m - % d - % h - % m - % s ) ; $ name = arthur - backup - $ backup _ date velero backup create $ name - workflows \\ - - namespace = $ velero _ namespace \\ - - include -", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backup _ date velero backup create $ name - workflows \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - include - resources = workflows \\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io,", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, secrets, configmaps \\ - - storage - location = $ storage _ location \\ - - wait backing up messaging infrastructure the arthur platform uses velero to take a backup of the kafka ( and zookeeper ) deployment state and ebs volumes. to manually back up kafka, run the following commands : shell # you need to configure this by getting the name of the backup storage location # eg : ` velero backup -", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "following commands : shell # you need to configure this by getting the name of the backup storage location # eg : ` velero backup - location get ` or ` kubectl get backupstoragelocation - n < velero - namespace > ` storage _ location = \" put your storage location here \" arthur _ namespace = \" put your arthur namespace here \" velero _ namespace = \" put your velero namespace here \" $ backup _ date = $ ( date + % y - % m - % d - % h - % m - % s ) ; $", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##pace here \" $ backup _ date = $ ( date + % y - % m - % d - % h - % m - % s ) ; $ name = arthur - backup - $ backup _ date $ velero backup create $ name - messaging \\ - - namespace = $ velero _ namespace \\ - - include - namespaces = $ arthur _ namespace \\ - - selector ='app in ( cp - zookeeper, cp - kafka )'\\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles.", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##fka )'\\ - - exclude - resources = clusterrolebindings. rbac. authorization. k8s. io, clusterroles. rbac. authorization. k8s. io, controllerrevisions. apps, endpointslices. discovery. k8s. io, customresourcedefinitions. apiextensions. k8s. io, services, endpoints, configmaps, poddisruptionbudgets \\ - - storage - location = $ storage _ location \\ - - wait backing up rds postgres rds database backups are called snaps", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##budgets \\ - - storage - location = $ storage _ location \\ - - wait backing up rds postgres rds database backups are called snapshots. to manually create a snapshot of an rds database, execute the below script : shell $ backup _ date = $ ( date + % y - % m - % d - % h - % m - % s ) ; $ name = arthur - backup - $ backup _ date $ aws rds create - db - cluster - snapshot \\ - - db - cluster - snapshot - identifier $ name - snapshot \\ - - db -", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s create - db - cluster - snapshot \\ - - db - cluster - snapshot - identifier $ name - snapshot \\ - - db - cluster - identifier rds _ db _ name \\ - - profile aws _ profile _ name \\ - - region aws _ region [UNK] compatibilitythe command is only compatible for a multi - region rds database cluster. if you are using a single - region rds database, the command to use is aws rds create - db - snapshot. for more information, please refer to the aws documentation : multi - region rds cluster and aws cl", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s rds create - db - snapshot. for more information, please refer to the aws documentation : multi - region rds cluster and aws cli command single - region rds instance and aws cli command updated 3 months ago table of contents backing up clickhouse data backing up enrichments backing up enrichments infrastructure backing up enrichments workflows backing up messaging infrastructure backing up rds postgres source : https : / / docs. arthur. ai / docs / backing - up - the - arthur - platform", "metadata": {"source": "https://docs.arthur.ai/docs/backing-up-the-arthur-platform", "row": 19, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 20 text : token sequence ( llm ) jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by token sequence ( llm ) suggest editsgenerative text model outputs are computer - generated text that mimics human language patterns and structures based on patterns learned from a large dataset. in arthur, these models are listed under the token sequence model type. some common examples of generative text models are : headline summarization question and answering ( chatbots ) formatted data in arthur depending on how you built your generative model and what", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tive text models are : headline summarization question and answering ( chatbots ) formatted data in arthur depending on how you built your generative model and what you are looking to track, there are different types of data that you can track in the platform. attribute ( user text input ) output ( text output ) output likelihood ( token likelihood ) ( optional ) non input attribute ( numeric or categorical ) dulce est desipere in locoacta, non verba [ { \" acta \" : 0. 7, \", \" : 0. 3, \" non \" : 0. 8, \" verba \" :", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ { \" acta \" : 0. 7, \", \" : 0. 3, \" non \" : 0. 8, \" verba \" : 0. 34 } ] political topicsi vis amari amacastigat ridendo mores [ { \" cast \" : 0. 56, \" igat \" : 0. 4, \" ridendo \" : 0. 24, \" mores \" : 0. 67 } ] entertainment topic predict function and mapping teams need to specify the relationship between the prediction and ground truth columns to help set up their arthur model's environment to calculate default performance metrics. here is an", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "need to specify the relationship between the prediction and ground truth columns to help set up their arthur model's environment to calculate default performance metrics. here is an example of what that might look like : registering predict / gt mapping # # registering columns arthur _ model. build _ token _ sequence _ model ( input _ column = \" user _ input \", output _ text _ column = \" output _ text \" # # optional for model types with token likelihood output _ likelihood _ column = \" token _ likelihoods \" ) available metrics when onboarding token sequence models, you have a number of default metrics available to you within the ui", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= \" token _ likelihoods \" ) available metrics when onboarding token sequence models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) when teams onboard a token sequence model. find out more about how to use these metrics in the performance metrics section. for metric definitions, check out the glossary. metricmetric typeaverage token likelihoodperformancelikelihood stabilityper", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in the performance metrics section. for metric definitions, check out the glossary. metricmetric typeaverage token likelihoodperformancelikelihood stabilityperformanceaverage sequence lengthperformanceinference countingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. some things of note : for unstructured data types ( like text and image ), feature drift is calculated for", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics in the drift and anomaly section. some things of note : for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. additionally, generative text models create text input and output that can be tracked with multivariate drift. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftmultivariate drift for prompts ( text input ) multivariate driftmultivariate drift for predictions ( text output ) multivariate drift note : teams can", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##iate drift for prompts ( text input ) multivariate driftmultivariate drift for predictions ( text output ) multivariate drift note : teams can evaluate drift for inference data at different intervals with our python sdk and query service ( for example, data coming into the model now compared to a month ago ). user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / generative - llm", "metadata": {"source": "https://docs.arthur.ai/docs/generative-llm", "row": 20, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 21 text : airgap kubernetes cluster ( k8s ) install jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metric", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##multiclass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##csamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfi", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplat", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesback", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by airgap kubernetes cluster ( k8s ) installsuggest editsensure your k8s cluster is ready for arthur platform installation by following installing arthur pre - requisites guide. preparing container registries prepare your private container image registry for arthur artifacts by creating the following list of repositories : admin console : arthurai / dex arthurai / kotsadm", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "private container image registry for arthur artifacts by creating the following list of repositories : admin console : arthurai / dex arthurai / kotsadm arthurai / kotsadm - migrations arthurai / local - volume - fileserver arthurai / local - volume - provider arthurai / minio arthurai / postgres application : arthurai / alert - service arthurai / alpine arthurai / api - service arthurai / argocli arthurai / argoexec arthurai / aws - cli arthurai / beta - client arthurai / busybox arthurai / clickhouse - operator", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ai / argoexec arthurai / aws - cli arthurai / beta - client arthurai / busybox arthurai / clickhouse - operator arthurai / clickhouse - server arthurai / client arthurai / cp - kafka arthurai / cp - kafka - connect arthurai / cp - schema - registry arthurai / cp - zookeeper arthurai / custom - hpa arthurai / dataset - service arthurai / ingestion - service arthurai / kafka - connect - monitor arthurai / kafka - exporter arthurai / kafka - prometheus", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ion - service arthurai / kafka - connect - monitor arthurai / kafka - exporter arthurai / kafka - prometheus - jmx - exporter arthurai / kubectl arthurai / mc arthurai / metric - service arthurai / metrics - exporter arthurai / minio arthurai / model - server arthurai / model - server - controller arthurai / postgresql arthurai / python - jobs arthurai / python - spark - jobs arthurai / pytorch - jobs arthurai / query - service arthurai / redis arthurai / scala", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthurai / python - spark - jobs arthurai / pytorch - jobs arthurai / query - service arthurai / redis arthurai / scala - spark - jobs arthurai / schema - service arthurai / workflow - controller arthurai / zookeeper - exporter as an example, here's how you can create a new arthurai / alert - service repository on aws ecr. shellexport aws _ region = < your _ region > aws ecr create - repository - - repository - name = arthurai / alert - service download installation files go to the download portal using the url", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> aws ecr create - repository - - repository - name = arthurai / alert - service download installation files go to the download portal using the url and the password provided by arthur. select the \" bring my own cluster \" option click the \u201c download license \u201d button to download your license in the yaml file. download the \" kots airgap bundle \" and the \" arthur airgap bundle \". setup for installation make sure you're in the correct kubectl environment context before running the installer. shellkubectl config current - context install the kots kubectl extension", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "kubectl environment context before running the installer. shellkubectl config current - context install the kots kubectl extension on your local machine : shellcurl https : / / kots. io / install bash if the linux workstation you're running kubectl from is also in the airgap environment ; download the \" kots cli \" from the download portal and install it like below : shelltar zxf kots _ linux _ amd64. tar. gz # move it to a location that's on your path sudo mv kots /", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##f kots _ linux _ amd64. tar. gz # move it to a location that's on your path sudo mv kots / usr / local / bin / kubectl - kots [UNK] \" kots cli \" and \" kots airgap bundle \" must be installed at the same time and therefore will be on the same version. if your workstation is a mac, you can download the latest kots cli darwin binary version from https : / / kots. io /. start installation push the admin console images to your private registry : shellkubectl ko", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "binary version from https : / / kots. io /. start installation push the admin console images to your private registry : shellkubectl kots admin - console push - images. / kotsadm. tar. gz [ your private registry host ] / arthurai \\ - - registry - username [ read - write username ] \\ - - registry - password [ read - write password ] as an option, you can also pre - upload the application images to your private registry before running the installer : shellkubectl kots admin - console push - images. / arthur - x", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the application images to your private registry before running the installer : shellkubectl kots admin - console push - images. / arthur - x. x. x. airgap [ your private registry host ] / arthurai \\ - - registry - username [ read - write username ] \\ - - registry - password [ read - write password ] install the admin console ( see here for { doc } namespace - scoped installs < k8s _ install _ namespace _ scoped > ) : shellkubectl kots install arthur \\ - - no - port - forward \\ -", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s _ install _ namespace _ scoped > ) : shellkubectl kots install arthur \\ - - no - port - forward \\ - - namespace arthur \\ - - shared - password [ provide an admin console password ] \\ - - kotsadm - namespace arthurai \\ - - kotsadm - registry [ your private container image repository ] \\ - - registry - username [ read - write username ] \\ - - registry - password [ read - write password ] create a port forwarding tunnel to admin console. go to http : / / localhost : 8800 to access the", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ read - write password ] create a port forwarding tunnel to admin console. go to http : / / localhost : 8800 to access the admin console : shellkubectl kots admin - console - - namespace arthur follow the instructions on the admin console to complete your installation by providing the private registry details and arthur - x. x. x. airgap bundle. [UNK] upload process can take couple of hours so ensure your laptop does not go to sleep. you may follow the instructions airgap kubernetes cluster ( k8s ) install with cli to install the", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "laptop does not go to sleep. you may follow the instructions airgap kubernetes cluster ( k8s ) install with cli to install the admin console and arthur app programmatically using the cli only configure arthur. verify installation monitor the admin console dashboard for the application status to become ready. to see the progress of the deployment, monitor the deployment status with thekubectl cli : shellkubectl get deployment, statefulset, pod - n arthur if anything is showing pending, it is likely you need to add more / bigger nodes to your cluster. customize", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", statefulset, pod - n arthur if anything is showing pending, it is likely you need to add more / bigger nodes to your cluster. customize installation configure graphs on admin console by clicking on the configure prometheus address button and providing your prometheus endpoint ( e. g., http : / / kube - prometheus - stack - prometheus. monitoring. svc. cluster. local : 9090 ). updated 3 months ago table of contents preparing container registries download installation files setup for installation start installation verify installation customize installation source : https : /", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##90 ). updated 3 months ago table of contents preparing container registries download installation files setup for installation start installation verify installation customize installation source : https : / / docs. arthur. ai / docs / airgap - kubernetes - cluster - k8s - install", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install", "row": 21, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 22 text : text jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by textsuggest editstext input models are a type of machine learning model that operates on text data, such as natural language text found in documents, emails, social media posts, and other forms of written communication. these models are designed to learn the patterns and relationships between words and phrases in order to perform tasks such as sentiment analysis, language translation, and text classification. text input models can be built using a variety of techniques, including neural networks, decision trees", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "perform tasks such as sentiment analysis, language translation, and text classification. text input models can be built using a variety of techniques, including neural networks, decision trees, and support vector machines. embeddings in arthur text models in arthur take in text input, i. e. raw text of the documents or social media posts teams are using to predict. while there are a few enrichments ( namely anomaly detection ) that use model embeddings, arthur computes these embeddings internally. currently, arthur does not take in embeddings or vector inputs. tokenization in arthur text inputs are tokenized in arthur", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "embeddings internally. currently, arthur does not take in embeddings or vector inputs. tokenization in arthur text inputs are tokenized in arthur for both anomaly detection and explainability. to create these tokens, arthur has a few different text delimiters. text delimiters here are the different delimiters available in arthur. namedefined in arthurdescriptioncomma \", \" splits on a single comma. comma _ plus \", + \" splits on one or more commas. not _ word \" \\ w + \" splits on any character that is not a word. pipe \" \" splits", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "+ \" splits on one or more commas. not _ word \" \\ w + \" splits on any character that is not a word. pipe \" \" splits on a single pipe. pipe _ plus \" + splits on one or more pipes. whitespace \" \\ s + \" splits on whitespace. updated 3 months ago table of contents embeddings in arthur tokenization in arthur source : https : / / docs. arthur. ai / docs / text", "metadata": {"source": "https://docs.arthur.ai/docs/text", "row": 22, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 23 text : creating a connection to arthur jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 jump tointroductioncreating a connection to arthurarthur api reference", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 jump tointroductioncreating a connection to arthurarthur api referencearthur sdk clientarthur sdk documentationtiontest explainability locallypowered by jump tointroductioncreating a connection to arthurarthur api referencearthur sdk clientarthur sdk documentationtiontest explainability locallypowered by creating a connection to arthurthis page will help you get started with arthur api. there are a few ways to create a connection to arthur. creating an api key to access the arthur platform outside of arthur, you specify your access", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur api. there are a few ways to create a connection to arthur. creating an api key to access the arthur platform outside of arthur, you specify your access rights. one way to do that is through an api key. to get an api key from arthur, users need to go into the settings of their organizational dashboard. from there, they need to click on the api keys tab and click the create api key button. [UNK] key = = header in api exampleswhen using the api example structure in this documentation, you should place your api key in the header section of the call you are trying to build. connecting to a specific arthur", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the api example structure in this documentation, you should place your api key in the header section of the call you are trying to build. connecting to a specific arthur model within the sdk after connecting to your arthur instance, whether you are using the api or arthur python sdk, it is common for teams to want to connect to a specific arthur model. there are two ways to identify a specific model within arthur, shown below : partner model id : the partner model id is an id given to the model by the customer team / user that onboarded the model. this can be found in the ui under the details section of your model.", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "id given to the model by the customer team / user that onboarded the model. this can be found in the ui under the details section of your model. arthur model id : the arthur model id is a unique id given to the model by the arthur platform. this can be found in your arthur instance in the url of your model. connecting to the python sdk the arthur python sdk, discussed in more detail here, is one of the most common ways teams interact with the arthur platform in a notebook environment. connect to platform when connecting to the arthur platform with the sdk, there are two api keyusername passwordim", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the arthur platform in a notebook environment. connect to platform when connecting to the arthur platform with the sdk, there are two api keyusername passwordimport arthurai as arthurai arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \", password = \" < your _ password > \" ) import arthurai as arthurai arthur = arthurai ( url = \" your _ organizaions _ arthur _ url \", login = \" username \", password = \" password \" ) specifying organization", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##l = \" your _ organizaions _ arthur _ url \", login = \" username \", password = \" password \" ) specifying organization id if you log in with your username and password and are a member of multiple organizations within arthur, you must specify which organization you want to connect to within the notebook. this can be done by adding the organization _ id to your connection. pythonimport arthurai as arthurai connection = arthurai ( url = \" your _ organizaions _ arthur _ url \", login = \" username \", password = \" password \", organization _ id =", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= \" your _ organizaions _ arthur _ url \", login = \" username \", password = \" password \", organization _ id = \" org _ id \" ) you should contact your system administrator if you are unsure what your organization id is. you can also see a list of all possible organization ids you have access to in a notebook by running this python script : pythonconnection = arthurai ( url = \" your _ organizaions _ arthur _ url \", login = \" username \", password = \" password \", verify _ ssl = false ) import requests headers", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ arthur _ url \", login = \" username \", password = \" password \", verify _ ssl = false ) import requests headers = {'content - type':'application / json ','authorization': connection. client. session. headers ['authorization'] } session = requests. session ( ) response = session. get ( \" https : / / < your _ organizations _ url > / api / v3 / organizations? page _ size = 1000000 \", headers = headers, verify = false ) print ( response. json ( ) ) connect", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ organizations? page _ size = 1000000 \", headers = headers, verify = false ) print ( response. json ( ) ) connect to specific arthur model either of these can be used to connect to arthur with this code : python # # using partner model id arthur _ model = connection. get _ model ( place _ partner _ id, id _ type = \" partner _ model _ id \" ) # # using arthur model id arthur _ model = connection. get _ model ( place _ model _ id, id _ type = \" id \" ) connecting to the api to connect to the api, users have the", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". get _ model ( place _ model _ id, id _ type = \" id \" ) connecting to the api to connect to the api, users have the same permissions as above. source : https : / / docs. arthur. ai / reference", "metadata": {"source": "https://docs.arthur.ai/reference", "row": 23, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 24 text : kubernetes cluster ( k8s ) install with namespace scope privileges jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelco", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ry classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manually", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsde", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installation", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##atform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisite", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ing platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by kubernetes cluster ( k8s ) install with namespace scope privilegessuggest editsif you would like to install the arthur platform with namespace scoped privileges, there are certain components that will fail since they will need cluster - level access. these cluster - level components are crds ( custom resource definitions ) which are required for the proper functioning and operation of the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will need cluster - level access. these cluster - level components are crds ( custom resource definitions ) which are required for the proper functioning and operation of the arthur platform. however, these crds only need to be installed once with cluster - admin privileges, and elevated access is not required for normal usage of the platform. [UNK] the admin console and arthur application can be installed at the cluster - level or namespace - scope independent of each other. crds leveraged by arthur platform the arthur platform makes use of the following two crds : argo workflows : kubernetes - native open - source workflow manager", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur platform the arthur platform makes use of the following two crds : argo workflows : kubernetes - native open - source workflow manager clickhouse operator : column - oriented olap datastore installing admin console within a namespace by default, the admin console is installed at the cluster level, available in all namespaces. if you would like to install the admin console only within a specific namespace, you can use the following flag to the kots command : shellkubectl kots install arthur \\ - - use - minimal - rbac - - skip - rbac - check", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to the kots command : shellkubectl kots install arthur \\ - - use - minimal - rbac - - skip - rbac - check since the admin console will not have access to the cluster, certain preflight checks will fail. the cluster admin is responsible for ensuring sufficient resources are provisioned with the correct version of k8s. installing cluster - level crds for arthur from nexus since the arthur platform requires crds for normal operation, these will need to be installed by the cluster admin before installing arthur itself, in no particular order. the instructions below show how to download the crd charts from", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will need to be installed by the cluster admin before installing arthur itself, in no particular order. the instructions below show how to download the crd charts from our publicly hosted repository. argo workflows : shellhelm repo add arthurai - released https : / / repository. arthur. ai / repository / charts - - username < nexus - username > - - password < nexus - password > helm install argo - workflows - crd arthurai - released / argo - workflows - crd - - version 0. 19. 1 - arthur - 1 clickhouse operator : shellhelm repo", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- released / argo - workflows - crd - - version 0. 19. 1 - arthur - 1 clickhouse operator : shellhelm repo add arthurai - released https : / / repository. arthur. ai / repository / charts - - username < nexus - username > - - password < nexus - password > helm install clickhouse - operator - crd arthurai - released / clickhouse - operator - crd - - version 0. 19. 2 - arthur - 1 [UNK] reach out to our sales team if you do not have credentials to our nexus repository. installing cluster - level crds for arthur from", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "2 - arthur - 1 [UNK] reach out to our sales team if you do not have credentials to our nexus repository. installing cluster - level crds for arthur from airgap bundle if you are in an air - gapped environment with no access to the public internet, the crd charts are also available in the airgap bundle provided to you. the instructions below show how to extract the charts from the airgap bundle. shelltar - xvf arthur - < version >. airgap cd arthur - < version > tar - xvf app. tar. gz cd app argo workflows : shellhelm install", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ">. airgap cd arthur - < version > tar - xvf app. tar. gz cd app argo workflows : shellhelm install argo - workflows - crd argo - workflows - crd - 0. 14. 0 - arthur - 2. tgz clickhouse operator : shellhelm install clickhouse - operator - crd arthurai - released / clickhouse - operator - crd - 0. 18. 4 - arthur - 2. tgz you can verify the crds have been installed successfully by executing the following command : shellkubectl get crd", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- arthur - 2. tgz you can verify the crds have been installed successfully by executing the following command : shellkubectl get crd grep - ie'argoclickhouse'now that we have the prereqs installed with elevated access, we can now switch over to namespace - scoped access to complete the installation either using the admin console using the admin console or using the cli. updated 3 months ago table of contents crds leveraged by arthur platform installing admin console within a namespace installing cluster - level crds for arthur from nexus installing cluster - level crds", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "contents crds leveraged by arthur platform installing admin console within a namespace installing cluster - level crds for arthur from nexus installing cluster - level crds for arthur from airgap bundle source : https : / / docs. arthur. ai / docs / kubernetes - cluster - k8s - install - with - namespace - scope - privileges", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges", "row": 24, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 25 text : externalizing the relational database jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by externalizing the relational databasesuggest editsif desired, you can bring your own postgres instance to use as your arthur's relational database. follow the steps on this page to prepare your postgres instance. first, deploy your postgres instance in your desired environment with appropriate ingress firewall configuration. create databases for the arthur platform. create database arthurai create database alert _ service ; create database dataset _ service", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "environment with appropriate ingress firewall configuration. create databases for the arthur platform. create database arthurai create database alert _ service ; create database dataset _ service ; create database metric _ service ; - - for stand alone instance create user arthurai with password'supersecret'; - - for rds instance create role arthurai with password'supersecret'login ; revoke all privileges on database postgres from arthurai ; grant all privileges on database arthurai to arthurai ; grant all privileges on database alert _ service to arthurai ; grant all privileges on database dataset _ service to arthurai ;", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "database arthurai to arthurai ; grant all privileges on database alert _ service to arthurai ; grant all privileges on database dataset _ service to arthurai ; grant all privileges on database metric _ service to arthurai ; if you have been using the embedded database and you wish to switch to using an external postgres, backup the embedded database and restore it to the new external postgres with pg _ dump and pg _ restore. connecting to the database using ssl / tls if your postgres instance supports ssl / tls connections, and you want to connect to your external database with an encrypted connection", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "tls if your postgres instance supports ssl / tls connections, and you want to connect to your external database with an encrypted connection, you simply need to set database ssl mode in the initial configuration. by default, this is set to disable. however, you can enable an encrypted connection using the value require. { note } an externally managed postgres instance is strongly recommended for production - grade installs. updated 3 months ago table of contents connecting to the database using ssl / tls source : https : / / docs. arthur. ai / docs / externalizing", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ago table of contents connecting to the database using ssl / tls source : https : / / docs. arthur. ai / docs / externalizing - the - relational - database", "metadata": {"source": "https://docs.arthur.ai/docs/externalizing-the-relational-database", "row": 25, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 26 text : regression jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by regressionimage regression models within arthursuggest editsregression models predict a numeric outcome. in arthur, these models are listed under the regression model type. some common examples of image regression are : predict user age based on a photograph predict house price from the image of the home formatted data in arthur image regression models require two columns : image input and numeric output. when onboarding a reference dataset ( and setting a model schema )", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data in arthur image regression models require two columns : image input and numeric output. when onboarding a reference dataset ( and setting a model schema ), you need to specify a target column for each inference's ground truth. many teams also choose to onboard metadata for the model ( i. e., any information you want to track about your inferences ) as non - input attributes. attribute ( image input ) prediction ( numeric ) ground truth ( numeric ) non - input attribute ( numeric or categorical ) image _ 1. jpg45. 3462. 42high school educationimage _", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") non - input attribute ( numeric or categorical ) image _ 1. jpg45. 3462. 42high school educationimage _ 2. jpg55. 153. 2graduate degree predict function and mapping these are some examples of common values teams need to onboard for their regression models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth mappingexample prediction function # # single column ground truth output _ mapping = {'prediction _ column':'gt _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping ) # # example prediction function for binary classification def predict ( x ) : return model. predict ( x ) available metrics when onboarding regression models", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ mapping ) # # example prediction function for binary classification def predict ( x ) : return model. predict ( x ) available metrics when onboarding regression models, several default metrics are available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a regression model. learn more about these metrics in the performance metrics section. metricmetric typeroot mean squared errorperformancemean absolute errorper", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "regression model. learn more about these metrics in the performance metrics section. metricmetric typeroot mean squared errorperformancemean absolute errorperformancer squaredperformanceinference countingestionaverage predictioningestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. learn more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case, text ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note :", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##feature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams can evaluate drift for inference data at different intervals with our python sdk and query service ( for example, data coming into the model now compared to a month ago ). user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / image - regression - 2", "metadata": {"source": "https://docs.arthur.ai/docs/image-regression-2", "row": 26, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 27 text : generative text jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by generative textsuggest editsthis page discusses the basics of setting up a generative text model and onboarding it to arthur scope to monitor generative performance. getting started the first step is to import functions from the arthurai package and establish a connection with arthur. python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \" ) preparing data for arthur arthur scope does not need your model object itself to monitor performance - only predictions are required all you need to monitor your model with arthur is to upload the predictions your model makes. here's how to format predictions for common generative text model schemas. use the arthur data type tokens for tokenized input and output", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "makes. here's how to format predictions for common generative text model schemas. use the arthur data type tokens for tokenized input and output texts. arthur expects a list of strings as below for tokenized data. python [ { \" input _ text \" : \" this is the raw input to my model \", \" input _ tokens \" : [ \" this \", \" is \", \" the \", \" raw \", \" input \", \" to \", \" my \", \" model \" ], \" output _ text \" : \" this is model generated text \", \" output _ tokens \"", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to \", \" my \", \" model \" ], \" output _ text \" : \" this is model generated text \", \" output _ tokens \" : [ \" this \", \" is \", \" model \", \" generated \", \" text \" ] } ] use the arthur data type token _ likelihoods for generated outputs of tokens and their likelihoods. arthur expects this data type to be formatted as an array of maps from token strings to float likelihoods. each array index should correspond to one token in the generated sequence. if supplying both tokens and token _ likelihoods for predicted values, the two arrays", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s. each array index should correspond to one token in the generated sequence. if supplying both tokens and token _ likelihoods for predicted values, the two arrays must be equal in length. python [ { \" input _ text \" : \" this is the raw input to my model \", \" input _ tokens \" : [ \" this \", \" is \", \" the \", \" raw \", \" input \", \" to \", \" my \", \" model \" ], \" output _ text \" : \" this is model generated text \", \" output _ tokens \" : [ \" this \", \" is \"", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" ], \" output _ text \" : \" this is model generated text \", \" output _ tokens \" : [ \" this \", \" is \", \" model \", \" generated \", \" text \" ], \" output _ probs \" : [ { \" this \" : 0. 4, \" the \" : 0. 5, \" a \" : 0. 1 }, { \" is \" : 0. 8, \" could \" : 0. 1, \" may \" : 0. 1 }, { \" model \" : 0. 33, \" human \" : 0. 33, \" robot \" : 0", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" may \" : 0. 1 }, { \" model \" : 0. 33, \" human \" : 0. 33, \" robot \" : 0. 33 }, { \" generated \" : 0. 9, \" written \" : 0. 03, \" dreamt \" : 0. 07 }, { \" text \" : 0. 7, \" rant \" : 0. 2, \" story \" : 0. 1 } ] } ] arthur supports maps of up to 5 token - float key pairs. the arthur sdk provides helper functions for mapping openai response objects or log tensor arrays to arthur format. see", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "up to 5 token - float key pairs. the arthur sdk provides helper functions for mapping openai response objects or log tensor arrays to arthur format. see the sdk reference for more guidance on usage. registering a generative text model each generative text model is created with a name and with output _ type = outputtype. tokensequence. we also need to specify an input type, which in this case will be inputtype. nlp for a text to text model. here, we register a token sequence model with nlp input specifying a text _ delimiter of not _ word : pythonarthur _", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "text model. here, we register a token sequence model with nlp input specifying a text _ delimiter of not _ word : pythonarthur _ nlp _ model = arthur. model ( name = \" nlpquickstart \", input _ type = inputtype. nlp, model _ type = outputtype. tokensequence, text _ delimiter = textdelimiter. not _ word ) arthur uses the text delimiter to tokenize model input texts and generated texts and track derived insights like sequence length. you can also register your own pre - tokenized values with arthur for more", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to tokenize model input texts and generated texts and track derived insights like sequence length. you can also register your own pre - tokenized values with arthur for more complex tokenizers. if the registered model uses a custom tokenizer, this is the recommended process outlined in the below section on building a generative text model. below, we show different ways of building a generative text model that depends on which attributes you want to monitor for your model. building a generative text model to build a generative text model in the arthur sdk, use the build _ token _ sequence _ model method on the arthur model. here we add", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model to build a generative text model in the arthur sdk, use the build _ token _ sequence _ model method on the arthur model. here we add one attribute for the input text and one attribute for the model output or generated text. both of these attributes will have the unstructured _ text value type in the arthurmodel after calling this method - this means that this data is saved as a string in each inference. you should build your model this way if you will only monitor its input and output text and not monitor any of its token processing or likelihood scores. pythonarthur _ nlp _ model. build _ token", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will only monitor its input and output text and not monitor any of its token processing or likelihood scores. pythonarthur _ nlp _ model. build _ token _ sequence _ model ( input _ column ='input _ text ', output _ text _ column ='generated _ text') registering pre - tokenized text optionally, token sequence models also support adding token information. in the below example, the tokenized input text is specified in theinput _ token _ column and the final tokens selected for the generated output are specified in theoutput _ token _ column. this method builds a model with four attributes to monitor", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "column and the final tokens selected for the generated output are specified in theoutput _ token _ column. this method builds a model with four attributes to monitor for your generative text model. while the text attributes will still have the unstructured _ text value type, the token attributes will have the tokens value type means that these attributes are represented as a list of tokens for each inference. you should build your model this way if you are going to monitor the inferences in their tokenized form as well as in their text form - this may help distinguish performance behaviors due to the base model from performance behaviors due to the token", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s in their tokenized form as well as in their text form - this may help distinguish performance behaviors due to the base model from performance behaviors due to the tokenization. pythonarthur _ nlp _ model. build _ token _ sequence _ model ( input _ column ='input _ text ', output _ text _ column ='generated _ text ', input _ token _ column ='input _ tokens ', output _ token _ column ='output _ tokens') registering tokens with likelihoods you can attach likelihoods to the generated tokens by specifying the output _ likelihood _ column : pythonarthur", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s') registering tokens with likelihoods you can attach likelihoods to the generated tokens by specifying the output _ likelihood _ column : pythonarthur _ nlp _ model. build _ token _ sequence _ model ( input _ column ='input _ text ', output _ text _ column ='generated _ text ', input _ token _ column ='input _ tokens ', output _ token _ column ='output _ tokens ', output _ likelihood _ column ='output _ probs') it is not required to specify both a output _ token _ column and an output _ likelihood _ column -", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "likelihood _ column ='output _ probs') it is not required to specify both a output _ token _ column and an output _ likelihood _ column - if only the output _ likelihood _ column is specified, greedy decoding will be assumed. registering a ground truth sequence lastly, adding a ground truth sequence to the model is optional. ground truth has the same tokenization support as model input and output texts. pythonarthur _ nlp _ model. build _ token _ sequence _ model ( input _ column ='input _ text ', output _ text _ column ='generated _ text ', ground _ truth _ text _", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sequence _ model ( input _ column ='input _ text ', output _ text _ column ='generated _ text ', ground _ truth _ text _ column ='ground _ truth _ text') adding inference metadata we now have a model schema with input, predicted value, and ground truth data defined. additionally, we can add non - input data attributes to track other information associated with each inference but not necessarily part of the model pipeline. for generative text models, tracking production signals as performance feedback is often of interest. here, we add one continuous attribute and one boolean attribute to measure the success of our model for", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", tracking production signals as performance feedback is often of interest. here, we add one continuous attribute and one boolean attribute to measure the success of our model for a use case. pythonarthur _ nlp _ model. add _ attribute ( name ='edit _ duration ', value _ type = valuetype. float, stage = stage. noninputdata ) arthur _ nlp _ model. add _ attribute ( name ='accepted _ by _ user ', value _ type = valuetype. boolean, stage = stage. noninputdata ) reviewing the model schema before you register your model with", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", value _ type = valuetype. boolean, stage = stage. noninputdata ) reviewing the model schema before you register your model with arthur by calling arthur _ model. save ( ) you can call arthur _ model. review ( ) the model schema to check that it is correct. for a tokensequence model with nlp input, the model schema should look similar to this : python name stage value _ type categorical is _ unique 0 text _ attr pipeline _ input unstructured _ text false true 1 pred _ value predicted _ value unstructured _ text false false..", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0 text _ attr pipeline _ input unstructured _ text false true 1 pred _ value predicted _ value unstructured _ text false false... 2 pred _ tokens predicted _ value token _ likelihoods false false 3 non _ input _ 1 non _ input _ data float false false... finishing onboarding once you have finished formatting your reference data and your model schema looks correct use thearthur _ model. review ( ), you are finished registering your model and its attributes, ready to complete onboarding your model. to finish onboarding your tokensequence model, the following steps apply", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are finished registering your model and its attributes, ready to complete onboarding your model. to finish onboarding your tokensequence model, the following steps apply, which is the same for nlp models as it is for models of any inputtype and outputtype : finishing _ onboarding. md sending inferences since we've already formatted the data, we can use the send _ inferences method of the sdk to upload the inferences to arthur. this functionality is also available directly through the api. pythonarthur _ nlp _ model. send _ inferences ( [ { \" input _ text \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "functionality is also available directly through the api. pythonarthur _ nlp _ model. send _ inferences ( [ { \" input _ text \" : \" this is the raw input to my model \", \" input _ tokens \" : [ \" this \", \" is \", \" the \", \" raw \", \" input \", \" to \", \" my \", \" model \" ], \" output _ text \" : \" this is model generated text \", \" output _ tokens \" : [ \" this \", \" is \", \" model \", \" generated \", \" text \" ], \" output", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" output _ tokens \" : [ \" this \", \" is \", \" model \", \" generated \", \" text \" ], \" output _ probs \" : [ { \" this \" : 0. 4, \" the \" : 0. 5, \" a \" : 0. 1 }, { \" is \" : 0. 8, \" could \" : 0. 1, \" may \" : 0. 1 }, { \" model \" : 0. 33, \" human \" : 0. 33, \" robot \" : 0. 33 }, { \" generated \" : 0. 9, \" written \"", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". 33, \" human \" : 0. 33, \" robot \" : 0. 33 }, { \" generated \" : 0. 9, \" written \" : 0. 03, \" dreamt \" : 0. 07 }, { \" text \" : 0. 7, \" rant \" : 0. 2, \" story \" : 0. 1 } ] } ] ) arthur supports maps of up to 5 token - float key pairs. the arthur sdk provides a helper function to map tensor arrays into an arthur format. see the sdk reference for more guidance on usage enrichments for an overview of configuring", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "helper function to map tensor arrays into an arthur format. see the sdk reference for more guidance on usage enrichments for an overview of configuring enrichments for nlp models, see the { doc } / user - guide / walkthroughs / enrichments guide. explainability is not currently supported for tokensequence models, but anomaly detection will be enabled by default. updated 3 months ago table of contents getting started preparing data for arthur registering a generative text model building a generative text model adding inference metadata reviewing the model schema finishing onboarding sending inferences enrichments source : https : / /", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "generative text model building a generative text model adding inference metadata reviewing the model schema finishing onboarding sending inferences enrichments source : https : / / docs. arthur. ai / docs / generative - text", "metadata": {"source": "https://docs.arthur.ai/docs/generative-text", "row": 27, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 28 text : cv onboarding jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by cv onboardingsuggest editsthis page shows the basics of setting up computer vision ( cv ) models and onboarding arthur scope to monitor vision - specific performance. getting started the first step is to import functions from the arthurai package and establish a connection with arthur. python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \" ) registering a cv model each computer vision model is created with input _ type = inputtype. image and with specified width and height dimensions for the processed images. here, we register a classification model on 1024x1024 images : pythonarthur _ cv _ model = arthur. model ( name = \" imagequickstart \", input _", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "on 1024x1024 images : pythonarthur _ cv _ model = arthur. model ( name = \" imagequickstart \", input _ type = inputtype. image, model _ type = outputtype. multiclass, pixel _ height = 1024, pixel _ width = 1024 ) you can send images to the arthur platform with any dimensions, and we'll keep the original you send as wellas a resized copy in the model dimensions. if you enable explainability for your model, the resized versions will be passed to it to generate explanations. the different outputtype values currently supported for computer vision", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "if you enable explainability for your model, the resized versions will be passed to it to generate explanations. the different outputtype values currently supported for computer vision models are classification, regression, and object detection. formatting data computer vision models require the same structure as tabular and nlp models. however, the attribute value for image attributes should be a valid path to the image file for that inference. here is an example of a valid reference _ data data frame to build an arthurmodel with : python image _ attr pred _ value ground _ truth non _ input _ 1 0'img _ path / img _", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthurmodel with : python image _ attr pred _ value ground _ truth non _ input _ 1 0'img _ path / img _ 0. png'0. 1 0 0. 2 1'img _ path / img _ 1. png'0. 05 0 - 0. 3 2'img _ path / img _ 2. png'0. 02 1 0. 7... 3'img _ path / img _ 3. png'0. 8 1 1. 2 4'img _ path / img _ 4. png", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ path / img _ 3. png'0. 8 1 1. 2 4'img _ path / img _ 4. png'0. 4 0 - 0. 5... non - input attributes any non - pixel features to be tracked in images for performance comparison or bias detection should be added as non - input attributes. for example, metadata about people's identities captured in images for a cv model should be included as non - input attributes. reviewing the model schema before you call arthur _ model. save ( ) you can call arthur _ model. review ( ) the model schema to", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attributes. reviewing the model schema before you call arthur _ model. save ( ) you can call arthur _ model. review ( ) the model schema to check that your data is parsed correctly. for an image model, the model schema should look like this : python name stage value _ type categorical is _ unique 0 image _ attr pipeline _ input image false true 1 pred _ value predicted _ value float false false... 2 ground _ truth ground _ truth integer true false 3 non _ input _ 1 non _ input _ data float false false... object detection formatting bounding boxes if using an", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ truth integer true false 3 non _ input _ 1 non _ input _ data float false false... object detection formatting bounding boxes if using an object detection model, bounding boxes should be formatted as lists in the form : [ class _ id, confidence, top _ left _ x, top _ left _ y, width, height ] the first two components of the bounding box list represent the classification being made within the bounding box. theclass _ id represents the id of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction ( 0. 0 for", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "id represents the id of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction ( 0. 0 for completely unconfident and 1. 0 for completely confident ). the next four components of the bounding box list represent the location of the bounding box within the image : the top _ left _ x and top _ left _ y represent the x and y pixel coordinates of the top - left corner of the bounding box. these pixel coordinates are calculated from the origin, which is in the top left corner of the image. this means that each coordinate is calculated by counting", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ing box. these pixel coordinates are calculated from the origin, which is in the top left corner of the image. this means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. the width represents the number of pixels the bounding box covers from left to right and the height represents the number of pixels the bounding box covers from top to bottom. so using the following model schema as an example : python name stage value _ type 0 image _ attr pipeline _ input image 1 label ground _ truth bounding _ box 2 objects _ detected predicted _ value bounding _ box a valid data", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0 image _ attr pipeline _ input image 1 label ground _ truth bounding _ box 2 objects _ detected predicted _ value bounding _ box a valid dataset would look like python # image _ attr objects _ detected ground _ truth non _ input _ 1 0,'img _ path / img _ 0. png ', [ [ 0, 0. 98, 12, 20, 50, 25 ], [ 0, 1, 14, 22, 48, 29 ], 0. 2 [ 1, 0. 47, 92, 140, 80, 36 ] ] 1,'img _ path /", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "48, 29 ], 0. 2 [ 1, 0. 47, 92, 140, 80, 36 ] ] 1,'img _ path / img _ 1. png ', [ [ 1, 0. 22, 4, 5, 14, 32 ] ] [ 1, 1, 25, 43, 49, 25 ] - 0. 3 #... #... finishing onboarding once you have finished formatting your reference data and your model schema looks correct using thearthur _ model. review ( ), you are finished locally configuring your model and its attributes - so", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "your model schema looks correct using thearthur _ model. review ( ), you are finished locally configuring your model and its attributes - so you are ready to complete onboarding your model. to finish onboarding your cv model, the following steps apply, which is the same for cv models as it is for models of any inputtype and outputtype : finishing _ onboarding. md enrichments for an overview of configuring enrichments for image models, see the enabling enrichments section. for a step - by - step walkthrough of setting up the explainability enrichment for image models, see the", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", see the enabling enrichments section. for a step - by - step walkthrough of setting up the explainability enrichment for image models, see the assets required for explainability section. updated 3 months ago table of contents getting started registering a cv model formatting data non - input attributes reviewing the model schema object detection formatting bounding boxes finishing onboarding enrichments source : https : / / docs. arthur. ai / docs / cv - onboarding", "metadata": {"source": "https://docs.arthur.ai/docs/cv-onboarding", "row": 28, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 29 text : organizations and users jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by organizations and userssuggest editsby default, a new organization, \" my organization \" is created by the installer for convenience. you can also create new organizations using the api with the superadmin user. log in with superadmin credentials the first thing you will need is a superadmin authorization token. to obtain this, you will need to make a post request to your organization's / login endpoint with the password set in", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##admin authorization token. to obtain this, you will need to make a post request to your organization's / login endpoint with the password set in the admin console config page. textjsonpost / login { \" login \" : \" superadmin \", \" password \" : \" < superadmin - password > \" } the response will look like this : json { \" id \" : \" ed1dcb56 - 352a - 4130 - 8f52 - 1fd1225196b1 \", \" first _ name \" : \" super \", \" last", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##a - 4130 - 8f52 - 1fd1225196b1 \", \" first _ name \" : \" super \", \" last _ name \" : \" admin \", \" email \" : \" [ email protected ] \", \" username \" : \" superadmin \", \" roles \" : null, \" active \" : true, \" show _ intro _ sequence \" : true, \" help _ mode _ enabled \" : false, \" created _ at \" : \" 2021 - 08 - 09t19 : 57 : 44. 92047z \" } the response will also include a set -", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "created _ at \" : \" 2021 - 08 - 09t19 : 57 : 44. 92047z \" } the response will also include a set - cookie http header with an authorization token. ` copy the authorization token value and use it in subsequent requests as your auth token. set - cookie : authorization = < authorization - token > ; path = / ; expires = mon, 30 aug 2021 16 : 51 : 07 gmt ; secure ; curl example bashcurl - - location - - request post'https : / / < your - domain > / api / v3 / login'- - header '", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##curl - - location - - request post'https : / / < your - domain > / api / v3 / login'- - header'content - type : application / json'- - data - raw'{ \" login \" : \" superadmin \", \" password \" : \" < superadmin - password > \" }'- v create a new organization to create a new organization, you will need to make a post request to / organizations with the body specifying the name. ensure you are using a super admin authentication token to make this request. textjsonpost / organizations { \"", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "organizations with the body specifying the name. ensure you are using a super admin authentication token to make this request. textjsonpost / organizations { \" name \" : \" my - new - organization \" } the response will look like this : json { \" id \" : \" 38faff8b - 4edf - 44c5 - b103 - aeca4ea71110 \", \" name \" : \" my - new - organization \", \" plan \" : \" enterprise \", \" created _ at \" : \" 2021 - 08 - 18t19 : 51 : 22. 291504554", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" plan \" : \" enterprise \", \" created _ at \" : \" 2021 - 08 - 18t19 : 51 : 22. 291504554z \" } remember to save the id ; you will need this to add users to your organization. curl example bashcurl - - location - - request post'< your - domain > / api / v3 / organizations'- - header'content - type : application / json'- - header'authorization : < your - superadmin - access - control - token >'- - data - raw'{ \" name \" : \" my - new - organization", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": < your - superadmin - access - control - token >'- - data - raw'{ \" name \" : \" my - new - organization \" }'- v create the first user in an organization to create a new user in the new organization, you will need to make a post request to / users? organization _ id = < your _ organization _ id > using a super admin authentication token. you can set the role of the new user to administrator, model owner, or user. refer to the platform access control for the description of the roles. textjsonpost / users? organization _ id = <", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", model owner, or user. refer to the platform access control for the description of the roles. textjsonpost / users? organization _ id = < your _ organization _ id > { \" username \" : \" newuser \", \" email \" : \" [ email protected ] \", \" password \" : \" g00dp @ $ $ w0rd! \", \" first _ name \" : \" new \", \" last _ name \" : \" user \", \" roles \" : [ \" administrator \" ], \" alert _ notifications _ enabled \" : true } the response will look like this. json {", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" roles \" : [ \" administrator \" ], \" alert _ notifications _ enabled \" : true } the response will look like this. json { \" id \" : \" b6554927 - 9ac4 - 4531 - bf76 - fe640b8223b7 \", \" first _ name \" : \" new \", \" last _ name \" : \" user \", \" email \" : \" [ email protected ] \", \" username \" : \" newuser \", \" roles \" : null, \" active \" : true, \" show _ intro _ sequence \" : true,", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" username \" : \" newuser \", \" roles \" : null, \" active \" : true, \" show _ intro _ sequence \" : true, \" help _ mode _ enabled \" : true, \" created _ at \" : \" 2021 - 08 - 18t20 : 20 : 18. 535137592z \" } you can now log in to the dashboard as this user. curl example this action can be performed as either the super administrator or an organization administrator. if you'd like to use an organization administrator, repeat the login api call performed earlier with the credentials for that user and save the returned authorization", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "administrator. if you'd like to use an organization administrator, repeat the login api call performed earlier with the credentials for that user and save the returned authorization token. bashcurl - - location - - request post'https : / / < your - domain > / api / v3 / users? organization _ id = < your - organization - id >'- - header'content - type : application / json'- - header'authorization : < your - superadmin - token >'- - data - raw'{ \" username \" : \" < username > \", \" email \" : \" <", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "superadmin - token >'- - data - raw'{ \" username \" : \" < username > \", \" email \" : \" < email - address > \", \" password \" : \" < password > \", \" first _ name \" : \" < first - name > \", \" last _ name \" : \" < last - name > \", \" roles \" : [ \" administrator \" ], \" alert _ notifications _ enabled \" : true }'adding additional users although you can continue to create users through the api, it is generally easier to create an administrator user and then invite additional users from the", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "}'adding additional users although you can continue to create users through the api, it is generally easier to create an administrator user and then invite additional users from the ui. to add additional users this way, login to arthur ai with an administrator user on a web browser and follow these steps : in the top right corner, you will see a series of icons. click on the organization icon that looks like a tree with three nodes. you will see a dropdown menu. click on manage members under the heading, invite members, you can type in the email address of the person you wish to invite. that person will receive email instructions for", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "on manage members under the heading, invite members, you can type in the email address of the person you wish to invite. that person will receive email instructions for creating a user in the organization. once the new user follows the emailed instructions, they can log in with their newly created username and password. you will then be able to view that new user on this manage members page. as an administrator, you can continue to use this page to manage users and roles. adding existing users to existing organizations via api : to add an existing user to an existing organization, create a patch request to / organizations / < org _ id > /", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "existing users to existing organizations via api : to add an existing user to an existing organization, create a patch request to / organizations / < org _ id > / users. supplying in the body a json object defining the role ( administrator, model owner, or user ) you want to add the user with. any attributes other than roles that are supplied in the body will affect the user across all organizations that the user is a part of. textjsonpatch / organizations / < org _ id > / users [ { \" user _ id \" : \" b6554927 - 9ac4 - 4531 - bf76", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "< org _ id > / users [ { \" user _ id \" : \" b6554927 - 9ac4 - 4531 - bf76 - fe640b8223b7 \", \" role \" : \" model owner \" }, { \" user _ id \" : \" b6554927 - 9ac4 - 4531 - bf76 - fe640b8223b7 \", \" role \" : \" model owner \" } ] the response will look like this. json { \" updated _ user _ count \" : 10 } updated 3 months ago table of contents log", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "owner \" } ] the response will look like this. json { \" updated _ user _ count \" : 10 } updated 3 months ago table of contents log in with superadmin credentials curl example create a new organization curl example create the first user in an organization curl example adding additional users adding existing users to existing organizations via api : source : https : / / docs. arthur. ai / docs / organizations - and - users", "metadata": {"source": "https://docs.arthur.ai/docs/organizations-and-users", "row": 29, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 30 text : source : https : / / staging. docs. arthur. ai / docs / pages - in - the - arthur - platform", "metadata": {"source": "https://staging.docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 30, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 31 text : explainability jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by explainabilityunderstand why your model is making decisionssuggest editsexplainability is one of the core pillars of the arthur scope platform. teams utilize explainability to build trust with valuable insights into how their ml models make decisions. they use it to explore patterns, investigate problems, and ensure compliance with explanations for their model outputs. finally, they can use it to debug by understanding not only why predictions are being made but also evaluating how", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ensure compliance with explanations for their model outputs. finally, they can use it to debug by understanding not only why predictions are being made but also evaluating how changes to model inputs change predictive outputs. levels of explainability in arthur ui explainability can be evaluated at multiple levels within the arthur platform. these levels include : global explainability ( tabular models ) global explainability in machine learning models refers to the ability to understand and explain a model's overall behavior and decision - making process across its entire dataset or population. teams often use global explainability as a gut check to ensure that the features they consider the most important are", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- making process across its entire dataset or population. teams often use global explainability as a gut check to ensure that the features they consider the most important are the ones driving decisions. teams can also drive value from global explanations through interactions with other metrics within arthur scope. in the arthur ui, for example, global explainability can be visualized in conjunction with a data drift of choice at the bottom of the overview page. in this figure, we can see that pay _ 0 is the most important feature. using explainability in conjunction with other metrics can help teams understand and debug models and drive actions such as model re", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0 is the most important feature. using explainability in conjunction with other metrics can help teams understand and debug models and drive actions such as model retraining or new feature / architecture exploration. [UNK] explanations are an aggregate of local explanationswithin arthur, global explanations are created by aggregating the absolute value of local explanations. the absolute value is taken because, as you will see in local explainability, features can positively or negatively impact whether or not a prediction is made. by taking the average absolute value of all local explanations, global explanations are a measure of feature impact ( not necessarily positive or negative ). local explainability", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "made. by taking the average absolute value of all local explanations, global explanations are a measure of feature impact ( not necessarily positive or negative ). local explainability local explainability in machine learning refers to the ability to explain the reasoning behind a model's prediction for a particular instance or input. teams can access local explainability for inferences within an arthur models inference tab. select positive predicted attribute : teams need to select a positive predicted attribute when running local explanations because it focuses the explanation on a specific prediction of interest. i. e., we are generating explanations for what causes that prediction to occur. for example, in binary classification,", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "explanation on a specific prediction of interest. i. e., we are generating explanations for what causes that prediction to occur. for example, in binary classification, this helps us read the explanation to understand whether the feature is driving the prediction for that class in a positive way or a negative way by driving the prediction for the other class instead. this is the same for multiclass models ; however, unlike binary, for negative importance scores, we cannot attribute this negative importance to the other class. this is because there are more than two classes. it could be driving predictions to any other predicted class instead. tabular inferences we can visualize", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "class. this is because there are more than two classes. it could be driving predictions to any other predicted class instead. tabular inferences we can visualize local importance scores for each model input feature for tabular inferences. here we can see what if capabilities \" what if \" local explainability functionality refers to the ability to interactively explore and understand the impact of changing input features on a model's prediction for a particular instance or input. to utilize what - ifs for a specific inference, a user needs to toggle on the \" what - if \" functionality in the arthur ui for that inference. then they can change any", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for a specific inference, a user needs to toggle on the \" what - if \" functionality in the arthur ui for that inference. then they can change any feature and evaluate how it changes the model's predictions or relative importance scores. this example shows how different inputs for this inference affected the local importance scores for the inference and the predicted probabilities. text inferences for text models, teams can visualize the words that drive predictions. note : explainability is only available for classification and regression text models. we do not currently have explainability functionality for generative text models. instead, teams can look into token likelihood for generative", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for classification and regression text models. we do not currently have explainability functionality for generative text models. instead, teams can look into token likelihood for generative models. in this example, we can see that the model mispredicted consult _ history _ and _ phy instead of urology. we can use explainability to examine the top tokens that drove this misprediction between the two classes. is bag of words : bag of words is a text representation technique in natural language processing that converts a document into a collection of its words. when \" is bag of words \" is turned on, explainability scores are calculated", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "technique in natural language processing that converts a document into a collection of its words. when \" is bag of words \" is turned on, explainability scores are calculated per word - not considering where duplicate words are placed in the text. however, when it is turned off, each duplicate word is treated as a unique token. so, placement in the text is taken into consideration. tokenization : features for text input models are often called tokens. as described further in the text input section, arthur default to create word - based tokens based on whitespace. if whitespace does not make sense in your use case, make sure to set", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "section, arthur default to create word - based tokens based on whitespace. if whitespace does not make sense in your use case, make sure to set up a different tokenization when enabling explainability. image inferences for image classification / regression models, teams can visualize the segments of the image that drive different predictions. querying more levels of explainability with the python sdk beyond the explainability functionality provided within the ui, many teams choose to pull custom reports or charts with our arthur query service. common examples can be found here : querying explainability available post - hoc explainers in arthur arthur supports open - source lime", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "charts with our arthur query service. common examples can be found here : querying explainability available post - hoc explainers in arthur arthur supports open - source lime and shap for explainability within the platform. explaineravailable for tabular modelsavailable for text modelsavailable for image modelslimexxxshapx please reference our enabling enrichments section in the model onboarding section for recommendations regarding which explainer to use. available arthur schemas explainability is available for all model types except object detection and generative text. updated 3 months ago what \u2019 s nextlearn more about enabling explainability in", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "explainability is available for all model types except object detection and generative text. updated 3 months ago what \u2019 s nextlearn more about enabling explainability in general in the enabling enrichments section of model onboarding. however, if you are having specific troubles check out our pages on troubleshooting and debugging explainability enablement. enabling enrichmentstroubleshooting explainabilitytable of contents levels of explainability in arthur ui global explainability ( tabular models ) local explainability querying more levels of explainability with the python sdk available post - hoc explainers in arthur available arthur schemas source : https :", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "local explainability querying more levels of explainability with the python sdk available post - hoc explainers in arthur available arthur schemas source : https : / / docs. arthur. ai / docs / explainability", "metadata": {"source": "https://docs.arthur.ai/docs/explainability", "row": 31, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 32 text : arthur permissions by standard roles jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by arthur permissions by standard rolessuggest edits model readeralert managerusermodel writerfree tier usermodel ownerorg manageradministratororg creatorsuper adminuserreadxxxxxxmodelreadxxxxxxtagreadxxxxxxalert _ rulereadxxxxxxalertreadxxxxxxalertresolvexxxxxxalert _ notification _ configre", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##xxalert _ rulereadxxxxxxalertreadxxxxxxalertresolvexxxxxxalert _ notification _ configreadxxxxxxalert _ notification _ configwritexxxxxxalert _ notification _ configdeletexxxxxxinsightreadxxxxxxinsightresolvexxxxxxalert _ summary _ configreadxxxxxxalert _ summary _ configwritexxxxxxalert _ summary _ configdeletexxxxxxalert _ summary _ subscriberreadxxxxxxalert _", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##writexxxxxxalert _ summary _ configdeletexxxxxxalert _ summary _ subscriberreadxxxxxxalert _ summary _ subscriberwritexxxxxxalert _ summary _ subscriberdeletexxxxxxreference _ datareadxxxxxraw _ datareadxxxxxqueryexecutexxxxxmetric _ queryreadxxxxxxenrichment _ configreadxxxxxxorganization _ usagereadxxxxxmodel _ pinned _ columns _ globalreadxxxxxxmodelwritexxxxxmo", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##xxxxorganization _ usagereadxxxxxmodel _ pinned _ columns _ globalreadxxxxxxmodelwritexxxxxmodeldeletexxxxxtagwritexxxxxtagdeletexxxxxalert _ rulewritexxxxxalert _ ruledeletexxxxxmetric _ querywritexxxxxmetric _ querydeletexxxxxreference _ datawritexxxxraw _ datawritexxxxenrichment _ configwritexxxxorganization _ metricsreadxxxxservice _ accountreadxxxxservice _ account", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##richment _ configwritexxxxorganization _ metricsreadxxxxservice _ accountreadxxxxservice _ accountwritexxxxservice _ accountdeletexxxxground _ truthwritexxxxinvite _ userwritexxxxuserwritexxxuserdeletexxxauthentication _ configreadxxxauthentication _ configwritexxxalertnotifyxxalert _ summarynotifyxxinvite _ userwritexxmodel _ pinned _ columns _ globalwritexxmodel _ pinned _ columns _ globaldeletexxorganization", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ifyxxinvite _ userwritexxmodel _ pinned _ columns _ globalwritexxmodel _ pinned _ columns _ globaldeletexxorganization _ globalreadxxorganization _ globalwritexxorganizationreadxxorganizationdeletexxcustom _ rolesreadxxcustom _ roleswritexxcustom _ rolesdeletexxsystem _ configwritexmodel _ statuswritexenrichment _ statuswritexupdated about 2 months ago source : https : / / docs. arthur. ai / docs / arthur - permissions - by -", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##writexupdated about 2 months ago source : https : / / docs. arthur. ai / docs / arthur - permissions - by - standard - roles", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles", "row": 32, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 33 text : configuring for high availability jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser -", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ssionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explain", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstra", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityextern", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##installation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess control", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestorin", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##sarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by configuring for high availabilitysuggest editsintroduction the arthur platform is built to run in a high availability configuration, ensuring that the application can function in the event of a data center outage, hardware outages, or other similar infrastructure issues. in order to take advantage of this, there are a few requirements in how your infrastructure is setup : installing across 3 availability zones specifying the correct instance types configur", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to take advantage of this, there are a few requirements in how your infrastructure is setup : installing across 3 availability zones specifying the correct instance types configuring the cluster for auto - scaling notes about this document { note } note that this document is written using aws terminology, as this is one of the environments / infrastructure that arthur uses internally for our environments. however, these setup steps should work across various cloud providers using similar features. { note } note that this document is written with the pre - requisite that you are installing arthur in a high availability configuration. at the minimum, this means that there should be 3 instances across", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is written with the pre - requisite that you are installing arthur in a high availability configuration. at the minimum, this means that there should be 3 instances across which arthur is deployed. installing across 3 availability zones in order to ensure continuous operation during an availability zone ( az ) outage, arthur must be installed on a cluster that has 3 availability zones. this ensures that in the event of one az outage that the rest of the components can still operate. to do this in aws, create 3 separate auto - scaling groups ( asgs ) - one for each az. you can configure which az an asg provisions", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "aws, create 3 separate auto - scaling groups ( asgs ) - one for each az. you can configure which az an asg provisions instances into when you create the asg. when arthur deploys, the stateful services ( eg : databases, messaging queues, etc. ) will be balanced across the 3 azs automatically using kubernetes pod anti - affinity rules ( pods will not schedule onto nodes where there already exists another pod that is of the same component ). specifying the correct instance types generally speaking, the best way to ensure you have deployed the correct instance types is to monitor resource", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is of the same component ). specifying the correct instance types generally speaking, the best way to ensure you have deployed the correct instance types is to monitor resource utilization across the cluster to determine when your services are hitting resource limits. when initially configuring a cluster for arthur, we recommend 3 nodes, where each node has at least 16 vcpus and 64g of ram ( eg : an m5a. 4xlarge instance type ). this is a good starting point for a general - purpose cluster that will grow with your production usage. configuring the cluster for auto - scaling arthur's stateless", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "good starting point for a general - purpose cluster that will grow with your production usage. configuring the cluster for auto - scaling arthur's stateless components horizontally auto - scale, but in order to take the maximum advantage of this, you will need to configure and install an additional component that performs node autoscaling ( eg : adding more instances ). aws specifies how to setup cluster autoscaling in their documentation. generally speaking, it involves setting up an iam role and granting permissions to autoscale the cluster, and then installing a third - party component to perform the autoscaling (", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "setting up an iam role and granting permissions to autoscale the cluster, and then installing a third - party component to perform the autoscaling ( eg : cluster - autoscalerupdated 3 months ago table of contents introduction notes about this document installing across 3 availability zones specifying the correct instance types configuring the cluster for auto - scaling source : https : / / docs. arthur. ai / docs / configuring - for - high - availability", "metadata": {"source": "https://docs.arthur.ai/docs/configuring-for-high-availability", "row": 33, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 34 text : enrichments jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by enrichmentsenrich your monitoring process with state - of - the - art techniquessuggest editsenrichments are additional services that the arthur platform provides for state - of - the - art proactive model monitoring. enrichments in arthur anomaly detection : monitor and alert on incoming changes to your data distribution ( compared to the reference dataset ) based on complex interactions between features hotspots : automatically illuminate segments of underperformance within incoming inferences", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( compared to the reference dataset ) based on complex interactions between features hotspots : automatically illuminate segments of underperformance within incoming inferences explainability : understand why your model is making decisions, by computing the importance of individual features from your data on your model's outcomes bias mitigation : methods for model post - processing that improve the fairness of outcomes without re - deploying your model once activated, these enrichments are automatically computed on arthur's backend, with results viewable in the online ui dashboard and queryable from arthur's api. available enrichments by different model types due to the specialized", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backend, with results viewable in the online ui dashboard and queryable from arthur's api. available enrichments by different model types due to the specialized nature of enrichments, they are only available for certain model types. model typeanomaly detectionbias mitigationexplainabilityhot spotstabular classificationxxxxtabular regressionxxtext classificationxxtext regressionxxtext sequence generation ( llm ) x ( on inputs ) cv classificationxxcv regressionxxcv object detectionx viewing enabled enrichments in the ui you are also able to view the enrichments enabled for your specific model within the arthur ui by", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##xxcv object detectionx viewing enabled enrichments in the ui you are also able to view the enrichments enabled for your specific model within the arthur ui by clicking on the details sections of the model's overview page. enrichment workflows as enrichments are add - ons meant to enrich standard model monitoring, they run on their own workflows within arthur. updated 3 months ago table of contents enrichments in arthur available enrichments by different model types viewing enabled enrichments in the ui enrichment workflows source : https : / / docs. arthur. ai / docs / what - are - enrichments", "metadata": {"source": "https://docs.arthur.ai/docs/what-are-enrichments", "row": 34, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 35 text : data drift metrics jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by data drift metricstrack the stability of your model by comparing real - world data to a reference datasetsuggest editsdata drift is one of the top causes of model performance decay over time. data drift measures how much the input data stream to the model changes over time. tracking data drift over time can help teams identify when models are no longer performing as expected and take proactive steps to maintain or improve their performance. teams use data drift", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data drift over time can help teams identify when models are no longer performing as expected and take proactive steps to maintain or improve their performance. teams use data drift to illuminate and debug issues like : upstream data issues, such as a third - party data provider changing their tagging of missing data fromnull to - 1 data quality issues, such as a faulty sensor tagging a feature 10x lower natural demographic shifts, such as an interesting segment of new users from a younger age group using your platform sudden changes in relationships, such as the covid - 19 pandemic, immediately shift relationships between features and predictions selecting", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from a younger age group using your platform sudden changes in relationships, such as the covid - 19 pandemic, immediately shift relationships between features and predictions selecting samples for comparison data drift metrics are essentially metrics built to compare two samples of the same data distribution. it calculates how much that distribution drifts from one sample to another. this section talks about choosing those two samples of comparison. using a reference dataset the reference dataset is a representative sample of the input features your model ingests. it is used to compute baseline model analytics. by capturing the data distribution you expect your model to receive, arthur can detect,", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "features your model ingests. it is used to compute baseline model analytics. by capturing the data distribution you expect your model to receive, arthur can detect, surface, and diagnose data drift before it impacts results. more information about reference datasets can be found in the documentation examples of how reference data is structured for different model types can be found in the model input / output types section. best practices for selecting a reference dataset can be found in thecreating an arthur model object section comparing points in time while the arthur ui is designed to compare production data against a reference dataset, arthur can compare any two distribution samples", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "an arthur model object section comparing points in time while the arthur ui is designed to compare production data against a reference dataset, arthur can compare any two distribution samples. one of the most popular ways teams use this is by comparing two samples of production data to one another ( i. e., how has data from one week, one month, or one quarter ago drifted compared to now ). an example of this query can be seen in querying data drift resource. type of data drift metrics we have established that data drift metrics are a comparison of distributions. now, we can look at the distributions that ml teams often compare.", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data drift metrics we have established that data drift metrics are a comparison of distributions. now, we can look at the distributions that ml teams often compare. feature drift feature drift ( also known as covariate drift ) refers to the changes in the distribution of input variables to a machine learning model. metrics in arthur for feature drift arthur allows teams to decide between the most common metrics for feature drift for easy comparison within the ui. metrics availablepsikldivergencejsdivergencehellingerdistancehypothesistest prediction drift prediction drift tracks the discrepancy of your ml model outputs over time. [UNK]", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gencejsdivergencehellingerdistancehypothesistest prediction drift prediction drift tracks the discrepancy of your ml model outputs over time. [UNK] drift as a proxy for concept driftteams may be familiar with the different types of distributional drifts that can occur within ml systems, the two most popular being covariate and concept drift. covariate, known in arthur as feature drift, refers to the distributions of features going into the model changing. concept drift refers to a changing relationship between the inputs ( features ) and outputs ( predictions ) of the model. while concept drift is best tracked with model accuracy metrics", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "drift refers to a changing relationship between the inputs ( features ) and outputs ( predictions ) of the model. while concept drift is best tracked with model accuracy metrics, in situtations where there is not ground truth prediction drift is a common proxy for tracking concept drift. see more in this blog post : https : / / neptune. ai / blog / concept - drift - best - practices metrics in arthur for prediction drift since prediction drift is another univariate drift technique, it has all the same options as feature drift for available univariate drift metrics. metrics availablepsikldivergencejsdivergencehellinger", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "it has all the same options as feature drift for available univariate drift metrics. metrics availablepsikldivergencejsdivergencehellingerdistancehypothesistest multivariate drift ( anomaly detection ) the previous statistical drift metrics listed for both feature and prediction drift are univariate metrics of data drift. this means they only track one attribute at a time, which is incredibly useful for diagnosing specific issues within a feature. however, sometimes teams want to explore the changing relationships between features. this is the purpose of multivariate drift. metrics in arthur for multivariate drift currently,", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sometimes teams want to explore the changing relationships between features. this is the purpose of multivariate drift. metrics in arthur for multivariate drift currently, in the arthur platform, arthur provides one multivariate drift metric based on the average of our model - based anomaly score technique. metrics availablemultivariate drift using drift to drive action in practice, data drift is best used as a technique to instigate action within teams. to drive this action, teams have to use different features within arthur. investigating features in tabular models while data drift is commonly used as a proxy for performance for models that do not receive ground", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "have to use different features within arthur. investigating features in tabular models while data drift is commonly used as a proxy for performance for models that do not receive ground truth soon after the time of prediction ( if ever ), it is also. this gif shows an example of how data drift can be used in conjunction with feature importance to track down the root cause of model underperformance. in the arthur ui, there are two charts below distributional drift to enable quick evaluation : feature importance x drift : using drift in conjunction with feature importance allows teams to understand how impactful drifted features are in modeling predictions attribute distribution : evaluate the numerical", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": feature importance x drift : using drift in conjunction with feature importance allows teams to understand how impactful drifted features are in modeling predictions attribute distribution : evaluate the numerical or categorical distributions of the attribute that have drifted to understand the cause of univariate drift. anomaly detection + multivariate drift while univariate drift can be tracked for noninputdata attributes, the most common data distributional drift tracked for text and image models is multivariate drift ( or anomaly detection ). for a more detailed look at how teams use our anomaly detection enrichment to drive value, please refer to the anomaly detection page. updated about 2 months", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detection ). for a more detailed look at how teams use our anomaly detection enrichment to drive value, please refer to the anomaly detection page. updated about 2 months ago table of contents selecting samples for comparison using a reference dataset comparing points in time type of data drift metrics feature drift prediction drift multivariate drift ( anomaly detection ) using drift to drive action investigating features in tabular models anomaly detection + multivariate drift source : https : / / docs. arthur. ai / docs / drift - and - anomaly", "metadata": {"source": "https://docs.arthur.ai/docs/drift-and-anomaly", "row": 35, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 36 text : installing arthur pre - requisites jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser -", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ssionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explain", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstra", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityextern", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##installation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess control", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestorin", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##sarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by installing arthur pre - requisitessuggest editsthis is a guide to help you prepare your existing kubernetes cluster for installing the arthur platform. the examples use helm 3. make sure you're in the correct kubectl environment context before running the installer. install ingress nginx nginx is an enterprise - grade cloud - agnostic open source ingress controller that can be used to", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the installer. install ingress nginx nginx is an enterprise - grade cloud - agnostic open source ingress controller that can be used to access the arthur application. shellhelm repo add ingress - nginx https : / / kubernetes. github. io / ingress - nginx helm repo update helm upgrade - - install - n ingress - system \\ - - create - namespace \\ ingress - nginx \\ ingress - nginx / ingress - nginx [ optional ] to monitor nginx using prometheus and add an aw", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##inx \\ ingress - nginx / ingress - nginx [ optional ] to monitor nginx using prometheus and add an aws managed ssl certificate, create a values. yaml file with following contents - shellcontroller : metrics : enabled : true servicemonitor : enabled : true additionallabels : release : \" kube - prometheus - stack \" service : annotations : service. beta. kubernetes. io / aws - load - balancer - backend - protocol : http service. beta. kubernetes. io / aws -", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es. io / aws - load - balancer - backend - protocol : http service. beta. kubernetes. io / aws - load - balancer - connection - idle - timeout : \" 60 \" service. beta. kubernetes. io / aws - load - balancer - cross - zone - load - balancing - enabled : \" true \" service. beta. kubernetes. io / aws - load - balancer - ssl - cert : < acm certificate arn > service. beta. kubernetes. io / aws - load - balance", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##r - ssl - cert : < acm certificate arn > service. beta. kubernetes. io / aws - load - balancer - ssl - ports : https service. beta. kubernetes. io / aws - load - balancer - type : < nlb > # optional annotation that creates a network load balancer. defaults to elb ( classic load lalancer ) service. beta. kubernetes. io / aws - load - balancer - ssl - negotiation - policy : elbsecuritypolicy - tls - 1", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##netes. io / aws - load - balancer - ssl - negotiation - policy : elbsecuritypolicy - tls - 1 - 2 - 2017 - 01 service. beta. kubernetes. io / aws - load - balancer - internal : true # optional annotation that creates a non - internet - facing loadbalancer. defaults to false targetports : http : \" tohttps \" allowsnippetannotations : \" true \" config : http - snippet : server { listen 2443 ; return 308 https : / / $ host $ request", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##otations : \" true \" config : http - snippet : server { listen 2443 ; return 308 https : / / $ host $ request _ uri ; } use - forwarded - headers : \" true \" ingressclassresource : name : nginx enabled : true default : false controllervalue : \" k8s. io / internal - ingress - nginx \" # default : k8s. io / ingress - nginx containerport : http : 8080 tohttps : 2443 https : 80 upgrade or install the helm chart with the values.", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ress - nginx containerport : http : 8080 tohttps : 2443 https : 80 upgrade or install the helm chart with the values. yaml you created. shellhelm upgrade - - install - n ingress - system \\ - - create - namespace \\ ingress - nginx \\ ingress - nginx / ingress - nginx \\ - f values. yaml if you need to install nginx in the same namespace as arthur ( not recommended ) and want to use our network - policy to restrict ingress to the arthur application, use the below command to add labels to", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##pace as arthur ( not recommended ) and want to use our network - policy to restrict ingress to the arthur application, use the below command to add labels to the pods and services. the network - policy allows traffic between pods and services that have these labels. shellhelm upgrade - - install - n arthur - - set controller. podlabels. network - app = arthurai, controller. service. labels. network - app = arthurai, defaultbackend. podlabels. network - app = arthurai,. service. labels. network - app = arthurai \\ ingress - nginx \\ ingress - ng", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##els. network - app = arthurai,. service. labels. network - app = arthurai \\ ingress - nginx \\ ingress - nginx / ingress - nginx look up the hostname for the ingress and configure it in your dns ( e. g. arthur. mydomain. com ). shellkubectl get svc - n ingress - system ingress - nginx - controller - ojsonpath ='{. status. loadbalancer. ingress [ * ]. hostname }'install prometheus installing the chart", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- ojsonpath ='{. status. loadbalancer. ingress [ * ]. hostname }'install prometheus installing the chart shellhelm repo add \\ prometheus - community \\ https : / / prometheus - community. github. io / helm - charts helm repo update shellhelm upgrade - - install - n monitoring \\ - - create - namespace \\ kube - prometheus - stack \\ prometheus - community / kube - prometheus - stack \\ - f / path / to / values. yaml # see below for contents of this file helm", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- community / kube - prometheus - stack \\ - f / path / to / values. yaml # see below for contents of this file helm upgrade - - install - n monitoring \\ - - create - namespace \\ prometheus - adapter \\ prometheus - community / prometheus - adapter note : the values. yaml is not incremental. helm uses a single values. yaml file, so all configuration must be present in the same values. yaml file. if you are doing this step by step, you must re - apply the prior changes to values. yaml file.", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the same values. yaml file. if you are doing this step by step, you must re - apply the prior changes to values. yaml file. setting up retention for grafana and prometheus by default, prometheus and grafana will use local pod storage to store metrics / dashboards. these metrics / dashboards will be lost if the pod restarts for any reason. to avoid this and keep the metrics / dashboards for a longer period of time, add the following to your values. yaml to use a persistent volume store : prometheus : prometheusspec : servicemon", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "period of time, add the following to your values. yaml to use a persistent volume store : prometheus : prometheusspec : servicemonitorselectorniluseshelmvalues : false retention : 30d # metrics rolled over every 30 days retentionsize : 49gib # size of metrics retained before they are rolled over storagespec : volumeclaimtemplate : spec : storageclassname : gp2 accessmodes : [ \" readwriteonce \" ] resources : requests : storage : 50gi # size of disk for metrics grafana : persistence : type : pvc enabled", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" readwriteonce \" ] resources : requests : storage : 50gi # size of disk for metrics grafana : persistence : type : pvc enabled : true storageclassname : gp2 accessmodes : - readwriteonce size : 1gi # size of disk for dashboards run the following command to apply the updated configurations ( replace the path to the values. yaml file ) : shellhelm upgrade - - install - n monitoring \\ kube - prometheus - stack \\ prometheus - community / kube - prometheus - stack \\ - f / path / to / values. yaml setting", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##etheus - stack \\ prometheus - community / kube - prometheus - stack \\ - f / path / to / values. yaml setting up ingress for prometheus, alertmanager and grafana grafana and prometheus are useful to have exposed on an ingress route so that cluster administrators can access real - time telemetry and observe the behavior of the arthur platform. please note that grafana comes with a default username and password which should be changed immediately once installed. we also highly recommend installing prometheus and grafana within a vpc where the domains will not be exposed to the", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "which should be changed immediately once installed. we also highly recommend installing prometheus and grafana within a vpc where the domains will not be exposed to the public internet. the steps to enable ingress are : copy the values. yaml file below with the ingress configuration make the following edits to the yaml file that describe your environment : the ingressclassname if you installed using the nginx chart above, this should be nginx. if you are using a custom nginx ingressclass, this will be the name of that ingress class if you unsure what your ingressclass is called,", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are using a custom nginx ingressclass, this will be the name of that ingress class if you unsure what your ingressclass is called, run kubectl get ingressclass the url hostnames that you want to expose for these services note - these url hostnames will need to be published dns entries run the following command to deploy ( replace the path to the values. yaml file ) helm upgrade - - install - n monitoring \\ kube - prometheus - stack \\ prometheus - community / kube - prometheus - stack \\ - f / path / to", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "monitoring \\ kube - prometheus - stack \\ prometheus - community / kube - prometheus - stack \\ - f / path / to / values. yaml to confirm this is working, navigate to the url hostname defined in the values. yaml and you should be taken to the front page for either grafana or prometheus. change the default password for grafana. here is the values. yaml file that configures ingress for prometheus / grafana : prometheus : ingress : enabled : true ingressclassname : nginx # confirm this is", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ingress for prometheus / grafana : prometheus : ingress : enabled : true ingressclassname : nginx # confirm this is correct or replace me hosts : - prometheus. mydomain. com # replace me alertmanager : ingress : enabled : true ingressclassname : nginx hosts : - alertmanager. mydomain. com # replace me grafana : ingress : enabled : true ingressclassname : nginx # confirm this is correct or replace me hosts : - grafana. mydomain. com # replace me verifying the install verify", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##name : nginx # confirm this is correct or replace me hosts : - grafana. mydomain. com # replace me verifying the install verify that prometheus crds are installed : shellkubectl api - resources grep monitoring verify that prometheus is up and running : shellkubectl - - namespace monitoring get pods - l \" release = kube - prometheus - stack \" if everything is installed correctly, the following command should not return \" serviceunavailable \" : shellkubectl get - - raw / apis / custom. metrics. k8", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "command should not return \" serviceunavailable \" : shellkubectl get - - raw / apis / custom. metrics. k8s. io / v1beta1 monitoring and alerting for the arthur platform when you are ready to setup monitoring and alerting, please reach out to your arthur support representative and we can share additional details on this. prometheus alerts can be configured to trigger when certain rules that are setup to track prometheus metrics violate for a period of time, which is customizable. for more information see the prometheus alerting documentation these rules can then be", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##eus metrics violate for a period of time, which is customizable. for more information see the prometheus alerting documentation these rules can then be configured to send via a notification channel ( eg : email, slack, etc ) so that someone can be notified. this is the job of the alert manager. install metrics server example : shellhelm repo add bitnami https : / / charts. bitnami. com / bitnami helm repo update helm upgrade - - install - n monitoring \\ - - create - namespace \\ metrics - server \\ bitnami / metrics - server \\ - - set api", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "upgrade - - install - n monitoring \\ - - create - namespace \\ metrics - server \\ bitnami / metrics - server \\ - - set apiservice. create = true \\ - - set - - extraargs. kubelet - preferred - address - types = internalip verify that you can retrieve metric snapshots. shellkubectl top node configure the cluster - autoscaler in a production environment, it is vital to ensure that there are enough resources ( memory and cpu ) available for pods to get scheduled on the kubernetes cluster. please follow the instructions for your", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to ensure that there are enough resources ( memory and cpu ) available for pods to get scheduled on the kubernetes cluster. please follow the instructions for your cloud provider to install the cluster - autoscaler on your cluster. verify that the cluster - autoscaler is successfully installed. shellkubectl get deployments - n kube - system grep - i cluster - autoscaler cloud provider - specific configuration if installing on an existing amazon aws eks, follow the additional steps deploying on amazon aws eksupdated 14 days ago table of contents install ingress nginx install prometheus installing", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", follow the additional steps deploying on amazon aws eksupdated 14 days ago table of contents install ingress nginx install prometheus installing the chart setting up retention for grafana and prometheus setting up ingress for prometheus, alertmanager and grafana verifying the install monitoring and alerting for the arthur platform install metrics server configure the cluster - autoscaler cloud provider - specific configuration source : https : / / docs. arthur. ai / docs / kubernetes - preparation", "metadata": {"source": "https://docs.arthur.ai/docs/kubernetes-preparation", "row": 36, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 37 text : multiclass classification jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by multiclass classificationsuggest editsmulticlass classification models predict one class from more than two potential classes. in arthur, these models fall into the category of classification and are represented by the multiclass model type. some common examples of text multiclass classification are : is the sentiment of this tweet neutral, positive, or negative? what category does this document fall into? similar to binary classification, these models frequently output not only the predicted class", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "tweet neutral, positive, or negative? what category does this document fall into? similar to binary classification, these models frequently output not only the predicted class but also a probability for each class predicted. the highest probability class is then the predicted output. in these cases, a threshold does not need to be provided to arthur and it will automatically track the highest probability class as the predicted output. formatted data in arthur text multiclass classification models require three things to be specified in their schema : text input, predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "their schema : text input, predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( text input ) probability of prediction aprobability of prediction bprobability of prediction cground truthnon - input attribute ( numeric or categorical ) alea iacta est. 90. 05. 05amalecastigat ridendo mores.. 46. 14. 40bfemaleplenus venter non", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##cta est. 90. 05. 05amalecastigat ridendo mores.. 46. 14. 40bfemaleplenus venter non studet libenter.. 16. 17. 71cfemale predict function and mapping these are some examples of common values teams need to onboard for their multi - class classification models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. there are 2 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". there are 2 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth mappingexample prediction function # # option 1 : multiple prediction columns, single ground truth column # map each predictedvalue attribute to its corresponding groundtruth value. output _ mapping _ 1 = {'pred _ class _ one _ column':'one ','pred _ class _ two _ column':", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ mapping _ 1 = {'pred _ class _ one _ column':'one ','pred _ class _ two _ column':'two ','pred _ class _ three _ column':'three'} # build arthur model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = output _ mapping _ 1 ) # # option 2 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. output _ mapping _ 2", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") # # option 2 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. output _ mapping _ 2 = {'pred _ class _ one _ column':'gt _ class _ one _ column ','pred _ class _ two _ column':'gt _ class _ two _ column ','pred _ class _ three _ column':'gt _ class _ three _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping _ 2 ) # # example prediction function for binary classification def predict ( x ) : return model. predict _ proba ( x ) available metrics when onboarding tabular classification models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the -", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a multiclass classification model. find out more about these metrics in the performance metrics section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestion", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##city ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case text ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams are able to evaluate drift for inference data at", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ure driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. for text models, however, the only attribute required to onboard a model is the image attribute. so, it is only possible to monitor non - input attributes for fairness in image", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", however, the only attribute required to onboard a model is the image attribute. so, it is only possible to monitor non - input attributes for fairness in image models. metricmetric typeaccuracy ratefairnesstrue positive rate ( equal opportunity ) fairnesstrue negative ratefairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai /", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / text - multi - class - classification - 1", "metadata": {"source": "https://docs.arthur.ai/docs/text-multi-class-classification-1", "row": 37, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 38 text : sagemaker jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by sagemakerusing sagemaker data capturesuggest editsmodels deployed with aws sagemaker can be configured to automatically push their real - time inferences to arthur scope by utilizing sagemaker data capture. this guide walks through setting up that integration and utilizing a lambda function to send data capture to log files to be ingested by the arthur platform. prerequisites the model for which inferences are being ingested has already been onboarded onto arthur", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "log files to be ingested by the arthur platform. prerequisites the model for which inferences are being ingested has already been onboarded onto arthur. the sagemaker model schema matches that of its arthur model counterpart. sagemaker configuration aws sagemaker offers two features that enable this arthur integration : real - time endpoints & data capture. endpoints are apis that expose a trained model. users can use the api to retrieve predictions from the hosted model in the endpoint. data capture is a feature that logs the inputs and outputs of each prediction from the hosted model endpoints. to enable data capture in a way", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the endpoint. data capture is a feature that logs the inputs and outputs of each prediction from the hosted model endpoints. to enable data capture in a way that accurately logs all input and output data needed for the arthur integration, a configuration must be passed in when deploying an endpoint ( see below ). configuring data capture through the sagemaker sdk an extended description of the following configuration can be found in the \" sagemaker python sdk \" tab of the sagemaker data capture documentation. pythonfrom sagemaker. model import model from sagemaker. model _ monitor import datacaptureconfig s3", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the sagemaker data capture documentation. pythonfrom sagemaker. model import model from sagemaker. model _ monitor import datacaptureconfig s3 _ capture _ upload _ path = f \" s3 : / / { bucket - name } / { model - specific - path } / datacapture \" model = model (... ) data _ capture _ config = datacaptureconfig ( enable _ capture = true, sampling _ percentage = 100, destination _ s3 _ uri = s3 _ capture _ upload _ path, capture _ options = ['request ', '", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "percentage = 100, destination _ s3 _ uri = s3 _ capture _ upload _ path, capture _ options = ['request ','response'], ) model. deploy ( data _ capture _ config = data _ capture _ config,... ) this integration requires that datacaptureconfig be set such that : capture _ options includes both request and response to record model inputs and outputs for each inference sampling _ percentage is set to 100to comprehensively ingest all new inferences enable _ capture is set to true configuring data capture through the sagemaker api users", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is set to 100to comprehensively ingest all new inferences enable _ capture is set to true configuring data capture through the sagemaker api users can also call the createendpoint api to create a real - time endpoint via the api. to ensure that this endpoint is deployed with data capture enabled, it must receive an endpointconfigname that matches an endpointconfig created using the createendpointconfig api with the following specifications : {..., \" datacaptureconfig \" : { \" capturecontenttypeheader \" : { \" csvconte", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "specifications : {..., \" datacaptureconfig \" : { \" capturecontenttypeheader \" : { \" csvcontenttypes \" : [ \" string \" ], \" jsoncontenttypes \" : [ \" string \" ] }, \" captureoptions \" : [ { \" capturemode \" : \" input \" }, { \" capturemode \" : \" output \" } ], \" destinations3uri \" : \" string \", \" enablecapture \" : true, \" initialsamplingpercentage \" : 100, \" kmskeyid \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" string \", \" enablecapture \" : true, \" initialsamplingpercentage \" : 100, \" kmskeyid \" : \" string \" }, \" endpointconfigname \" : \" string \",... } this integration requires that datacaptureconfig be set such that : capturecontenttypeheader be specified to an arthur - supported content type ( see section below ). if no csvcontenttypes or jsoncontenttypes are specified, sagemaker will by default base64 encode when capturing the data. this content type is currently not", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##types or jsoncontenttypes are specified, sagemaker will by default base64 encode when capturing the data. this content type is currently not supported by the arthur platform. captureoptions be set to both the input and output capture modes. enablecapture be set to true. initialsamplingpercentage be set to 100. supported data formats aws sagemaker algorithms can accept and produce numerous mime types for the http payloads used in retrieving predictions from endpoint - hosted models. the mime type utilized in an endpoint invocation also corresponds to the format of the data captured inference. the", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##eving predictions from endpoint - hosted models. the mime type utilized in an endpoint invocation also corresponds to the format of the data captured inference. the arthur platform supports the following mime types / data formats for those types : mime type : text / csv 37, self - emp - not - inc, 227253, preschool, 1, married - civ - spouse, sales, husband, white, male, 0, 0, 30, mexico \\ n24, private, 211129, bachelors, 13, never - married, exec - managerial, other - relative, white, female,", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\\ n24, private, 211129, bachelors, 13, never - married, exec - managerial, other - relative, white, female, 0, 0, 60, united - states \\ n each inference is represented as an ordered row of comma - separate values, where each value represents a feature in the inference these features must be specified in the same order as their counterparts in the corresponding arthur model if multiple inferences are included in a single call to invoke _ endpoint, each inference is separated by \\ n mime type : application / json arthur currently supports two unique json formats, described with", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##voke _ endpoint, each inference is separated by \\ n mime type : application / json arthur currently supports two unique json formats, described with examples below. option 1 : column - ordered list of feature - values json { \" instances \" : [ { \" features \" : [ 1. 5, 16, \" teststringa \", false ] }, { \" features \" : [ 2. 0, 12, \" teststringb \", true ] } ] } each inference is represented as a new object inside a json array the upper - level key mapping to this inference array is named one of the", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] } ] } each inference is represented as a new object inside a json array the upper - level key mapping to this inference array is named one of the following : instances, predictions each object within this json array is a key mapping to an ordered array of features the second level key mapping to this feature array is named one of the following : features, probabilities option 2 : feature - name keys to values map json { \" predictions \" : [ { \" closest _ cluster \" : 5, \" distance _ to _ cluster \" : 36. 5 }, { \" closest _ cluster \" : 2, \" distance _ to", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ cluster \" : 5, \" distance _ to _ cluster \" : 36. 5 }, { \" closest _ cluster \" : 2, \" distance _ to _ cluster \" : 90. 3 } ] } each inference is represented as an object inside a json array the upper - level key mapping to this inference array is named one of the following : instances, predictions each object within this json array has keys representing feature names mapping to their corresponding feature values. the names of these features cannot be any one of the following : instances, predictions, features, probabilities specifying partner inference id on arthur - ingested data capture inference", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "these features cannot be any one of the following : instances, predictions, features, probabilities specifying partner inference id on arthur - ingested data capture inferences the arthur platform enforces that each uploaded inference has a partner inference id, which is a unique identifier used as the matching mechanism for joining ground truth data. arthur's sagemaker integration populates the arthur inference id from two possible sources in sagemaker. the default is to use sagemaker's eventid, which is a random id auto - generated by sagemaker for each request. sagemaker's eventid is captured in the eventmetadata /", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##id, which is a random id auto - generated by sagemaker for each request. sagemaker's eventid is captured in the eventmetadata / eventid field of the data capture output files. as another option, sagemaker allows invoke - endpoint api callees to specify an inferenceid ( or inference - id ) to a call when using the api, sdk function, or cli to invoke an endpoint. when inferenceid is specified, sagemaker appends an eventmetadata / inferenceid field to the data capture event. both approaches generate a single eventid or inferenceid", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "specified, sagemaker appends an eventmetadata / inferenceid field to the data capture event. both approaches generate a single eventid or inferenceid for each call to invoke - endpoint. if an inferenceid is specified, arthur will use it as the arthur partner inference id. otherwise, it will default to the sagemaker eventid. one tricky part about sagemaker's invoke - endpoint api it allows requesting multiple inferences in a single invoke - endpoint api call. in this case, the sagemaker eventid or callee - specified inferenceid would be shared by all inference", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "single invoke - endpoint api call. in this case, the sagemaker eventid or callee - specified inferenceid would be shared by all inferences in the call and would not be unique. when this occurs, the arthur integration will append an index number to either the eventid or inferenceid based on the inference order in the call to invoke - endpoint. when ingesting data capture inferences from sagemaker, the following table describes the partner inference id any given inference is assigned on the arthur platform. sagemaker invoke call without inference id provided in invoke endpoint with inference id", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "partner inference id any given inference is assigned on the arthur platform. sagemaker invoke call without inference id provided in invoke endpoint with inference id provided in invoke endpoint single inference in invoke endpoint eventid inferenceid multiple inferences in invoke endpoint eventid _ { index _ within _ invoke _ endpoint _ call } inferenceid _ { index _ within _ invoke _ endpoint _ call } inferenceid and eventid refer to the inference id and event id, respectively, provided when calling invoke endpoint either through the api or boto3 sdk", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and eventid refer to the inference id and event id, respectively, provided when calling invoke endpoint either through the api or boto3 sdk. index _ within _ invoke _ endpoint _ call refers to the index of the specific inference within a group of multiple / mini - batch inferences sent through the invoke - endpoint call. for example, for an invoke - endpoint call including csv data 1, 2, 3, 4 \\ n5, 6, 7, 8 \\ n and inference id abcdefg - 12345, the inference containing the features 1, 2", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", 4 \\ n5, 6, 7, 8 \\ n and inference id abcdefg - 12345, the inference containing the features 1, 2, 3, 4 would have a partner inference id of abcdefg - 12345 _ 0 on the arthur platform and the inference containing the features 5, 6, 7, 8 would have a partner inference id of abcdefg - 12345 _ 1. ( s3 _ batch _ ingestion ) = aws lambda setup this section provides an example of a single - lambda - per - arthur - model setup. the following code is meant to serve as an", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= aws lambda setup this section provides an example of a single - lambda - per - arthur - model setup. the following code is meant to serve as an example and can be implemented in a variety of ways that fit your organization's tech stack. this build sets up an s3 object creation lambda trigger to run the function whenever sagemaker writes a file to the bucket. this sample code will then pull the file and upload it to arthur. in its current form, the code assumes that all s3 object notifications will be for the single model for the configured arthur _ model _ id. see the following sections for the configurations", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the code assumes that all s3 object notifications will be for the single model for the configured arthur _ model _ id. see the following sections for the configurations required to use the lambda. lambda function configurations to create the lambda function, go to the aws lambda console, click \" create function \" and select \" use a blueprint \", then search for and select s3 - get - object - python. then ensure you select or create an execution role with access to your data capture upload bucket ( s ). finally, ensure the following configurations are set and create the function : timeout : 15 min 0 sec ( must be", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data capture upload bucket ( s ). finally, ensure the following configurations are set and create the function : timeout : 15 min 0 sec ( must be set after lambda function creation in the \" configuration \" / \" general configuration \" tab ) environment variables arthur _ host : the host url for your arthur deployment ( or https : / / app. arthur. ai / for saas customers ) arthur _ model _ id : the id assigned to the arthur model to accept the new inferences arthur _ access _ token : an access token for the arthur platform this can be replaced by the retrieval of this token at lambda runtime the token", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "new inferences arthur _ access _ token : an access token for the arthur platform this can be replaced by the retrieval of this token at lambda runtime the token must, at the very least, provide raw _ data write access lambda function trigger source : s3 bucket : your data capture configured s3 bucket event : all object create events prefix : path to your model's specific data capture output directory suffix :. jsonl aws s3 trigger overlap error aws prevents multiple s3 triggers from applying to the same subset ( s ) of files. therefore, be careful in specifying your prefix / suffix and the files they indicate", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "multiple s3 triggers from applying to the same subset ( s ) of files. therefore, be careful in specifying your prefix / suffix and the files they indicate. for example, in the following setup of s3 triggers, aws would raise errors because of overlap with. jsonl files in / modela / datacapture ( triggers a + c ) as well as overlap with. tar. gz files in / modela ( triggers b + c ) : trigger a : ( prefix : s3 : / / bucket / modela / datacapture ) ( suffix :. jsonl ) trigger b : ( prefix", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a : ( prefix : s3 : / / bucket / modela / datacapture ) ( suffix :. jsonl ) trigger b : ( prefix : s3 : / / bucket / modela ) ( suffix :. tar. gz ) trigger c : ( prefix : s3 : / / bucket / modela ) ( suffix : unspecified ) in the above cases, aws will still successfully create the lambda but will then raise the following error at the top of their ui : your lambda function \" lambda - function - name \" was successfully created, but an error occurred when creating the trigger : configuration is ambiguously defined", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of their ui : your lambda function \" lambda - function - name \" was successfully created, but an error occurred when creating the trigger : configuration is ambiguously defined. cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type. lambda code pythonimport urllib. parse import boto3 import os import requests s3 = boto3. client ('s3') arthur _ model _ id = os. environ [ \" arthur _ model _ id \" ] # 12345678 - 1234 - 1234 - 1234 - 1234567890", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on [ \" arthur _ model _ id \" ] # 12345678 - 1234 - 1234 - 1234 - 1234567890ab arthur _ host = os. environ [ \" arthur _ host \" ] # https : / / app. arthur. ai / if arthur _ host [ - 1 ]! ='/': arthur _ host + ='/'# ensure trailing slash exists # todo by user # fill in code to retrieve an arthur api key arthur _ access _ token = os. environ [ \" arthur _ access _ token \" ] arthur _ endpoint = f", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to retrieve an arthur api key arthur _ access _ token = os. environ [ \" arthur _ access _ token \" ] arthur _ endpoint = f \" api / v3 / models / { arthur _ model _ id } / inferences / integrations / sagemaker _ data _ capture \" def lambda _ handler ( event, context ) : # get the object from the event bucket = event ['records'] [ 0 ] ['s3'] ['bucket'] ['name'] key = urllib. parse. unquote _ plus ( event ['records'] [ 0", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' bucket'] ['name'] key = urllib. parse. unquote _ plus ( event ['records'] [ 0 ] ['s3'] ['object'] ['key'], encoding ='utf - 8') try : s3 _ object = s3. get _ object ( bucket = bucket, key = key ) datacapture _ body = s3 _ object. get ('body') request _ url = urllib. parse. urljoin ( arthur _ host, arthur _ endpoint ) print ( f \"", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' ) request _ url = urllib. parse. urljoin ( arthur _ host, arthur _ endpoint ) print ( f \" request : post { request _ url } \" ) response = requests. post ( request _ url, files = {'inference _ data': ('smdatacapture. jsonl ', datacapture _ body, s3 _ object ['contenttype'] ) }, headers = {'authorization': arthur _ access _ token } ) print ( f \" response : { response. content } \" ) except exception as e :", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s = {'authorization': arthur _ access _ token } ) print ( f \" response : { response. content } \" ) except exception as e : print ( e ) print ('error getting object { } from bucket { }.'' make sure they exist and your bucket is in the same region as this function. '. format ( key, bucket ) ) raise e summary with your sagemaker endpoint deployed ( with data capture configured ) and a lambda function ready for s3 updates, you can send requests to your sagemaker endpoint to generate predictions. the predictions will be logged as files in s3 by data", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ready for s3 updates, you can send requests to your sagemaker endpoint to generate predictions. the predictions will be logged as files in s3 by data capture, and the lambda function will upload the inferences to arthur, where you can see them in the dashboard. updated 3 months ago table of contents prerequisites sagemaker configuration configuring data capture through the sagemaker sdk configuring data capture through the sagemaker api supported data formats specifying partner inference id on arthur - ingested data capture inferences aws lambda setup lambda function configurations lambda function trigger lambda code summary source : https : /", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "specifying partner inference id on arthur - ingested data capture inferences aws lambda setup lambda function configurations lambda function trigger lambda code summary source : https : / / docs. arthur. ai / docs / sagemaker - data - capture", "metadata": {"source": "https://docs.arthur.ai/docs/sagemaker-data-capture", "row": 38, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 39 text : what does ongoing maintenance look like? jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser -", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ssionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explain", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstra", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityextern", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##installation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess control", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestorin", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##sarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by what does ongoing maintenance look like? suggest editsupdated 3 months ago source : https : / / docs. arthur. ai / docs / what - does - ongoing - maintenance - look - like", "metadata": {"source": "https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like", "row": 39, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 40 text : composing advanced functions jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by composing advanced functionssuggest editsto see functions you can use with this syntax, checkout the aggregation functions and the transformation functions guides. any function with a parameter with the type signature [ string or nested ] is able to accept the following as a value : a string constant that represents a property of the model an object of the form : json { \" alias _ ref \" : \" string \", \" nested _ function \" : { \"", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "property of the model an object of the form : json { \" alias _ ref \" : \" string \", \" nested _ function \" : { \"... \" : \"... \" } } only one of alias _ ref or nested _ function may be present at a time. see the following explanations of each. alias references the alias _ ref field allows specifying another selected column's alias to use as input to the function. this example uses an alias _ ref to pull in another column to the add function : json { \" select \" : [ { \" function \" : \" abs \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "an alias _ ref to pull in another column to the add function : json { \" select \" : [ { \" function \" : \" abs \", \" alias \" : \" absloan \", \" parameters \" : { \" property \" : \" loan \" } }, { \" function \" : \" add \", \" alias \" : \" plus2 \", \" parameters \" : { \" left \" : 2, \" right \" : { \" alias _ ref \" : \" absloan \" } } } ] } this request returns : json { \" query _ result \" : [ { \" absloan \" : 55", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" absloan \" } } } ] } this request returns : json { \" query _ result \" : [ { \" absloan \" : 55. 45, \" plus2 \" : 57. 45 } ] } nested functions the nested _ function field allows specifying another function definition to use as input. here's an example of how to calculate absolute error for a regression model. in this example, we pass the nested subtract function as input to the abs function via the nested _ function object for the property parameter of abs : json { \" select \" : [ { \" function \" :", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "input to the abs function via the nested _ function object for the property parameter of abs : json { \" select \" : [ { \" function \" : \" abs \", \" alias \" : \" abs _ error \", \" parameters \" : { \" property \" : { \" nested _ function \" : { \" function \" : \" subtract \", \" alias \" : \" error \", \" parameters \" : { \" left \" : \" predicted _ fico _ score \", \" right \" : { \" alias _ ref \" : \" ground _ truth \" } } } } } }, { \" property \" :", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ score \", \" right \" : { \" alias _ ref \" : \" ground _ truth \" } } } } } }, { \" property \" : \" predicted _ fico _ score \" }, { \" property \" : \" consumer _ credit _ score \", \" alias \" : \" ground _ truth \" } ] } this query returns : json { \" query _ result \" : [ { \" consumer _ credit _ score \" : 660, \" predicted _ fico _ score \" : 688. 10004, \" abs _ error \" : 28. 100040000000035 }, { \" consumer _ credit", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##co _ score \" : 688. 10004, \" abs _ error \" : 28. 100040000000035 }, { \" consumer _ credit _ score \" : 663, \" predicted _ fico _ score \" : 681, \" abs _ error \" : 18 }, \"... \" ] } [UNK] you use the same function multiple times in a query, you need to give each one a distinct \" alias \". otherwise, the names will conflict. updated 3 months ago table of contents alias references nested functions source : https : / / docs. arthur. ai / docs / composing -", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "conflict. updated 3 months ago table of contents alias references nested functions source : https : / / docs. arthur. ai / docs / composing - functions", "metadata": {"source": "https://docs.arthur.ai/docs/composing-functions", "row": 40, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 41 text : hot spots jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by hot spotsautomatically illuminate underperforming segments in your datasuggest editswhen a system has high - dimensional data, finding the right data input regions, such as troubleshooting, becomes a difficult problem. hotspots automate identifying regions associated with poor ml performance to significantly reduce the time and error of finding such regions. arthur scope utilizes a proprietary tree - based algorithm to search out areas of underperformance and", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "performance to significantly reduce the time and error of finding such regions. arthur scope utilizes a proprietary tree - based algorithm to search out areas of underperformance and explain them through human - understandable language. find out more about the algorithm here : hot spots in arthur scope hot spots are under the insights tab in your arthur model dashboard. time intervals : to highlight when underperformance occurred, hot spots are calculated for specific segments of time. for batch models, this is every batch. for streaming models, this is every 7 days. performance threshold : in the ui, hot spots we currently only have accuracy hot spots available to detect performance", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "batch. for streaming models, this is every 7 days. performance threshold : in the ui, hot spots we currently only have accuracy hot spots available to detect performance under 40 %. subgroups : the segments of data that have been identified as underperforming in each hot spot clicking on a hot spot subgroup performance : the accuracy rate for this subgroup of data inference count : how many inferences were included in this subgroup subgroup rules : the rules that define the inferences included in this subgroup. rules are set up to incorporate any metadata sent to arthur, not just model features ( i. e., can include any non - input attributes", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this subgroup. rules are set up to incorporate any metadata sent to arthur, not just model features ( i. e., can include any non - input attributes provided to arthur ). to ensure actionable hot spots, we allow a maximum of 7 rules. view inferences button : click on this button to go to the inferences deep dive page automatically filtered to match the rules within this subgroup for continued exploration status button : change the status from new to acknowledged to alert other team members that you have evaluated this hot spot. available arthur schemas currently only available for tabular binary and multiclass classification models within arthur. understanding the algorithm", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "members that you have evaluated this hot spot. available arthur schemas currently only available for tabular binary and multiclass classification models within arthur. understanding the algorithm to learn more about the algorithm used for hot spots. please refer to the arthur algorithms documentation section. updated 3 months ago what \u2019 s nextlearn more about enabling enrichments for your model in the model onboarding section. otherwise, click on explainability to learn about another type of enrichment. enabling enrichmentsexplainabilitytable of contents hot spots in arthur scope clicking on a hot spot available arthur schemas understanding the algorithm source : https : / / docs.", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ainabilitytable of contents hot spots in arthur scope clicking on a hot spot available arthur schemas understanding the algorithm source : https : / / docs. arthur. ai / docs / hot - spots", "metadata": {"source": "https://docs.arthur.ai/docs/hot-spots", "row": 41, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 42 text : sending inferences jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by sending inferencessuggest editsnow that you have registered your model successfully, you can connect your production pipeline to arthur. creating arthur connection to be able to send inference data to the platform, you will need to create a connection to not only your arthur platform but also the model the inferences are being tracked for. information about creating your api key and connecting to the arthur platform / model objects can be found in the ui guide. formatting inference", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s are being tracked for. information about creating your api key and connecting to the arthur platform / model objects can be found in the ui guide. formatting inference data the first thing you need to do to send inferences to arthur is to format the data into a structure arthur will understand. this will follow a similar structure to the formatting to onboard your reference dataset. however, there are some differences in added attributes to point out. the following attributes are formatted the exact same as your reference dataset and are required for all inferences sent. model attributes : all features the model uses to create predictions model predictions : model predictions for", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "same as your reference dataset and are required for all inferences sent. model attributes : all features the model uses to create predictions model predictions : model predictions for each row of data the next two parameters are required for inference datasets. however, these are not explicitly required for teams onboarding inferences with the arthur python sdk. inference timestamp : typically refers to the time of model prediction if the inference timestamp is not specified, the sdk will auto - populate this field with the time the inferences were logged into the arthur platform partner inference id : a way to match specific inferences in arthur against your", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- populate this field with the time the inferences were logged into the arthur platform partner inference id : a way to match specific inferences in arthur against your other systems and update your inferences with ground truth labels as they become available in the future. the most appropriate choice for a partner inference id depends on your specific circumstances, but common strategies include using existing ids and joining metadata with non - unique ids. if you already have existing ids that are unique to each inference and easily attached to future ground truth labels, you can simply use those ( casting to strings if needed ). another common approach is constructing a partner inference id", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inference and easily attached to future ground truth labels, you can simply use those ( casting to strings if needed ). another common approach is constructing a partner inference id from multiple pieces of metadata. for example, if your model makes predictions about your customers at least once daily, you might construct your partner inference ids as { customer _ id } - { date }. this would be easy to reconstruct when sending ground truth labels much later : simply look up the labels for all the customers passed to the model on a given day and append that date to their id. if you don \u2019 t supply partner inference ids, the sd", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the customers passed to the model on a given day and append that date to their id. if you don \u2019 t supply partner inference ids, the sdk will generate them for you and return them to your send _ inferences ( ) call. these can be kept for future reference or discarded if you \u2019 ve already sent ground truth values or don \u2019 t plan to in the future. [UNK] partner inference id with internal distinct idsmany ml models in production do not receive ground truth at the time of prediction. the technique that arthur uses to onboard ground truth later utilizes your partner inference id. teams that want to take advantage of metric", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ground truth at the time of prediction. the technique that arthur uses to onboard ground truth later utilizes your partner inference id. teams that want to take advantage of metrics, unique arthur enrichments, and popular reporting workflows that require ground truth will have to have a linkable connection between their data base and ours ( through partner inference id ). finally, the remaining information can be onboarded to arthur but does not need to be : non - input attributes : specifying values for non - input attributes is not required at the time of inference, i. e., you can send inferences with null non - input attributes [UNK] inference", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for non - input attributes is not required at the time of inference, i. e., you can send inferences with null non - input attributes [UNK] inference data is immutableyou may not update non - input attribute data ( or any inference data for that matter ) after sending it to the arthur platform. the only value that can be updated in arthur is ground truth, which we will see in the next section. send inferences to arthur inferences are commonly sent to arthur using our python sdk but can also be sent with our api or 3rd party integrations. python sdk ( quick integration ) : the most common way", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur using our python sdk but can also be sent with our api or 3rd party integrations. python sdk ( quick integration ) : the most common way to log inferences is with the python sdk. this can be done by adding our send _ inferences ( ) to your model's prediction function. teams must connect to arthur within their prediction script, run predictions, and send results to arthur. however, this option would have you add latency to the speed with which your model generates inferences. for more efficient approaches, see options 2 and 3. python sdk ( streaming uploads ) : if you write", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with which your model generates inferences. for more efficient approaches, see options 2 and 3. python sdk ( streaming uploads ) : if you write your model's inputs and outputs to a data stream, you can add a listener to that stream to log those inferences with arthur. for example, if you have a kafka topic, you might add a new arthur consumer group to listen to new events and pass them to the send _ inferences ( ) method. if your inputs and predictions live in different topics or you want to add non - input data from another topic, you might use kafka streams to", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "method. if your inputs and predictions live in different topics or you want to add non - input data from another topic, you might use kafka streams to join the various topics before sending them to arthur. python sdk ( inference upload jobs ) : another option is to read data from the rest and send it into the arthur platform. depending on their architecture, some teams choose a job or event - driven approach. they often have jobs that look up inferences since the last run, run a script that formats and writes the data into parquet files, and then use the python sdk function send _ bulk _ inferences (", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "last run, run a script that formats and writes the data into parquet files, and then use the python sdk function send _ bulk _ inferences ( ) to send the parquet files to the arthur platform. json payload function : for model deployments that do not have a python script to run, teams often choose to send inferences to our api through json payload. 3rd party integration : arthur has several integrations with third - party services, frameworks, and platforms. check out our integrations page to explore more potential integrations. python sdkjson payloadinference upload jobs # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and platforms. check out our integrations page to explore more potential integrations. python sdkjson payloadinference upload jobs # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # new code to fetch the arthurmodel # connect to arthur import os from arthurai import arthurai arthur = arthurai ( url = \" https : / / app. arthur. ai \", access _ key = os. envir", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ai import arthurai arthur = arthurai ( url = \" https : / / app. arthur. ai \", access _ key = os. environ [ \" arthur _ api _ key \" ] ) # retrieve the arthur model arthur _ model = arthur. get _ model ( os. environ [ \" arthur _ partner _ model _ id \" ], id _ type ='partner _ model _ id') # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # your original model prediction function # which can be on its own as a python script # or wrapped by an api like a flask app def predict ( ) : # get data to apply model to inference _ data =... # generate inferences # in this example, the predictions are classification probabilities predictions = model. predict _ proba (... ) # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this example, the predictions are classification probabilities predictions = model. predict _ proba (... ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # new part of your model's prediction script # send new inferences to arthur arthur _ model. send _ inferences ( inference _ data, predictions = predictions ) # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ model. send _ inferences ( inference _ data, predictions = predictions ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # return predictions updated 3 months ago table of contents creating arthur connection formatting inference data send inferences to arthur source : https : / / docs. arthur. ai / docs / sending - inferences - 1", "metadata": {"source": "https://docs.arthur.ai/docs/sending-inferences-1", "row": 42, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 43 text : platform readiness for existing cluster installs jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser -", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ssionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explain", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstra", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityextern", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##installation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess control", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestorin", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##sarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by platform readiness for existing cluster installssuggest editsthe arthur platform can be installed on an on - prem or cloud - based pre - provisioned kubernetes cluster, so all the data and controls adhere to existing corporate practices. arthur cluster installs faqs sysadmin what kind of privileges / hardware does the sysadmin installing the arthur platform need? the sysadmin will need a works", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s sysadmin what kind of privileges / hardware does the sysadmin installing the arthur platform need? the sysadmin will need a workstation with the following requirements : running linux or macos. the kots cli installer does not support windows. sudo / root access. to install the kots cli plugin for kubectl. connection to the kubernetes cluster, using kubectl, and privileges to deploy k8s objects at either cluster or namespace scope ( at least ). ( recommended ) access to the internet. for downloading the installer, plug", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s objects at either cluster or namespace scope ( at least ). ( recommended ) access to the internet. for downloading the installer, plugins and fetching updates. how can i download the artifacts required for installing the arthur platform? all artifacts required for installing the arthur platform are available on a customer - specific password - protected portal, which your sales team can give you access to. it is recommended that the portal is accessible from within your corporate network, since the artifacts are around mutiple gbs in size. does my kubernetes cluster need access to the internet? the arthur platform can be installed without", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the artifacts are around mutiple gbs in size. does my kubernetes cluster need access to the internet? the arthur platform can be installed without internet access, once all the required files are downloaded and available locally. however, we recommend access to the internet from the kubernetes cluster for an efficient install and upgrade experience. please inform your sales team about any network restrictions and optionally, if its possible to { ref } whitelist specific urls < requirements _ for _ online _ installation >. cloud providers which kubernetes distributions that arthur supports out - of - the - box? arthur is architected to", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ for _ online _ installation >. cloud providers which kubernetes distributions that arthur supports out - of - the - box? arthur is architected to run on any distribution of kubernetes, however certain commercial distributions are untested. the arthur application is validated / tested on : amazon aws eks which cloud providers has arthur been tested on? the arthur platform has been tested on the following cloud providers : amazon aws what container runtimes does arthur support? containers in the arthur platform run on the following container runtimes : docker ( slated to be deprecated in kubernetes 1", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur support? containers in the arthur platform run on the following container runtimes : docker ( slated to be deprecated in kubernetes 1. 24 ) containerd kubernetes server what version ( s ) of kubernetes server does arthur support? please refer to the requirements documentation. can the arthur platform be scoped to a dedicated namespace? the arthur platform can be deployed and scoped to a specific namespace, though there are some cluster - level customresourcedefinitions that need to be pre - installed. see details here. what are the minimum resource requirements for operating the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster - level customresourcedefinitions that need to be pre - installed. see details here. what are the minimum resource requirements for operating the arthur platform? optimal performance of the arthur platform is ensured on a 6 node cluster ( though test clusters can be provisioned with 3 nodes ) with each node having 16 cpus, 32gb memory ( ram ) and 1000 gb storage with at least 3000 iops. however, please reach out to your sales team for a tailored configuration custom to your projected workloads. is there a default storageclass defined on the kubernetes cluster? the kubernetes cluster must have", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "configuration custom to your projected workloads. is there a default storageclass defined on the kubernetes cluster? the kubernetes cluster must have a default storageclass defined before starting the arthur platform installation. if a default storageclass does not exist, adding the storageclass. kubernetes. io / is - default - class : \" true \" annotation to a storageclass should remedy this requirement. what ingress controller are you planning to use to access the arthur platform? is it already installed? the arthur platform needs to expose a couple of services so the application is accessible outside the cluster. all compatible ku", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "access the arthur platform? is it already installed? the arthur platform needs to expose a couple of services so the application is accessible outside the cluster. all compatible kubernetes ingress controllers should work, though { ref } nginx ingress controller < k8s _ install _ prep _ install _ ingress > installed in a separate namespace is recommended. are there any securitycontext requirements on the kubernetes cluster? the arthur platform is architected to leverage as few permissions as deemed necessary for optimal functioning. no container is run as root. all processes are owned by non - system users. please reach out", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "leverage as few permissions as deemed necessary for optimal functioning. no container is run as root. all processes are owned by non - system users. please reach out to your sales team if you have specific securitycontext requirements. does arthur support running on selinux environments? the arthur platform requires selinux to be running in permissive mode, if enabled. are there any network policies configured on the kubernetes cluster? pods of the arthur platform will need to communicate with each other. by default, pods can communicate with each other. please reach out to your sales team if you have custom network policies configured on the kuber", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with each other. by default, pods can communicate with each other. please reach out to your sales team if you have custom network policies configured on the kubernetes cluster. how many ip addresses should be available for the arthur platform? the arthur platform is architected to be scalable, using resources on - demand. given the dynamic nature of the infrastructure involved, we recommend at least 128 ip address cidr blocks attached to the relevant subnets. however, this number can increase as more models are onboarded to the platform. are there any namespace - level constraints enforced on the kubernetes cluster? please let your sales", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "increase as more models are onboarded to the platform. are there any namespace - level constraints enforced on the kubernetes cluster? please let your sales team know if there are any constraints configured at the namespace - level on the kubernetes cluster, as this will help prepare for a smooth installation experience. are there any cluster - level constraints enforced on the kubernetes cluster? please let your sales team know if there are any specific cluster - level contraints configured on the kubernetes cluster, as this will help prepare for a smooth installation experience. does the kubernetes cluster have access to", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ints configured on the kubernetes cluster, as this will help prepare for a smooth installation experience. does the kubernetes cluster have access to a private / public container registry? the kubernetes cluster on which the arthur platform will be installed must have connectivity to a container registry. the sysadmin performing the installation must also have read / write access to the same container registry. does the kubernetes cluster have access to a private / public pypi / conda registry? the kubernetes cluster on which the arthur platform will be installed must have connectivity to a pypi / con", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "pypi / conda registry? the kubernetes cluster on which the arthur platform will be installed must have connectivity to a pypi / conda registry, which ensures optimum utilization of the features of the platform. other considerations does your enterprise have a software procurement process? please keep your sales team informed of any software procurement process that maybe in place before installing new software, and potential turnaround times for such processes. do you want to deploy arthur on infrastructure that isn't mentioned above ( eg : cloud providers, kubernetes distributions, etc. )? if so, please inform your sales team as soon", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "isn't mentioned above ( eg : cloud providers, kubernetes distributions, etc. )? if so, please inform your sales team as soon as possible so we can setup an architecture review between your platform team and arthur's platform team. can any of the arthur platform components be externalized, so its not managed by arthur? the platform supports the use of aws s3 as well as most s3 compatible systems as the object / blob store. the embedded metadata database can be replaced by a recent version of postgres. a managed service for s3 and / or postgres is recommended for", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". the embedded metadata database can be replaced by a recent version of postgres. a managed service for s3 and / or postgres is recommended for production - grade installs. can the arthur platform be deployed on a kubernetes cluster that is shared with other applications? the arthur platform has been architected to be highly scalable and reliable. based on usage ( number of models ) and load ( data ingestion ), pods are scaled in short periods of time to ensure efficient operation. as such, if other applications will be installed on the same kubernetes platform, talk to your sales team about provision", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of time to ensure efficient operation. as such, if other applications will be installed on the same kubernetes platform, talk to your sales team about provisioning dedicated nodegroups for the cluster. does the arthur platform support different organizations / business units using the same application? yes. see our guide on user and org management. updated 3 months ago table of contents arthur cluster installs faqs sysadmin cloud providers kubernetes server other considerations source : https : / / docs. arthur. ai / docs / platform - readiness - for - existing - cluster - installs", "metadata": {"source": "https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs", "row": 43, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 44 text : multiclass classification jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by multiclass classificationsuggest editsmulticlass classification models predict one class from more than two potential classes. in arthur, these models fall into the classification category and are represented by the multiclass model type. some common examples of image multiclass classification are : what breed of dog is in this photo? what part of the car is damaged in this photo? similar to binary classification, these models frequently output the predicted class and a probability for each class", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this photo? what part of the car is damaged in this photo? similar to binary classification, these models frequently output the predicted class and a probability for each class predicted. the highest probability class is then the predicted output. in these cases, a threshold does not need to be provided to arthur, and it will automatically track the highest probability class as the predicted output. formatted data in arthur image multiclass classification models require three things to be specified in their schema : the image input, the predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "input, the predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for the model ( i. e., any information you want to track about your inferences ) as non - input attributes. attribute ( image input ) probability of prediction aprobability of prediction bprobability of prediction cground truthnon - input attribute ( numeric or categorical ) image _ 1. jpg. 90. 05. 05amaleimage _ 2. jpg. 46. 14. 40bfemaleimage _ 3. jpg. 16.", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". 90. 05. 05amaleimage _ 2. jpg. 46. 14. 40bfemaleimage _ 3. jpg. 16. 17. 71cfemale predict function and mapping these are some examples of common values teams need to onboard for their multiclass classification models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. there are 2 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth mappingexample prediction function # # option 1 : multiple prediction columns, single ground truth column # map each predictedvalue attribute to its corresponding groundtruth value. output _ mapping _ 1 = {'pred _ class _ one _ column':'one ','pred _ class _ two _ column':'two ','pred _ class _", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "class _ one _ column':'one ','pred _ class _ two _ column':'two ','pred _ class _ three _ column':'three'} # build arthur model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = output _ mapping _ 1 ) # # option 2 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. output _ mapping _ 2 = {'pred _ class _ one _", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. output _ mapping _ 2 = {'pred _ class _ one _ column':'gt _ class _ one _ column ','pred _ class _ two _ column':'gt _ class _ two _ column ','pred _ class _ three _ column':'gt _ class _ three _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping _ 2 ) # # example prediction function for binary", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping _ 2 ) # # example prediction function for binary classification def predict ( x ) : return model. predict _ proba ( x ) available metrics when onboarding multiclass classification models, several default metrics are available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a multiclass classification model.", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a multiclass classification model. learn more about these metrics in the performance metrics section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. learn more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case, text ) drift is calculated with multi", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case, text ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams can evaluate drift for inference data at different intervals with our python sdk and query service ( for example, data coming", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "driftmultivariate drift note : teams can evaluate drift for inference data at different intervals with our python sdk and query service ( for example, data coming into the model now compared to a month ago ). fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. for text models, however, the image attribute is the only attribute required to onboard a model. so, monitoring non - input attributes for fairness in image models is only possible. metricmetric typeaccuracy ratefairnesstrue positive rate (", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "so, monitoring non - input attributes for fairness in image models is only possible. metricmetric typeaccuracy ratefairnesstrue positive rate ( equal opportunity ) fairnesstrue negative ratefairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / image - multi - class - classification - 2", "metadata": {"source": "https://docs.arthur.ai/docs/image-multi-class-classification-2", "row": 44, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 45 text : custom rbac jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by custom rbacsuggest editsmanaging rbac and organizations for sso users for customers using sso ( on - prem only ), arthur can set up a fully customizable rbac. please follow the below : when setting up your identity provider via the yaml configuration, supply a global role name and set of permissions under globalroledefs that your identity provider will authenticate users with. this configuration will create the global role in", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a global role name and set of permissions under globalroledefs that your identity provider will authenticate users with. this configuration will create the global role in the arthur authorization system when it is applied. see the { ref } creating global roles for managing organizations and rbac policies guide < creating _ global _ roles _ in _ arthur _ config > for more information. that global role can then create custom role mappings for each organization : during organization creation, including the role configuration json ( see below, for example ) in the request body when calling the organizations endpoint. after an organization is created, create or add custom", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "role configuration json ( see below, for example ) in the request body when calling the organizations endpoint. after an organization is created, create or add custom _ roles by sending the role configuration json ( see below, for example ) in the request body when calling authorization custom roles endpoint. users logging in through your idp must now have a valid known role in their token when accessing the arthur platform. arthur will use this role to authenticate that the user is a member of the organization and determine their permissions. managing roles and permissions understanding permissions a permission is a combination of a resource and an action. for", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a member of the organization and determine their permissions. managing roles and permissions understanding permissions a permission is a combination of a resource and an action. for example raw _ data read, users write, models delete. for a full list of available permissions. please see arthur permissions by standard roles. for a directory of permissions by api endpoint, please see arthur permissions by endpoint. create custom roles the create organization custom roles endpoint is available for customers using sso to operate on custom roles for each organization. a few notes : this endpoint only operates on permission scopes within each organization. permissions", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "customers using sso to operate on custom roles for each organization. a few notes : this endpoint only operates on permission scopes within each organization. permissions with global scope ( such as creating a new organization ) cannot be granted via this endpoint, those permissions must be assigned to a role with global privileges via the arthur idp configuration yaml. see { ref } creating global roles for managing organizations and rbac policies guide < creating _ global _ roles _ in _ arthur _ config > for more information. roles can have a list of permissions to allow and / or a list of other roles to inherit permissions", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur _ config > for more information. roles can have a list of permissions to allow and / or a list of other roles to inherit permissions from. role names cannot conflict with arthur permissions by standard roles supplied permissions must be valid, known as arthur permissions. roles can inherit the permissions of other roles that are either arthur permissions by standard roles or roles also defined in the same organization. unknown inherited role names will be rejected. get custom roles to retrieve a list of roles defined for an organization, use : get organization custom roles. to filter on specific roles, pass a comma - separated list of", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to retrieve a list of roles defined for an organization, use : get organization custom roles. to filter on specific roles, pass a comma - separated list of role names in a roles query parameter. for example : / authorization / custom _ roles? roles = role1, role2. if you wish to return all roles, simply leave out the query parameter or pass \" * \" as role. delete custom roles to delete a role or multiple roles from an organization, use delete organization custom roles. specify which roles to delete in the json request body. for example, to delete a single role : json", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "use delete organization custom roles. specify which roles to delete in the json request body. for example, to delete a single role : json { \" roles \" : [ \" role3 \" ] } to delete all roles pass \" *. \" [UNK] you do not specify an organization _ id, this will delete all custom roles you have created json { \" roles \" : [ \" * \" ] } example role configuration json below is an example json request body that creates three roles. role1 has 3 permissions defined, role2 gets additional permission and then inherits the 3 permissions from role", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "json request body that creates three roles. role1 has 3 permissions defined, role2 gets additional permission and then inherits the 3 permissions from role1, and role3 inherits the permissions from arthur's default \" model owner \" role. for more details on the expected schema for each endpoint, see api documentation. json { \" roles \" : [ { \" role _ name \" : \" role1 \", \" permissions \" : [ { \" resource \" : \" metric _ data \", \" action \" : \" read \" }, { \" resource \" : \" metric _ data \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" resource \" : \" metric _ data \", \" action \" : \" read \" }, { \" resource \" : \" metric _ data \", \" action \" : \" write \" }, { \" resource \" : \" tag \", \" action \" : \" read \" } ] }, { \" role _ name \" : \" role2 \", \" permissions \" : [ { \" resource \" : \" user _ self \", \" action \" : \" read \" } ], \" inherited _ role _ names \" : [ \" role1 \" ] }, { \" role _ name \" : \" role3 \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ], \" inherited _ role _ names \" : [ \" role1 \" ] }, { \" role _ name \" : \" role3 \", \" inherited _ role _ names \" : [ \" model owner \" ] } ] } updated 3 months ago table of contents managing rbac and organizations for sso users managing roles and permissions understanding permissions create custom roles get custom roles delete custom roles example role configuration json source : https : / / docs. arthur. ai / docs / custom - rbac", "metadata": {"source": "https://docs.arthur.ai/docs/custom-rbac", "row": 45, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 46 text : access control jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by access controlsuggest editsarthur supports a variety of mechanisms for authentication, who a user is, and authorization, what a user can do ( rbac ). by default, arthur will use a built - in authentication and authorization system. in on - prem installations, cluster administrators can optionally configure an external identity provider ( idp ) to control user authentication and authorization. see the sections below for an overview of arthur standard access control", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ly configure an external identity provider ( idp ) to control user authentication and authorization. see the sections below for an overview of arthur standard access control and arthur sso access control. updated 3 months ago source : https : / / docs. arthur. ai / docs / single - sign - on", "metadata": {"source": "https://docs.arthur.ai/docs/single-sign-on", "row": 46, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 47 text : arthur permissions by endpoint jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by arthur permissions by endpointsuggest editsfor more details on the expected schema for each endpoint, see authorization api documentation. if an endpoint is not listed here, there is no authorization restriction on that endpoint, and all authenticated users should be able to access it. for more details on the expected schema for each endpoint, see authorization. if an endpoint is not listed here, there is no", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to access it. for more details on the expected schema for each endpoint, see authorization. if an endpoint is not listed here, there is no authorization restriction on it, and all authenticated users should be able to access it. org, user, and authorization related permissions permissionendpointrestresource in systemactionget all organizations / organizationsgetorganization _ globalreadcreate an organization / organizationspostorganization _ globalwriteget an organization / organizations / { organization _ id } getorganizationreaddelete an organization / organizations / { organization _ id } deleteorganization", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "organization / organizations / { organization _ id } getorganizationreaddelete an organization / organizations / { organization _ id } deleteorganizationdeleteadd users to an organization / organizations / { organization _ id } / userspatchuserwritecreate user / userspostuserwriteget all users / usersgetuserreadget current user / users / megetn / an / aupdate current user / users / mepatchn / an / aget user / users / { user _ id } getuserreadupdate user / users / { user _ id } patchuserwritedelete", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ aget user / users / { user _ id } getuserreadupdate user / users / { user _ id } patchuserwritedelete user / users / { user _ id } deleteuserdeleteinvite a new user / users / invite _ userspostinvite _ userwriteget current authentication info / users / me / auth _ infogetn / an / aget permssions for a role / authorization / permissionsgetn / an / acheck authorization for a permission and role / authorization / authorizepostn / an / acreate custom roles for rbac / authorization / custom", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ an / acheck authorization for a permission and role / authorization / authorizepostn / an / acreate custom roles for rbac / authorization / custom _ rolespostcustom _ roleswritedelete custom roles for rbac / authorization / custom _ rolesdeletecustom _ rolesdeleteget configured custom roles for rbac / authorization / custom _ rolesgetcustom _ rolesreadget current organization of session / organizations / currentgetn / an / aset current organization of session / organizations / currentputn / an / aget usage / usage / { rollup } getorganization _ metricsread model", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "current organization of session / organizations / currentputn / an / aget usage / usage / { rollup } getorganization _ metricsread model and data related permissions permissionendpointrestresource in systemactionget models / modelsgetmodelreadcreate a model / modelspostmodelwriteget a model's health score ( most recent ) / models / healthgetmodelreadget model / models / { model _ id } getmodelreadupdate model / models / { model _ id } putmodelwritedelete model / models / { model _ id } delete", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##modelreadupdate model / models / { model _ id } putmodelwritedelete model / models / { model _ id } deletemodeldeleteget model bias groups / models / { model _ id } / bias _ groupsgetmodelreadget attributes / models / { model _ id } / attributesgetmodelreadupdate attributes / models / { model _ id } / attributesputmodelwriteedit model attributes / models / { model _ id } / attributespatchmodelwritedelete model attributes / models / { model _ id } / attributesdeletemodeldel", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ { model _ id } / attributespatchmodelwritedelete model attributes / models / { model _ id } / attributesdeletemodeldeleteget an attribute / models / { model _ id } / attributes / { attribute _ id } getmodelreadupdate an attribute / models / { model _ id } / attributes / { attribute _ id } putmodelwritedelete an attribute / models / { model _ id } / attributes / { attribute _ id } deletemodeldeleteget tags / tagsgettagreadupdate a tag / tags / { tag _ name } put", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ attribute _ id } deletemodeldeleteget tags / tagsgettagreadupdate a tag / tags / { tag _ name } puttagwritedelete a tag / tags / { tag _ name } deletetagdeleteget model groups / model _ groupsgetmodelreadget a model group / model _ groups / { model _ group _ id } getmodelreadupdate a model group / model _ groups / { model _ group _ id } patchmodelwritedelete a model group / model _ groups / { model _ group _ id } deletemodeldeleteget", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "group _ id } patchmodelwritedelete a model group / model _ groups / { model _ group _ id } deletemodeldeleteget a model group's versions / model _ groups / { model _ group _ id } / versionsgetmodelreadget latest version for a model group / model _ groups / { model _ group _ id } / versions / latestgetmodelreadretrieve the prediction and explanation for an inference / models / { model _ id } / what _ ifpostraw _ datareadgenerate on - demand explanation for an inference / models / { model _ id", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ model _ id } / what _ ifpostraw _ datareadgenerate on - demand explanation for an inference / models / { model _ id } / inferences / { partner _ inference _ id } / explanationgetraw _ datareadsave inferences / models / { model _ id } / inferencespostraw _ datawritesave inferences from file / models / { model _ id } / inferences / filepostraw _ datawriteupdate inferences / models / { model _ id } / inferencespatchraw _ datawriteupdate ground truth / models / {", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "datawriteupdate inferences / models / { model _ id } / inferencespatchraw _ datawriteupdate ground truth / models / { model _ id } / ground _ truthpatchground _ truthwriteget image inference / models / { model _ id } / inferences / images / { image _ id } getraw _ datareadget batch information for batch of a model / models / { model _ id } / batches / { batch _ id } getraw _ datareadcloses a batch / models / { model _ id } / batches / { batch _ id } patch", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ id } getraw _ datareadcloses a batch / models / { model _ id } / batches / { batch _ id } patchraw _ datawriteget inference / models / { model _ id } / inferences / query / { partner _ inference _ id } getraw _ datareadget all datasets for a model / models / { model _ id } / datasetsgetraw _ datareadget reference data information for model / models / { model _ id } / reference _ datagetreference _ datareadcloses a reference dataset / models / { model", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for model / models / { model _ id } / reference _ datagetreference _ datareadcloses a reference dataset / models / { model _ id } / reference _ datapatchreference _ datawriteuploads a parquet or json file containing reference set data / models / { model _ id } / reference _ datapostreference _ datawriteexecute query / models / { model _ id } / inferences / querypostquery ( [UNK] see footnote ) executeexecute query and retrun data drift values / models / { model _ id } / inferences /", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##query ( [UNK] see footnote ) executeexecute query and retrun data drift values / models / { model _ id } / inferences / query / data _ driftpostqueryexecuteexecute query and return psi buckets / models / { model _ id } / inferences / query / data _ drift _ psi _ bucket _ calculation _ tablepostqueryexecuteexecute query and return scatterplot distributions / models / { model _ id } / inferences / query / distributionpostqueryexecuteget bias mitigation curves / models / { model _ id", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ { model _ id } / inferences / query / distributionpostqueryexecuteget bias mitigation curves / models / { model _ id } / bias _ mitigation _ curves / attributes / { attribute _ id } getraw _ datareadget bias mitigation curves / models / { model _ id } / enrichments / bias _ mitigation / curvesgetraw _ datareadfind hotspots / models / { model _ id } / enrichments / hotspots / findgetraw _ datareadget metric queries / models / { model _ id } / metric", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ id } / enrichments / hotspots / findgetraw _ datareadget metric queries / models / { model _ id } / metricsgetmetric _ queryreadcreate a metric query / models / { model _ id } / metricspostmetric _ querywriteget a metric query / models / { model _ id } / metrics / { metric _ id } getmetric _ queryreadupdate a metric query / models / { model _ id } / metrics / { metric _ id } putmetric _ querywritedelete a metric query / models / { model _ id } / metric", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ id } / metrics / { metric _ id } putmetric _ querywritedelete a metric query / models / { model _ id } / metrics / { metric _ id } deletemetric _ querydeleteget all enrichment configs / models / { model _ id } / enrichmentsgetenrichment _ configreadupdate an enrichment config / models / { model _ id } / enrichmentspatchenrichment _ configwriteget explainability config / models / { model _ id } / enrichments / explainabilitygetenrichment _ configread", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##figwriteget explainability config / models / { model _ id } / enrichments / explainabilitygetenrichment _ configreadupdate explainability config / models / { model _ id } / enrichments / explainabilitypatchenrichment _ configwriteget anomaly detection config / models / { model _ id } / enrichments / anomaly _ detectiongetenrichment _ configreadupdate anomaly detection config / models / { model _ id } / enrichments / anomaly _ detectionpatchenrichment _ configwriteget bias mit", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detection config / models / { model _ id } / enrichments / anomaly _ detectionpatchenrichment _ configwriteget bias mitigation config / models / { model _ id } / enrichments / bias _ mitigationgetenrichment _ configreadupdate bias mitigation config / models / { model _ id } / enrichments / bias _ mitigationpatchenrichment _ configwriteget hotspots config / models / { model _ id } / enrichments / hotspotsgetenrichment _ confi", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##figwriteget hotspots config / models / { model _ id } / enrichments / hotspotsgetenrichment _ configreadupdate hotspots config / models / { model _ id } / enrichments / hotspotspatchenrichment _ configwriteget pinned columns / models / { model _ id } / pinned _ columnsgetpinned _ columnsreadset pinned columns / models / { model _ id } / pinned _ columnsputpinned _ columnswriteadd / remove pinned columns / models / { model _ id } / pinned _ columns", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model _ id } / pinned _ columnsputpinned _ columnswriteadd / remove pinned columns / models / { model _ id } / pinned _ columnspatchpinned _ columnswritedelete pinned columns / models / { model _ id } / pinned _ columnsdeletepinned _ columnsdelete [UNK] query requires both queryexecute permissions as well as either raw _ dataread or reference _ dataread permissions depending on what the posted query is. alert related permissions permissionendpointrestresource in systemactioncreate an alert rule / models / { model _ id } / alert", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is. alert related permissions permissionendpointrestresource in systemactioncreate an alert rule / models / { model _ id } / alert _ rulespostalert _ rulewriteget alert rules / models / { model _ id } / alert _ rulesgetalert _ rulereaddelete an alert rule / models / { model _ id } / alert _ rules / { alert _ rule _ id } deletealert _ ruledeleteedit an alert rule / models / { model _ id } / alert _ rules / { alert _ rule _ id } patchalert _ rulewritebulk resolve alert", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "alert rule / models / { model _ id } / alert _ rules / { alert _ rule _ id } patchalert _ rulewritebulk resolve alert for an alert rule / models / { model _ id } / alert _ rules / { alert _ rule _ id } / bulk _ alertspatchalertresolveget alerts / alertsgetalertreadget alert counts by model / alerts / model _ countsgetalertreadupdate alert status / alerts / { alert _ id } patchalertresolvesend manual alert notification / alerts / { alert _ id } / notification", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "alert status / alerts / { alert _ id } patchalertresolvesend manual alert notification / alerts / { alert _ id } / notificationspostalertnotifyget alert notification configurations / alert _ notification _ configurationsgetalert _ notification _ configreadcreate an alert notification configuration / alert _ notification _ configurationspostalert _ notification _ configwriteget an alert notification configuration / alert _ notification _ configurations / { configuration _ id } getalert _ notification _ configreadedit an alert notification configuration / alert _ notification _ configurations / { configuration _ id } patchalert _ notification", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "getalert _ notification _ configreadedit an alert notification configuration / alert _ notification _ configurations / { configuration _ id } patchalert _ notification _ configwritedelete an alert notification configuration / alert _ notification _ configurations / { configuration _ id } deletealert _ notification _ configdeleteget insights / models / { model _ id } / insightsgetinsightreadupdate the status of insights / models / { model _ id } / insightspatchinsightresolveget an insight / models / { model _ id } / insights / { insight _ id } getins", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "id } / insightspatchinsightresolveget an insight / models / { model _ id } / insights / { insight _ id } getinsightwriteupdate the status of an insight / models / { model _ id } / insights / { insight _ id } patchinsightresolveget all alert summary configurations / alert _ summary _ configurationsgetalert _ summary _ configreadcreate an alert summary configuration / alert _ summary _ configurationspostalert _ summary _ configwriteget an alert summary configuration / alert _ summary _ configurations / { configuration _ id } getalert _ summary", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##postalert _ summary _ configwriteget an alert summary configuration / alert _ summary _ configurations / { configuration _ id } getalert _ summary _ configreaddelete an alert summary configuration / alert _ summary _ configurations / { configuration _ id } deletealert _ summary _ configdeleteupdate an alert summary configuration / alert _ summary _ configurations / { configuration _ id } patchalert _ summary _ configwriteget all alert summary subscribers / alert _ summary _ configurations / { configuration _ id } / subscribersgetalert _ summary _ subscriberreadcreate an alert", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "all alert summary subscribers / alert _ summary _ configurations / { configuration _ id } / subscribersgetalert _ summary _ subscriberreadcreate an alert summary subscriber / alert _ summary _ configurations / { configuration _ id } / subscriberspostalert _ summary _ subscriberwriteget an alert summary subscriber / alert _ summary _ configurations / { configuration _ id } / subscribers / { subscriber _ id } getalert _ summary _ subscriberreadupdate an alert summary subscriber / alert _ summary _ configurations / { configuration _ id } / subscribers / { subscriber _ id", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##scriberreadupdate an alert summary subscriber / alert _ summary _ configurations / { configuration _ id } / subscribers / { subscriber _ id } patchalert _ summary _ subscriberwritedelete an alert summary subscriber / alert _ summary _ configurations / { configuration _ id } / subscribers / { subscriber _ id } deletealert _ summary _ subscriberdeletesend manual alert summary notifications to subscribers / alert _ summary _ configurations / { configuration _ id } / notificationspostalert _ summarynotifyupdated about 2 months ago table of contents org, user,", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "summary _ configurations / { configuration _ id } / notificationspostalert _ summarynotifyupdated about 2 months ago table of contents org, user, and authorization related permissions model and data related permissions alert related permissions source : https : / / docs. arthur. ai / docs / arthur - permissions - by - endpoint", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-permissions-by-endpoint", "row": 47, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 48 text : nlp onboarding jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by nlp onboardingsuggest editsthis page walks through the basics of setting up a natural language processing ( nlp ) model and onboarding it to arthur scope to monitor language - specific performance. getting started the first step is to import functions from the arthurai package and establish a connection with arthur scope. python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "connection with arthur scope. python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \" ) registering an nlp model each nlp model is created with a name and with input _ type = inputtype. nlp. here, we register a classification model on text specifying a text _ delimiter of not _ word : pythonarthur _ nlp _ model = arthur. model (", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "register a classification model on text specifying a text _ delimiter of not _ word : pythonarthur _ nlp _ model = arthur. model ( name = \" nlpquickstart \", input _ type = inputtype. nlp, model _ type = outputtype. multiclass, text _ delimiter = textdelimiter. not _ word ) the different outputtype values currently supported for nlp models are classification, multi - labeling, and regression. text delimiter nlp models optionally allow specifying a text _ delimiter, which specifies how a raw document is split into token", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and regression. text delimiter nlp models optionally allow specifying a text _ delimiter, which specifies how a raw document is split into tokens. if a text delimiter is not provided, a default text _ delimiter will be textdelimiter. not _ word. this delimiter will ignore punctuation and tokenize text based only on the words present. however, suppose punctuation and non - word text needs to be considered by your model. in that case, you should consider using other options for a delimiter to ensure those other pieces of text are processed by your nl", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "considered by your model. in that case, you should consider using other options for a delimiter to ensure those other pieces of text are processed by your nlp model. for a full list of available text delimiters with examples, see the textdelimiter constant documentation in our sdk reference. additionally, arthur supports sending the pre - tokenized text. for steps on registering tokens with arthur, see our generative text walkthrough. formatting reference / inference data column names can contain only alphanumeric and underscore characters. the rest of the string values can have additional characters as raw text. python", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ inference data column names can contain only alphanumeric and underscore characters. the rest of the string values can have additional characters as raw text. python text _ attr pred _ value ground _ truth non _ input _ 1 0'here - is some text'0. 1 0 0. 2 1'saying a whole lot'0. 05 0 - 0. 3 2'of important things!'0. 02 1 0. 7 3'with all kinds of chars?!'0. 2 0 0. 1... 4'but attribute / column names'0. 6 1 - 0.", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "kinds of chars?!'0. 2 0 0. 1... 4'but attribute / column names'0. 6 1 - 0. 6 5'can only use underscore.'0. 9 1 - 0. 9... reviewing the model schema before you register your model with arthur by calling arthur _ model. save ( ), you can call arthur _ model. review ( ) the model schema to check that your data is parsed correctly. for an nlp model, the model schema should look like this : python name stage value _ type categorical is _ unique 0 text", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "parsed correctly. for an nlp model, the model schema should look like this : python name stage value _ type categorical is _ unique 0 text _ attr pipeline _ input unstructured _ text false true 1 pred _ value predicted _ value float false false... 2 ground _ truth ground _ truth integer true false 3 non _ input _ 1 non _ input _ data float false false... finishing onboarding once you have finished formatting your reference data and your model schema looks correct using arthur _ model. review ( ), you are finished registering your model and its attributes - so you are ready", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "reference data and your model schema looks correct using arthur _ model. review ( ), you are finished registering your model and its attributes - so you are ready to complete onboarding your model. to finish onboarding your nlp model, the following steps apply, which is the same for nlp models as it is for models of any inputtype and outputtype : finishing _ onboarding. md enrichments for an overview of configuring enrichments for nlp models, see the { doc } / user - guide / walkthroughs / enrichments guide. for a step - by - step walkthrough", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", see the { doc } / user - guide / walkthroughs / enrichments guide. for a step - by - step walkthrough of setting up the explainability enrichment for nlp models, see { ref } nlp _ explainability. updated 3 months ago table of contents getting started registering an nlp model text delimiter formatting reference / inference data reviewing the model schema finishing onboarding enrichments source : https : / / docs. arthur. ai / docs / nlp - onboarding", "metadata": {"source": "https://docs.arthur.ai/docs/nlp-onboarding", "row": 48, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 49 text : aggregation functions jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by aggregation functionssuggest editsfor an explanation of nested functions, see the composing functions guide mathematical functions average take the average of a property. query request : json { \" select \" : [ { \" function \" : \" avg \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ]", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < avg _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" avg \", \" alias \" : \" avgage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" avgage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" avgage \" : 55. 45 } ] } back to top average of absolute values take the average of absolute values of a property. query request : json { \" select \" : [ { \" function \" : \" avgabs \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" }", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < avg _ abs _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" avgabs \", \" alias \" : \" avgafloat \", \" parameters \" : { \" property \" : \" afloat \" } } ] } sample", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##bs \", \" alias \" : \" avgafloat \", \" parameters \" : { \" property \" : \" afloat \" } } ] } sample response : json { \" query _ result \" : [ { \" avgafloat \" : 55. 45 } ] } back to top max take the max of a property. query request : json { \" select \" : [ { \" function \" : \" max \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" }", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < max _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" max \", \" alias \" : \" maxage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" maxage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" maxage \" : 55. 45 } ] } back to top min take the min of a property. query request : json { \" select \" : [ { \" function \" : \" min \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < min _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" min \", \" alias \" : \" minage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" minage \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" minage \" : 17 } ] } back to top sum take the sum of a property. query request : json { \" select \" : [ { \" function \" : \" sum \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < sum _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" sum \", \" alias \" : \" sumage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" sumage \" : 1745 } ] } back to top variance take the variance of", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} sample response : json { \" query _ result \" : [ { \" sumage \" : 1745 } ] } back to top variance take the variance of a property. query request : json { \" select \" : [ { \" function \" : \" variance \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < variance > [", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < variance > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" variance \", \" alias \" : \" varage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" varage \" : 10. 234 } ] } back to top standard deviation take the standard deviation of a property. query request : json { \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" varage \" : 10. 234 } ] } back to top standard deviation take the standard deviation of a property. query request : json { \" select \" : [ { \" function \" : \" stddev \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < stddev > [ float ] \" } ] }", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < stddev > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" stddev \", \" alias \" : \" stddevage \", \" parameters \" : { \" property \" : \" age \" } } ] } sample response : json { \" query _ result \" : [ { \" stddevage \" : 3. 199 } ] } back to top count count count a property. query request : json { \" select \" : [", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "stddevage \" : 3. 199 } ] } back to top count count count a property. query request : json { \" select \" : [ { \" function \" : \" count \", \" alias \" : \" < alias _ name > [ optional string ] \" } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < count > [ int ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" count \" } ], \" filter \" : [ { \" property \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sample request : json { \" select \" : [ { \" function \" : \" count \" } ], \" filter \" : [ { \" property \" : \" income \", \" comparator \" : \" lt \", \" value \" : 90000 } ] } sample response : json { \" query _ result \" : [ { \" count \" : 5432 } ] } back to top count if conditionally count a property. query request : json { \" select \" : [ { \" function \" : \" countif \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "select \" : [ { \" function \" : \" countif \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < string or number to compare with property > \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < count > [ int ] \" } ] } see end", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < count > [ int ] \" } ] } see endpoint overview for a list of valid comparators. sample request : json { \" select \" : [ { \" function \" : \" countif \", \" alias \" : \" michigan _ count \", \" parameters \" : { \" property \" : \" state \", \" comparator \" : \" eq \", \" value \" : \" michigan \" } } ] } sample response : json { \" query _ result \" : [ { \" michigan _ count \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" value \" : \" michigan \" } } ] } sample response : json { \" query _ result \" : [ { \" michigan _ count \" : 5432 } ] } back to top categories count count the number of categories of a property. to be used for discrete features. query request : json { \" select \" : [ { \" function \" : \" categoriescount \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string ] \" } } ] } query response : json { \" query", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" parameters \" : { \" property \" : \" < attribute _ name > [ string ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < categories _ count > [ int ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" categoriescount \", \" alias \" : \" categoriescountzipcode \", \" parameters \" : { \" property \" : \" zipcode \" } } ] } sample response : json { \" query _ result \" : [", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" parameters \" : { \" property \" : \" zipcode \" } } ] } sample response : json { \" query _ result \" : [ { \" categoriescountzipcode \" : 732 } ] } back to top rate calculates the rate of a condition on a column. query request : json { \" select \" : [ { \" function \" : \" rate \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" comparator \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < string or number to compare with property > \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < rate _ value > [ float ] \" } ] } see endpoint overview for a list of valid comparators. sample request : calculate the positive predictive rate, with predictions classified as positive", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" } ] } see endpoint overview for a list of valid comparators. sample request : calculate the positive predictive rate, with predictions classified as positive when pos _ class above. 5 ( standard definition of positive predictive rate ). json { \" select \" : [ { \" function \" : \" rate \", \" alias \" : \" pos _ rate \", \" parameters \" : { \" property \" : \" pos _ class \", \" comparator \" : \" gt \", \" value \" : 0. 5 } } ] } response : json { \" query _ result \" : [ {", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##parator \" : \" gt \", \" value \" : 0. 5 } } ] } response : json { \" query _ result \" : [ { \" pos _ rate \" : \" 0. 1 \" } ] } back to top distributions distribution return the distribution of a column with a specified number of bins. you may specify one of either num _ bins or bin _ thresholds. for num _ bins,, if there is not enough data, fewer than the specified number of bins will be returned. for bin _ thresholds, there will be a bucket below your lowest bin and a bucket", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data, fewer than the specified number of bins will be returned. for bin _ thresholds, there will be a bucket below your lowest bin and a bucket above your highest bin, i. e. n + 1 buckets when supplied n thresholds. query request with num _ bins : json { \" select \" : [ { \" function \" : \" distribution \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" num _ bins \" : \" < number", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" num _ bins \" : \" < number _ of _ bins > [ int ] \" } } ] } query request with bin _ thresholds : json { \" select \" : [ { \" function \" : \" distribution \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" bin _ thresholds \" : \" < list _ of _ floats > [ list [", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" < attribute _ name > [ string or nested ] \", \" bin _ thresholds \" : \" < list _ of _ floats > [ list [ int ] ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : [ { \" lower \" : \" < bin _ lower _ bound > [ float ] \", \" upper \" : \" < bin _ upper _ bound > [ float ] \", \" count \" : \" < bin _ count > [ float ] \" } ] } ] } lower bounds are inclusive while upper bounds", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> [ float ] \", \" count \" : \" < bin _ count > [ float ] \" } ] } ] } lower bounds are inclusive while upper bounds are exclusive, so a bucket response like this : json { \" lower \" : 30, \" upper \" : 50, \" count \" : \" < bin _ count > [ float ] \" } would include values such as 30, 40, or 49. 999, but not 50. sample request with num _ bins : json { \" select \" : [ { \" function \" : \" distribution \", \" parameters \" : { \" property \" : \" fico", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "bins : json { \" select \" : [ { \" function \" : \" distribution \", \" parameters \" : { \" property \" : \" fico _ predicted \", \" num _ bins \" : 50 } } ] } sample response with num _ bins : json { \" query _ result \" : [ { \" distribution \" : [ { \" lower \" : 500, \" upper \" : 600, \" count \" : 1000 }, { \" lower \" : 600, \" upper \" : 700, \" count \" : 5000 }, { \" lower \" : 700, \" upper \" : 800, \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "lower \" : 600, \" upper \" : 700, \" count \" : 5000 }, { \" lower \" : 700, \" upper \" : 800, \" count \" : 2000 }, { \" lower \" : 800, \" upper \" : 850, \" count \" : 550 } ] } ] } sample request with bin _ thresholds : json { \" select \" : [ { \" function \" : \" distribution \", \" parameters \" : { \" property \" : \" fico _ predicted \", \" bin _ thresholds \" : [ 600, 700 ] } } ] } sample response with bin _ thresholds : json", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "fico _ predicted \", \" bin _ thresholds \" : [ 600, 700 ] } } ] } sample response with bin _ thresholds : json { \" query _ result \" : [ { \" distribution \" : [ { \" upper \" : 600, \" count \" : 1000 }, { \" lower \" : 600, \" upper \" : 700, \" count \" : 5000 }, { \" lower \" : 700, \" count \" : 2550 } ] } ] } back to top quantile get the quantile of a column at the specified point. the level parameter between 0 and 1 specifies the cut point, with", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] } back to top quantile get the quantile of a column at the specified point. the level parameter between 0 and 1 specifies the cut point, with 0. 5 representing the median, 0. 9 the 90th percentile, and so on. query request : json { \" select \" : [ { \" function \" : \" quantile \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" level \" : \" < quantile level > [ float ( 0. 0", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" < attribute _ name > [ string or nested ] \", \" level \" : \" < quantile level > [ float ( 0. 0, 1. 0 ) ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < quantile > [ constant ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" quantile \", \" alias \" : \" medianage \", \" parameters \" : { \" property \" : \" age \", \" level \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" quantile \", \" alias \" : \" medianage \", \" parameters \" : { \" property \" : \" age \", \" level \" : \" 0. 5 \" } } ] } sample response : json { \" query _ result \" : [ { \" medianage \" : 47 } ] } decile get the decile of a property. query request : json { \" select \" : [ { \" function \" : \" deciles \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : { \" max \" : \" < max _ value > [ number ] \", \" min \" : \" < min _ value > [ number ] \", \" q1 \" : \" < first decile > [ number ] \", \" q2 \" : \" < second decile > [ number", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] \", \" q1 \" : \" < first decile > [ number ] \", \" q2 \" : \" < second decile > [ number ] \", \" q3 \" : \" < third decile > [ number ] \", \" q4 \" : \" < fourth decile > [ number ] \", \" q5 \" : \" < fifth decile > [ number ] \", \" q6 \" : \" < sixth decile > [ number ] \", \" q7 \" : \" < seventh decile > [ number ] \", \" q8 \" : \" < eighth decile > [ number", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] \", \" q7 \" : \" < seventh decile > [ number ] \", \" q8 \" : \" < eighth decile > [ number ] \", \" q9 \" : \" < ninth decile > [ number ] \", } } ] } sample request : json { \" select \" : [ { \" function \" : \" deciles \", \" alias \" : \" likelihooddeciles \", \" parameters \" : { \" property \" : \" likelihood \" } } ] } sample response : json { \" query _ result \" : [ { \" likelihooddeciles \" : { \" max", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" likelihood \" } } ] } sample response : json { \" query _ result \" : [ { \" likelihooddeciles \" : { \" max \" : 0. 9427181028155817, \" min \" : 0. 04451819608908539, \" q1 \" : 0. 5422967935750383, \" q2 \" : 0. 6379949435686291, \" q3 \" : 0. 7381396364893235, \" q4 \" : 0. 80248464746", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##1, \" q3 \" : 0. 7381396364893235, \" q4 \" : 0. 802484647469614, \" q5 \" : 0. 8282093987794898, \" q6 \" : 0. 8474456853909271, \" q7 \" : 0. 8662622658358209, \" q8 \" : 0. 884452391809424, \" q9 \" : 0. 901106156817971 }", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 0. 884452391809424, \" q9 \" : 0. 901106156817971 } } ] } back to top arg values arg max take the value of a property at which a different property as at its maximum. query request : json { \" select \" : [ { \" function \" : \" argmax \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" argument \" : \" < attribute _ name > [ string or nested ] \", \" value \" : \" < attribute", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" parameters \" : { \" argument \" : \" < attribute _ name > [ string or nested ] \", \" value \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < arg _ max _ value > [ constant ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" argmax \", \" alias \" : \" mostexpensivezipcode \", \" parameters \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : [ { \" function \" : \" argmax \", \" alias \" : \" mostexpensivezipcode \", \" parameters \" : { \" argument \" : \" zipcode \", \" value \" : \" homeprice \" } } ] } sample response : json { \" query _ result \" : [ { \" mostexpensivezipcode \" : 94027 } ] } back to top arg min take the value of a property at which a different property as at its minimum. query request : json { \" select \" : [ { \" function \" : \" argmin \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "property at which a different property as at its minimum. query request : json { \" select \" : [ { \" function \" : \" argmin \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" argument \" : \" < attribute _ name > [ string or nested ] \", \" value \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < arg _ min _ value >", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < arg _ min _ value > [ constant ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" argmin \", \" alias \" : \" leastexpensivezipcode \", \" parameters \" : { \" argument \" : \" zipcode \", \" value \" : \" homeprice \" } } ] } sample response : json { \" query _ result \" : [ { \" leastexpensivezipcode \" : 469", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ice \" } } ] } sample response : json { \" query _ result \" : [ { \" leastexpensivezipcode \" : 46953 } ] } back to top any if returns the selected property for any row which matches the provided condition. a common use case for this function is querying data with unique identifiers ( as described in this guide grouped inference queries ). query request : json { \" select \" : [ { \" function \" : \" anyif \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" anyif \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < string or number to compare with property > \", \" result \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" <", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < result > [ constant ] \" } ] } see endpoint overview for a list of valid comparators. sample request : json { \" select \" : [ { \" function \" : \" anyif \", \" alias \" : \" some _ michigan _ price \", \" parameters \" : { \" property \" : \" state \", \" comparator \" : \" eq \", \" value \" : \" michigan \", \" result \" : \" homeprice \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" state \", \" comparator \" : \" eq \", \" value \" : \" michigan \", \" result \" : \" homeprice \" } } ] } sample response : json { \" query _ result \" : [ { \" some _ michigan _ price \" : 459312 } ] } back to top regional feature importance returns the regional importance score for a particular attribute. this is the average of the absolute value of the explainability value for all the inferences. this is only available if explainability has been enabled for the model. query request : json { \" select \" : [ { \" function", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the inferences. this is only available if explainability has been enabled for the model. query request : json { \" select \" : [ { \" function \" : \" regionalfeatureimportance \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" attribute _ name \" : \" < pipeline _ input _ attribute _ name > [ string ] \", \" predicted _ attribute _ name \" : \" < predicted _ attribute _ name > [ string ] \", \" explanation _ algorithm \" : \" [ limeshap ] \" } } ] } query response : j", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "predicted _ attribute _ name > [ string ] \", \" explanation _ algorithm \" : \" [ limeshap ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < feature _ importance _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" regionalfeatureimportance \", \" parameters \" : { \" attribute _ name \" : \" age \", \" predicted _ attribute _ name \" : \" prediction _ 0 \", \" explanation _ algorithm", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "parameters \" : { \" attribute _ name \" : \" age \", \" predicted _ attribute _ name \" : \" prediction _ 0 \", \" explanation _ algorithm \" : \" lime \" } } ], \" filter \" : [ { \" property \" : \" inference _ timestamp \", \" comparator \" : \" gte \", \" value \" : \" 2020 - 12 - 01t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 12 - 22t11 : 00 :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inference _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 12 - 22t11 : 00 : 00z \" } ] } sample response : json { \" query _ result \" : [ { \" age \" : 0. 001406118694451489 } ] } back to top regional feature importances returns the regional importance scores for all of the pipeline input attributes. this is the average of the absolute value of the explainability value for all the inferences for each pipeline input attribute. this is only available if explainability has been enabled for the model", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the absolute value of the explainability value for all the inferences for each pipeline input attribute. this is only available if explainability has been enabled for the model. query request : json { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" predicted _ attribute _ name \" : \" < predicted _ attribute _ name > [ string ] \", \" explanation _ algorithm \" : \" [ limeshap ] \" } } ] } query response : json { \" query _ result", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ string ] \", \" explanation _ algorithm \" : \" [ limeshap ] \" } } ] } query response : json { \" query _ result \" : [ { \" < attribute _ name > \" : \" < feature _ importance _ value > [ float ] \", \" < attribute _ name > \" : \" < feature _ importance _ value > [ float ] \", \" < attribute _ name > \" : \" < feature _ importance _ value > [ float ] \", \" < attribute _ name > \" : \" < feature _ importance _ value > [ float ] \", \" < attribute _ name > \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] \", \" < attribute _ name > \" : \" < feature _ importance _ value > [ float ] \", \" < attribute _ name > \" : \" < feature _ importance _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" parameters \" : { \" predicted _ attribute _ name \" : \" prediction _ 0 \", \" explanation _ algorithm \" : \" lime \" } } ], \" filter \" : [ { \" property \" : \" inference _ timestamp \", \" comparator \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" lime \" } } ], \" filter \" : [ { \" property \" : \" inference _ timestamp \", \" comparator \" : \" gte \", \" value \" : \" 2020 - 12 - 01t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 12 - 22t11 : 00 : 00z \" } ] } sample response : json { \" query _ result \" : [ { \" explainer _ attribute \" : \" pay _ 0 \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "00z \" } ] } sample response : json { \" query _ result \" : [ { \" explainer _ attribute \" : \" pay _ 0 \", \" regionalfeatureimportance \" : 0. 055036517803945396 }, { \" explainer _ attribute \" : \" pay _ 2 \", \" regionalfeatureimportance \" : 0. 026880464089676884 }, { \" explainer _ attribute \" : \" pay _ 3 \", \" regionalfeatureimportance \" : 0. 024027", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##4 }, { \" explainer _ attribute \" : \" pay _ 3 \", \" regionalfeatureimportance \" : 0. 024027941129616155 }, { \" explainer _ attribute \" : \" limit _ bal \", \" regionalfeatureimportance \" : 0. 022367882999425544 }, { \" explainer _ attribute \" : \" pay _ amt2 \", \" regionalfeatureimportance \" : 0. 019145911247181836 }, { \" explainer _ attribute", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" regionalfeatureimportance \" : 0. 019145911247181836 }, { \" explainer _ attribute \" : \" pay _ amt1 \", \" regionalfeatureimportance \" : 0. 019052984358794038 }, { \" explainer _ attribute \" : \" pay _ amt3 \", \" regionalfeatureimportance \" : 0. 012942233755875516 }, { \" explainer _ attribute \" : \" pay _ 5 \", \" regionalfeatureimport", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##2942233755875516 }, { \" explainer _ attribute \" : \" pay _ 5 \", \" regionalfeatureimportance \" : 0. 011911442095349226 }, { \" explainer _ attribute \" : \" pay _ 4 \", \" regionalfeatureimportance \" : 0. 010464962507962139 }, { \" explainer _ attribute \" : \" pay _ 6 \", \" regionalfeatureimportance \" : 0. 00891260261770653 }, {", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" pay _ 6 \", \" regionalfeatureimportance \" : 0. 00891260261770653 }, { \" explainer _ attribute \" : \" bill _ amt4 \", \" regionalfeatureimportance \" : 0. 007211523900878019 }, { \" explainer _ attribute \" : \" bill _ amt5 \", \" regionalfeatureimportance \" : 0. 006279087267628024 }, { \" explainer _ attribute \" : \" bill _ amt1", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : 0. 006279087267628024 }, { \" explainer _ attribute \" : \" bill _ amt1 \", \" regionalfeatureimportance \" : 0. 006221344024007549 }, { \" explainer _ attribute \" : \" pay _ amt4 \", \" regionalfeatureimportance \" : 0. 005310133724715099 }, { \" explainer _ attribute \" : \" pay _ amt6 \", \" regionalfeatureimportance \" : 0. 00", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##9 }, { \" explainer _ attribute \" : \" pay _ amt6 \", \" regionalfeatureimportance \" : 0. 004135643379284112 }, { \" explainer _ attribute \" : \" marriage \", \" regionalfeatureimportance \" : 0. 004089899824740581 }, { \" explainer _ attribute \" : \" education \", \" regionalfeatureimportance \" : 0. 003931984513777395 }, { \" explainer _ attribute \" :", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##featureimportance \" : 0. 003931984513777395 }, { \" explainer _ attribute \" : \" pay _ amt5 \", \" regionalfeatureimportance \" : 0. 0033734464617669853 }, { \" explainer _ attribute \" : \" sex \", \" regionalfeatureimportance \" : 0. 0029222783744727687 }, { \" explainer _ attribute \" : \" bill _ amt6 \", \" regionalfeatureimportance \"", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##727687 }, { \" explainer _ attribute \" : \" bill _ amt6 \", \" regionalfeatureimportance \" : 0. 002707692309829875 }, { \" explainer _ attribute \" : \" bill _ amt2 \", \" regionalfeatureimportance \" : 0. 001955133839877692 }, { \" explainer _ attribute \" : \" bill _ amt3 \", \" regionalfeatureimportance \" : 0. 001779632159", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ attribute \" : \" bill _ amt3 \", \" regionalfeatureimportance \" : 0. 001779632159224476 }, { \" explainer _ attribute \" : \" age \", \" regionalfeatureimportance \" : 0. 001406118694451494 } ] } rankeditemmaxk returns the maximum position of a specified ranked list item across all ranked list data for a model. maximum position in this case refers to the largest index the item sits at in a ranked list array, not the highest ranking. if the specified item", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model. maximum position in this case refers to the largest index the item sits at in a ranked list array, not the highest ranking. if the specified item is not in any array, the function will return none. for use with ranked list type attributes only. the property name should have the _ item _ id suffix appended to it. query request : json { \" select \" : [ { \" function \" : \" rankeditemmaxk \", \" parameters \" : { \" property \" : < ranked _ list _ attr _ name > _ item _ id, \" item _ filter \" : < item _ ident", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { \" property \" : < ranked _ list _ attr _ name > _ item _ id, \" item _ filter \" : < item _ identifier > } } ] } query response : json { \" query _ result \" : [ { \" rankeditemmaxk \" : < maxk > } ] } sample request : json { \" select \" : [ { \" function \" : \" rankeditemmaxk \", \" parameters \" : { \" property \" : \" recommendations _ item _ id \", \" item _ filter \" : \" item1 \" } } ] } sample response : json {", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" property \" : \" recommendations _ item _ id \", \" item _ filter \" : \" item1 \" } } ] } sample response : json { \" query _ result \" : [ { \" rankeditemmaxk \" : 4 } ] } back to topupdated about 2 months ago table of contents mathematical functions average average of absolute values max min sum variance standard deviation count count count if categories count rate distributions distribution quantile decile arg values arg max arg min any if regional feature importance regional feature importances rankeditemmaxk source : https : / / docs. arthur. ai / doc", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "max arg min any if regional feature importance regional feature importances rankeditemmaxk source : https : / / docs. arthur. ai / docs / aggregation - functions", "metadata": {"source": "https://docs.arthur.ai/docs/aggregation-functions", "row": 49, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 50 text : arthur algorithms jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by arthur algorithmssuggest editsanomaly detection for an explanation of how to enable anomaly detection with arthur, please see the anomaly detection anomaly scores are computed by training a model on the reference set you provide to arthur and using that model to assign an anomaly score to each inference you send to arthur. scores of 0. 5 are given to \" typical \" examples from your reference set, while higher scores are given to more anomalous inferences, and lower", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "scores of 0. 5 are given to \" typical \" examples from your reference set, while higher scores are given to more anomalous inferences, and lower scores are given to instances that the model judges as similar to the reference data with high confidence. how anomaly detection works we calculate anomaly scores with an isolation forest algorithm. this algorithm works by building what is essentially a density model of the data by iteratively isolating data points from one another. because anomalies tend to be farther away from other points and occur less frequently, they are easier to isolate from other points, so we can use a data point's \" ease", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to be farther away from other points and occur less frequently, they are easier to isolate from other points, so we can use a data point's \" ease of isolation \" to describe its anomaly. the method is based on the paper linked here. the isolation forest \" method takes advantage of two [ quantitative properties of anomalies ] : i ) they are the minority consisting of fewer instances and ii ) they have attribute - values that are very different from those of normal instances. in other words, anomalies are'few and different ', which make them more susceptible to isolation than normal points. \" the idea is to build a", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in other words, anomalies are'few and different ', which make them more susceptible to isolation than normal points. \" the idea is to build a binary tree that randomly segments the data until each instance can be uniquely selected ( or a maximum height is reached ). anomalous instances will take fewer steps to become isolated on the tree because of the abovementioned properties. in the example above, we can see that the in - distribution x _ i takes many more steps to reach than the out - of - distribution x _ 0 of course, using a single randomly generated tree would be noisy. so we train multiple trees to", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "steps to reach than the out - of - distribution x _ 0 of course, using a single randomly generated tree would be noisy. so we train multiple trees to construct an isolation forest of multiple trees and use the average path length, noting that average path lengths converge : the isolation forest algorithm is highly efficient compared to other density estimation techniques because the individual trees can be built from data samples without losing performance. when you add a reference set to your model in arthur, we fit an isolation forest model to that data to compute an anomaly score for the inferences your model receives. generating anomaly scores the path length, or the number of steps taken to", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "forest model to that data to compute an anomaly score for the inferences your model receives. generating anomaly scores the path length, or the number of steps taken to reach the partition that a data point belongs to, varies between 0 and n - 1 where n is the number of data points in the training set. following our intuition above, the shorter the path length, the more anomalous a point is. to measure anomaly, an anomaly score between 0 and 1 is generated by normalizing the path length by the average path length and applying an inverse logarithmic scale. in particular, the anomaly scores algorithm shown below is the average", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "normalizing the path length by the average path length and applying an inverse logarithmic scale. in particular, the anomaly scores algorithm shown below is the average path length to data pointx from a collection of isolation trees and c ( n ) is the average path length given the number of data points in the dataset n. every inference you send to arthur will be evaluated against the trained isolation forest model and given an anomaly score. this can be seen in the anomaly score contour of 64 normally distributed points : at arthur, we also visualize the distribution of anomaly scores among all the inferences your model has retrieved since training. when", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tour of 64 normally distributed points : at arthur, we also visualize the distribution of anomaly scores among all the inferences your model has retrieved since training. when you select an inference in our ui you'll see where it falls on this distribution : interpreting anomaly scores the resulting anomaly scores can be interpreted in the following way : if instances return s very close to 1, then they are definitely anomalies if instances have s smaller than 0. 5, then they are quite safe to be regarded as normal instances if all the instances return s approximately at 0. 5, then the entire sample does not really have any distinct anomaly bias mitiga", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to be regarded as normal instances if all the instances return s approximately at 0. 5, then the entire sample does not really have any distinct anomaly bias mitigation if you are interested in mitigation capabilities, we're happy to discuss your needs and what approaches would work best for you. within the arthur product, we offer postprocessing methods and encourage the exploration of alternate ( pre - or in - processing ) methods if your data science team has the bandwidth to do so. we currently have one postprocessing method for the product : the threshold mitigator. threshold mitigator this algorithm is an extension of", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "do so. we currently have one postprocessing method for the product : the threshold mitigator. threshold mitigator this algorithm is an extension of the threshold optimizer implemented in the fairlearn library, which in turn is based on moritz hardt's 2016 paper introducing the method. the intuition behind this algorithm is based on the idea that if the underlying black box is biased, then different groups have different predicted probability distributions \u2014 let's call the originally predicted probability, for example, x the score s _ x. then, for a fixed threshold t, p ( s _ x > t x in a )", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "predicted probability, for example, x the score s _ x. then, for a fixed threshold t, p ( s _ x > t x in a ) > p ( sx > t x in b ), where _ b indicates membership in the disadvantaged group. for example, the default threshold is t = 0. 5, where we predict y ^ = 1 if s > 0. 5. however, we might be able to mitigate bias in the predictions by choosing group - specific thresholds for the decision rule. so, this algorithm generally proceeds as follows : try every possible threshold t from 0 to 1, where for any", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "group - specific thresholds for the decision rule. so, this algorithm generally proceeds as follows : try every possible threshold t from 0 to 1, where for any x, we predict y ^ = 1 if and only if s _ x > t. calculate each threshold's group - specific positivity, true positive, and false positive rates. let's say we're trying to satisfy demographic parity. we want the group - wise positivity rates to be equal \u2014 p ( y ^ = 1 a ) = p ( y ^ = 1 b ). then, for any given positivity rate, the threshold", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "equal \u2014 p ( y ^ = 1 a ) = p ( y ^ = 1 b ). then, for any given positivity rate, the threshold t _ a that achieves this positivity rate might differ from the threshold t _ b that achieves this positivity rate. this also hints at why we need the notion of tradeoff curves rather than a single static threshold : if we want to achieve a positivity rate of 0. 3, we'll need different t _ a, t _ b than if we wanted to achieve a positivity rate of 0. 4. then, once we '", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' ll need different t _ a, t _ b than if we wanted to achieve a positivity rate of 0. 4. then, once we've generated the curves, we pick a specific point on the curve, such as a positivity rate of 0. 3. when making predictions on the future and using the corresponding t _ a, t _ b to make predictions what's compatible? fairness constraints : we can generate curves satisfying each of the following constraints ( fairness metrics ) : demographic parity ( equal positivity rate ) equal opportunity ( equal true positive rate ) equalized odds ( equal tpr and", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( fairness metrics ) : demographic parity ( equal positivity rate ) equal opportunity ( equal true positive rate ) equalized odds ( equal tpr and fpr ) sensitive attributes : this algorithm can handle sensitive attributes that take on any number of discrete values ( i. e., not limited to binary sensitive attributes ). we can specify buckets for the continuous values for continuous sensitive attributes, then treat them like categorical attributes. the tradeoff curve generating the curve each group has its own curve. to generate a single curve, we generate all possible thresholds ( i. e. [ 0, 0. 001, 0.", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "group has its own curve. to generate a single curve, we generate all possible thresholds ( i. e. [ 0, 0. 001, 0. 002, 0. 003,..., 0. 999, 1. 00 ] ) \u2014 default 1000 total thresholds ; as described above, we calculate many common fairness metrics for each of those thresholds for the result if we were to use that threshold to determine predictions. visualizing the curve the tradeoff curve will look different depending on the fairness constraint we're trying to satisfy. for equal opportunity and demographic parity, where a single quantity must be", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "tradeoff curve will look different depending on the fairness constraint we're trying to satisfy. for equal opportunity and demographic parity, where a single quantity must be equalized across the two groups, the tradeoff curve plots the quantity to equalize on the x - axis and accuracy on the y - axis. understanding the curve artifact the curve artifact can be pulled down according to our docs here. this mitigation approach is somewhat constrained because we will generate a separate set of curves for each sensitive attribute to mitigate on ; for each constraint to follow. then, each set of curves entails a single curve for each sensitive attribute value", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "curves for each sensitive attribute to mitigate on ; for each constraint to follow. then, each set of curves entails a single curve for each sensitive attribute value. conceptually, the curves are organized something like the one below ; in the api, however, you'll be getting a single < list of ( x, y ) coordinate pairs that define the curve > at a time, with additional information about the attribute being mitigated, the constraint being targeted, and the feature value the specific list of pairs is for. * * mitigating on gender - - demographic parity * * gender = = male < list of (", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "feature value the specific list of pairs is for. * * mitigating on gender - - demographic parity * * gender = = male < list of ( x, y ) coordinate pairs that define the curve > gender = = female < list of ( x, y ) coordinate pairs that define the curve > gender = = nb < list of ( x, y ) coordinate pairs that define the curve > * * mitigating on gender - - equalized odds * * gender = = male < list of ( x, y ) coordinate pairs that define the curve > gender = = female < list of ( x, y ) coordinate", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "gender = = male < list of ( x, y ) coordinate pairs that define the curve > gender = = female < list of ( x, y ) coordinate pairs that define the curve > gender = = nb < list of ( x, y ) coordinate pairs that define the curve > * * mitigating on age - - demographic parity * * age < 35 < list of ( x, y ) coordinate pairs that define the curve > age > = 35 < list of ( x, y ) coordinate pairs that define the curve > summary of curves by constraint equal opportunity ( equal tpr ) : tpr vs. accuracy ;", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of ( x, y ) coordinate pairs that define the curve > summary of curves by constraint equal opportunity ( equal tpr ) : tpr vs. accuracy ; the accuracy - maximizing solution is the point along the x - axis with the highest accuracy ( y - axis ) for both groups. demographic parity ( equal selection rates ) : selection rate vs. accuracy ; the accuracy - maximizing solution is the point along the x - axis with the highest accuracy ( y - axis ) for both groups. equalized odds ( equal tpr and fpr ) : fpr vs. tpr ( canonical roc curve ) ; the accuracy -", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") for both groups. equalized odds ( equal tpr and fpr ) : fpr vs. tpr ( canonical roc curve ) ; the accuracy - maximizing solution is the point on the curves that are closest to the top left corner of the graph ( i. e., low fpr and high tpr ). choosing a set of thresholds ( point on curve ) as mentioned above, a single \" point \" on the solution curve for a given constraint and sensitive attribute corresponds to several thresholds mapping feature value to a threshold. but how do we choose which point on the curve to use? the default behavior \u2014 and", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sensitive attribute corresponds to several thresholds mapping feature value to a threshold. but how do we choose which point on the curve to use? the default behavior \u2014 and, in fact, the default in fairlearn \u2014 is to automatically choose the accuracy - maximizing thresholds subject to a particular fairness constraint. this might work in most cases ; however, accuracy is not necessarily the only benchmark an end user will care about. one client, for example, needs to satisfy a particular positivity rate and is fine with sacrificing accuracy to do so. what our feature introduces that goes beyond what's available in fairlearn", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "positivity rate and is fine with sacrificing accuracy to do so. what our feature introduces that goes beyond what's available in fairlearn is the ability to try different thresholds ; see what hypothetical predictions would be ; and see what hypothetical results / metrics ( e. g., positivity rate, tpr, etc. ) would look like if a certain set of thresholds was applied. the ultimate choice of thresholds is up to data scientists'needs, etc. hotspots for an explanation of how to enable hotspots with arthur, please see the hot spots we might have an ml", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "scientists'needs, etc. hotspots for an explanation of how to enable hotspots with arthur, please see the hot spots we might have an ml model deployed in production and some monitoring in place. we might notice that performance degrades from classic performance metrics or drift monitoring combined with explainability techniques. we \u2019 ve identified that our model is failing ; the next step is to identify why our model is failing. this process would involve slicing and dicing our input data that caused model degradation. we want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data that caused model degradation. we want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks or retraining our models on those regions. this basically boils down to the time - consuming task of finding needles in a haystack. what if we could reverse engineer the process and surface all of the needles, i. e., input regions associated with poor performance, directly to the user? we can with hotspots! how hotspots works the outputs of a model are encoded as a special classification task to partition data to separate out the poor - performing", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with hotspots! how hotspots works the outputs of a model are encoded as a special classification task to partition data to separate out the poor - performing data points from the correct predictions / classifications on a per - batch basis for batch models or a per 7 - day window for streaming models. hotspot enrichments are used to surface input regions where the model is currently underperforming for inferences. hotspots are extracted from a custom arthur tree model, where nodes are associated with particular input regions and have associated performance metrics, e. g., a node with 70 % accuracy with data points where variable x", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "where nodes are associated with particular input regions and have associated performance metrics, e. g., a node with 70 % accuracy with data points where variable x is less than 1000. nodes are candidates for hotspots. depending on user - specified thresholds, e. g., a threshold of 71 % accuracy, the tree is traversed until all nodes with less than 71 %, such as our node with 70 % accuracy, have been identified and returned to the user as hotspots, not including the hotspot nodes'children, which would be either ( 1 ) purer than the hotspot node and therefore in", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "hotspots, not including the hotspot nodes'children, which would be either ( 1 ) purer than the hotspot node and therefore in further violation of the, e. g., 71 % threshold or ( 2 ) pure nodes with correct inferences, which are not of interest to the user for remediation purposes. see our blog post here for a more in - depth overview. updated 5 months ago table of contents anomaly detection how anomaly detection works generating anomaly scores interpreting anomaly scores bias mitigation threshold mitigator hotspots how hotspots works source : https : / / docs. arthur", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "generating anomaly scores interpreting anomaly scores bias mitigation threshold mitigator hotspots how hotspots works source : https : / / docs. arthur. ai / docs / arthur - algorithms", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-algorithms", "row": 50, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 51 text : managing alerts jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by managing alertssuggest editsany team that has suffered from alert fatigue knows that alerts are only as good as your ability to manage and take action on them. knowing this, there are a few ways in which teams can work and interact with alerts within arthur, all focused on the ability for teams to derive value and track their alerts. recent organization alert overview upon opening the ui, teams begin to interact with alerts at an organizational", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the ability for teams to derive value and track their alerts. recent organization alert overview upon opening the ui, teams begin to interact with alerts at an organizational level. on the control panel home page, teams can see the most recent critical alerts triggered within their organization and a high - level description. from this page, teams can quickly assess which model's to dive deeper into from a recent alerting perspective. alert management per model there is an alert tab within each arthur model dashboard, as well, that provides a management dashboard for all alerts triggered for that particular model. here teams can : view recent history of alerts :", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model dashboard, as well, that provides a management dashboard for all alerts triggered for that particular model. here teams can : view recent history of alerts : visualize the recent patterns of alerting over the past two weeks. understand when new critical alerts or warnings appeared and in what quantity. create alert rules : described in the alerting overview tab. this allows teams to create some of the most common alert rules that we see in an easy, predefined structure. filter and sort triggered alerts : sort alerts by metrics and severity to immediately find the alerts of interest. operationalize alert management : teams can", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". filter and sort triggered alerts : sort alerts by metrics and severity to immediately find the alerts of interest. operationalize alert management : teams can tag alerts into different categories through the jira board - like structure. while teams can determine the best way to operationalize these categories for these teams, we typically see : categoryuse case descriptionnewnew alerts coming into the model that require investigation. acknowledgedalerts being investigated or looked into by a team member, reducing the repetition of investigation or work within teams. resolvedalerts that have been investigated and resolved, either through the team triggering retraining, solving an", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", reducing the repetition of investigation or work within teams. resolvedalerts that have been investigated and resolved, either through the team triggering retraining, solving an upstream data issue, or even flagging the alert as a false positive and potentially changing the threshold / alert rules managing alert rules within each model's alerting dashboard, teams can view and manage all model alert rules by clicking the manage alert rules button. understand all alert rules created for your model ( enabled or disabled ), their severity, and the date they were last updated in one central place. investigating a specific alert - ( root cause analysis ) after clicking on an alert,", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", their severity, and the date they were last updated in one central place. investigating a specific alert - ( root cause analysis ) after clicking on an alert, users are taken to an overview page for the alert. this page describes more about this alert, including a historical list of when it was triggered. it also provides three common analysis charts for the alert to allow teams to begin their root cause analysis into the cause. these charts include : the inference count over time : see how many inferences have been affected in creating this alert alerted metric over time : see how the specific metric of interest alerted metric over time ( for the attributes that experienced", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "many inferences have been affected in creating this alert alerted metric over time : see how the specific metric of interest alerted metric over time ( for the attributes that experienced the most drift ) : immediately evaluate if recent data distributional drifts may have been a root cause of the alert beyond some initial drilled - down charts for exploration, this alert details tile provides insights view in overview : jump to the model overview tab in the arthur ui to investigate further. this tab will have global filtered automatically applied to the window of time and / or data segments the alert was triggered on. send to relevant team - members or stakeholders : you can easily share the alert", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "applied to the window of time and / or data segments the alert was triggered on. send to relevant team - members or stakeholders : you can easily share the alert with others by hitting the send button on the top corner. this alert can be shared by email to anyone who has enabled email alert notifications for this model. otherwise, users can select copy link to copy a link to this alert details tile. updated 3 months ago table of contents recent organization alert overview alert management per model managing alert rules investigating a specific alert - ( root cause analysis ) source : https : / / docs. arthur. ai / docs / alerting -", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "managing alert rules investigating a specific alert - ( root cause analysis ) source : https : / / docs. arthur. ai / docs / alerting - dashboard", "metadata": {"source": "https://docs.arthur.ai/docs/alerting-dashboard", "row": 51, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 52 text : default access control jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by default access controlsuggest editsin both saas and on - prem installations, arthur ships with a built - in access control system that can be used to manage users, permissions, and access to organizations. this system has different capabilities than the sso - based paradigm. if your installation is using sso, please see the platform access control. authentication users authenticate to arthur using a username and password set when their account is created and can", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is using sso, please see the platform access control. authentication users authenticate to arthur using a username and password set when their account is created and can be changed later in the ui. users can also use the login api endpoint to retrieve a token with arthur apis. applications and automated systems can authenticate with arthur using api keys, which can be created in the arthur ui from the organization menu in the upper right corner, then clicking on manage api keys. [UNK] on using session keysit is not recommended to use api - keys for non - automated use cases as they are not tied to user identities and can obscure who", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[UNK] on using session keysit is not recommended to use api - keys for non - automated use cases as they are not tied to user identities and can obscure who is performing actions. as a best practice, use api keys minimally only in the systems that need automated access, and be sure to create a rotation practice to ensure safe keeping. authorization ( rbac ) the arthur standard access control system uses role - based access control ( rbac ) with a set of pre - defined roles. the available roles for users are user, model owner, administrator, and superadmin. if enrolled in multiple organizations, the user can have a different", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "roles. the available roles for users are user, model owner, administrator, and superadmin. if enrolled in multiple organizations, the user can have a different role in each organization. for a full list of permissions for these 4 standard roles, please reference arthur permissions by standard roles. user : has read - only access to the models and data within the organization. model owner : can onboard new models in the enrolled organization as well as send data, including reference data, inferences, and ground truth. administrator : organization - level administrator that has access to manage users and models within the organization. super admin : has full access", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inferences, and ground truth. administrator : organization - level administrator that has access to manage users and models within the organization. super admin : has full access to all data, models, and actions on the platform. can create new organizations and manage users. only available on - prem. [UNK] rolesif your installation uses sso, you can take advantage of creating custom roles to fine - tune user access to arthur resources. see the documentation on custom rbac for more information. adding users to an organization in the ui to complete this section, you must have the \" administrator \" role in your organization. click on the organization menu in the", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "adding users to an organization in the ui to complete this section, you must have the \" administrator \" role in your organization. click on the organization menu in the upper right corner and then \" manage members. \" from this screen, you can enter the emails of additional users to add to the organization, manage the roles of existing users, and remove users from the organization. [UNK] order for email - based user invites to work, your installation must have an email integration set up. if not, you can use the arthur apito create user accounts directly in your organization. adding users to an organization in the api arthur also supports managing users via automated", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "not, you can use the arthur apito create user accounts directly in your organization. adding users to an organization in the api arthur also supports managing users via automated workflows using the rest api. to create a user in your organization, you must have administrator privileges or access to the super admin user for your arthur on - prem installation. the following apis are helpful for managing users : create user update user send user invites switching between organizations if a user is invited to multiple organizations, they can switch between them in the ui. users can click on the organization menu in the upper right corner and choose one of the other available organizations from", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "organizations, they can switch between them in the ui. users can click on the organization menu in the upper right corner and choose one of the other available organizations from that menu to switch to it. if no other organizations appear, that user cannot access any other organizations. updated 3 months ago table of contents authentication authorization ( rbac ) adding users to an organization in the ui adding users to an organization in the api switching between organizations source : https : / / docs. arthur. ai / docs / default - access - control", "metadata": {"source": "https://docs.arthur.ai/docs/default-access-control", "row": 52, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 53 text : welcome to arthur scope! jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by welcome to arthur scope! as your team's data science operations center, arthur helps enterprise teams monitor, measure and optimize ai performance at scale. suggest editslooking to operationalize your machine learning systems in production? arthur's observability platform helps enterprise teams monitor, measure, and improve machine learning at scale. what happens when ai meets the real world? as the ai performance company, arthur helps data scientists, product", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "monitor, measure, and improve machine learning at scale. what happens when ai meets the real world? as the ai performance company, arthur helps data scientists, product owners, and business leaders accelerate model operations at scale. our platform monitors, measures, and improves machine learning models for better results across three core areas : accuracy, explainability, and fairness. video overview got 4 minutes? check out a video overview of our product : get started today jump into the quickstart guide or start learning about the arthur platform. updated 3 months ago what \u2019 s nextquickstarttable of contents what happens when ai meets the real world? video", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "start learning about the arthur platform. updated 3 months ago what \u2019 s nextquickstarttable of contents what happens when ai meets the real world? video overview get started today source : https : / / docs. arthur. ai / docs / getting - started", "metadata": {"source": "https://docs.arthur.ai/docs/getting-started", "row": 53, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 54 text : api documentation - arthur documentation api documentation arthur documentation link overview getting started arthur model arthur attribute reference set model inferences concepts and terminology user guide algorithmic bias explainability enrichments batch ingestion from s3 sparkml integration guide authentication and deployment sdk install guide sdk docs api api query guide endpoint overview aggregation functions model evaluation functions transformation functions explainability data drift alert rules guide examples on github redoc source : https : / / legacy. docs. arthur. ai / api - documentation / v3 - api - docs. html", "metadata": {"source": "https://legacy.docs.arthur.ai/api-documentation/v3-api-docs.html", "row": 54, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 55 text : pages in the arthur scope platform jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by pages in the arthur scope platformsuggest editsorganization wide in arthur scope, an organization is the highest level of grouped control within arthur utilized to organize and manage access to resources that exist on the platform. users can be added to multiple organizations and given roles that provide them with read and / or write access to some subset of that organization's resources, as defined by their user role. home page each organization's home page", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with read and / or write access to some subset of that organization's resources, as defined by their user role. home page each organization's home page works as a control panel for all the models in production. organization - wide model dashboard the organization - wide model dashboard provides a single pane of glass view into all your organization's models in production. model onboarding status model onboarding status provides a look into your model's creation phase. this is the place to check after onboarding to ensure your model is ready to receive inferences. pending : if your model status is pending, model creation will begin soon. please", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to check after onboarding to ensure your model is ready to receive inferences. pending : if your model status is pending, model creation will begin soon. please check back for updates or contact support if you don \u2019 t see any updates within a few minutes. creating : if your model status is creating, the model creation process is in - progress. please check back for updates or contact support if you don \u2019 t see any updates within 10 minutes. ready : if your model status is ready, the model creation process has been completed successfully! your model is now ready to ingest data. creation failed : if your model status is creation failed", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is ready, the model creation process has been completed successfully! your model is now ready to ingest data. creation failed : if your model status is creation failed, the model creation process was unsuccessful, and the necessary infrastructure was not provisioned fully. while in this state, your model cannot infrastructure ingest any data. you may try re - saving the model or contact support if the problem persists. health scores health scores provide a quick insight into the model's overall performance from three important aspects. accuracy : classification : f1 score. regression : 1 - normalized mae. ingestion : variance of normalized time periods between ing", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from three important aspects. accuracy : classification : f1 score. regression : 1 - normalized mae. ingestion : variance of normalized time periods between ingestion events. the variance of normalized volume differences between ingestion events. drift : 1 - average anomaly score these scores are classified into three color - coded sections : green, yellow, and red. settings users can access the settings for their organization by clicking on their initial button in the lower left corner. pages available within settings : account : change account setting information, such as your user password api keys : create and copy new api keys from members : a look at all", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "available within settings : account : change account setting information, such as your user password api keys : create and copy new api keys from members : a look at all members and roles in the arthur organization usage : evaluate monthly arthur platform usage notifications : organization - wide alert configuration, learn more at managing alerts changing organizations : users can have access to multiple organizations. to see what organizations they can access and / or switch between, they can click on the switch button in the settings tab. arthur scope - model dashboards overview tab the overview tab provides a look into your model's performance over time, in particular performance metrics, fairness metric", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". arthur scope - model dashboards overview tab the overview tab provides a look into your model's performance over time, in particular performance metrics, fairness metrics, and data drift metrics. inference deep dive the inference deep dive page provides a look into the individual inferences. certain administrator - level permissions will have the ability to save and edit column presets by pinning or unpinning columns on this page. these pinning selections happen at a per - model level. for those with non - adminstrator permissions, users will see the columns pinned by the admin by default and are able to pin columns to", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". for those with non - adminstrator permissions, users will see the columns pinned by the admin by default and are able to pin columns to their view only. alert dashboard a control panel for all things alerting for your arthur model, including creating, managing, and triaging alerts in one dashboard. insights insights is a page dedicated to the information stored from enrichments within arthur. currently, insights only contain insights about hot spots. updated about 2 months ago table of contents organization wide home page organization - wide model dashboard settings arthur scope - model dashboards overview tab inference deep dive alert dashboard insights source : https : /", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "table of contents organization wide home page organization - wide model dashboard settings arthur scope - model dashboards overview tab inference deep dive alert dashboard insights source : https : / / docs. arthur. ai / docs / pages - in - the - arthur - platform", "metadata": {"source": "https://docs.arthur.ai/docs/pages-in-the-arthur-platform", "row": 55, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 56 text : token likelihood jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by token likelihoodsuggest editsthere are currently no standard post - hoc explainability techniques for generative text ( or token sequence ) models. however, teams looking to better understand their models'outputs can turn to token likelihood for insights. [UNK] likelihood availability by llm typeteams may choose from a number of llm providers ( openai, anthropic, cohere, etc ) to build their model. all of these model types", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from a number of llm providers ( openai, anthropic, cohere, etc ) to build their model. all of these model types can be monitored with arthur. however, not every llm provider allows the option for outputting token likelihood. to track token likelihood in arthur, teams must provide token likelihood outputs from an llm that outputs them. understanding token likelihood the token likelihood is a number between 0 and 1 that quantifies the model \u2019 s level of surprise that this token was the next predicted token of the sentence. if a token has a low likelihood ( close to 0 ), it means that the", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "level of surprise that this token was the next predicted token of the sentence. if a token has a low likelihood ( close to 0 ), it means that the model is more unsure about selecting this token. while a likelihood close to 1 indicates that the model is very confident in predicting this token. for example, if i was writing the sentence : i need to pack my backpack and _ _ _ _ we can see from this example how token likelihood works. textbooks is seen as a much more likely next word for the sentence than other terms like umbrella. in llm models, each token predicted has a likelihood. what it looks like in arthur", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "much more likely next word for the sentence than other terms like umbrella. in llm models, each token predicted has a likelihood. what it looks like in arthur in arthur scope, token likelihood can be found, per inference provided for, in the inferences tab or the ui for text token sequence types. the color of tokens ranges from green to red, with bright green being the most likely and bright red being the least for tokens in each inference. tracking likelihood metrics as performance we've seen how looking at individual inferences token likelihood can provide insight into a single prediction, but what if we wanted to use likelihood to assess", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "as performance we've seen how looking at individual inferences token likelihood can provide insight into a single prediction, but what if we wanted to use likelihood to assess trends? average token likelihood the average token likelihood is created for each inference by taking the average likelihood score of predicted tokens in that inference. the average of these scores is then calculated for inferences within whatever time interval specified, i. e., daily, weekly, snapshot, etc. the average token likelihood is a way to evaluate how confident your model is in its overall predictions. by tracking this over time, teams can better track this confidence. likelihood stability likelihood stability looks", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a way to evaluate how confident your model is in its overall predictions. by tracking this over time, teams can better track this confidence. likelihood stability likelihood stability looks at how stable your likelihood is between tokens for each inference. updated 3 months ago table of contents understanding token likelihood what it looks like in arthur tracking likelihood metrics as performance average token likelihood likelihood stability source : https : / / docs. arthur. ai / docs / token - likelihood", "metadata": {"source": "https://docs.arthur.ai/docs/token-likelihood", "row": 56, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 57 text : registering model attributes manually jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by registering model attributes manuallysuggest editsthis guide is useful for teams that want to manually register all their model attributes or register a model without a reference dataset. setting up to individually onboard model attributes setting up your model to add each attribute individually is similar to the typical starting point for creating arthur model objects. teams must define their arthur model object : pythonmodel = arthur. model ( partner _ model _ id = f \" creditrisk", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "creating arthur model objects. teams must define their arthur model object : pythonmodel = arthur. model ( partner _ model _ id = f \" creditrisk _ batch _ qs - { datetime. now ( ). strftime ('% y % m % d % h % m % s') } \", display _ name = \" credit risk batch \", input _ type = inputtype. tabular, output _ type = outputtype. multiclass, is _ batch = true ) more information on defining this and all the steps can be found on the creating arthur model objects page here. set up arthur", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##class, is _ batch = true ) more information on defining this and all the steps can be found on the creating arthur model objects page here. set up arthur input attributes teams looking to manually onboard models can start by manually adding all pipelineinput and noninput attributes to their arthur model. [UNK] null values for attributesunless otherwise specified below in different types, null values are allowed for different input types. null and nan values are allowed for onboarding through the arthur sdk. on the other hand, null values are allowed when onboarding with the api, but null values are not. numerical attribute numerical attributes are input attributes meant to", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". on the other hand, null values are allowed when onboarding with the api, but null values are not. numerical attribute numerical attributes are input attributes meant to track continuous numerical values. they can be manually added to a model in arthur using the add _ attribute function. pythonfrom arthurai. common. constants import stage, valuetype # adds a float input attribute directly to the model arthur _ model. add _ attribute ( name = \" num _ attr _ name \", value _ type = valuetype. float, stage = stage. modelpipelineinput ) [UNK] numerical attributes as categoricalwhen arthur is", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "name \", value _ type = valuetype. float, stage = stage. modelpipelineinput ) [UNK] numerical attributes as categoricalwhen arthur is inferring the model schema, float and integer columns are assumed to be categorical if there are fewer than 20 unique values and if float values are all whole numbers. string and boolean columns are always assumed to be categorical for tabular models. python # # ensure that numerical attributes are valued as numerical and not categorical arthur _ model. get _ attribute ( \" num _ attr _ name \", stage = stage. modelpipelineinput ).", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "not categorical arthur _ model. get _ attribute ( \" num _ attr _ name \", stage = stage. modelpipelineinput ). categorical = false teams may also choose to specify the exact value type. the options are \" integer \" or \" float \". pythonarthur _ model. get _ attribute ( \" num _ attr _ name \", stage = stage. modelpipelineinput ). value _ type ='integer'categorical attribute categorical attributes are attributes that represent a finite group of values ( or categories ). they can be manually added to a model in arthur using the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "categorical attribute categorical attributes are attributes that represent a finite group of values ( or categories ). they can be manually added to a model in arthur using the add _ attribute function. pythonfrom arthurai. common. constants import stage, valuetype # adds a float input attribute directly to the model arthur _ model. add _ attribute ( name = \" cat _ attr _ name \", value _ type = valuetype. string, stage = stage. modelpipelineinput ) [UNK] all possible production attributes are specifiedattributes that are set to categorical must have at least one column. in edge cases where", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lineinput ) [UNK] all possible production attributes are specifiedattributes that are set to categorical must have at least one column. in edge cases where this is not possible, the category list can be set using a single \" dummy \" category ( e. g., [ \" n / a \" ] ). while new categories will be taken in by the platform, they will not be utilized in drift calculations or segmented visualization in the ui. so, it is important to ensure that all potential categories are listed before onboarding. setting possible categories based on the callout above, teams may manually specify the potential categories.", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is important to ensure that all potential categories are listed before onboarding. setting possible categories based on the callout above, teams may manually specify the potential categories. python # # set categories to n / a arthur _ model. get _ attribute ( \" cat _ attr _ name \", stage = stage. modelpipelineinput ). categories = [ \" n / a \" ] # # set categories to a list arthur _ model. get _ attribute ( \" cat _ attr _ name \", stage = stage. modelpipelineinput ). categories = [ \" n / a \", \" bachelors \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attr _ name \", stage = stage. modelpipelineinput ). categories = [ \" n / a \", \" bachelors \", \" masters \", \" highschool \" ] ] ] setting attribute labels when teams have set up numerical encoding for their categorical variables, providing the mapping back to human understanding for the arthur platform may be useful. this will make it easier for end users to utilize the ui to understand categorical attributes better. python # labels the value 0 for the attribute'education _ level'# to have the label'elementary ', etc. arthur _ model. set _ attribute _ labels (", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the value 0 for the attribute'education _ level'# to have the label'elementary ', etc. arthur _ model. set _ attribute _ labels ('education _ level ', { 0 :'elementary ', 1 :'middle ', 2 :'high ', 3 :'university'} ) timestamp attribute timestamp attributes are model features that represent a date / time. these are frequently found in time series models. pythonfrom arthurai. common. constants import stage, valuetype # adds a timestamp input attribute directly to the model arthur _ model. add _ attribute ( name =", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". common. constants import stage, valuetype # adds a timestamp input attribute directly to the model arthur _ model. add _ attribute ( name = \" timestamp _ attribute _ name \", value _ type = valuetype. timestamp, stage = stage. modelpipelineinput ) it is important to note that the datetime object being put into arthur, must be in a datetime format and include a timezone. a common example of how to set up these transformations can be seen below : python # # example of converting to pandas datetime # # this function will need to change depending on how", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to set up these transformations can be seen below : python # # example of converting to pandas datetime # # this function will need to change depending on how your time strings are formatted def get _ timestamps ( x ) : new _ time = x. split ( '.') [ 0 ] return datetime. strptime ( new _ time,'% y - % m - % d % h : % m : % s') df ['timestamps'] = df ['timestamps']. apply ( lambda x : get _ timestamps ( x ) )", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "['timestamps'] = df ['timestamps']. apply ( lambda x : get _ timestamps ( x ) ) # # ensure appropriate tzinfo timezone added df ['timestamp'] = df ['timestamp']. apply ( lambda x : x. replace ( tzinfo = pytz. utc ) ) [UNK] and nan values are not allowedfor timestamp attributes, null and nan values are not allowed within arthur. time series attribute time series attributes are model features that represent a value over time. pythonfrom arthurai. common", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and nan values are not allowed within arthur. time series attribute time series attributes are model features that represent a value over time. pythonfrom arthurai. common. constants import stage, valuetype # adds a time series input attribute directly to the model arthur _ model. add _ attribute ( name = \" time _ series _ attribute _ name \", value _ type = valuetype. timeseries, stage = stage. modelpipelineinput ) it is important to note that the timeseries object being put into arthur must be formatted as a list of dicts with \" timestamp \" and \" value \" keys. the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that the timeseries object being put into arthur must be formatted as a list of dicts with \" timestamp \" and \" value \" keys. the timestamps must be formatted according to the restrictions on timestamp attributes. the values must be floats. [UNK] must be data for every timestamp on a regular time intervaleach time series attribute should be considered to have some regular time interval ( eg. 1 day, 1 week, etc. ) at which the value it is recording is polled. the value thus must be recorded on consistent time intervals, and if data is not recorded on a given timestam", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "at which the value it is recording is polled. the value thus must be recorded on consistent time intervals, and if data is not recorded on a given timestamp in that consistent interval, a data point with a null value must still be recorded. text ( nlp ) attribute unstructured text attributes refer to input text for nlp models. they can be manually added to a model in arthur using the add _ attribute function. the arthurattribute type of unstructuredtext is designed to be used for nlp models only. pythonfrom arthurai. common. constants import stage, valuetype # adds a", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##structuredtext is designed to be used for nlp models only. pythonfrom arthurai. common. constants import stage, valuetype # adds a float input attribute directly to the model arthur _ model. add _ attribute ( name = \" nlp _ input _ attribute _ name \", value _ type = valuetype. unstructuredtext, stage = stage. modelpipelineinput ) generative text models teams wanting to monitor generative text models should refer to the generative text model onboarding guide. this provides a step - by - step walkthrough in manually onboarding those models. image attribute image", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to the generative text model onboarding guide. this provides a step - by - step walkthrough in manually onboarding those models. image attribute image attributes refer to the images input to computer vision models. they can be manually added to a model in arthur using the add _ image _ attribute function. pythonmodel. add _ image _ attribute ( \" imagecolumnname \" ) the imagecolumnname string contains the column's name in your future reference or inference data frames containing the path to each image. unique identifier a unique identifier attribute within arthur is created to specify unique values within the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "reference or inference data frames containing the path to each image. unique identifier a unique identifier attribute within arthur is created to specify unique values within the platform. these string - type categorical attributes within arthur specify unique values for every category. pythonfrom arthurai. common. constants import stage, valuetype # adds a float input attribute directly to the model arthur _ model. add _ attribute ( name = \" cat _ unique _ attr _ name \", value _ type = valuetype. string, stage = stage. modelpipelineinput ) # # specify this category as unique arthur _ model. get _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value _ type = valuetype. string, stage = stage. modelpipelineinput ) # # specify this category as unique arthur _ model. get _ attribute ( \" cat _ unique _ attr _ name \", stage = stage. modelpipelineinput ). unique = true # # specify no categories arthur _ model. get _ attribute ( \" cat _ unique _ attr _ name \", stage = stage. modelpipelineinput ). categories = [ ] [UNK] not onboard your partner inference id with reference datawhile the partner inference id ( your internal teams inference identifier ) is the most common unique id", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ ] [UNK] not onboard your partner inference id with reference datawhile the partner inference id ( your internal teams inference identifier ) is the most common unique identifier within arthur, this is something that you can specify ( or arthur will create ) when you send inferences onto the platform. this is not specified when building out a reference dataset. set up arthur predicted / ground truth attributes after sending all input attribute information, teams can specify their model's predicted and ground truth attributes. this will depend on the model task type they are trying to onboard. [UNK] values are not allowed for predicted or ground truth attributesnull values are", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "truth attributes. this will depend on the model task type they are trying to onboard. [UNK] values are not allowed for predicted or ground truth attributesnull values are not supported for predicted or ground truth attributes. classification to add output attributes for classification tasks, teams must first specify what type of classification model they want to onboard. they can choose between : binary classification with columns for each predicted attribute and ground truth value multi - class classification with columns for each predicted attribute and ground truth value either binary or multi - class classification with columns for each predicted attribute but only a single column for ground truth the type of classification you choose should be based on the sc", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "binary or multi - class classification with columns for each predicted attribute but only a single column for ground truth the type of classification you choose should be based on the schema you expect for onboarding reference or inference data later. binary classification if you expect your inference schema to consist of two predictions and two ground truth columns for your binary classification task, then you should utilize the add _ binary _ classifier _ output _ attributes function. in this function, you need to provide : prediction to ground truth mapping : mapping of each predicted column to its corresponding ground truth column positive predicted attribute : the positive class is the class that is related to your objective", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to ground truth mapping : mapping of each predicted column to its corresponding ground truth column positive predicted attribute : the positive class is the class that is related to your objective function. for example, if you want to classify whether the objects are present in a given scenario. so all the data samples where objects are predicted present will be considered positively predicted. python # map predictedvalue attributes to their corresponding groundtruth attributes pred _ to _ ground _ truth _ map = {'pred _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'} # add the ground truth and predicted attributes", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##d _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'} # add the ground truth and predicted attributes to the model # specifying that the ` pred _ 1 ` attribute is the # positive predicted attribute, which means it corresponds to the # probability that the binary target attribute is 1 arthur _ model. add _ binary _ classifier _ output _ attributes ( positive _ predicted _ attr ='pred _ 1 ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map ) multi - class classification if you expect your", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map ) multi - class classification if you expect your inference schema to consist of multiple predictions and their corresponding multiple ground truth columns for your classification task, then you should utilize the add _ multiclass _ classifier _ output _ attributes function. in this function, you need to provide : prediction to ground truth mapping : mapping of each predicted column to its corresponding ground truth column positive predicted attribute : the positive class is the class that is related to your objective function. for example, if you want to classify whether the objects are present in", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "positive predicted attribute : the positive class is the class that is related to your objective function. for example, if you want to classify whether the objects are present in a given scenario. so all the data samples where objects are predicted present will be considered positively predicted. python # map predictedvalue attributes to their corresponding groundtruth attributes pred _ to _ ground _ truth _ map = { \" dog \" : \" dog _ gt \", \" cat \" : \" cat _ gt \", \" horse \" : \" horse _ gt \" } # add the ground truth and predicted attributes to the model arthur _ model. add _ multiclass _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" horse \" : \" horse _ gt \" } # add the ground truth and predicted attributes to the model arthur _ model. add _ multiclass _ classifier _ output _ attributes ( pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map ) single column classification single - column classification is very similar to previous techniques ; however, there is only a single ground truth column in this technique. prediction to ground truth mapping : mapping of each predicted column to its corresponding ground truth value positive predicted attribute : the positive class is the class that is related to your objective function. for example, if", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of each predicted column to its corresponding ground truth value positive predicted attribute : the positive class is the class that is related to your objective function. for example, if you want to classify whether the objects are present in a given scenario. so all the data samples where objects are predicted present will be considered positively predicted. ground _ truth _ column : you must specify the single - column ground truth python # map predictedvalue attribute to its corresponding groundtruth attribute value. # this tells arthur that the ` pred _ survived ` column represents # the probability that the ground truth column has the value 1 pred _ to _ ground _ truth _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur that the ` pred _ survived ` column represents # the probability that the ground truth column has the value 1 pred _ to _ ground _ truth _ map = { \" pred _ value \" : 1 } # add the ground truth and predicted attributes to the model, # specifying which attribute represents ground truth and # which attribute represents the predicted value. arthur _ model. add _ classifier _ output _ attributes _ gtclass ( positive _ predicted _ attr ='pred _ value ', pred _ to _ ground _ truth _ class _ map = pred _ to _ ground _ truth _ map, ground", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' pred _ value ', pred _ to _ ground _ truth _ class _ map = pred _ to _ ground _ truth _ map, ground _ truth _ column ='gt _ column') regression to manually specify your regression output, teams need to specify a prediction to ground truth mapping with the following : predicted value : the column that contains your numerical predicted output ground truth value : the column that contains the ground truth pythonfrom arthurai. common. constants import valuetype # map predictedvalue attributes to their corresponding groundtruth attributes pred _ to _ ground _ truth _ map = { \" pred", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s import valuetype # map predictedvalue attributes to their corresponding groundtruth attributes pred _ to _ ground _ truth _ map = { \" pred _ value \" : \" gt _ value \", } # add the ground truth and predicted attributes to the model arthur _ model. add _ regression _ output _ attributes ( pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map, value _ type = valuetype. float ) object detection to manually specify your object detection models, teams need to specify the following : predicted attribute name : this is the column name that will store your predicted", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") object detection to manually specify your object detection models, teams need to specify the following : predicted attribute name : this is the column name that will store your predicted bounding boxes ground truth attribute name : this is the name of the column with the true labeled bounding boxes class labels : all potential object labels for that your model is detecting predicted _ attribute _ name = \" objects _ detected \" ground _ truth _ attribute _ name = \" label \" class _ labels = ['cat ','dog ','person'] arthur _ model. add _ object _ detection _ output _ attributes ( predicted _ attribute _ name, ground _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "','dog ','person'] arthur _ model. add _ object _ detection _ output _ attributes ( predicted _ attribute _ name, ground _ truth _ attribute _ name, class _ labels ) generative text ( llm ) teams wanting to monitor generative text models should refer to the generative text model onboarding guide. this provides a step - by - step walkthrough in manually onboarding those models. setting reference data later for teams that have chosen to manually onboard all of their model attributes to ensure that they were inferred correctly but still want to include a reference dataset for drift calculations, they", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "chosen to manually onboard all of their model attributes to ensure that they were inferred correctly but still want to include a reference dataset for drift calculations, they can! after manually creating the model schema above, this can be done by setting the reference dataset. python # reference dataframe of model inputs reference _ set = pd. dataframe (.... ) # produce model predictions on reference set # in this example, the predictions are classification probabilities preds = model. predict _ proba ( reference _ set ) # assign the column corresponding to the positive class # as the ` pred ` attribute in the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ds = model. predict _ proba ( reference _ set ) # assign the column corresponding to the positive class # as the ` pred ` attribute in the reference data reference _ set [ \" pred \" ] = preds [ :, 1 ] # set ground truth labels reference _ set [ \" gt \" ] =... # configure the arthurmodel to use this dataframe as reference data arthur _ model. set _ reference _ data ( data = reference _ set ) updated 2 months ago table of contents setting up to individually onboard model attributes set up arthur input attributes numerical attribute categorical attribute timestamp", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= reference _ set ) updated 2 months ago table of contents setting up to individually onboard model attributes set up arthur input attributes numerical attribute categorical attribute timestamp attribute time series attribute text ( nlp ) attribute image attribute unique identifier set up arthur predicted / ground truth attributes classification regression object detection generative text ( llm ) setting reference data later source : https : / / docs. arthur. ai / docs / registering - model - in - sdk - piece - by - piece", "metadata": {"source": "https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece", "row": 57, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 58 text : enabling enrichments jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by enabling enrichmentssuggest editsas discussed in detail in the enrichments section above, many teams require more than standard inference logging monitoring. for this reason, arthur provides enrichments. functions within enrichment enablement there are three main functionalities for toggling enrichments on and off within arthur. these are : getting the current configuration of the enrichment : understand whether or not the enrichment is enabled / disabled. this can be validated in a notebook environment", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". these are : getting the current configuration of the enrichment : understand whether or not the enrichment is enabled / disabled. this can be validated in a notebook environment ( as shown in the below examples ) or within the arthur ui for the model under details. enable the enrichment : functionality to turn the enrichment on. this is not currently available in the ui and must be done with the python sdk or an api call. disable the enrichment : functionality to turn the enrichment off. this is not currently available in the ui and must be done with the python sdk or an api call. anomaly detection anomaly detection allows users to go beyond un", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this is not currently available in the ui and must be done with the python sdk or an api call. anomaly detection anomaly detection allows users to go beyond univariate drift analysis and look at complex interactions between features that may cause drift. to learn more in detail about anomaly detection, please refer to its doc's section here : enable anomaly detection anomaly detection is the only enrichment automatically enabled within arthur, as long as your model has a reference dataset attached to it. python # # using the arthur python sdk # view current configuration arthur _ model. get _ enrichment ( enrichment. anomalydetection ) # enable arthur _ model", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # using the arthur python sdk # view current configuration arthur _ model. get _ enrichment ( enrichment. anomalydetection ) # enable arthur _ model. update _ enrichment ( enrichment. anomalydetection, true, { } ) # disable arthur _ model. update _ enrichment ( enrichment. anomalydetection, false, { } ) explainability one of the most commonly enabled enrichments is explainability. explainability allows teams to build trust with valuable insights into how models make decisions. it allows teams to understand why predictions are being made and evaluate how changes to model input change predictions. you can learn more about how", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "into how models make decisions. it allows teams to understand why predictions are being made and evaluate how changes to model input change predictions. you can learn more about how to use our explainability capabilities here, but model asset requirements arthur can automatically calculate explanations ( feature importances ) for every prediction your model makes. to make this possible, we package up your model in a way that allows us to call it'spredict function, which allows us to calculate explanations. we require a few things from your end : a python script that wraps your model's predict function for image models, a second function, load _ image is also required", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a few things from your end : a python script that wraps your model's predict function for image models, a second function, load _ image is also required. a directory containing the above file, along with any serialized model files, and other supporting code a requirements. txt with the dependencies to support the above more detailed explanations about the model assets required here. pythonentrypoint fileexample requirements. txt # # enabling explainability arthur _ model. enable _ explainability ( df = x _ train. head ( 50 ), project _ directory = \" / path / to / model _ folder / \",", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ explainability ( df = x _ train. head ( 50 ), project _ directory = \" / path / to / model _ folder / \", requirements _ file = \" requirements. txt \", user _ predict _ function _ import _ path = \" model _ entrypoint \", ignore _ dirs = [ \" folder _ to _ ignore \" ] # optionally exclude directories within the project folder from being bundled with predict function ) # # common examples per model type can be found in the model input / output pages # example _ entrypoint. py sk _ model = joblib. load ( \". /", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model type can be found in the model input / output pages # example _ entrypoint. py sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( x ) boto3 > = 1. 0 numpy > = 1. 0 pandas > = 1. 0 scikit - learn > = 0. 23. 0 find more information about troubleshooting explainability here. hot spots hot spots help teams automatically surface rule - based segments of your data for underperformance. learn more about", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "about troubleshooting explainability here. hot spots hot spots help teams automatically surface rule - based segments of your data for underperformance. learn more about how in the hot spots section of the documentation here. enable hot spots since hot spots rely only on inference data, no additional configuration is needed to enable them within the platform. python # view current configuration arthur _ model. get _ enrichment ( enrichment. hotspots ) # enable arthur _ model. update _ enrichment ( enrichment. hotspots, true, { } ) # disable arthur _ model. update _ enrichment ( enrichment. hotspots, false, { }", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "enrichment. hotspots, true, { } ) # disable arthur _ model. update _ enrichment ( enrichment. hotspots, false, { } ) bias mitigation as a reminder about a few of bias mitigation's \" only's \" that affect enrichment enablement. bias mitigation is only available for binary classification models it can only be enabled if at least one model attribute is marked as monitor _ for _ bias = true so by default, any binary classifier that you want to enable bias mitigation for will automatically train a mitigation model for all attributes marked as monitor _ for _", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", any binary classifier that you want to enable bias mitigation for will automatically train a mitigation model for all attributes marked as monitor _ for _ bias = true. enable bias mitigation default bias mitigation ( all marked attributes ) bias mitigation for a specific attribute # view current configuration arthur _ model. get _ enrichment ( enrichment. biasmitigation ) # enable arthur _ model. update _ enrichment ( enrichment. biasmitigation, true, { } ) # or arthur _ model. enable _ bias _ mitigation ( ) updated 3 months ago what \u2019 s nextsending inferencesasset", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", { } ) # or arthur _ model. enable _ bias _ mitigation ( ) updated 3 months ago what \u2019 s nextsending inferencesassets required for explainabilitytroubleshooting explainabilitytable of contents functions within enrichment enablement anomaly detection enable anomaly detection explainability model asset requirements hot spots enable hot spots bias mitigation enable bias mitigation source : https : / / docs. arthur. ai / docs / enabling - enrichments", "metadata": {"source": "https://docs.arthur.ai/docs/enabling-enrichments", "row": 58, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 59 text : data preparation for arthur jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by data preparation for arthurwhat type of data do i need to prepare for the arthur platform? suggest editsat a high level, teams often want to know what they need to set up to onboard arthur before starting the onboarding process. as shown below, the assets teams need to prepare to use arthur are directly related to how they want to use arthur for their models. we can see that the only assets required to monitor inferences actively are your model", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "use arthur are directly related to how they want to use arthur for their models. we can see that the only assets required to monitor inferences actively are your model structure and inference data. however, teams that wish to use more aspects of the arthur platform than seeing inputs and outputs need to onboard / send additional assets to arthur. model structure a model structure, otherwise known as an arthur model schema, defines a wireframe for what arthur should expect as inputs and outputs to your model. by recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". by recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built for your model in arthur. example model input and output to define this wireframe, teams must provide either example model input and output data ( as in a reference dataset ) or define all model inputs ( features ) and outputs ( predictions or ground truth ) as well as their expected values. these values must be a type allowed by arthur. at a high level, these are the available arthur inputs : available input value typesallowed data typesadditional information requirednumericalinte", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "allowed by arthur. at a high level, these are the available arthur inputs : available input value typesallowed data typesadditional information requirednumericalinteger, floatcategoricalinteger, string, boolean, floatspecified available categoriestimestampdatetimemust be time zone aware. cannot be null or nantext ( nlp ) stringgenerative sequence ( llm ) stringa tokens ( and optional token likelihood ) column is also requiredimage. jpg,. png,. gifunique identifierstringtime serieslist of dictionaries with \" timestamp", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##image. jpg,. png,. gifunique identifierstringtime serieslist of dictionaries with \" timestamp \" and \" value \" keystimestamps must be timezone aware datetimes and values must be floats. these are the available arthur outputs : arthur output typeallowed data typesnotesclassificationinteger, floatcannot be null or nanregressioninteger, floatcannot be null or nanobject detectionliteral array of bounding box valuescannot be null or nangenerative sequence unstructured textstringcannot be null or nangen", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionliteral array of bounding box valuescannot be null or nangenerative sequence unstructured textstringcannot be null or nangenerative sequence token likelihoodarray of float valuesthis is optional. cannot be null or nanranked listlist of dictionaries with \" item _ id \", \" label \", and \" score \" keys \" item _ id \" and \" label \" values must be strings, \" score \" value must be a float. \" label \" and \" score \" keys are optional. for more specific examples of troubleshooting onboarding for specific model input and output types, please", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "float. \" label \" and \" score \" keys are optional. for more specific examples of troubleshooting onboarding for specific model input and output types, please refer to registering model attributes manually. map of data relationship as well as information about how our model predictions relate to our ground truth attributes. this is used to help prepare the arthur platform to calculate performance metrics correctly. teams can register this structure about their models manually ( registering model attributes manually ). however, the most common way teams define their model structure in arthur is by having it automatically inferred by onboarding their reference dataset with the python sdk. reference dataset", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "way teams define their model structure in arthur is by having it automatically inferred by onboarding their reference dataset with the python sdk. reference dataset a reference dataset is a representative sample of input features for your model. in other words, it is a sample of what is typical or expected for your model. typically, teams onboard their model's training or validation data for reference. in the arthur platform, your reference dataset provides the baseline for data drift and anomaly metrics. therefore, for those techniques to appear, a reference dataset must be set for your arthur model object. the only required stage to be included", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "anomaly metrics. therefore, for those techniques to appear, a reference dataset must be set for your arthur model object. the only required stage to be included in the reference dataset is modelpipelineinput. but we also recommend including data from the predictedvalue, groundtruth, and noninputdata stages so that arthur can also measure drift in those attributes over time. note : as mentioned in the data drift metric section, univariate data drift metrics can be calculated in the python sdk by comparing inferences to one another without a reference dataset. for more information on best practices / guidelines for", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics can be calculated in the python sdk by comparing inferences to one another without a reference dataset. for more information on best practices / guidelines for selecting your reference dataset, please refer to the creating arthur model object page. model assets model assets are only required for teams that wish to enable explainability for their model. explainability, as described better in the explainability section, can be used to understand the model's decisions better. these assets are further described in ( add explainability section ), but they include : requirements. txt file : a file containing all the requirements needed to run your predict function model :", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in ( add explainability section ), but they include : requirements. txt file : a file containing all the requirements needed to run your predict function model : the compressed runnable model used to generate predictions ( typically. pkl format ) entrypoint python file : this file contains your python predict function that will be used to generate predictions to produce explanations inputs : list outputs : list pythonimport \u2026 model = load. model ( \u2018 pkl _ model \u2019 ) # # python predict function # # takes in and returns a list def predict ( x ) : return model. predict _ proba ( x ) inference data for teams", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# python predict function # # takes in and returns a list def predict ( x ) : return model. predict _ proba ( x ) inference data for teams that want to use arthur for the primary purpose of monitoring, it is essential to set up active monitoring of the data and predictions your model is running on in production. in arthur, these rows of data are called inferences. data associated with each inference might include ( 1 ) input data, ( 2 ) model predictions, and / or ( 3 ) corresponding ground truth. teams with the most success monitoring work to automate or create a consistent process for sending inferences to the platform", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and / or ( 3 ) corresponding ground truth. teams with the most success monitoring work to automate or create a consistent process for sending inferences to the platform. inference ground truth while ground truth can be sent simultaneously as inference data, many ml models do not receive ground truth labels during prediction. in these instances, teams can set up a process to match ground truth in the platform with outside labels. note on sending large data to arthur while arthur supports ingesting reference data, inferences, and ground truth labels, there are some constraints on ingestion that are worth being aware of. clients are responsible for ensuring that data sent to", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inferences, and ground truth labels, there are some constraints on ingestion that are worth being aware of. clients are responsible for ensuring that data sent to arthur respects these limits except for where called out below : arthur's ingress proxy has a 4gb limit on the size of any request payload that's made to arthur. requests which exceed this 4gb limit will error out indicating that the body was too large. arthur's ingestion - service ( responsible for ingesting data ) has a fixed amount of memory available. by default, this is 2gb, but is configurable through the administrative interface", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for ingesting data ) has a fixed amount of memory available. by default, this is 2gb, but is configurable through the administrative interface or deployment scripts. requests exceeding this limit may cause ingestion - service to run out of memory and crash. sdk users that are ingesting images using send _ bulk _ inferences, send _ bulk _ ground _ truths or set _ reference _ data will have their files chunked so that they respect these limits. updated 2 months ago table of contents model structure example model input and output map of data relationship reference dataset model assets inference data inference ground truth note on", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "limits. updated 2 months ago table of contents model structure example model input and output map of data relationship reference dataset model assets inference data inference ground truth note on sending large data to arthur source : https : / / docs. arthur. ai / docs / preparing - for - onboarding", "metadata": {"source": "https://docs.arthur.ai/docs/preparing-for-onboarding", "row": 59, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 60 text : pagerduty jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by pagerdutysuggest editswith the arthur + pagerduty integration, you can notify on - call teams in pagerduty of alerts arthur triggers. to set up this integration, follow these steps : step 1 : set up your email integration in pagerduty there are three ways to configure an email integration in pagerduty. please follow one of the three options in the pagerd", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##rduty there are three ways to configure an email integration in pagerduty. please follow one of the three options in the pagerduty email integration guide to retrieve your integration email address. step 2 : configure your integration in arthur to configure the pagerduty integration for a model in arthur, you can send a post request to the / alert _ notification _ configurations. model _ id - uuid of the model this alert notification configuration belongs to. type - type of notification to send. in this case, \" pagerduty \". destination - the integration email address", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this alert notification configuration belongs to. type - type of notification to send. in this case, \" pagerduty \". destination - the integration email address obtained in step 1. enabled - whether or not the notification configuration is enabled. it defaults to true. example query request : json { \" model _ id \" : \" < model _ id > [ string ] \", \" type \" : \" [ pagerduty ] \", \" destination \" : \" < [ email protected ] > [ string ] \", } for more information on configuring alert notifications, please see the notification section of the alert", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "< [ email protected ] > [ string ] \", } for more information on configuring alert notifications, please see the notification section of the alerting page. step 3 : start monitoring! your integration is now ready to use! when an alert is triggered in arthur for this model, an incident will be created in your pagerduty. updated 3 months ago table of contents step 1 : set up your email integration in pagerduty step 2 : configure your integration in arthur step 3 : start monitoring! source : https : / / docs. arthur. ai / docs / pagerdut", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##figure your integration in arthur step 3 : start monitoring! source : https : / / docs. arthur. ai / docs / pagerduty", "metadata": {"source": "https://docs.arthur.ai/docs/pagerduty", "row": 60, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 61 text : transformation functions jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by transformation functionssuggest editsfor transformation functions, it will be helpful to include theproperty in the request to help associate the transformation function values, for example : json { \" select \" : [ { \" property \" : \" < attribute _ name > [ string ] \" }, { \" function \" : \" roundtimestamp \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" :", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" function \" : \" roundtimestamp \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string ] \", \" time _ interval \" : \" [ secondminutehourdaymonthyear ] \" } } ] } we omit property for brevity in the following examples. for an explanation of nested functions, see the guide composing functions. round timestamp rounds a timestamp property to the provided time interval. this function requires one property which must be an attribute of type datetime", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "functions. round timestamp rounds a timestamp property to the provided time interval. this function requires one property which must be an attribute of type datetime and one parameter, time _ interval. query request : json { \" select \" : [ { \" function \" : \" roundtimestamp \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" time _ interval \" : \" [ secondminutehourdaymonthyear ] \" } } ] } query response :", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or nested ] \", \" time _ interval \" : \" [ secondminutehourdaymonthyear ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < rounded _ timestamp > [ string ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" roundtimestamp \", \" parameters \" : { \" property \" : \" inference _ timestamp \", \" time _ interval \" : \" day \" } } ] } sample response : json", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { \" property \" : \" inference _ timestamp \", \" time _ interval \" : \" day \" } } ] } sample response : json { \" query _ result \" : [ { \" roundtimestamp \" : \" 2020 - 08 - 10t00 : 00 : 00. 000z \" }, { \" roundtimestamp \" : \" 2020 - 08 - 09t00 : 00 : 00. 000z \" }, { \" roundtimestamp \" : \" 2020 - 08 - 08t00 : 00 : 00. 000z \" } ] } back to top label by max column given a", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##stamp \" : \" 2020 - 08 - 08t00 : 00 : 00. 000z \" } ] } back to top label by max column given a list of columns, returns a string column with the name of the column containing the max value for the row. for example, this function can be used to extract the max probability from a vector of probability properties. query request : json { \" select \" : [ { \" function \" : \" labelbymaxcolumn \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" properties \" : [ \" < property", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##n \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" properties \" : [ \" < property name > [ string ] \" ] } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < one of the properties from the given list > [ string ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" labelbymaxcolumn \", \" alias \" : \" classprediction \", \" parameters \" : {", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": [ { \" function \" : \" labelbymaxcolumn \", \" alias \" : \" classprediction \", \" parameters \" : { \" properties \" : [ \" class _ 1 \", \" class _ 2 \", \" class _ 3 \" ] } } ] } sample response : json { \" classprediction \" : [ { \" classprediction \" : \" class _ 1 \" }, { \" classprediction \" : \" class _ 1 \" }, { \" classprediction \" : \" class _ 2 \" } ] } back to top if / then / else given", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" class _ 1 \" }, { \" classprediction \" : \" class _ 2 \" } ] } back to top if / then / else given a column and a condition, returns the \" then \" value if the condition is true on that column, otherwise returns the \" else \" value. query request : json { \" select \" : [ { \" function \" : \" if \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < property name > [ string or nested ] \", \" comparator \" : \" < gt", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" parameters \" : { \" property \" : \" < property name > [ string or nested ] \", \" comparator \" : \" < gtltgtelteeqne > \", \" value \" : \" < any value to compare to > \", \" then \" : \" < any value to return when true > \", \" else \" : \" < any value to return when false > \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < then or else value based on the conditional > \" } ]", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ result \" : [ { \" < function _ name / alias _ name > \" : \" < then or else value based on the conditional > \" } ] } sample request : json { \" select \" : [ { \" function \" : \" if \", \" alias \" : \" predicted _ class \", \" parameters \" : { \" property \" : \" positive _ probability \", \" comparator \" : \" gte \", \" value \" : 0. 6, \" then \" : \" positive \", \" else \" : \" netivate \" } } ] } sample response : json { \" query _ result \"", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" then \" : \" positive \", \" else \" : \" netivate \" } } ] } sample response : json { \" query _ result \" : [ { \" predicted _ class \" : \" positive \" }, { \" predicted _ class \" : \" negative \" }, { \" predicted _ class \" : \" positive \" } ] } back to top bin continuous this function bins a continuous value based on supplied thresholds. the bins will be formed as : [ < threshold _ 1, threshold _ 1 < = x < threshold _ 2,..., threshold _ ( n - 1 ) < = x", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": [ < threshold _ 1, threshold _ 1 < = x < threshold _ 2,..., threshold _ ( n - 1 ) < = x < threshold _ ( n ), threshold _ ( n ) < x ]. the response bins will be labeled with an integer id corresponding to the ordered bin, starting at 1. if n thresholds are given, n + 1 bins will be returned. query request : json { \" select \" : [ { \" function \" : \" bincontinuous \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" :", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" function \" : \" bincontinuous \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < property name > [ string or nested ] \", \" bin _ thresholds \" : [ \" < threshold _ 1 > [ number ] \", \" < threshold _ 2 > [ number ] \", \" < threshold _ 3 > [ number ] \" ] } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < bin _ id > [", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < bin _ id > [ int ] \" } ] } sample request : json { \" select \" : [ { \" property \" : \" age \" }, { \" function \" : \" bincontinuous \", \" alias \" : \" agebin \", \" parameters \" : { \" property \" : \" age \", \" bin _ thresholds \" : [ 18, 65, 95 ] } } ] } sample response : json { \" query _ result \" : [ { \" age \"", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "thresholds \" : [ 18, 65, 95 ] } } ] } sample response : json { \" query _ result \" : [ { \" age \" : 10, \" agebin \" : 1 }, { \" age \" : 20, \" agebin \" : 2 }, { \" age \" : 70, \" agebin \" : 3 } ] } back to top bins to quantiles returns an array of values representing the quantiles based on the number of bins passed to the function. for example if you supply \" num _ bins \" : \" 10 \", then this query will return the", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "number of bins passed to the function. for example if you supply \" num _ bins \" : \" 10 \", then this query will return the value in your data at the 10 %, 20 %,..., 90 %, 100 % quantiles. query request : json { \" select \" : [ { \" function \" : \" binstoquantiles \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" num _ bins \" :", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" num _ bins \" : \" < num _ bins > [ int ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : [ \" float \" ] } ] } sample request : json { \" select \" : [ { \" function \" : \" binstoquantiles \", \" alias \" : \" quantiles \", \" parameters \" : { \" property \" : \" age \"", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" binstoquantiles \", \" alias \" : \" quantiles \", \" parameters \" : { \" property \" : \" age \", \" num _ bins \" : 10 } } ] } sample response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : [ 19, 28, 37, 46, 55, 64, 73, 82, 91 ] } ] } back to top date diff returns the difference of two timestamps in units. valid units are : second, minute, hour, day, week, month, quarter", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "top date diff returns the difference of two timestamps in units. valid units are : second, minute, hour, day, week, month, quarter, and year. query request : json { \" select \" : [ { \" function \" : \" datediff \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" unit \" : \" [ secondminutehourdayweekmonthquarteryear ] \", \" start _ date \" : \" < attribute _ name > [ string or nested ] \", \" end _ date \" : \" < attribute", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] \", \" start _ date \" : \" < attribute _ name > [ string or nested ] \", \" end _ date \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" difference [ int ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" datediff \", \" alias \" : \" date _ diff \", \" parameters \" : { \" unit \" : \" second \", \" start", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" datediff \", \" alias \" : \" date _ diff \", \" parameters \" : { \" unit \" : \" second \", \" start _ date \" : \" inference _ timestamp \", \" end _ date \" : \" prev _ timestamp \" } } ] } sample response : json { \" query _ result \" : [ { \" date _ diff \" : 100 } ] } back to top neighbor returns the value of the column offset rows next to this row in the ordering. default is the value that is returned when the offset goes out of bounds on the row set. it is", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "column offset rows next to this row in the ordering. default is the value that is returned when the offset goes out of bounds on the row set. it is recommended to use this function in a subquery with an order _ by clause to get consistent ordering. query request : json { \" select \" : [ { \" function \" : \" neighbor \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" offset \" : \" < offset > [ int ] \", \" property \" : \" < attribute _ name > [ string or nested ] \", \" default \"", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" < offset > [ int ] \", \" property \" : \" < attribute _ name > [ string or nested ] \", \" default \" : \" < default _ value > [ any ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" neighbor _ value [ any ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" neighbor \", \" alias \" : \" prev _ timestamp \", \" parameters \" : { \" property \" : \" inference", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" function \" : \" neighbor \", \" alias \" : \" prev _ timestamp \", \" parameters \" : { \" property \" : \" inference _ timestamp \", \" offset \" : - 1, \" default \" : null } } ], \" order _ by \" : [ { \" property \" : \" inference _ timestamp \", \" direction \" : \" desc \" } ] } sample response : json { \" query _ result \" : [ { \" prev _ timestamp \" : \" 2021 - 06 - 15t00 : 00 : 00. 000z \" } ] } back", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": [ { \" prev _ timestamp \" : \" 2021 - 06 - 15t00 : 00 : 00. 000z \" } ] } back to top arithmetic add, subtract, multiply, and divide are valid arithmetic functions. each takes two columns as input and returns the result of the arithmetic expression. query request : json { \" select \" : [ { \" function \" : \" [ addsubtractmultiplydivide ] \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" left \" : \" < attribute _ name", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" left \" : \" < attribute _ name > [ string or nested ] \", \" right \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" expression _ result [ number ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" multiply \", \" alias \" : \" double _ home", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ] } sample request : json { \" select \" : [ { \" function \" : \" multiply \", \" alias \" : \" double _ home _ value \", \" parameters \" : { \" left \" : \" home _ value \", \" right \" : 2 } } ] } sample response : json { \" query _ result \" : [ { \" double _ home _ value \" : 20000 } ] } sample nested request to compute ( home _ value + car _ value ) * 2 json { \" select \" : [ { \" function \" : \" multiply \", \" alias \" : \" double", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value + car _ value ) * 2 json { \" select \" : [ { \" function \" : \" multiply \", \" alias \" : \" double _ loans \", \" parameters \" : { \" left \" : { \" nested _ function \" : { \" function \" : \" add \", \" alias \" : \" total _ loan \", \" parameters \" : { \" left \" : \" home _ value \", \" right \" : \" car _ value \" } } }, \" right \" : 2 } } ] } sample nested response : json { \" query _ result \" : [ { \" double _", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} } }, \" right \" : 2 } } ] } sample nested response : json { \" query _ result \" : [ { \" double _ loans \" : 20000 } ] } back to top absolute value take the absolute value of a property. query request : json { \" select \" : [ { \" function \" : \" abs \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" property \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < abs _ value > [ float ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" abs \", \" alias \" : \" abs _ delta \", \" parameters \" : { \" property \" : \" delta \" } } ] } sample response : json { \" query _ result \" : [ { \" abs _ delta \" : 55. 45 }", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" delta \" } } ] } sample response : json { \" query _ result \" : [ { \" abs _ delta \" : 55. 45 } ] } back to top logical functions equals, and, and or are valid logical functions. each takes two columns as input and returns the result of the logical expression. these follow the same api as arithmetic functions query request : json { \" select \" : [ { \" function \" : \" [ equalsandor ] \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" left \" : \" < attribute _ name >", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" left \" : \" < attribute _ name > [ string or nested ] \", \" right \" : \" < attribute _ name > [ string or nested ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" expression _ result [ 0 or 1 ] \" } ] } sample request : json { \" select \" : [ { \" function \" : \" equals \", \" alias \" : \" has _ phd", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" } ] } sample request : json { \" select \" : [ { \" function \" : \" equals \", \" alias \" : \" has _ phd \", \" parameters \" : { \" left \" : \" education \", \" right \" : 4 } } ] } sample response : json { \" query _ result \" : [ { \" has _ phd \" : 1 } ] } sample nested request to compute has _ phd or has _ masters json { \" select \" : [ { \" function \" : \" or \", \" alias \" : \" has _ higher _ education \", \" parameters \" : { \"", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" select \" : [ { \" function \" : \" or \", \" alias \" : \" has _ higher _ education \", \" parameters \" : { \" left \" : { \" nested _ function \" : { \" function \" : \" equals \", \" alias \" : \" has _ phd \", \" parameters \" : { \" left \" : \" education \", \" right \" : 4 } } }, \" right \" : { \" nested _ function \" : { \" function \" : \" equals \", \" alias \" : \" has _ masters \", \" parameters \" : { \" left \" : \" education \",", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" function \" : \" equals \", \" alias \" : \" has _ masters \", \" parameters \" : { \" left \" : \" education \", \" right \" : 3 } } } } } ] } sample nested response : json { \" query _ result \" : [ { \" has _ higher _ education \" : 1 } ] } back to topupdated 3 months ago table of contents round timestamp label by max column if / then / else bin continuous bins to quantiles date diff neighbor arithmetic absolute value logical functions source : https : / / docs. arthur. ai / doc", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "else bin continuous bins to quantiles date diff neighbor arithmetic absolute value logical functions source : https : / / docs. arthur. ai / docs / transformation - functions", "metadata": {"source": "https://docs.arthur.ai/docs/transformation-functions", "row": 61, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 62 text : time series jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by time seriessuggest editstime series input models are a type of machine learning model that operates on time series data, or data which measures a certain value over time, such as credit card balance over time. these models can perform tasks such as predictions or recommendations based on past patterns. formatted data in arthur time series input models require the following data formatting : json [ { \" timestamp \" : \" 2023 - 10 - 05t", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data in arthur time series input models require the following data formatting : json [ { \" timestamp \" : \" 2023 - 10 - 05t00 : 00 : 00z \", \" value \" : 1 }, { \" timestamp \" : \" 2023 - 10 - 06t00 : 00 : 00z \", \" value \" : 4 } ] arthur requires that all times will be present in a given series according to a regular interval ( eg. one value each day ). there is an upper bound of 500 timestamps in a single time series inference. arthur supports sending time series data in", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##g. one value each day ). there is an upper bound of 500 timestamps in a single time series inference. arthur supports sending time series data in json files or dataframes. updated about 2 months ago table of contents formatted data in arthur source : https : / / docs. arthur. ai / docs / time - series", "metadata": {"source": "https://docs.arthur.ai/docs/time-series", "row": 62, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 63 text : source : https : / / staging. docs. arthur. ai / docs / quickstart", "metadata": {"source": "https://staging.docs.arthur.ai/docs/quickstart", "row": 63, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 64 text : config template jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by config templatesuggest editsthe configuration template for arthur version 3. 4. 0 is below : yamlapiversion : kots. io / v1beta1 kind : configvalues metadata : creationtimestamp : null name : arthur spec : values : iam _ permission _ type : default : access _ keys advanced _ cache _ options : default : \" 0 \" advanced _ messaging _ connect _ cpu", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "values : iam _ permission _ type : default : access _ keys advanced _ cache _ options : default : \" 0 \" advanced _ messaging _ connect _ cpu _ limits : default : \" 2 \" advanced _ messaging _ connect _ cpu _ limits _ not _ validate : { } advanced _ messaging _ connect _ cpu _ requests : default : \" 1 \" advanced _ messaging _ connect _ cpu _ requests _ not _ validate : { } advanced _ messaging _ connect _ heap _ options : default : - xms1g - xmx3g advanced _ messaging _ connect _ memory _ limits : default : 4gi advanced _ messaging", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ options : default : - xms1g - xmx3g advanced _ messaging _ connect _ memory _ limits : default : 4gi advanced _ messaging _ connect _ memory _ limits _ not _ validate : { } advanced _ messaging _ connect _ memory _ requests : default : 2gi advanced _ messaging _ connect _ memory _ requests _ not _ validate : { } advanced _ olap _ options : default : \" 0 \" advanced _ other : default : \" 0 \" value : \" 1 \" alert _ service _ update _ rule _ metrics : default : \" 0 \" api _ token _ ttl : default :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0 \" value : \" 1 \" alert _ service _ update _ rule _ metrics : default : \" 0 \" api _ token _ ttl : default : \" 24 \" arthur _ user _ id : default : \" 1000 \" audit _ log _ event _ bridge _ bus _ name : { } audit _ log _ event _ bridge _ bus _ region : { } audit _ log _ event _ bridge _ detail _ type : default : events. arthur. ai audit _ log _ event _ bridge _ source : default : arthur - audit - log audit _ log _ sink _ destination : default : none batch _ workflow _ parallelism", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ event _ bridge _ source : default : arthur - audit - log audit _ log _ sink _ destination : default : none batch _ workflow _ parallelism : default : \" 120 \" beta _ ui : default : \" 0 \" beta _ ui _ alternate _ site : default : \" 0 \" beta _ ui _ hostname : { } bootstrap _ job _ backoff _ limit : default : \" 100 \" bootstrap _ job _ ttl : default : \" 86400 \" cache _ cpu _ limits : { } cache _ cpu _ limits _ not _ validate : { } cache _ cpu _ requests : {", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" 86400 \" cache _ cpu _ limits : { } cache _ cpu _ limits _ not _ validate : { } cache _ cpu _ requests : { } cache _ cpu _ requests _ not _ validate : { } cache _ memory _ limits : { } cache _ memory _ limits _ not _ validate : { } cache _ memory _ requests : { } cache _ memory _ requests _ not _ validate : { } cache _ password : default : supersecret value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck +", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##cret value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok cache _ replicas : default : \" 0 \" cicd _ credentials : default : \" 0 \" cluster _ nodes : # only relevant for \" fixed \" cluster sizes. enter the number of nodes in the cluster. this number cannot be decreased from the current value unless it's greater", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "only relevant for \" fixed \" cluster sizes. enter the number of nodes in the cluster. this number cannot be decreased from the current value unless it's greater than ` 6 `. default : \" 1 \" value : \" 3 \" config _ job _ and _ workflow _ retention : default : \" 0 \" database _ admin _ password : default : supersecret value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok database _ hostname : # leave the default configuration to use the embedded database. if you would like to use an external postgres instance, provide the hostname here and follow this guide : https : / / docs. arthur. ai / platform - management / installation / externalize _ postgres. html. default : database - primary database _ password : default : super", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". arthur. ai / platform - management / installation / externalize _ postgres. html. default : database - primary database _ password : default : supersecret value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok database _ port : value : \" 5432 \" database _ ssl _ mode : # this option allows you to enable", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##0gj7rhsyhok database _ port : value : \" 5432 \" database _ ssl _ mode : # this option allows you to enable ssl communication between services and the postgres database. see https : / / www. postgresql. org / docs / 10 / libpq - ssl. html for full descriptions of each option. by default, the postgres database has ssl disabled. default : disable database _ username : default : arthurai default _ messaging _ partition _ count : default : \" 3 \" value : \" 1 \" disable _ ss", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##sable database _ username : default : arthurai default _ messaging _ partition _ count : default : \" 3 \" value : \" 1 \" disable _ ssl _ redirect _ on _ ingress : default : \" 0 \" email _ selection : default : none enable _ audit _ log : default : \" 0 \" enable _ olap _ backup : default :'\" 0 \"'enable _ olap _ backup _ user : default : \" 0 \" enable _ password _ rotation _ cache : default : \" 0 \" enable _ password _ rotation _ olap : default : \" 0 \" existing _ database _ primary _ pv", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "password _ rotation _ cache : default : \" 0 \" enable _ password _ rotation _ olap : default : \" 0 \" existing _ database _ primary _ pvc : { } existing _ or _ vm : default : existing _ cluster fixed _ or _ autoscale : # the ` fixed ` mode is recommended for clusters with a fixed number of nodes. the ` autoscale ` mode is used for clusters that can autoscale and automatically expand their node count. value : fixed full _ name _ override : default : arthurai global _ identity _ provider : default : none global _ model _ limit _ count : default", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value : fixed full _ name _ override : default : arthurai global _ identity _ provider : default : none global _ model _ limit _ count : default : \" 500 \" global _ model _ limits : default : \" 0 \" global _ workflow _ parallelism : default : \" 150 \" http _ proxy : { } # relevant if you are using explainability and your organization is behind a proxy server. if pip and / or conda need to route through the proxy server to pull down public packages this will set the environment variable http _ proxy to the supplied value. ex. http : / / sysproxy. my", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "server to pull down public packages this will set the environment variable http _ proxy to the supplied value. ex. http : / / sysproxy. my - company. com : port http _ proxy _ user : { } https _ proxy : { } https _ proxy _ user : { } ingestion _ service _ cpu _ limits : { } ingestion _ service _ cpu _ limits _ not _ validate : { } ingestion _ service _ cpu _ requests : { } ingestion _ service _ cpu _ requests _ not _ validate : { } ingestion _ service _ memory _ limits :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ requests : { } ingestion _ service _ cpu _ requests _ not _ validate : { } ingestion _ service _ memory _ limits : { } ingestion _ service _ memory _ limits _ not _ validate : { } ingestion _ service _ memory _ requests : { } ingestion _ service _ memory _ requests _ not _ validate : { } ingress _ ambassador _ enabled : default : \" false \" ingress _ class : default : nginx ingress _ hostname : value : arthur. mydomain. ai ingress _ namespace _ label _ key : value", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": default : nginx ingress _ hostname : value : arthur. mydomain. ai ingress _ namespace _ label _ key : value : name ingress _ namespace _ label _ value : value : ingress - system ingress _ nginx _ additional _ hostname : value : \" \" irsa _ annotations : { } irsa _ annotations _ user : default : eks. amazonaws. com / role - arn : arn : aws : iam : : 111122223333 : role / my - role k8 _ storageclass :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- arn : arn : aws : iam : : 111122223333 : role / my - role k8 _ storageclass : # provide kubernetes storageclass profile. use'gp2'for amazon eks,'default'if you're using embedded kubernetes provided by the installer value : default kafka _ ecosystem _ common _ replication _ calc : default : \" 1 \" max _ arthur _ replicas : default : \" 1 \" max _ messaging _ partition _ count : default : \" 3 \" max _ model _ server _ replicas : default : \"", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s : default : \" 1 \" max _ messaging _ partition _ count : default : \" 3 \" max _ model _ server _ replicas : default : \" 2 \" messaging _ cpu _ limit : default : \" 1 \" messaging _ heap : default : - xmx2g - xms1g messaging _ memory _ limit _ and _ request : default : 2560mi messaging _ rack _ aware _ enabled : default : \" 0 \" messaging _ rack _ label : default : topology. kubernetes. io / zone messaging _ replicas : default : \" 3 \" messaging _ sa _ create : default : \" 0 \"", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": topology. kubernetes. io / zone messaging _ replicas : default : \" 3 \" messaging _ sa _ create : default : \" 0 \" messaging _ sa _ fullnameoverride : { } messaging _ zookeeper _ timeout : default : \" 20000 \" meta _ replicas : default : \" 0 \" metric _ service _ update _ default _ metrics : default : \" 0 \" min _ arthur _ replicas : default : \" 1 \" model _ servers _ always _ on : # for use with what - if and on - demand explainability. see https : / / docs. arthur. ai", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ servers _ always _ on : # for use with what - if and on - demand explainability. see https : / / docs. arthur. ai / user - guide / explainability. html if set to \" true \", then on - demand and what - if explanations are available, but uses additional cluster resources, 1 cpu and 1 gb memory per model with explainability enabled. if set to \" false \", on - demand and what - if explanations are unavailable, but less cluster usage when there is no data being sent. regardless of the setting here, streaming explainability will be available if enabled. this only effects what", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", but less cluster usage when there is no data being sent. regardless of the setting here, streaming explainability will be available if enabled. this only effects what - if and on - demand explanations. default : \" true \" network _ policy _ enabled : default : \" 0 \" no _ proxy : { } # relevant if you are using explainability and your organization is behind a proxy server. if pip and / or conda need to route through the proxy server to pull down public packages this will set the environment variable no _ proxy to the supplied value. ex. localhost, 127. 0. 0. 1,. my", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "public packages this will set the environment variable no _ proxy to the supplied value. ex. localhost, 127. 0. 0. 1,. my - company. com no _ proxy _ user : { } number _ of _ olap _ backups _ to _ keep : default : \" 7 \" oidc _ identity _ provider _ config _ yaml : { } oidc _ identity _ provider _ config _ yaml _ user : { } olap _ backup _ s3 _ bucket : default : arthurai olap _ backup _ s3 _ bucket _ region : default : us", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { } olap _ backup _ s3 _ bucket : default : arthurai olap _ backup _ s3 _ bucket _ region : default : us - east - 1 olap _ backup _ s3 _ endpoint : default : s3. us - east - 1. amazonaws. com olap _ backup _ s3 _ path : default : olap _ backups olap _ backup _ service _ account : default : arthurai - arthurai olap _ cpu _ limits : { } olap _ cpu _ limits _ not _ validate : { } olap _ cpu _ requests : default :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "olap _ cpu _ limits : { } olap _ cpu _ limits _ not _ validate : { } olap _ cpu _ requests : default : 1000m olap _ cpu _ requests _ not _ validate : { } olap _ database _ operator _ password : # the olap database is installed along with a kubernetes operator to manage it. this operator needs credentials to access the database. we recommend overwriting the default password below. default : 5ugyldj2ulhrdegz5t value : ch / 0gntnbotnbqpxmzx4gupc", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "5ugyldj2ulhrdegz5t value : ch / 0gntnbotnbqpxmzx4gupcrnjqsnwtpot6fwgq9q4iy7chiqlefq3snnzgxynft4gsyince3khyimr7eebbtgbe5siuy / abpaysrsjexfo + 1vypbp176bp + zq = olap _ database _ user _ password : # password used internally in our application to query the olap database, currently only supports alpha", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##p + zq = olap _ database _ user _ password : # password used internally in our application to query the olap database, currently only supports alpha - numeric characters. default : eq3ibo8ugh5zqjkqwueeysrr value : ch / 0gntnbotnbqppngjggvcjsmpls / l8oro + uggq / rstcrycj2r / grxr8unr + u3plpij + ulmdxggfirtko6ptsclboqkoelxqdvr1jeqsthczi", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##r + u3plpij + ulmdxggfirtko6ptsclboqkoelxqdvr1jeqsthczi / btfovla = = olap _ memory _ limits : { } olap _ memory _ limits _ not _ validate : { } olap _ memory _ requests : default : 1gi olap _ memory _ requests _ not _ validate : { } olap _ node _ label _ key : { } olap _ node _ label _ value : { } olap _ replicas : default : \" 1 \" olap _ zookeeper _ cpu _", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ } olap _ node _ label _ value : { } olap _ replicas : default : \" 1 \" olap _ zookeeper _ cpu _ limits : { } olap _ zookeeper _ cpu _ limits _ not _ validate : { } olap _ zookeeper _ cpu _ requests : default : 500m olap _ zookeeper _ cpu _ requests _ not _ validate : { } olap _ zookeeper _ heap _ options : default : - xms4g - xmx4g olap _ zookeeper _ memory _ limits : { } olap _ zookeeper _ memory _ limits _", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- xms4g - xmx4g olap _ zookeeper _ memory _ limits : { } olap _ zookeeper _ memory _ limits _ not _ validate : { } olap _ zookeeper _ memory _ requests : default : 1gi olap _ zookeeper _ memory _ requests _ not _ validate : { } password _ rotation _ cron _ schedule : default : 0 0 1 * / 6 * pending _ batch _ workflows _ limit : default : \" 100 \" prometheus _ host : # leave the default configuration if you're using the embedded k8s. provide your prom", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "limit : default : \" 100 \" prometheus _ host : # leave the default configuration if you're using the embedded k8s. provide your prometheus hostname if you're running your own k8s. default : http : / / kube - prometheus - stack - prometheus. monitoring. svc. cluster. local prometheus _ labels : # if your prometheus installation requires labels to identify servicemonitors and prometheusrules, add them here. they should be in yaml format just as you would specify inside the \" metadata. labels \" block. do not", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##etheusrules, add them here. they should be in yaml format just as you would specify inside the \" metadata. labels \" block. do not indent. default : prometheus : monitor app : prometheus prometheus _ namespace : default : monitoring prometheus _ port : # leave the default configuration if you're using the embedded k8s. provide your prometheus hostname if you're running your own k8s. default : \" 9090 \" pypi _ registry _ conda : { } # this is set as a channel in the '. condarc", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". default : \" 9090 \" pypi _ registry _ conda : { } # this is set as a channel in the '. condarc'file. do not include'https : / /'prefix ( e. g. repository. arthur. ai / repository / conda - proxy / main ). pypi _ registry _ conda _ user : { } pypi _ registry _ index : { } # this maps to the'index key'in the'pip. conf'file. do not include'https : / /'prefix ( e. g repository. arthur. ai /", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "key'in the'pip. conf'file. do not include'https : / /'prefix ( e. g repository. arthur. ai / repository / pypi - virtual / pypi ). pypi _ registry _ index _ url : { } # this maps to the'index - url'key in the'pip. conf'file. do not include'https : / /'prefix ( e. g. repository. arthur. ai / repository / pypi - virtual / simple ). pypi _ registry _ index _ url _ user : { } p", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". arthur. ai / repository / pypi - virtual / simple ). pypi _ registry _ index _ url _ user : { } pypi _ registry _ index _ user : { } pypi _ registry _ password : default : bo4mxhdaevso / 029ytugz98wk7qpcxepa1p / uvqg4cy4uy1b3 + yn5q = = value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoa", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok pypi _ registry _ password _ user : value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5yt", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "+ ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok pypi _ registry _ username : { } pypi _ registry _ username _ user : { } raw _ anaconda _ config : { } raw _ anaconda _ config _ user : { } raw _ pypi _ config : { } raw _ pypi _ config _ user : { } rbac", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "user : { } raw _ pypi _ config : { } raw _ pypi _ config _ user : { } rbac _ privileges : # change to \" cluster _ scope \" to install crds too default : namespace _ scope run _ as _ root : default : \" 0 \" value : \" 0 \" s3 _ access _ key _ id : default : access _ key value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##htx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok s3 _ access _ key _ id _ user : default : access _ key value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ez", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok s3 _ bucket : default : arthurai s3 _ bucket _ user : default : arthurai s3 _ region : default : us - east - 1 s3 _ region _ user : default : us - east - 1 s3 _ secret _ access _ key : default : secret _ key value : vwc3tne9cpzobsxihtx9u / 34ky + ma6", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ access _ key : default : secret _ key value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok s3 _ secret _ access _ key _ user : default : secret _ key value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaea", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok s3 _ url : default : http : / / minio : 9000 s3 _ url _ user : default : http : / / minio : 9000 saml _ identity _ provider _ config _ yaml : { } saml _ identity _ provider _ con", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": / / minio : 9000 saml _ identity _ provider _ config _ yaml : { } saml _ identity _ provider _ config _ yaml _ user : { } secondary _ token _ validation _ key : value : aj3zici / ycntt3qr3watmndnezzqta8w9ijcohjnfmteio6lrcnukw = = ses _ region : { } ses _ role : { } show _ advanced _ arthur _ microservice _ options : default : \" 0 \" show _ advanced _ messaging :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ses _ role : { } show _ advanced _ arthur _ microservice _ options : default : \" 0 \" show _ advanced _ messaging : default : \" 0 \" value : \" 1 \" show _ hidden _ variables : default : \" 0 \" value : \" 0 \" show _ token _ signing _ and _ validation _ options : default : \" 0 \" signing _ cert : { } signing _ cert _ user : { } signing _ private _ key : { } signing _ private _ key _ user : { } single _ or _ ha : # the ` single ` configuration is a minimal deployment suitable for non", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { } signing _ private _ key _ user : { } single _ or _ ha : # the ` single ` configuration is a minimal deployment suitable for non - production environments. for production deployment, select ` ha `. value : single smtp _ from : { } # provide the email address to send alerts from ( e. g. [ email protected ] ) smtp _ host : { } # provide the address of the smtp server ( e. g. smtp. arthur. ai ) smtp _ password : value : vwc3tne9cpzobsxihtx9u / 34ky +", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "smtp. arthur. ai ) smtp _ password : value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok smtp _ port : { } smtp _ user : { } superadmin _ email : default : [ email protected ] superadmin _ firstname : default : super superadmin _ lastname : default : admin", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##admin _ email : default : [ email protected ] superadmin _ firstname : default : super superadmin _ lastname : default : admin superadmin _ password : default : supersecret value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok superadmin _ username : value : superadmin token _ signing", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##x3ezmi694tux0gj7rhsyhok superadmin _ username : value : superadmin token _ signing _ primary _ key : value : ysdfzjg5i83kmbj + whqmu / ejdq7ttthipadccrm + iqcdtofiul7dzztblfkb0e2u0 + uj74tuix28ognxpm + pkmklc1yx2uvj use _ external _ blob _ storage : # select \" yes \" if and only if you", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##pm + pkmklc1yx2uvj use _ external _ blob _ storage : # select \" yes \" if and only if you are supplying your own s3 compatible storage, otherwise select \" no \" to use the embedded blob storage. default : \" no \" use _ external _ postgres : default : \" no \" use _ raw _ python _ repository _ configs : # the pypi registry section is only relevant when using the explainability enrichment ( https : / / docs. arthur. ai / user - guide / enrichments. html # explainability ). # provide", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "using the explainability enrichment ( https : / / docs. arthur. ai / user - guide / enrichments. html # explainability ). # provide your private pypi registry if you have an airgapped enrivonment or your model requirements file includes packages only hosted in a private repository. # leaving this section blank will cause the public pypi to be used. if the public pypi is inaccessible from the cluster, the explainability feature will not work. default : \" no \" use _ smtp : default : \" 0 \" workflow _ ttl _ seconds : default : \" 3600 \"", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "not work. default : \" no \" use _ smtp : default : \" 0 \" workflow _ ttl _ seconds : default : \" 3600 \" workflow _ ttl _ seconds _ after _ success : default : \" 60 \" status : { } do note that these parameters are sorted alphabetically. unfortunately, this is how the'packager'software we use for our installer outputs the list of parameters. in reality, these parameters should be grouped based on their purpose. most of these parameters can be commented, unless you are advised by arthur support to configure them. it's also important to point", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "their purpose. most of these parameters can be commented, unless you are advised by arthur support to configure them. it's also important to point out that all'default'values will be ignored by the installer. for reference, this is the same configuration template, grouped by topic, and with only the most necessary parameters uncommented : yamlapiversion : kots. io / v1beta1 kind : configvalues metadata : creationtimestamp : null name : arthur spec : values : # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": creationtimestamp : null name : arthur spec : values : # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # install privileges. # change to \" namespace _ scope \" for restricted permissions. # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". # change to \" namespace _ scope \" for restricted permissions. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # values : namespace _ scope, cluster _ scope rbac _ privileges : value : cluster _ scope # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": namespace _ scope, cluster _ scope rbac _ privileges : value : cluster _ scope # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ingress # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # ingress # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ingress _ hostname : value : arthur. mlops. company. com ingress _ nginx _ additional _ hostname : value : \" a1788baaec5c4473aa", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". company. com ingress _ nginx _ additional _ hostname : value : \" a1788baaec5c4473aa4ec3bf4ef81bb5. xxxxxxxxxx. us - east - 1. elb. amazonaws. com \" ingress _ class : # values : \" nginx \", \" ambassador \" value : nginx ingress _ ambassador _ enabled : value : \" false \" ingress _ namespace _ label _ key : value : name ingress _ namespace _ label _ value : value : ingress - system di", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "false \" ingress _ namespace _ label _ key : value : name ingress _ namespace _ label _ value : value : ingress - system disable _ ssl _ redirect _ on _ ingress : value : \" 0 \" # ingress for new ui beta _ ui : value : \" 1 \" beta _ ui _ alternate _ site : value : \" 1 \" beta _ ui _ hostname : value : \" arthur. mlops. company. com \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "com \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # installation type # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # the ` single ` configuration is a minimal deployment suitable for non - production environments. # for production deployment, select ` ha `. single _ or _ ha : value : ha # the ` fixed ` mode is recommended for clusters with a fixed number of nodes. the ` autoscale ` mode is", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or _ ha : value : ha # the ` fixed ` mode is recommended for clusters with a fixed number of nodes. the ` autoscale ` mode is used for # clusters that can autoscale and automatically expand their node count. #'autoscale'will assume a minimum of 6 nodes, do not set for'autoscale'for clusters with < 6 nodes fixed _ or _ autoscale : value : fixed # only relevant for \" fixed \" cluster sizes. enter the number of nodes in the cluster. this number # cannot be decreased from the current value unless it's greater than ` 6 `. cluster", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sizes. enter the number of nodes in the cluster. this number # cannot be decreased from the current value unless it's greater than ` 6 `. cluster _ nodes : value : \" 3 \" # provide kubernetes storageclass profile. use'gp2'or'gp3'for amazon eks,'default'if you're using # embedded kubernetes provided by the installer k8 _ storageclass : value : gp3 # network policy network _ policy _ enabled : value : \" 0 \" # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "policy network _ policy _ enabled : value : \" 0 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # security and authentication # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # run _ as _ root : value : \" 0 \" arthur _ user _ id : value : \" 1000 \" # single sigh on # values : \" none \", \" oidc \", \" saml \" global _ identity _ provider : value", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" 1000 \" # single sigh on # values : \" none \", \" oidc \", \" saml \" global _ identity _ provider : value : none oidc _ identity _ provider _ config _ yaml : { } oidc _ identity _ provider _ config _ yaml _ user : { } saml _ identity _ provider _ config _ yaml : { } saml _ identity _ provider _ config _ yaml _ user : { } # iam integration - values : \" access _ keys \", \" irsa \", \" iam node roles \"", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ yaml _ user : { } # iam integration - values : \" access _ keys \", \" irsa \", \" iam node roles \" iam _ permission _ type : value : \" irsa \" irsa _ annotations : value : eks. amazonaws. com / role - arn : arn : aws : iam : : 123456789012 : role / arthur - eks - role irsa _ annotations _ user : value : eks. amazonaws. com / role - arn : arn : aws : iam :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ annotations _ user : value : eks. amazonaws. com / role - arn : arn : aws : iam : : 123456789012 : role / arthur - eks - role # ssl / token signature show _ token _ signing _ and _ validation _ options : value : \" 0 \" signing _ cert : { } signing _ cert _ user : { } signing _ private _ key : { } signing _ private _ key _ user : { } # token _ signing _ primary _ key : # value : ysdfzjg5i83", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "signing _ private _ key _ user : { } # token _ signing _ primary _ key : # value : ysdfzjg5i83kmbj + whqmu / ejdq7ttthipadccrm + iqcdtofiul7dzztblfkb0e2u0 + uj74tuix28ognxpm + pkmklc1yx2uvj # secondary _ token _ validation _ key : # value : aj3zici / ycntt3qr3watmndnezzqta8w9ijco", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "validation _ key : # value : aj3zici / ycntt3qr3watmndnezzqta8w9ijcohjnfmteio6lrcnukw = = # api _ token _ ttl : # value : \" 24 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # s3 integration # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # select \"", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # select \" yes \" if and only if you are supplying your own s3 compatible storage, # otherwise select \" no \" to use the embedded blob storage. use _ external _ blob _ storage : value : \" yes \" s3 _ access _ key _ id : { } s3 _ access _ key _ id _ user : { } s3 _ secret _ access _ key : { } s3 _ secret _ access _ key _ user : { } s3", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "id _ user : { } s3 _ secret _ access _ key : { } s3 _ secret _ access _ key _ user : { } s3 _ url : { } s3 _ url _ user : { } s3 _ bucket : value : arthur - s3 - eks s3 _ bucket _ user : value : arthur - s3 - eks s3 _ region : value : us - east - 1 s3 _ region _ user : value : us - east - 1 # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": value : us - east - 1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # superadmin configuration # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # superadmin _ email : value : [ email protected ] superadmin _ firstname : value : super superadmin _ lastname : value : admin superadmin _ password : value : password1234 superadmin _ username : value :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "super superadmin _ lastname : value : admin superadmin _ password : value : password1234 superadmin _ username : value : superadmin # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # smtp configuration # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # smtp configuration # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # values : \" ses \", \" smtp \", \" none \" email _ selection : value : none use _ smtp", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # values : \" ses \", \" smtp \", \" none \" email _ selection : value : none use _ smtp : value : \" 0 \" # provide the email address to send alerts from ( e. g. [ email protected ] ) smtp _ from : { } # provide the address of the smtp server ( e. g. smtp. arthur. ai ) smtp _ host : { } smtp _ password : value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck +", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "password : value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok smtp _ port : { } smtp _ user : { } # ses configuration ses _ region : { } ses _ role : { } # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ses _ role : { } # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # prometheus integration # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # leave the default configuration if you're using the embedded k8s. # provide your prometheus hostname if you're running your own k8s. prometheus _ host : value : http : / / kube - prometheus -", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "hostname if you're running your own k8s. prometheus _ host : value : http : / / kube - prometheus - stack - prometheus. monitoring. svc. cluster. local # if your prometheus installation requires labels to identify servicemonitors and prometheusrules, # add them here. they should be in yaml format just as you would specify inside the \" metadata. labels \" block. # do not indent. prometheus _ labels : value : prometheus : monitor app : prometheus prometheus _ namespace : value : monitoring #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "indent. prometheus _ labels : value : prometheus : monitor app : prometheus prometheus _ namespace : value : monitoring # leave the default configuration if you're using the embedded k8s. # provide your prometheus hostname if you're running your own k8s. prometheus _ port : value : \" 9090 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # private python registry # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # use _ raw _ python _ repository _ configs : # the pypi registry section is only relevant when using the explainability enrichment # ( https : / / docs. arthur. ai / user - guide / enrichments. html # explainability ). # provide your private pypi registry if you have an airgapped enrivonment or your model requirements file # includes", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "html # explainability ). # provide your private pypi registry if you have an airgapped enrivonment or your model requirements file # includes packages only hosted in a private repository. # leaving this section blank will cause the public pypi to be used. if the public pypi is inaccessible from the # cluster, the explainability feature will not work. value : \" no \" # this is set as a channel in the '. condarc'file. do not include'https : / /'prefix # ( e. g. repository. arthur. ai / repository / conda - proxy /", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' file. do not include'https : / /'prefix # ( e. g. repository. arthur. ai / repository / conda - proxy / main ). pypi _ registry _ conda : { } pypi _ registry _ conda _ user : { } # this maps to the'index key'in the'pip. conf'file. do not include'https : / /'prefix # ( e. g repository. arthur. ai / repository / pypi - virtual / pypi ). pypi _ registry _ index : { } # this maps to the '", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". ai / repository / pypi - virtual / pypi ). pypi _ registry _ index : { } # this maps to the'index - url'key in the'pip. conf'file. do not include'https : / /'prefix # ( e. g. repository. arthur. ai / repository / pypi - virtual / simple ). pypi _ registry _ index _ url : { } pypi _ registry _ index _ url _ user : { } pypi _ registry _ index _ user : { } pypi _ registry _ password", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i _ registry _ index _ url _ user : { } pypi _ registry _ index _ user : { } pypi _ registry _ password : value : password1234 pypi _ registry _ password _ user : value : password1234 pypi _ registry _ username : { } pypi _ registry _ username _ user : { } raw _ anaconda _ config : { } raw _ anaconda _ config _ user : { } raw _ pypi _ config : { } raw _ pypi _ config _ user : {", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##fig _ user : { } raw _ pypi _ config : { } raw _ pypi _ config _ user : { } # relevant if you are using explainability and your organization is behind a proxy server. # if pip and / or conda need to route through the proxy server to pull down public packages this will set # the environment variable no _ proxy to the supplied value. ex. localhost, 127. 0. 0. 1,. my - company. com no _ proxy : { } no _ proxy _ user : { } # relevant if you are using explainability and", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". 1,. my - company. com no _ proxy : { } no _ proxy _ user : { } # relevant if you are using explainability and your organization is behind a proxy server. # if pip and / or conda need to route through the proxy server to pull down public packages this will # set the environment variable http _ proxy to the supplied value. ex. http : / / sysproxy. my - company. com : port http _ proxy : { } http _ proxy _ user : { } https _ proxy : { } https _ proxy _ user : { } # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { } http _ proxy _ user : { } https _ proxy : { } https _ proxy _ user : { } # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # postgres integration # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # postgres integration # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # use _ external _ postgres : value : \" yes \" database _ admin _ password : value : password _ for _ rds _ admin _ user database _ host", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "postgres : value : \" yes \" database _ admin _ password : value : password _ for _ rds _ admin _ user database _ hostname : # leave the default configuration to use the embedded database. if you would like to use an external # postgres instance, provide the hostname here and follow this guide : # https : / / docs. arthur. ai / platform - management / installation / externalize _ postgres. html. value : arthur - db. cluster - xptowtzabcd. us - east - 1. rds. amazonaws. com database _ username", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value : arthur - db. cluster - xptowtzabcd. us - east - 1. rds. amazonaws. com database _ username : value : arthurai database _ password : value : password _ for _ rds _ arthurai _ user database _ port : value : \" 5432 \" database _ ssl _ mode : # this option allows you to enable ssl communication between services and the postgres database. # see https : / / www. postgresql. org / docs / 10 / libpq - ssl. html for full descriptions of each option. #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "www. postgresql. org / docs / 10 / libpq - ssl. html for full descriptions of each option. # by default, the postgres database has ssl disabled. value : disable existing _ database _ primary _ pvc : { } # meta _ replicas : # value : \" 0 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # olap db settings # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # olap password olap _ database _ operator _ password : # the olap database is installed along with a kubernetes operator to manage it. # this operator needs credentials to access the database. we recommend overwriting the default password below. value : gntnbotnbqpxmzx4gupcrnjqsnwtpot6fwgq9q4iy7chiq", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "gntnbotnbqpxmzx4gupcrnjqsnwtpot6fwgq9q4iy7chiqlefq3snnzgxynft4gsyince3khyimr7eebbtgbe5siuy olap _ database _ user _ password : # password used internally in our application to query the olap database, # currently only supports alpha - numeric characters. value : gntnbotnbqpxmzx4gupcrnjqsnwtpot6fwgq9q4iy7chiq", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "gntnbotnbqpxmzx4gupcrnjqsnwtpot6fwgq9q4iy7chiqlefq3snnzgxynft4gsyince3khyimr7eebbtgbe5siuy # enable _ password _ rotation _ olap : # value : \" 0 \" # password _ rotation _ cron _ schedule : # value : 0 0 1 * / 6 * # # olap backup # enable _ olap _ backup : # value :'\" 0 \"'# enable _ olap _ backup _ user :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "* # # olap backup # enable _ olap _ backup : # value :'\" 0 \"'# enable _ olap _ backup _ user : # value : \" 0 \" # number _ of _ olap _ backups _ to _ keep : # value : \" 7 \" # olap _ backup _ s3 _ bucket : # value : denisd - s3 - eks # olap _ backup _ s3 _ bucket _ region : # value : us - east - 1 # olap _ backup _ s3 _ endpoint : # value : s3. us - east - 1. amazonaw", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": us - east - 1 # olap _ backup _ s3 _ endpoint : # value : s3. us - east - 1. amazonaws. com # olap _ backup _ s3 _ path : # value : olap _ backups # olap _ backup _ service _ account : # value : arthurai - arthurai # # olap optimization # advanced _ olap _ options : # value : \" 0 \" # olap _ cpu _ limits : { } # olap _ cpu _ limits _ not _ validate : { } # olap _ cpu _ requests : # value : 1000", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ limits : { } # olap _ cpu _ limits _ not _ validate : { } # olap _ cpu _ requests : # value : 1000m # olap _ cpu _ requests _ not _ validate : { } # olap _ memory _ limits : { } # olap _ memory _ limits _ not _ validate : { } # olap _ memory _ requests : # value : 1gi # olap _ memory _ requests _ not _ validate : { } # olap _ node _ label _ key : { } # olap _ node _ label _ value : { } # olap", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "validate : { } # olap _ node _ label _ key : { } # olap _ node _ label _ value : { } # olap _ replicas : # value : \" 1 \" # olap _ zookeeper _ cpu _ limits : { } # olap _ zookeeper _ cpu _ limits _ not _ validate : { } # olap _ zookeeper _ cpu _ requests : # value : 500m # olap _ zookeeper _ cpu _ requests _ not _ validate : { } # olap _ zookeeper _ heap _ options : # value : - xms4g - x", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cpu _ requests _ not _ validate : { } # olap _ zookeeper _ heap _ options : # value : - xms4g - xmx4g # olap _ zookeeper _ memory _ limits : { } # olap _ zookeeper _ memory _ limits _ not _ validate : { } # olap _ zookeeper _ memory _ requests : # value : 1gi # olap _ zookeeper _ memory _ requests _ not _ validate : { } # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ate : { } # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # cache settings # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # cache _ password : value : vwc3tne9cpzobsxihtx9u / 34ky + ma6p8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ez", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8veb9bck + iqcaeaoargzgeff7ozogxo3m05qy5ytuix3ezmi694tux0gj7rhsyhok # cache _ replicas : # value : \" 0 \" # advanced _ cache _ options : # value : \" 0 \" # enable _ password _ rotation _ cache : # value : \" 0 \" # # cache optimization # cache _ cpu _ limits : { } # cache _ cpu _ limits _ not _ validate : { } # cache _ cpu _ requests : { } # cache _ cpu _ requests", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { } # cache _ cpu _ limits _ not _ validate : { } # cache _ cpu _ requests : { } # cache _ cpu _ requests _ not _ validate : { } # cache _ memory _ limits : { } # cache _ memory _ limits _ not _ validate : { } # cache _ memory _ requests : { } # cache _ memory _ requests _ not _ validate : { } # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # kafka optimization settings # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # default _ messaging _ partition _ count : # value : \" 1 \" # max _ messaging _ partition _ count : # value : \" 3 \" # messaging _ replicas : # value : \" 3 \" # kafka _ ecosystem _ common _ replication _ calc : # value : \" 1 \" # messaging _ cpu _ limit : #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" 3 \" # kafka _ ecosystem _ common _ replication _ calc : # value : \" 1 \" # messaging _ cpu _ limit : # value : \" 1 \" # advanced _ messaging _ connect _ cpu _ limits : # value : \" 2 \" # advanced _ messaging _ connect _ cpu _ limits _ not _ validate : { } # advanced _ messaging _ connect _ cpu _ requests : # value : \" 1 \" # advanced _ messaging _ connect _ cpu _ requests _ not _ validate : { } # messaging _ heap : # value : - xmx2g - xms1g # messaging _", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cpu _ requests _ not _ validate : { } # messaging _ heap : # value : - xmx2g - xms1g # messaging _ memory _ limit _ and _ request : # value : 2560mi # advanced _ messaging _ connect _ heap _ options : # value : - xms1g - xmx3g # advanced _ messaging _ connect _ memory _ limits : # value : 4gi # advanced _ messaging _ connect _ memory _ limits _ not _ validate : { } # advanced _ messaging _ connect _ memory _ requests : # value : 2gi # advanced _ messaging _ connect _ memory", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ not _ validate : { } # advanced _ messaging _ connect _ memory _ requests : # value : 2gi # advanced _ messaging _ connect _ memory _ requests _ not _ validate : { } # messaging _ rack _ aware _ enabled : # value : \" 0 \" # messaging _ rack _ label : # value : topology. kubernetes. io / zone # messaging _ sa _ create : # value : \" 0 \" # messaging _ sa _ fullnameoverride : { } # messaging _ zookeeper _ timeout : # value : \" 20000 \" # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##nameoverride : { } # messaging _ zookeeper _ timeout : # value : \" 20000 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # audit log settings # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # audit log settings # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # enable _ audit _ log : # value : \" 0 \" # audit _ log _ event _ bridge _ bus _ name : { } # audit _ log _ event _ bridge _ bus _ region", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value : \" 0 \" # audit _ log _ event _ bridge _ bus _ name : { } # audit _ log _ event _ bridge _ bus _ region : { } # audit _ log _ event _ bridge _ detail _ type : # value : events. arthur. ai # audit _ log _ event _ bridge _ source : # value : arthur - audit - log # audit _ log _ sink _ destination : # value : none # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # admin console settings # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # advanced _ other : # value : \" 1 \" # show _ advanced _ arthur _ microservice _ options : # value : \" 0 \" # show _ advanced _ messaging : # value : \" 1 \" # show _ hidden _ variables : # value : \" 0 \" # config _ job _ and _ workflow _", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": # value : \" 1 \" # show _ hidden _ variables : # value : \" 0 \" # config _ job _ and _ workflow _ retention : # value : \" 0 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # backend performance optimization settings # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # # # # # # # # # # # # backend performance optimization settings # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # limits # global _ model _ limit _ count : # value : \" 500 \" # global _", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # # limits # global _ model _ limit _ count : # value : \" 500 \" # global _ model _ limits : # value : \" 0 \" # global _ workflow _ parallelism : # value : \" 150 \" # # job limits # batch _ workflow _ parallelism : # value : \" 120 \" # bootstrap _ job _ backoff _ limit : # value : \" 100 \" # bootstrap _ job _ ttl : # value : \" 86400 \" # pending _ batch _ workflows _ limit : # value : \" 100 \"", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##trap _ job _ ttl : # value : \" 86400 \" # pending _ batch _ workflows _ limit : # value : \" 100 \" # workflow _ ttl _ seconds : # value : \" 3600 \" # workflow _ ttl _ seconds _ after _ success : # value : \" 60 \" # # ingestion service optimizations # ingestion _ service _ cpu _ limits : { } # ingestion _ service _ cpu _ limits _ not _ validate : { } # ingestion _ service _ cpu _ requests : { } # ingestion _ service _ cpu _", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ limits _ not _ validate : { } # ingestion _ service _ cpu _ requests : { } # ingestion _ service _ cpu _ requests _ not _ validate : { } # ingestion _ service _ memory _ limits : { } # ingestion _ service _ memory _ limits _ not _ validate : { } # ingestion _ service _ memory _ requests : { } # ingestion _ service _ memory _ requests _ not _ validate : { } # # explainability optimizations # model _ servers _ always _ on : # # for use with what - if and on", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "not _ validate : { } # # explainability optimizations # model _ servers _ always _ on : # # for use with what - if and on - demand explainability. see https : / / docs. arthur. ai / user - guide / explainability. html # # if set to \" true \", then on - demand and what - if explanations are available, but uses additional cluster # # resources, 1 cpu and 1 gb memory per model with explainability enabled. if set to \" false \", on - demand # # and what - if explanations are unavailable, but less cluster usage when there is no data", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ability enabled. if set to \" false \", on - demand # # and what - if explanations are unavailable, but less cluster usage when there is no data being sent. # # regardless of the setting here, streaming explainability will be available if enabled. # # this only effects what - if and on - demand explanations. # value : \" true \" # max _ model _ server _ replicas : # value : \" 2 \" # metric _ service _ update _ default _ metrics : # value : \" 0 \" # alert _ service _ update _ rule _ metrics : # value : \" 0 \" # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ metrics : # value : \" 0 \" # alert _ service _ update _ rule _ metrics : # value : \" 0 \" # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # internal arthur use # # # # # # # # # # # # # # # # #", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # internal arthur use # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # cicd _ credentials : # value : \" 0 \" # existing _ or _ vm : # value : existing _ cluster # full _ name _ override :", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##cd _ credentials : # value : \" 0 \" # existing _ or _ vm : # value : existing _ cluster # full _ name _ override : # value : arthurai # min _ arthur _ replicas : # value : \" 1 \" # max _ arthur _ replicas : # value : \" 1 \" status : { } ps : this template assumes integration with s3 and rds database, using irsa ( iam roles for service accounts ) configured. most of these settings can be modified in the admin console ui. the config screen will include fields for all the settings in the template : some of", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "most of these settings can be modified in the admin console ui. the config screen will include fields for all the settings in the template : some of the settings in this template can cause an installation to fail, if they are not correctly set. some of the critical parameters are : yaml k8 _ storageclass : value : gp3 this parameter defines the storage class that will be used to create the persistent volumes for the cluster. having the wrong storage class defined here will cause the installer to fail to provision storage, which will compromise the installation. make sure to have the correct value set before running the installer. ya", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will cause the installer to fail to provision storage, which will compromise the installation. make sure to have the correct value set before running the installer. yaml ingress _ hostname : value : arthur. mlops. company. com ingress _ nginx _ additional _ hostname : value : \" a1788baaec5c4473aa4ec3bf4ef81bb5. xxxxxxxxxx. us - east - 1. elb. amazonaws. com \" these parameters will configure which ingress addresses arthur will accept. by definition, arthur", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "east - 1. elb. amazonaws. com \" these parameters will configure which ingress addresses arthur will accept. by definition, arthur will reject requests sent to addresses that are not in this list, even if they are correct ( for instance, accessing arthur through https : / / localhost does not work ). if these hostname parameters do not match existing load balancer addresses, arthur will be inaccessible, even once the instance is successfully installed. this can be reconfigured after installation, though it is recommended to ensure the proper value at installation time. updated 3 months ago source : https", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". this can be reconfigured after installation, though it is recommended to ensure the proper value at installation time. updated 3 months ago source : https : / / docs. arthur. ai / docs / config - template", "metadata": {"source": "https://docs.arthur.ai/docs/config-template", "row": 64, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 65 text : restoring the arthur platform jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by restoring the arthur platformsuggest editsthis document details restoring various arthur platform components from existing backups. restore rds postgres use the aws rds documentation to restore the database from an existing rds snapshot. please ensure that you correctly match the following configuration of the db from which the snapshot was taken : the connection port the vpc and security group ids db subnet group db instance type any other configuration which might", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "db from which the snapshot was taken : the connection port the vpc and security group ids db subnet group db instance type any other configuration which might be overridden this operation might take a while, and the db must show as available before proceeding to install the platform. install the arthur platform [UNK] database is readyonly proceed to installing the arthur platform after the restored database shows as \" available \" in the rds console. install the arthur platform either using the airgap kubernetes cluster ( k8s ) install ` or online kubernetes cluster ( k8s ) install. although most configurations for", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gap kubernetes cluster ( k8s ) install ` or online kubernetes cluster ( k8s ) install. although most configurations for the arthur platform should remain the same, the following two configurations might need to be updated : the \" meta database \" section of the admin console should point to the newly restored db instance. [UNK] restore cluster is pointing to the right locationit is very critical to update the configuration to point to the newly restored db instance. failure to complete this step will cause data corruption. update the ingress url in the \" network \" section of the admin console. wait for the platform", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "failure to complete this step will cause data corruption. update the ingress url in the \" network \" section of the admin console. wait for the platform to come back online before proceeding to the next steps. all deployments and statefulsets should be completely stood up ( eg : all pods should be ready and \" running \" ) and all jobs should be \" completed \". $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get pods - n $ arthur _ namespace name ready status restarts age argo - workflows - server - 75fc4d9d", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ctl get pods - n $ arthur _ namespace name ready status restarts age argo - workflows - server - 75fc4d9d55 - wfsqc 1 / 1 running 0 12h argo - workflows - workflow - controller - 7b95b66b94 - 66hrs 1 / 1 running 0 119m arthurai - alert - service - 858784dd7f - 4kgq2 1 / 1 running 0 4h58m arthurai - api - service - 7fc58f4958 - trcvg 1 / 1", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##q2 1 / 1 running 0 4h58m arthurai - api - service - 7fc58f4958 - trcvg 1 / 1 running 0 4h58m arthurai - custom - hpa - 646bb978dd - t9b68 1 / 1 running 0 12h arthurai - dataset - service - 86c8dd54cc - bwwtr 1 / 1 running 0 4h58m arthurai - frontend - 78cc85fbc5 - ffx79 1 / 1 running 0 12h arthurai - frontend - beta - 5cb", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- frontend - 78cc85fbc5 - ffx79 1 / 1 running 0 12h arthurai - frontend - beta - 5cb8756f68 - 8hljq 1 / 1 running 0 12h arthurai - frontend - classic - 5ff79bd579 - rhqv8 1 / 1 running 0 12h arthurai - ingestion - service - 5f7896bf5c - jxwvk 1 / 1 running 0 12h arthurai - ingestion - service - 5f7896bf5c", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##c - jxwvk 1 / 1 running 0 12h arthurai - ingestion - service - 5f7896bf5c - vzdn2 1 / 1 running 0 4h58m arthurai - kafka - connect - monitor - 54cfcc8f7d - dcgr6 1 / 1 running 2 12h arthurai - metric - service - 78f85cb548 - s65dj 1 / 1 running 0 12h arthurai - query - service - 64d7c9f846 - h2ms9 1 / 1 running 0 2d12", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "1 running 0 12h arthurai - query - service - 64d7c9f846 - h2ms9 1 / 1 running 0 2d12h arthurai - schema - service - 69b8c484bd - thhkr 1 / 1 running 0 4h58m cache - master - 0 1 / 1 running 0 12h cache - slave - 0 1 / 1 running 0 17h cache - slave - 1 1 / 1 running 0 119m chi - olap - installation - arthur - 0 - 0 - 0 2 / 2 running 0 119m chi - olap - installation - arthur -", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "119m chi - olap - installation - arthur - 0 - 0 - 0 2 / 2 running 0 119m chi - olap - installation - arthur - 0 - 1 - 0 2 / 2 running 0 12h chi - olap - installation - arthur - 0 - 2 - 0 2 / 2 running 0 17h database - primary - 0 1 / 1 running 0 12h database - read - 0 1 / 1 running 0 17h database - read - 1 1 / 1 running 0 119m kafka - exporter - 744dbd8476 - wwztw 1 / 1 running 0 45h", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "1 running 0 119m kafka - exporter - 744dbd8476 - wwztw 1 / 1 running 0 45h kotsadm - 5db494c84f - b9vtq 1 / 1 running 0 119m kotsadm - minio - 0 1 / 1 running 0 17h kotsadm - rqlite - 0 1 / 1 running 0 12h messaging - 0 2 / 2 running 0 17h messaging - 1 2 / 2 running 2 11h messaging - 2 2 / 2 running 0 119m messaging - connect - 5db8c6fbc", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##h messaging - 1 2 / 2 running 2 11h messaging - 2 2 / 2 running 0 119m messaging - connect - 5db8c6fbcf - jk7st 2 / 2 running 0 45h messaging - connect - 5db8c6fbcf - jstq7 2 / 2 running 0 119m messaging - cp - zookeeper - 0 3 / 3 running 0 17h messaging - cp - zookeeper - 1 3 / 3 running 0 2d11h messaging - cp - zookeeper - 2 3 / 3 running 0 119m messaging - schema - registry - 7c646d8", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0 2d11h messaging - cp - zookeeper - 2 3 / 3 running 0 119m messaging - schema - registry - 7c646d8c7 - mxshj 2 / 2 running 0 2d12h messaging - schema - registry - 7c646d8c7 - q77bh 2 / 2 running 0 119m messaging - schema - registry - 7c646d8c7 - z5s4v 2 / 2 running 0 45h olap - installation - zookeeper - 0 3 / 3 running 0 2d12h olap - installation - zookeeper - 1 3", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "2 running 0 45h olap - installation - zookeeper - 0 3 / 3 running 0 2d12h olap - installation - zookeeper - 1 3 / 3 running 0 17h olap - installation - zookeeper - 2 3 / 3 running 0 119m olap - operator - 7999d4fdb8 - kkprt 2 / 2 running 0 119m $ kubectl get jobs - n $ arthur _ namespace name completions duration age arthurai - additional - images - bootstrap - xmmes 0 / 1 16s 16s arthurai - api - key - bootstra", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "duration age arthurai - additional - images - bootstrap - xmmes 0 / 1 16s 16s arthurai - api - key - bootstrap - cfrhy 0 / 1 16s 16s arthurai - database - migration - hhlpc 0 / 1 16s 16s arthurai - default - entities - bootstrap - lddho 0 / 1 15s 16s arthurai - meter - events - connector - deploy - xvxgz 0 / 1 15s 15s arthurai - model - health - score - connector - deploy - qhvg1 0 / 1 15s 15", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##z 0 / 1 15s 15s arthurai - model - health - score - connector - deploy - qhvg1 0 / 1 15s 15s arthurai - query - service - migration - 17ehe 0 / 1 15s 15s clickhouse - v23 - migration - job - xynkq 0 / 1 15s 15s messaging - config - 031e7f1c 0 / 1 15s 15s restore clickhouse data the arthur platform ships with a kubernetes cronjob that executes a clickhouse restore that is scheduled never to run. to restore click", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data the arthur platform ships with a kubernetes cronjob that executes a clickhouse restore that is scheduled never to run. to restore clickhouse data, execute the following commands : get the name of the clickhouse - backup that coincides with the kafka / enrichments / workflow backups that you are restoring using the clickhouse pod itself - shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl exec chi - olap - installation - arthur - 0 - 0 - 0 - n $ arthur _ namespace - c backup - - clickhouse - backup", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "exec chi - olap - installation - arthur - 0 - 0 - 0 - n $ arthur _ namespace - c backup - - clickhouse - backup list < < < output - truncated - for - brevity > > > 2023 / 05 / 12 15 : 12 : 16. 199434 info select * from system. macros logger = clickhouse chi - olap - installation - arthur - 0 - 0 - arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 10. 33mib 11 / 05 / 2023 00 : 00 : 14 remote tar, regular chi", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##3 - 05 - 11 - 00 - 00 - 07 10. 33mib 11 / 05 / 2023 00 : 00 : 14 remote tar, regular chi - olap - installation - arthur - 0 - 1 - arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 10. 33mib 11 / 05 / 2023 00 : 00 : 15 remote tar, regular chi - olap - installation - arthur - 0 - 2 - arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 10. 33mib 11 / 05 / 2023 00 :", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 10. 33mib 11 / 05 / 2023 00 : 00 : 15 remote tar, regular chi - olap - installation - arthur - 0 - 0 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 10. 33mib 12 / 05 / 2023 00 : 00 : 14 remote tar, regular chi - olap - installation - arthur - 0 - 1 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 10. 33mib", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "installation - arthur - 0 - 1 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 10. 33mib 12 / 05 / 2023 00 : 00 : 14 remote tar, regular chi - olap - installation - arthur - 0 - 2 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 10. 33mib 12 / 05 / 2023 00 : 00 : 15 remote tar, regular 2023 / 05 / 12 15 : 12 : 18. 317324 info clickhouse connection closed logger = clickhouse using aws s", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "tar, regular 2023 / 05 / 12 15 : 12 : 18. 317324 info clickhouse connection closed logger = clickhouse using aws s3 cli - shell $ aws s3 ls s3 : / / < s3 - bucket - name > / < backup - path > / - - profile aws _ profile pre chi - olap - installation - arthur - 0 - 0 - arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 / pre chi - olap - installation - arthur - 0 - 0 - arthur - clickhouse - backup - 2023", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- 11 - 00 - 00 - 07 / pre chi - olap - installation - arthur - 0 - 0 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 / pre chi - olap - installation - arthur - 0 - 1 - arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 / pre chi - olap - installation - arthur - 0 - 1 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 / pre chi - olap - installation - arthur - 0 - 2 - arthur - click", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backup - 2023 - 05 - 12 - 00 - 00 - 06 / pre chi - olap - installation - arthur - 0 - 2 - arthur - clickhouse - backup - 2023 - 05 - 11 - 00 - 00 - 07 / pre chi - olap - installation - arthur - 0 - 2 - arthur - clickhouse - backup - 2023 - 05 - 12 - 00 - 00 - 06 / extract the arthur _ backup _ name from the backups. the backups are named in the clickhouse _ node _ name - arthur _ backup _ name format. for example, chi - olap - installation - arthur -", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "backups are named in the clickhouse _ node _ name - arthur _ backup _ name format. for example, chi - olap - installation - arthur - 0 - 0 - arthur - clickhouse - backup - 2022 - 05 - 12 - 00 - 00 - 06 can be parsed into : clickhouse node name : chi - olap - installation - arthur - 0 - 0 arthur's backup name : arthur - clickhouse - backup - 2022 - 05 - 12 - 00 - 00 - 06 create the restore job, and configure it to use the arthur backup name from above shell $ arthur _ namespace", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "12 - 00 - 00 - 06 create the restore job, and configure it to use the arthur backup name from above shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl create job - - from = cronjob / clickhouse - restore - cronjob - n $ arthur _ namespace - o yaml clickhouse - restore - - dry - run = client - - save - config > clickhouse - restore. yaml $ backup _ name = \" arthur - clickhouse - backup - 2022 - 05 - 12 - 00 - 00 - 06", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "clickhouse - restore. yaml $ backup _ name = \" arthur - clickhouse - backup - 2022 - 05 - 12 - 00 - 00 - 06 \" # value extracted in above step $ sed - i - e \" s / insert - backup - name - here / $ backup _ name / \" clickhouse - restore. yaml $ cat clickhouse - restore. yaml grep - c2 \" name : backup _ name \" # verify the replacement is correct $ kubectl apply - f clickhouse - restore. yaml - n $ arthur _ namespace job. batch / clickhouse - restore created restore", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "$ kubectl apply - f clickhouse - restore. yaml - n $ arthur _ namespace job. batch / clickhouse - restore created restore messaging infrastructure the arthur platform restores kafka deployment state, persistentvolumes, and persistentvolumeclaims using velero. to restore the messaging infrastructure ( kafka and zookeeper ), run the following commands : delete the statefulsets related to messaging infrastructure that was created while installing the platform shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get sts - n $ arthur _ namespace", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "installing the platform shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get sts - n $ arthur _ namespace grep - i messaging # there should only be two stss returned $ kubectl delete sts messaging - n $ arthur _ namespace $ kubectl delete sts messaging - cp - zookeeper - n $ arthur _ namespace delete the persistentvolumeclaims related to messaging infrastructure that was created while installing the platform shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get pvc - n", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that was created while installing the platform shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get pvc - n $ arthur _ namespace grep - i messaging # the number of pvcs returned depends on your configuration $ kubectl get pvc - n $ arthur _ namespace grep - i messaging awk'{ print $ 1 }'xargs kubectl delete pvc - n $ arthur _ namespace confirm the persistentvolumes have automatically been deleted ( due to a'delete'retention policy ) shell $ arthur _ namespace =", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur _ namespace confirm the persistentvolumes have automatically been deleted ( due to a'delete'retention policy ) shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get pv - n $ arthur _ namespace grep - i messaging wc - l # should return 0 if the persistentvolumes still do not get deleted automatically after a few minutes, delete them manually shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl get pv - n $ arthur _ namespace grep - i messaging # the number of pvs returned", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "your arthur namespace here \" $ kubectl get pv - n $ arthur _ namespace grep - i messaging # the number of pvs returned depends on your configuration $ kubectl get pv - n $ arthur _ namespace grep - i messaging awk'{ print $ 1 }'xargs kubectl delete pv - n $ arthur _ namespace [UNK] sure restore steps are completedo not proceed until the above deletion commands have fully completed. check with the kubectl get < resource > commands. get the relevant velero backup by using the velero cl", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "commands have fully completed. check with the kubectl get < resource > commands. get the relevant velero backup by using the velero cli : shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup get - n $ velero _ namespace grep messaging name status errors warnings created expires storage location selector arthur - backup - 2023 - 05 - 11t15. 22. 37 - 04. 00 - messaging completed 0 0 2023 - 05 - 11 15 : 22 : 48 - 0400 edt 27d docs -", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". 37 - 04. 00 - messaging completed 0 0 2023 - 05 - 11 15 : 22 : 48 - 0400 edt 27d docs - demo - storage - location app in ( cp - kafka, cp - zookeeper ) $ velero restore create \\ - - from - backup \" arthur - backup - 2023 - 05 - 11t15. 22. 37 - 04. 00 - messaging \" \\ - - namespace $ velero _ namespace velero will update the pod specs, point to the pvs using the ebs volume snapshots, and restore the kuber", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ namespace velero will update the pod specs, point to the pvs using the ebs volume snapshots, and restore the kubernetes resources associated with kafka. wait for the messaging infrastructure and arthur platform to become \" ready \" shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl kots get apps - n $ arthur _ namespace slug status version arthur ready 3. 4. 0 restore enrichments the arthur platform uses velero to restore the enrichments infrastructure and workflows, which will require running 2 separate commands. restore enrichments infrastructure", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "enrichments the arthur platform uses velero to restore the enrichments infrastructure and workflows, which will require running 2 separate commands. restore enrichments infrastructure to restore the enrichments infrastructure, run the following commands : shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup get - n $ velero _ namespace grep enrichments name status errors warnings created expires storage location selector arthur - backup - 2023 - 05 - 11t15. 25. 24 - 04. 00 - enrichments completed 0 0 2023 - 05 - 11 15 : 25", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- 2023 - 05 - 11t15. 25. 24 - 04. 00 - enrichments completed 0 0 2023 - 05 - 11 15 : 25 : 33 - 0400 edt 27d default component in ( kafka - mover - init - connector, model _ server ) $ velero restore create \\ - - from - backup \" arthur - backup - 2023 - 05 - 11t15. 25. 24 - 04. 00 - enrichments \" \\ - - namespace $ velero _ namespace restore enrichments workflows restoring the workflows is a 2 - step process :", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s \" \\ - - namespace $ velero _ namespace restore enrichments workflows restoring the workflows is a 2 - step process : restore the workflows from the velero backup shell $ velero _ namespace = \" put your velero namespace here \" $ velero backup get - n $ velero _ namespace grep workflows name status errors warnings created expires storage location selector arthur - backup - 2022 - 09 - 23t11. 23. 25 - 04. 00 - workflows completed 0 0 2022 - 09 - 23 11 :", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- 2022 - 09 - 23t11. 23. 25 - 04. 00 - workflows completed 0 0 2022 - 09 - 23 11 : 24 : 35 - 0400 edt 27d default < none > $ velero restore create \\ - - from - backup \" arthur - backup - 2022 - 09 - 23t11. 23. 25 - 04. 00 - workflows \" \\ - - namespace $ velero _ namespace restore batch workflows which are recoverable using an arthur admin endpoint in one terminal window, port - forward to the dataset service : shell $", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "restore batch workflows which are recoverable using an arthur admin endpoint in one terminal window, port - forward to the dataset service : shell $ arthur _ namespace = \" put your arthur namespace here \" $ kubectl port - forward - n $ arthur _ namespace svc / arthurai - dataset - service 7899 : 80 in another terminal window, run the following commands : shell $ curl - k - xpost http : / / localhost : 7899 / api / v1 / workflows / batch / recover { \" message \" : \" success \" } smoke tests", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ localhost : 7899 / api / v1 / workflows / batch / recover { \" message \" : \" success \" } smoke tests and validation the restore process is now complete. all data should be restored and consistent from when the backup was taken. any data sent during or after the backup will need to be re - sent. perform any validation / smoke tests to ensure that the platform is operating. updated 3 months ago table of contents restore rds postgres install the arthur platform restore clickhouse data restore messaging infrastructure restore enrichments restore enrichments infrastructure restore enrichments workflows smoke tests and validation source :", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gres install the arthur platform restore clickhouse data restore messaging infrastructure restore enrichments restore enrichments infrastructure restore enrichments workflows smoke tests and validation source : https : / / docs. arthur. ai / docs / restoring - the - arthur - platform", "metadata": {"source": "https://docs.arthur.ai/docs/restoring-the-arthur-platform", "row": 65, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 66 text : ranked list outputs onboarding jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by ranked list outputs onboardingsuggest editsthis page walks through the basics of setting up a recommender system model ( ranked list output ) and onboarding it to arthur scope to monitor performance. the inputs to a recommender system model could be time series inputs or more traditional tabular inputs. getting started the first step is to import functions from the arthurai package and establish a connection with arthur scope. python # arthur imports from arthurai", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ular inputs. getting started the first step is to import functions from the arthurai package and establish a connection with arthur scope. python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \" ) registering a recommender system model each recommender system model is created with a name and with output _ type = outputtype. rankedlist. here, we register a recommender model : pythonarthur _", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "system model is created with a name and with output _ type = outputtype. rankedlist. here, we register a recommender model : pythonarthur _ model = arthur. model ( name = \" recsysquickstart \", input _ type = inputtype. tabular, model _ type = outputtype. rankedlist ) formatting reference / inference data column names can contain only alphanumeric and underscore characters. ranked list data can be uploaded to arthur either in a dataframe or a json file. typically, a json file is a more natural formatting for ranked list data. for a", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to arthur either in a dataframe or a json file. typically, a json file is a more natural formatting for ranked list data. for a recommender system model recommending a loan policy, the reference data might look like this : json { \" reference _ data \" : { \" id \" : \" 6euqxgjai11qr0genggvgh \", \" account _ id \" : \" 8klqsgjil78qr4gljkklsy \", \" recommendations \" : [ { \" label \" : \" loan policy 3 \", \" item _ id", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##78qr4gljkklsy \", \" recommendations \" : [ { \" label \" : \" loan policy 3 \", \" item _ id \" : \" 64nwp2mbjxd7ohg7xmxcia \", \" score \" : 90 }, { \" label \" : \" loan policy 1 \", \" item _ id \" : \" 0cozwivqahhgaryrtjd1v5 \", \" score \" : 83 }, ], \" gt \" : [ \" 0cozwivqahhgaryrtjd1v5 \", /", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "score \" : 83 }, ], \" gt \" : [ \" 0cozwivqahhgaryrtjd1v5 \", / / ids of relevant recommendations \" 72eaubslq047r3j9dxmcf4 \", ] },... / / more inferences here } data requirements the list of ranked list items should be sorted in rank order, such that the highest ranked item is first. each ranked list output model in arthur can have max 1000 total unique recommended items in its reference dataset. each ranked list output model can have max 100 recommendations per inference /", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list output model in arthur can have max 1000 total unique recommended items in its reference dataset. each ranked list output model can have max 100 recommendations per inference / ground truth. if the label or score metadata field in a ranked list item is specified for one inference, it must be specified for all of them. reviewing the model schema before you register your model with arthur by calling arthur _ model. save ( ), you can call arthur _ model. review ( ) on the model schema to check that your data was parsed correctly in your call to arthur _ model. build ( ). for a recommender system model, the", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model schema to check that your data was parsed correctly in your call to arthur _ model. build ( ). for a recommender system model, the model schema should look like this : python name stage value _ type categorical is _ unique 0 ranked _ list _ pred _ attr predicted _ value ranked _ list false false 1 ground _ truth ground _ truth array ( string ) false false... 2 non _ input _ 1 non _ input _ data float false false... finishing onboarding once you have finished formatting your reference data and your model schema looks correct using arthur _ model. review (", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "float false false... finishing onboarding once you have finished formatting your reference data and your model schema looks correct using arthur _ model. review ( ), you are finished registering your model and its attributes - so you are ready to complete onboarding your model. see this guide for further details on how to save your model, send inferences, and get performance results from arthur. these steps are the same for recommender system models as for models of any inputtype and outputtype. updated about 2 months ago table of contents getting started registering a recommender system model formatting reference / inference data data requirements reviewing the model sc", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and outputtype. updated about 2 months ago table of contents getting started registering a recommender system model formatting reference / inference data data requirements reviewing the model schema finishing onboarding source : https : / / docs. arthur. ai / docs / ranked - list - outputs - onboarding", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-outputs-onboarding", "row": 66, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 67 text : exporting platform configurations jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by exporting platform configurationssuggest editsafter a successful installation of the arthur platform, you can download the configuration that was used as a. yaml file by following the below steps : navigate to the admin console and login. click on the \" view files \" tab. go to upstream \u2192 userdata and click on config. yaml file. copy the contents of that file and save it for future use ( or", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "go to upstream \u2192 userdata and click on config. yaml file. copy the contents of that file and save it for future use ( or check - in to source control ). updated 3 months ago source : https : / / docs. arthur. ai / docs / exporting - platform - configurations", "metadata": {"source": "https://docs.arthur.ai/docs/exporting-platform-configurations", "row": 67, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 68 text : anomaly detection jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by anomaly detectionmonitor and alert on incoming changes to your data distribution based on complex interactions between featuressuggest editsgo beyond single distribution analysis and look at complex interactions between features that may cause drift ( multivariate ). both in low, but especially in high dimensional spaces where there is often data sparsity, anomaly detection can help capture changes in complex interactions between features. anomaly detection in practice anomaly detection as multivariate drift model - based monitoring", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data sparsity, anomaly detection can help capture changes in complex interactions between features. anomaly detection in practice anomaly detection as multivariate drift model - based monitoring techniques allow you to look at inferences as a whole ( not split by individual distributions ) to better understand how many anomalous or weird inputs your model is receiving. special use cases such as nlp and computer vision have different feature relationships than traditional tabular methods, which require model - based anomaly detection to capture their drift. search out anomalous inferences easily investigate anomalous inferences by filtering by individual anomaly scores in the inference tab. while also available for tab", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "drift. search out anomalous inferences easily investigate anomalous inferences by filtering by individual anomaly scores in the inference tab. while also available for tabular or text inputs, we can see this example in arthur below. in the example gif provided above, we are looking into a model trained to detect whether or not a satellite image is from france or russia. this model was only trained on satellite images of amusement parks. here, we can see the most anomalous inferences are of stadiums ( a context that the model was not trained to understand ). teams frequently use this feature to find potential data pipeline issues, select", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lous inferences are of stadiums ( a context that the model was not trained to understand ). teams frequently use this feature to find potential data pipeline issues, select inputs for re - training, and better understand the environment their model is running within. inference anomaly score distribution chart for each inference, teams can also visualize the anomaly score distribution chart. this chart represents all of the anomaly scores in your reference dataset as a point of comparison. inferences are ranked on a scale of 0 - 1, where 0 represents no anomaly, and 1 represents an anomalous input. raw anomaly score ( for text and image input types : as you", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0 - 1, where 0 represents no anomaly, and 1 represents an anomalous input. raw anomaly score ( for text and image input types : as you can see in the visual above, there is the ability to visualize raw anomaly score in the distribution chart. this is only available for text and image model types. the raw anomaly scores are the calculated loss of the model used to predict anomaly. due to the way anomalies are scored against the reference dataset, we recommend using the raw anomaly score to track and sort anomalies in instances such as image quality assurance where all images are the same. understanding the algorithm to", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "we recommend using the raw anomaly score to track and sort anomalies in instances such as image quality assurance where all images are the same. understanding the algorithm to learn more about the algorithms used for anomaly detection. please refer to the arthur algorithms documentation section. available arthur schemas anomaly detection can be enabled for models with any input or output type other than time series input type models. only a reference data set is required - this can be a set of the model's train or test data. once a reference set is uploaded, anomaly scores are calculated automatically. updated 2 months ago what \u2019 s nextlearn more about enabling enrichments", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or test data. once a reference set is uploaded, anomaly scores are calculated automatically. updated 2 months ago what \u2019 s nextlearn more about enabling enrichments for your model in the model onboarding section. otherwise, click on hot spots to learn about another type of enrichment. enabling enrichmentshot spotstable of contents anomaly detection in practice anomaly detection as multivariate drift search out anomalous inferences inference anomaly score distribution chart understanding the algorithm available arthur schemas source : https : / / docs. arthur. ai / docs / anomaly - detection", "metadata": {"source": "https://docs.arthur.ai/docs/anomaly-detection", "row": 68, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 69 text : metrics jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by metricssuggest editsmetrics are functions for measuring model performance. they might compare predicted values to ground truth, measure distributional shift, evaluate model fairness, surface explainability trends, track feature distributions, inference volumes, or anything else you can imagine. metrics are a foundational part of evaluating and exploring your models. arthur \u2019 s powerful metrics api gives you the defaults you need to hit the ground running and the flexibility to define", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "part of evaluating and exploring your models. arthur \u2019 s powerful metrics api gives you the defaults you need to hit the ground running and the flexibility to define model performance however it best suits your business. arthur \u2019 s metrics are defined as template queries in our query api format. these template queries are evaluated with your specified parameters, filters, and rollup. for example, when you \u2019 re viewing a feature drift chart in the ui, behind the scenes, the arthur dashboard is : evaluating the feature drift metric for your model specifying your selected drift metric parameter ( e. g., \u201c psi \u201d ) and your specified", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur dashboard is : evaluating the feature drift metric for your model specifying your selected drift metric parameter ( e. g., \u201c psi \u201d ) and your specified drift attribute parameter ( s ) ( e. g. \u201c age \u201d and \u201c fico score \u201d input attributes ) specifying your selected timeframe as a filter over the \u201c inference _ timestamp \u201d field specifying your selected rollup of \u201c day, \u201d \u201c hour \u201d, etc., to determine the granularity of the graph ( or \u201c batch _ id \u201d for batch models ) updated 3 months ago source : https : / / docs. arthur. ai", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ularity of the graph ( or \u201c batch _ id \u201d for batch models ) updated 3 months ago source : https : / / docs. arthur. ai / docs / metrics - 1", "metadata": {"source": "https://docs.arthur.ai/docs/metrics-1", "row": 69, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 70 text : arthur sdk reference documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar arthur sdk reference documentation toggle light / dark / auto color theme toggle table of contents sidebar arthur docs sdk home permissions by function arthuraitoggle child pages in navigation arthurai. clienttoggle child pages in navigation arthurai. client. apiv3 arthurai. client. authtoggle child pages in navigation arthurai. client. auth. authrefresher arthurai. client. clientto", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". client. authtoggle child pages in navigation arthurai. client. auth. authrefresher arthurai. client. clienttoggle child pages in navigation arthurai. client. client. new _ requests _ client arthurai. client. client. arthurai arthurai. client. helpertoggle child pages in navigation arthurai. client. helper. construct _ url arthurai. client. helper. get _ arthur _ internal _ user _ org arthurai. client. helper. get _ auth _ info arthurai. client. helper. get _ current _ org arthur", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ user _ org arthurai. client. helper. get _ auth _ info arthurai. client. helper. get _ current _ org arthurai. client. helper. user _ login arthurai. client. httptoggle child pages in navigation arthurai. client. http. arthurtoggle child pages in navigation arthurai. client. http. arthur. arthurhttpclient arthurai. client. http. basetoggle child pages in navigation arthurai. client. http. base. abstracthttpclient arthurai. client. http. requeststoggle child pages in navigation arthurai.", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "navigation arthurai. client. http. base. abstracthttpclient arthurai. client. http. requeststoggle child pages in navigation arthurai. client. http. requests. httpclient arthurai. client. validationtoggle child pages in navigation arthurai. client. validation. validate _ multistatus _ response _ and _ get _ failures arthurai. client. validation. validate _ response _ status arthurai. commontoggle child pages in navigation arthurai. common. constantstoggle child pages in navigation arthurai. common. constants. accuracymetric arthurai. common. constants. enrichment", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "navigation arthurai. common. constantstoggle child pages in navigation arthurai. common. constants. accuracymetric arthurai. common. constants. enrichment arthurai. common. constants. enrichmentstatus arthurai. common. constants. imagecontenttype arthurai. common. constants. imageresponsetype arthurai. common. constants. inferencetype arthurai. common. constants. inputtype arthurai. common. constants. listablestrenum arthurai. common. constants. modelstatus arthurai. common. constants. outputtype arthurai. common.", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". listablestrenum arthurai. common. constants. modelstatus arthurai. common. constants. outputtype arthurai. common. constants. role arthurai. common. constants. stage arthurai. common. constants. textdelimiter arthurai. common. constants. timestampinferencetype arthurai. common. constants. valuetype arthurai. common. exceptionstoggle child pages in navigation arthurai. common. exceptions. arthur _ excepted arthurai. common. logtoggle child pages in navigation arthurai. common. log. disable _ debug", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". exceptions. arthur _ excepted arthurai. common. logtoggle child pages in navigation arthurai. common. log. disable _ debug _ logging arthurai. common. log. enable _ debug _ logging arthurai. common. log. initialize _ logging arthurai. common. log. infofilter arthurai. coretoggle child pages in navigation arthurai. core. alertstoggle child pages in navigation arthurai. core. alerts. validate _ parameters _ for _ alert arthurai. core. alerts. alert arthurai. core. alerts. alertrule arthur", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "alerts. validate _ parameters _ for _ alert arthurai. core. alerts. alert arthurai. core. alerts. alertrule arthurai. core. alerts. alertrulebound arthurai. core. alerts. alertruleseverity arthurai. core. alerts. alertstatus arthurai. core. alerts. metric arthurai. core. alerts. metrictype arthurai. core. attributestoggle child pages in navigation arthurai. core. attributes. get _ attribute _ order _ stage arthurai. core. attributes. arthurattribute arthurai. core", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in navigation arthurai. core. attributes. get _ attribute _ order _ stage arthurai. core. attributes. arthurattribute arthurai. core. attributes. attributebin arthurai. core. attributes. attributecategory arthurai. core. auth _ infotoggle child pages in navigation arthurai. core. auth _ info. authinfo arthurai. core. basetoggle child pages in navigation arthurai. core. base. arthurbasejsondataclass arthurai. core. biastoggle child pages in navigation arthurai. core. bias. bias _ metricstoggle child pages", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthurbasejsondataclass arthurai. core. biastoggle child pages in navigation arthurai. core. bias. bias _ metricstoggle child pages in navigation arthurai. core. bias. bias _ metrics. biasmetrics arthurai. core. bias. bias _ wrappertoggle child pages in navigation arthurai. core. bias. bias _ wrapper. arthurbiaswrapper arthurai. core. bias. threshold _ mitigationtoggle child pages in navigation arthurai. core. bias. threshold _ mitigation. curves arthurai. core. bias. threshold _ mitigation.", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ggle child pages in navigation arthurai. core. bias. threshold _ mitigation. curves arthurai. core. bias. threshold _ mitigation. thresholdmitigation arthurai. core. data _ servicetoggle child pages in navigation arthurai. core. data _ service. datasetservice arthurai. core. data _ service. imagezipper arthurai. core. dataset _ validation _ utilstoggle child pages in navigation arthurai. core. dataset _ validation _ utils. ensure _ obj _ matches _ attr _ value _ type arthurai. core. dataset _", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". core. dataset _ validation _ utils. ensure _ obj _ matches _ attr _ value _ type arthurai. core. dataset _ validation _ utils. get _ first _ elem _ if _ valid _ list arthurai. core. dataset _ validation _ utils. obj _ value _ type _ mismatch _ err arthurai. core. dataset _ validation _ utils. valid _ rec _ obj arthurai. core. dataset _ validation _ utils. validate _ attr _ names arthurai. core. dataset _ validation _ utils. valid", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". core. dataset _ validation _ utils. validate _ attr _ names arthurai. core. dataset _ validation _ utils. validate _ series _ data _ type arthurai. core. dataset _ validation _ utils. validate _ token _ likelihoods _ type arthurai. core. decoratorstoggle child pages in navigation arthurai. core. decorators. log _ prediction arthurai. core. enrichment _ status _ waitertoggle child pages in navigation arthurai. core. enrichment _ status _ waiter. await _ enrichments _ ready arthurai. core. enrichment _ status _ waiter", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ggle child pages in navigation arthurai. core. enrichment _ status _ waiter. await _ enrichments _ ready arthurai. core. enrichment _ status _ waiter. enrichmentstatuswaiter arthurai. core. enrichment _ status _ waiter. statusforenrichment arthurai. core. inferencestoggle child pages in navigation arthurai. core. inferences. add _ inference _ metadata _ to _ dataframe arthurai. core. inferences. add _ predictions _ or _ ground _ truth arthurai. core. inferences. nest _ inference _ and _ ground _ truth _ data arthurai. core. inferences.", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or _ ground _ truth arthurai. core. inferences. nest _ inference _ and _ ground _ truth _ data arthurai. core. inferences. nest _ reference _ data arthurai. core. inferences. parse _ stage _ attributes arthurai. core. model _ status _ waitertoggle child pages in navigation arthurai. core. model _ status _ waiter. modelstatuswaiter arthurai. core. model _ utilstoggle child pages in navigation arthurai. core. model _ utils. check _ attr _ is _ bias arthurai. core. model _ utils. check _", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in navigation arthurai. core. model _ utils. check _ attr _ is _ bias arthurai. core. model _ utils. check _ has _ bias _ attrs arthurai. core. model _ utils. get _ positive _ predicted _ class arthurai. core. model _ utils. tensors _ to _ arthur _ inference arthurai. core. modelstoggle child pages in navigation arthurai. core. models. arthurmodel arthurai. core. models. arthurmodelgroup arthurai. core. models. explainabilityparameters arthurai. core. status _ waiterto", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthurai. core. models. arthurmodelgroup arthurai. core. models. explainabilityparameters arthurai. core. status _ waitertoggle child pages in navigation arthurai. core. status _ waiter. statuswaiter arthurai. core. utiltoggle child pages in navigation arthurai. core. util. can _ cast arthurai. core. util. dataframe _ like _ to _ list _ of _ dicts arthurai. core. util. intersection _ is _ non _ empty arthurai. core. util. is _ bool _ like arthurai. core.", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". core. util. intersection _ is _ non _ empty arthurai. core. util. is _ bool _ like arthurai. core. util. is _ date _ like arthurai. core. util. is _ float _ like arthurai. core. util. is _ int _ like arthurai. core. util. is _ list _ like arthurai. core. util. is _ str _ like arthurai. core. util. is _ valid _ datetime _ obj arthurai. core. util. retrieve _ json _ files arthurai. core", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "core. util. is _ valid _ datetime _ obj arthurai. core. util. retrieve _ json _ files arthurai. core. util. retrieve _ parquet _ files arthurai. core. util. series _ to _ df arthurai. core. util. standardize _ pd _ obj arthurai. core. util. update _ column _ in _ list _ of _ dicts arthurai. core. util. numpyencoder arthurai. core. viztoggle child pages in navigation arthurai. core. viz. style arthurai. core", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "util. numpyencoder arthurai. core. viztoggle child pages in navigation arthurai. core. viz. style arthurai. core. viz. utilstoggle child pages in navigation arthurai. core. viz. utils. get _ pred _ and _ gt _ attrs arthurai. core. viz. utils. savgol _ filter arthurai. core. viz. visualizertoggle child pages in navigation arthurai. core. viz. visualizer. datavisualizer arthurai. datasetstoggle child pages in navigation arthurai. datasets. arthur", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ai. core. viz. visualizer. datavisualizer arthurai. datasetstoggle child pages in navigation arthurai. datasets. arthur _ exampletoggle child pages in navigation arthurai. datasets. arthur _ example. arthurexample arthurai. datasets. arthur _ example. arthurexampleschema arthurai. datasets. downloadtoggle child pages in navigation arthurai. datasets. download. download _ from _ s3 arthurai. datasets. download. get _ file _ keys _ in _ s3 _ folder arthurai. datasets", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ from _ s3 arthurai. datasets. download. get _ file _ keys _ in _ s3 _ folder arthurai. datasets. download. load _ downloaded _ file arthurai. datasets. download. arthurdatasetsource arthurai. datasets. download. arthurexampledownloader arthurai. explainabilitytoggle child pages in navigation arthurai. explainability. arthur _ explainertoggle child pages in navigation arthurai. explainability. arthur _ explainer. arthurexplainer arthurai. explainability. arthur _ explainer. emptylimee", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in navigation arthurai. explainability. arthur _ explainer. arthurexplainer arthurai. explainability. arthur _ explainer. emptylimeexplanation arthurai. explainability. explanation _ packagertoggle child pages in navigation arthurai. explainability. explanation _ packager. explanationpackager arthurai. explainability. validationtoggle child pages in navigation arthurai. explainability. validation. validate _ predicted _ attribute _ order _ matches _ dataframe arthurai. explainability. validation. validate _ predicted _ attribute _ order _ matches _ remote arthurai. utiltoggle child", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "matches _ dataframe arthurai. explainability. validation. validate _ predicted _ attribute _ order _ matches _ remote arthurai. utiltoggle child pages in navigation arthurai. util. format _ time _ series _ attr _ timestamps arthurai. util. format _ timestamp arthurai. util. format _ timestamps arthurai. util. generate _ timestamps arthurai. util. is _ valid _ datetime _ string arthurai. util. normal _ random _ ints _ fixed _ sum arthurai. util. value _ type _ to _", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "datetime _ string arthurai. util. normal _ random _ ints _ fixed _ sum arthurai. util. value _ type _ to _ python _ type arthurai. version back to top toggle light / dark / auto color theme toggle table of contents sidebar arthur sdk python api reference # this page contains the complete python api reference for arthur \u2019 s sdk. for guides and concepts, see our main docs. essentials # there are a few touch points that are most commonly needed in the sdk. these are : the arthurai client to create a connection to arthur its arthurai. model", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a few touch points that are most commonly needed in the sdk. these are : the arthurai client to create a connection to arthur its arthurai. model ( ) method to create a new ( empty ) model its arthurai. get _ model ( ) to fetch an existing model already registered with arthur the arthurmodel class ( returned by both of the above methods ) to interact with a particular model its arthurmodel. build ( ) method to construct a new model from a dataframe its arthurmodel. save ( ) method to register a model with the arthur platform contents # arthur docs sdk home permissions by function", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a dataframe its arthurmodel. save ( ) method to register a model with the arthur platform contents # arthur docs sdk home permissions by function arthurai arthurai. client arthurai. common arthurai. core arthurai. datasets arthurai. explainability arthurai. util arthurai. version indices # index module index next permissions by function copyright \u00a9 2022, arthur made with sphinx and @ pradyunsg's furo on this page arthur sdk python api reference essentials contents indices source : https : / / sdk. docs. arthur. ai", "metadata": {"source": "https://sdk.docs.arthur.ai", "row": 70, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 71 text : model input / output types jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by model input / output typesunderstanding model schemas within arthur scopesuggest editsrole of a model schema in arthur scope arthur's model schema records important properties for your model's attributes, including their value type and stage. these data types define the data that enters and exits your model. the inputtype of a model specifies whether data enter your model as a tabular data frame, image, or raw text.", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that enters and exits your model. the inputtype of a model specifies whether data enter your model as a tabular data frame, image, or raw text. the outputtype of a model specifies the modeling task : whether your model predicts values for a regression task, probabilities for a classification task, bounding boxes for a computer vision object - detection task, or token sequences ( and their probabilities for generative tasks ). supported schemas in arthur scope arthur scope supports several input and output types. the input types supported are : the output types supported are : updated 2 months ago what \u2019 s nextquick look", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur scope supports several input and output types. the input types supported are : the output types supported are : updated 2 months ago what \u2019 s nextquick look at model monitoring in arthur by typetabulartextimagetable of contents role of a model schema in arthur scope supported schemas in arthur scope source : https : / / docs. arthur. ai / docs / model - input - output - types", "metadata": {"source": "https://docs.arthur.ai/docs/model-input-output-types", "row": 71, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 72 text : regression jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by regressionsuggest editsregression models predict a numeric outcome. in arthur, these models are listed under the regression model type. some common examples of text regression are : what is the predicted review score for written restaurant reviews? predict house price from description text formatted data in arthur text regression models require two columns : text input and numeric output. when onboarding a reference dataset ( and setting a model schema ), you need to specify", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "models require two columns : text input and numeric output. when onboarding a reference dataset ( and setting a model schema ), you need to specify a target column for each inference's ground truth. many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( text input ) prediction ( numeric ) ground truth ( numeric ) non - input attribute ( numeric or categorical ) dulce est desipere in loco45. 3462. 42high school educationsi vis amari ama55.", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "numeric or categorical ) dulce est desipere in loco45. 3462. 42high school educationsi vis amari ama55. 153. 2graduate degree predict function and mapping these are some examples of common values teams need to onboard for their regression models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth mappingexample prediction function # # single column ground truth output _ mapping = {'prediction _ column':'gt _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping ) # # example prediction function for binary classification def predict ( x ) : return model. predict ( x ) available metrics when onboarding regression models, there are several default metric", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prediction function for binary classification def predict ( x ) : return model. predict ( x ) available metrics when onboarding regression models, there are several default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a regression model. find out more about these metrics in the performance metrics section. metricmetric typeroot mean squared errorperformancemean absolute errorperformancer squared", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "more about these metrics in the performance metrics section. metricmetric typeroot mean squared errorperformancemean absolute errorperformancer squaredperformanceinference countingestionaverage predictioningestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non -", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case text ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams are able to", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##hypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichment", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / text - regression - 1", "metadata": {"source": "https://docs.arthur.ai/docs/text-regression-1", "row": 72, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 73 text : sending ground truth jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by sending ground truthsuggest editsone of the greatest differences between evaluating models in experimentation vs. monitoring in production is the delayed nature of responses. a model that predicts an action may have ground truth available immediately after prediction. one common example is models, common in the realm of advertising, used to predict whether or not a user will click on the advertisement. a bank using a model that predicts whether or not a customer will default on their loan", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "used to predict whether or not a user will click on the advertisement. a bank using a model that predicts whether or not a customer will default on their loan in the first 6 months will not know whether or not they were correct until 6 months have passed or the customer defaults on their loan. in rarer cases, though more often in models of unstructured data types ( like text or image ), ground truth may never be collected. due to these varying timelines of receiving ground truth data, many teams use data drift metrics as proxies for performance when ground truth is delayed. with these techniques, however, it", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of receiving ground truth data, many teams use data drift metrics as proxies for performance when ground truth is delayed. with these techniques, however, it is still best practice to format and send in your ground truth data when available. formatting ground truth data after receiving ground truth data, matching ground truth labels with the correct inferences within arthur is essential. to ensure this, arthur requires two values for every inference value you would like to send ground truth for : ground truth label : true label for that inference row partner inference id : a unique inference identifier meant to connect inferences with how teams keep track of inferences internally", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "label : true label for that inference row partner inference id : a unique inference identifier meant to connect inferences with how teams keep track of inferences internally sending ground truth data with python sdk with the need to wait for ground truth, there tend to be three main workflows for teams updating their arthur model to receive ground truth : at the time of prediction : some ml models run in systems where ground truth is provided nearly instantaneously after prediction. in these instances, it is best to include ground truth as an additional column to there. send _ inferences ( ) workflow. at the time of labeling : similar", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "instances, it is best to include ground truth as an additional column to there. send _ inferences ( ) workflow. at the time of labeling : similar to attaching send _ inferences within the python script where your model makes inferences, teams with a receiving ground truth workflow may choose to attach update _ inference _ ground _ truths ( ) to their python script in bulk : other teams may wait to onboard ground truth labels until a certain number of labels or time has passed. teams that choose to send labels in bulk from a data frame may either use. update _ inference _ ground _ truths ( ) or send _ bulk _", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "passed. teams that choose to send labels in bulk from a data frame may either use. update _ inference _ ground _ truths ( ) or send _ bulk _ ground _ truths ( ) for updating ground truth for more than 100k inferences at a time python sdk # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # we can collect a set of folder names each corresponding to a batch run, containing one or # more", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# # # # # # # # # # # # we can collect a set of folder names each corresponding to a batch run, containing one or # more parquet or json files with the input attributes columns, non - input attribute columns, and # prediction attribute columns as well as a \" partner _ inference _ id \" column with our unique # identifiers and an \" inference _ timestamp \" column inference _ batch _ dirs =... # then suppose we have a directory with one or more parquet or json files containing matching # \" partner _ inference _ id \" s and our ground truth attribute columns", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "suppose we have a directory with one or more parquet or json files containing matching # \" partner _ inference _ id \" s and our ground truth attribute columns as well as a # \" ground _ truth _ timestamp \" column ground _ truth _ dir =... # send the inferences to arthur for batch _ dir in inference _ batch _ dirs : batch _ id = batch _ dir. split ( \" / \" ) [ - 1 ] # use the directory name as the batch id arthur _ model. send _ bulk _ inferences ( directory _ path = batch _ dir, batch _ id = batch _ id", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "directory name as the batch id arthur _ model. send _ bulk _ inferences ( directory _ path = batch _ dir, batch _ id = batch _ id ) # send the ground truths to arthur arthur _ model. send _ bulk _ ground _ truths ( directory _ path = ground _ truth _ dir ) updating ground truth with the api ground truth can also be updated using the arthur api using either : the update inference ground truths endpoint the send inference file endpoint and specifying the ground truth file updated 2 months ago table of contents formatting ground truth data sending ground truth data with python sdk updating ground truth with the", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "specifying the ground truth file updated 2 months ago table of contents formatting ground truth data sending ground truth data with python sdk updating ground truth with the api source : https : / / docs. arthur. ai / docs / sending - ground - truth", "metadata": {"source": "https://docs.arthur.ai/docs/sending-ground-truth", "row": 73, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 74 text : upgrading jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by upgradingsuggest editsarthur's platform installer leverages an open - source solution called kubernetes off - the - shelf ( kots ). please refer to the below links for upgrading the platform components. upgrading the platform application upgrading the admin console upgrading the embedded kubernetes cluster for vm install updated 3 months ago source : https : / / docs. arthur. ai / docs / upgrading", "metadata": {"source": "https://docs.arthur.ai/docs/upgrading", "row": 74, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 75 text : virtual machine installation jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by virtual machine installationsuggest editsthis section covers the steps required for installing arthur on a virtual machine ( vm ). we have included separate steps required for online and air - gapped installations. the vm installs are not recommended for production - grade deployments. they are great for development and testing purposes. online virtual machine ( vm ) install go to the download portal using the url and the password provided by arthur. click the", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for development and testing purposes. online virtual machine ( vm ) install go to the download portal using the url and the password provided by arthur. click the \" download license \" button to download your license in yaml file. ssh into your virtual machine ( vm ) and run the command below to install the admin console : shellcurl - ssl https : / / k8s. kurl. sh / arthur sudo bash log in to the admin console at < yourhost > : 8800 using the provided password in the install output. follow the instruction to set up your secure connection with tls", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##min console at < yourhost > : 8800 using the provided password in the install output. follow the instruction to set up your secure connection with tls certificate. upload your license file. provide your configurations review the preflight checks to ensure that your machine meets the minimum requirements before proceeding with the installation. monitor the dashboard for the application status to become ready. to see the progress of the deployment, monitor the deployment status with kubectl cli : shell # reload your shell if you haven't bash - l kubectl get deployment, statefulset, pod - n < yournamespace > if", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "reload your shell if you haven't bash - l kubectl get deployment, statefulset, pod - n < yournamespace > if anything is showing pending, it is likely you need to add more / bigger nodes to your cluster. when using kubectl, you might run into a permission issue loading the kubernetes / admin. conf file. please remediate it by running the command below. shellsudo chmod + r / etc / kubernetes / admin. conf airgap virtual machine ( vm ) install go to the download portal using", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##d + r / etc / kubernetes / admin. conf airgap virtual machine ( vm ) install go to the download portal using the url and the password provided by arthur. select the \" embedded cluster \" option click the \u201c download license \u201d button to download your license in the yaml file. download the \" latest kurl embedded install \" and the \" latest arthur airgap bundle \". preparing the embedded cluster arthur leverages kubernetes as the base. this step installs the base kubernetes cluster and arthur's admin console on your vm with a single cli", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es as the base. this step installs the base kubernetes cluster and arthur's admin console on your vm with a single cli command. first, upload the kurl embedded install bundle on your vm instance. example : shellscp - i mykey. pem ~ / downloads / arthur. tar. gz ubuntu @ hostname : arthur. tar. gz unpack the bundle and install the embedded kubernetes cluster on your vm instance. shelltar xvf arthur. tar. gz cat install. sh sudo bash - s airgap save the", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es cluster on your vm instance. shelltar xvf arthur. tar. gz cat install. sh sudo bash - s airgap save the output from the install, including the kotsadm admin console url and the password. you now have a k8s cluster, kubectl cli, and the admin console installed on your vm. deploying the application to the embedded cluster load the admin console ui on port 8800 from your browser using the kotsadm url and the password you recorded earlier. follow the instructions on the admin console to complete your installation by providing the", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from your browser using the kotsadm url and the password you recorded earlier. follow the instructions on the admin console to complete your installation by providing the arthur - x. x. x. airgap bundle and necessary configurations. monitor the admin console dashboard for the application status to become ready. to see the progress of the deployment, monitor the deployment status with kubectl cli on the vm : shell # reload your shell if you haven't bash - l kubectl get deployment, statefulset, pod if anything is showing pending, it is likely you need to add more / bigger", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "t bash - l kubectl get deployment, statefulset, pod if anything is showing pending, it is likely you need to add more / bigger nodes to your cluster. note : you may also follow the instructions { doc } here < vm _ install _ airgapped _ cli > to install the admin console and arthur app programmatically using the cli only. airgap virtual machine ( vm ) install with cli if you prefer to install programmatically using cli only, follow the steps below. upload the license file and the arthur - x. x. x. airga", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to install programmatically using cli only, follow the steps below. upload the license file and the arthur - x. x. x. airgap bundle on your vm instance. example : shellscp - i mykey. pem ~ / downloads / test \\ customer. yaml ubuntu @ hostname : license. yaml scp - i mykey. pem ~ / downloads / arthur - x. x. x. airgap ubuntu @ hostname : arthur - x. x. x. airgap create a config. yaml file on the v", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##p ubuntu @ hostname : arthur - x. x. x. airgap create a config. yaml file on the vm instance using { doc } the configuration template < / platform - management / reference / config _ template >. run this install command from your vm's ssh session : shell kubectl kots install arthur \\ - - airgap - bundle. / arthur - x. x. x. airgap \\ - - license - file. / license. yaml \\ - - config - values. / config. yaml", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". airgap \\ - - license - file. / license. yaml \\ - - config - values. / config. yaml \\ - - namespace arthur \\ - - shared - password [ the kotsadm password you saved earlier ] updated 3 months ago table of contents online virtual machine ( vm ) install airgap virtual machine ( vm ) install preparing the embedded cluster deploying the application to the embedded cluster airgap virtual machine ( vm ) install with cli source : https : / / docs. arthur. ai / docs / virtual - machine - installation", "metadata": {"source": "https://docs.arthur.ai/docs/virtual-machine-installation", "row": 75, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 76 text : creating arthur model object jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by creating arthur model objectsuggest editsnow that we are ready, we can start onboarding a model to arthur. this page is a walkthrough of creating an arthurmodel object to monitor your ml model. tl ; dr an arthurmodel object sends and retrieves data important to your deployed ml system. the arthurmodel object is separate from the trained underlying model and makes predictions ; it is a wrapper for the", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s data important to your deployed ml system. the arthurmodel object is separate from the trained underlying model and makes predictions ; it is a wrapper for the underlying model to access arthur platform functionality. the general steps for creating an arthurmodelobject are as followed : creating a production - ready ml model prepare your model schema / reference dataset for arthur : define build out and save your arthur model object a quick overview of the code needed can be seen below. pythonarthur _ model = arthur. model ( partner _ model _ id = f \" creditrisk _ batch _ qs - { datetime. now (", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##arthur _ model = arthur. model ( partner _ model _ id = f \" creditrisk _ batch _ qs - { datetime. now ( ). strftime ('% y % m % d % h % m % s') } \", display _ name = \" credit risk batch \", input _ type = inputtype. tabular, output _ type = outputtype. multiclass, is _ batch = true ) prediction _ to _ ground _ truth _ map = { \" prediction _ 1 \" : 1 } # # building out with reference dataframe arthur _ model. build ( ref _ d", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ground _ truth _ map = { \" prediction _ 1 \" : 1 } # # building out with reference dataframe arthur _ model. build ( ref _ df, ground _ truth _ column = \" gt \", pred _ to _ ground _ truth _ map = prediction _ to _ ground _ truth _ map non _ input _ columns = ['age ','sex ','race ','education'] ) arthur _ model. save ( ) create a production ready model the first step to onboarding a model to arthur is to create a model ready for or already deployed to production. since arthur is model and", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a production ready model the first step to onboarding a model to arthur is to create a model ready for or already deployed to production. since arthur is model and platform agnostic, it does not matter how the model is built or where it is deployed. [UNK] - production monitoringsome teams use arthur \u2019 s techniques or platform to evaluate models pre - production. this is definitely an option. however, the same sentiment remains that you need to have a finished model to onboard to arthur and evaluate. creating arthur connection to be able to send inference data to the platform, you will need to create a connection to not only your arthur platform but also the", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "evaluate. creating arthur connection to be able to send inference data to the platform, you will need to create a connection to not only your arthur platform but also the model the inferences are being tracked for. information about creating your api key and connecting to the arthur platform / model objects can be found in the ui / platform guide. prepare model schema / reference dataset for arthur teams are required to define the model's structure or schema. a model structure, otherwise known as an arthur model schema, defines a wireframe for what arthur should expect as inputs and outputs to your model. by recording essential properties for your model '", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "as an arthur model schema, defines a wireframe for what arthur should expect as inputs and outputs to your model. by recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built for your model in arthur. when you are onboarding a model, arthur categorizes each attribute into a different stage, depending on the role of the attribute in the model pipeline : attribute stagedescriptionmodelpipelineinputall the features the model uses to create a prediction. for tabular, these are the features that go into the model", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##criptionmodelpipelineinputall the features the model uses to create a prediction. for tabular, these are the features that go into the model. these are the text or image inputs for text or nlp. predictedvalueoutput values ( or predictions ) that your model producesgroundtruthvalues you can provide to compare the model's outputs against performance metrics ( commonly referred to as target or label ) noninputdataany attributes that are not predictive features within the model but are additional metadata you would like to track. ( i. e., protected attributes like age, race, or", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are not predictive features within the model but are additional metadata you would like to track. ( i. e., protected attributes like age, race, or sex, or business - specific data like unique customer id ) attributes are analogous to the different columns that comprise your model's data. each attribute has a value type : these can be standard types like int and str, or datatypes for complex models like raw text and images. available input value typesallowed data typesadditional information requirednumericalinteger, floatcategoricalinteger, string, boolean, floatspecified available categoriestimesta", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data typesadditional information requirednumericalinteger, floatcategoricalinteger, string, boolean, floatspecified available categoriestimestampdatetimemust be time zone awaretext ( nlp ) stringgenerative sequence ( llm ) stringa tokens ( and optional token likelihood ) column is also requiredimage. jpg,. png,. gifunique identifierstringtime serieslist of dictionaries with \" timestamp \" and \" value \" keystimestamps must be timezone aware datetimes and values must be floats. as you log data over", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" timestamp \" and \" value \" keystimestamps must be timezone aware datetimes and values must be floats. as you log data over time with arthur, the model schema is used to type - check ingested data. this prevents analytics from being skewed by scenarios like int values suddenly replacing float values causing silent bugs. arthur also records attribute properties in the model schema, like the range of possible values an attribute has in your data. these properties are used to understand your data \u2019 s high - level structure, not to strictly enforce that future attributes have these same properties. some key things to keep in", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "properties are used to understand your data \u2019 s high - level structure, not to strictly enforce that future attributes have these same properties. some key things to keep in mind when onboarding models schemas are : verify categories listed : verifying the available categories for categorical attributes is important. you want to ensure that all expected categories appear or are added to each attribute's list of possible categories. this is because these categories will be used to calculate drift metrics and appear for advanced segmentation within the ui. other category values may be sent to the platform but will not be part of this functionality. specifying attribute bounds : teams can", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "advanced segmentation within the ui. other category values may be sent to the platform but will not be part of this functionality. specifying attribute bounds : teams can set specified attribute bounds for numerical attributes, such as minimum and maximum values. these are only used to make setting alert thresholds ( i. e., data - bound alerts on inferences passing an acceptable maximum value ) easier. high cardinality models ( over 1000 unique attributes ) : arthur does not allow for high - cardinality models ( i. e., models with many unique columns ) with more than 1000 unique attributes. high cardinality variables ( over 1000 unique categories", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- cardinality models ( i. e., models with many unique columns ) with more than 1000 unique attributes. high cardinality variables ( over 1000 unique categories ) : arthur does not allow for high - cardinality variables ( i. e., variables with many unique classes ) with more than 1000 unique categories. this is more common for non - input metadata attributes than model input attributes. in these cases, we recommend manually overwriting high cardinality variables ( over 50 unique categories ) : attributes do not need over 1000 unique categories to be considered high cardinality. an attribute with over 50 unique categories can be onboarded o arthur, but", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") : attributes do not need over 1000 unique categories to be considered high cardinality. an attribute with over 50 unique categories can be onboarded o arthur, but there is some feature loss for this attribute. this attribute will not be used when calculating enrichments ; this includes explainability, anomaly detection, and hot spots. monitoring specific attributes for bias : for attributes you know you want to track using fairness metrics, teams must designate those inferences for bias detection tracking. this is more explained later in the document here. typically, teams choose to infer this schema automatically from a reference dataset. however, a more in - depth", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "more explained later in the document here. typically, teams choose to infer this schema automatically from a reference dataset. however, a more in - depth look at how teams may choose to define schemas manually can be found here. selecting a reference dataset a reference dataset is a representative sample of input features for your model. in other words, it is a sample of what is typical or expected for your model. typically, teams onboard their model's training or validation data for reference. the reference dataset is a representative sample of the input features your model ingests. it is used to compute baseline model analytics", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or validation data for reference. the reference dataset is a representative sample of the input features your model ingests. it is used to compute baseline model analytics. by capturing the data distribution you expect your model to receive, arthur can detect, surface, and diagnose data drift before it impacts results. examples of how reference data is structured for different model types can be found in the model input / output types section. selecting a reference dataset : we typically recommend using your training dataset as the reference. this is because there is no better dataset representation of the patterns your model has been built to learn than the actual dataset", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "training dataset as the reference. this is because there is no better dataset representation of the patterns your model has been built to learn than the actual dataset it learned on. however, teams do often choose to use other datasets as a reference. in particular when : the training dataset is oversampled / not representative of real - world data patterns : this commonly occurs when your positive predicted attribute is rare ( i. e., tumor detection or credit card fraud ). the training dataset is incredibly large : reference datasets larger than 1gb can cause platform performance issues unless gradually onboarded in chunks. we recommend", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "). the training dataset is incredibly large : reference datasets larger than 1gb can cause platform performance issues unless gradually onboarded in chunks. we recommend using a sample of your training or validation dataset for a larger dataset ( typically, about 50, 000 to 100, 000 rows are sufficient to calculate drift metrics ). it is important to ensure this is still representative of the entire dataset. in particular, we recommend including samples with extreme values included in the sample. for help onboarding in chunks or configuring your sample, please contact arthur support ). in these instances, we recommend using your validation data", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sample. for help onboarding in chunks or configuring your sample, please contact arthur support ). in these instances, we recommend using your validation dataset, or another evaluated dataset with representative patterns, as the reference. define arthur model object after cleaning the data, we need to start defining the arthur model. to do this, we will need to provide both structural and user - defined information about the model : pythonarthur _ model = arthur. model ( partner _ model _ id = f \" creditrisk _ batch _ qs - { datetime. now ( ). strftime ('% y %", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ model _ id = f \" creditrisk _ batch _ qs - { datetime. now ( ). strftime ('% y % m % d % h % m % s') } \", display _ name = \" credit risk batch \", input _ type = inputtype. tabular, output _ type = outputtype. multiclass, is _ batch = true ) user - defined information model display name this name will appear for your model within the arthur ui. making this descriptive and intuitive is important, as it will be how most users search for and find your model. we recommend making this title", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "within the arthur ui. making this descriptive and intuitive is important, as it will be how most users search for and find your model. we recommend making this title description about that desired task your model is completing ( i. e., credit card fraud classification ). this is because you can create and track multiple model versions within the same display name. model partner id within arthur, the model partner id must be unique and cannot be duplicated across other models within your organization. often, teams may have an internal id or identifying hash for their models. we do not recommend making this id your model display name. as one, hashes are", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", teams may have an internal id or identifying hash for their models. we do not recommend making this id your model display name. as one, hashes are often hard to identify and search for without expert knowledge. and two, tracking multiple versions for a specific model use case is incredibly difficult. instead, teams are encouraged to onboard that specific model's identifier as the model partner id. [UNK] timestamps within partner idsto avoid any confusion later, it is best practice to construct a partner _ model _ id with a timestamp embedded in the string, so that each time the model is onboarded, a new partner", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "practice to construct a partner _ model _ id with a timestamp embedded in the string, so that each time the model is onboarded, a new partner _ model _ id is generated. structural information this is information about the model's structure, i. e., how it runs. while arthur is model agnostic, so it doesn't care how your model was built, we require you to specify some information so we can correctly set up your model's environment based on its specifications. data type also known as the model input type, this is the type of data that the model will use to make predictions. the", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "environment based on its specifications. data type also known as the model input type, this is the type of data that the model will use to make predictions. the four possible data types are tabular, image, text, and time series. task type also known as the model output type, this is the output your model will generate. the four possible model output types are multiclass ( all classification ), regression, object detection, and ranked list. this information is needed as the platform has to set up the correct default metric types within your model environment. batch v. streaming next, teams should specify how they plan to send data to arthur", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "platform has to set up the correct default metric types within your model environment. batch v. streaming next, teams should specify how they plan to send data to arthur. we have two choices : batch : lower frequency used ( and monitored ). typical high inference load at a time streaming : high frequency of use ( and monitoring ). typical lower inference load at a time so, if you have users sending data frequently ( e. g., daily ), we recommend streaming models. conversely, if you have users sending data infrequently ( e. g., monthly ), we'd recommend batching models for them.", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "conversely, if you have users sending data infrequently ( e. g., monthly ), we'd recommend batching models for them. however, if you want guarantees about when metrics are calculated or alerts fire and cannot wait for the scheduled jobs to run, we recommend using batch models. another call - out is that batch models generate alerts by batch ( i. e., a set threshold has been passed on average within a batch ). streaming models generate alerts by a designated lookback window ( i. e., the average over the past day, week, etc. ). more information", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "streaming models generate alerts by a designated lookback window ( i. e., the average over the past day, week, etc. ). more information on alerting can be found here. indicating a batch model means supplying an additional batch _ id to group your inferences. arthur will default to measuring performance for each batch rather than by the inference timestamps. prediction mapping finally, when teams build out their arthur model, they must specify a prediction mapping function. this ensures that arthur correctly identifies predictive and ground truth classes when setting up the model's environment and calculating metrics. this mapping needs to identify which columns are", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ensures that arthur correctly identifies predictive and ground truth classes when setting up the model's environment and calculating metrics. this mapping needs to identify which columns are prediction and ground truth but also map the labels within the columns to one another. examples of each model type's prediction mapping can be found in its model input / output types section. register arthur model object the simplest method of registering your attributes is to use the arthurmodel. build ( ) function parses a pandas dataframe of your reference dataset containing inputs, metadata, predictions, and ground truth labels. in addition, a pred _ to _ ground _ truth", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s dataframe of your reference dataset containing inputs, metadata, predictions, and ground truth labels. in addition, a pred _ to _ ground _ truth _ map is required, which tells arthur which of your attributes represent your model \u2019 s predicted values and how those predicted attributes correspond to your model \u2019 s ground truth attributes. python # # general format example # # pred _ mapping = { \" model _ inputs \" : \" model _ outputs \" } # map our prediction attribute to the ground truth value # this tells arthur that in the data you send to the platform, # the ` predicted _ probability ` column represents # the probability", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attribute to the ground truth value # this tells arthur that in the data you send to the platform, # the ` predicted _ probability ` column represents # the probability that the ground - truth column has the value 1 prediction _ to _ ground _ truth _ map = { \" prediction _ 1 \" : 1 } # # building out with reference dataframe arthur _ model. build ( ref _ df, ground _ truth _ column = \" gt \", pred _ to _ ground _ truth _ map = prediction _ to _ ground _ truth _ map ) examples of each model type's mapping can be found within their specific model input /", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ truth _ map = prediction _ to _ ground _ truth _ map ) examples of each model type's mapping can be found within their specific model input / output types descriptions. non - input attributes ( optional ) many teams want to track and monitor performance around metadata related to their model but are not necessarily model inputs or outputs. these features can be added as non - input attributes in the arthurmodel and must be specified in this build function. python # specifying additional non input attributes when building a model. # this tells arthur to monitor ['age ','sex ','race ','education'] # in", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "input attributes when building a model. # this tells arthur to monitor ['age ','sex ','race ','education'] # in the reference and inference data you send to the platform arthur _ model. build ( reference _ df, ground _ truth _ column ='ground _ truth _ label ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map, non _ input _ columns = ['age ','sex ','race ','education'] ) verify model and onboard to arthur as mentioned above, your model schema can", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' age ','sex ','race ','education'] ) verify model and onboard to arthur as mentioned above, your model schema can't be updated after you've saved your arthur model object. for this reason, many teams choose to review their model schema before sending it to the platform. this can be done in the sdk with pythonarthur _ model. review ( ) this creates a dataframe that contains all the information about the model schema. an example can be shown below : after reviewing everything, teams can save their model to arthur. this is done with the command below. python", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model schema. an example can be shown below : after reviewing everything, teams can save their model to arthur. this is done with the command below. pythonarthur _ model. save ( ) once you call arthur _ model. save ( ) arthur will handle creating the model and provisioning the necessary infrastructure to enable data ingestion for this model. if model creation fails, you may try re - saving the model or contact support if the problem persists. ( optional ) monitoring attributes for bias for some of the attributes in your model, you may want to pay particular attention to how your model \u2019 s outputs are potentially different for", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") monitoring attributes for bias for some of the attributes in your model, you may want to pay particular attention to how your model \u2019 s outputs are potentially different for each subpopulation of that attribute. we refer to this as monitoring an attribute for bias. when you set up to monitor a pipelineinput or noninput attribute for bias enables using fairness metrics for those attributes within your model ui. [UNK] monitoring for bias before sending inferencesyou must enable monitoring attributes for bias before sending inferences to the platform. based on whether the attributes of interest are categorical or continuous, you can follow the steps below for each attribute of interest", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "before sending inferences to the platform. based on whether the attributes of interest are categorical or continuous, you can follow the steps below for each attribute of interest you'd like to monitor for bias. categorical attributes for a categorical variable, each possible level of the attribute will be treated as a distinct sub - population for analysis. for example, if you had an attribute for \u201c gender, \u201d which comprised the three possible values male, female, and non - binary, then you would simply add the following to your model onboarding. pythonarthur _ model. get _ attribute ( \" sex \", stage = stage. model", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "then you would simply add the following to your model onboarding. pythonarthur _ model. get _ attribute ( \" sex \", stage = stage. modelpipelineinput ). monitor _ for _ bias = true continuous attributes for a continuous variable, you need to break the continuous range into a fixed number of groupings so that we can create sub - populations. you can do this by providing cutoff thresholds for each grouping. for example, if we have a continuous attribute called age, we can create three age brackets, such as < 35, 35 - 55, and > 55. we create these groups by providing the", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "continuous attribute called age, we can create three age brackets, such as < 35, 35 - 55, and > 55. we create these groups by providing the upper - cutoff values for each group. pythonarthur _ model. get _ attribute ( \" age \", stage = stage. modelpipelineinput ). monitor _ for _ bias = true arthur _ model. get _ attribute ( \" age \", stage = stage. modelpipelineinput ). set ( bins = [ none, 35, 55, none ] ) updated 2 months ago what \u2019 s nextmove onto enabling enrichments in arthur or", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "set ( bins = [ none, 35, 55, none ] ) updated 2 months ago what \u2019 s nextmove onto enabling enrichments in arthur or learn more about specifics around model attributes and stagesenabling enrichmentsattributes and stagesregistering model attributes manuallytable of contents tl ; dr create a production ready model creating arthur connection prepare model schema / reference dataset for arthur some key things to keep in mind when onboarding models schemas are : selecting a reference dataset define arthur model object user - defined information structural information register arthur model object non - input attributes ( optional ) verify model and onboard", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are : selecting a reference dataset define arthur model object user - defined information structural information register arthur model object non - input attributes ( optional ) verify model and onboard to arthur ( optional ) monitoring attributes for bias source : https : / / docs. arthur. ai / docs / creating - arthur - model - object", "metadata": {"source": "https://docs.arthur.ai/docs/creating-arthur-model-object", "row": 76, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 77 text : saml jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by samlsuggest [UNK] configurations are only supported in on - prem arthur installations this page provides a walk - through for how to configure your arthur installation to work with a saml compatible idp. in order to complete this guide, you need administrator access to your idp and access to your arthur installation's admin console configuration. additionally, you will either need access to the arthur superadmin user or be able to assume a role", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to your arthur installation's admin console configuration. additionally, you will either need access to the arthur superadmin user or be able to assume a role in your idp to give yourself rbac management permissions in arthur. this guide will walk through the following steps : configure the idp user groups and saml assertion configure the arthur service provider urls in the idp configure arthur to work with your idp apply the arthur idp yaml configuration create organization user roles to match the idp user groups test access cleaning up 1. configure the idp user groups", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur idp yaml configuration create organization user roles to match the idp user groups test access cleaning up 1. configure the idp user groups and saml assertion in order to properly map users to permissions, arthur requires an attribute in your saml assertion that contains information about the group memberships of the user. each group in the idp should correspond to a role in arthur's { doc } custom _ rbac permission system. this process can vary depending on your idp, but most idp's should have a user grouping mechanism, and a mechanism to configure attributes in the saml", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "depending on your idp, but most idp's should have a user grouping mechanism, and a mechanism to configure attributes in the saml assertions. for example using okta, under the saml application settings, admins can configure the saml assertion attributes to include group information under saml settings - > configure saml - > group attribute statements, then specifying a name for the attribute and a filter for the groups to include : setting this configuration produces the following attribute in the saml assertion ( in okta click \" preview the saml assertion \" button to see a sample", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to include : setting this configuration produces the following attribute in the saml assertion ( in okta click \" preview the saml assertion \" button to see a sample ) : xml < saml2 : attributestatement > < saml2 : attribute name = \" groups \" > < saml2 : attributevalue > everyone < / saml2 : attributevalue > < saml2 : attributevalue > admins < / saml2 : attributevalue > < saml2 : attributevalue > org - 1 - model - owners < / saml2 : attributevalue > < / saml2", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> < saml2 : attributevalue > org - 1 - model - owners < / saml2 : attributevalue > < / saml2 : attribute > < / saml2 : attributestatement > 2. configure the arthur service provider urls in the idp in order for your idp to speak to arthur, it needs to know where to find it. enter the following urls in your idp's configuration to arthur's saml endpoints : acs url ( sso url ) : https : / / < hostname > / api / v3 / sam", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "s saml endpoints : acs url ( sso url ) : https : / / < hostname > / api / v3 / saml / sso entity id : https : / / < hostname > / api / v3 / saml / sso start url : https : / / < hostname > / [UNK] your idp will be sending signed assertions to arthur, you will also need to generate and upload the public key ( certificate ) arthur will be using in your idp. this will be the same certificate you set in the arthur configuration below. please follow your own company policies", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( certificate ) arthur will be using in your idp. this will be the same certificate you set in the arthur configuration below. please follow your own company policies to obtain a certificate for arthur. if you have no internal guidelines, then use a tool like ssh - keygen to generate them 3. configure arthur to work with your idp additionally, arthur needs to know how to handshake with your idp. to do that, arthur requires the following information : your idp's metadata url or the metadata xml payload ( some idps require it be downloaded, either is fine ) one or more id", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "information : your idp's metadata url or the metadata xml payload ( some idps require it be downloaded, either is fine ) one or more idp administrator user groups that will be paired to global custom roles in arthur ( see here for a description of what these are for ) an understanding of your saml assertion and how to parse user information out of it with those three things available, it is possible to fill out arthur's idp configuration yaml. the next subsections explain each section of the arthur yaml configuration, and are followed by some complete examples further down. configuring the idps", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the next subsections explain each section of the arthur yaml configuration, and are followed by some complete examples further down. configuring the idps metadata url some idp's host their metadata xml at a public url, while others only have it available for download privately. to support either option, arthur has two configurations that can be used : yaml # use this option if your idp has a public url for its metadata metadataurl : \" link to idp metadata goes here \" # use this option if your idp does not have a public url and include the xml payload # make sure", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" link to idp metadata goes here \" # use this option if your idp does not have a public url and include the xml payload # make sure to indent the xml payload two spaces and make sure the x509certificate lines # do not have more than two leading whitespaces! metadataxml : <? xml version = \" 1. 0 \" encoding = \" utf - 8 \"? > < md : entitydescriptor... > < md : idpssodescriptor >... < ds : x509certificate > certificate line 1 cert", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ".. > < md : idpssodescriptor >... < ds : x509certificate > certificate line 1 cert line 2 cert line 3 cert line 4 last cert line < / ds : x509certificate > < / md : idpssodescriptor > < / md : entitydescriptor > { warning } if using the ` metadataxml ` configuration option, make sure to indent the entire xml payload two spaces. yaml expects multi - line values to be indented under the key ` metadataxml `. { warning } additionally", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ent the entire xml payload two spaces. yaml expects multi - line values to be indented under the key ` metadataxml `. { warning } additionally, the assertion's ` x509certificate ` xml attribute is a multi - line value within the xml. any new lines in the certificate value need to be indented only two spaces ( all the way to the left of the yaml value ). otherwise, the extra whitespaces introduces characters which will invalidate the certificate value. configure the arthur global roles arthur has the ability to create roles for the cluster administrators during the configuration of the", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "which will invalidate the certificate value. configure the arthur global roles arthur has the ability to create roles for the cluster administrators during the configuration of the idp. these roles are often needed by admins to configure rbac and create organizations for other users in the system. see { ref } creating _ global _ roles _ in _ arthur _ config for a deep dive on how to use global roles. this section of the yaml config is under the globalroledefs field. it accepts a list of role definitions that will be created when the configuration is applied. the names of the roles", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##g is under the globalroledefs field. it accepts a list of role definitions that will be created when the configuration is applied. the names of the roles in this section must match the user groups in your idp in order to be able to assume them in arthur. yaml globalroledefs : # here we can specify a list to define multiple global roles - name : \" idp - admin \" # change this name to match the cluster administrator group name in your idp permissions : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "your idp permissions : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete model : - read - write - delete parsing the idp saml assertion in order for arthur to communicate with your idp, it needs to understand the format of the saml assertion your idp uses. this section of the config falls under the assertionattributes yaml field. this section is designed to be flexible to support a variety of assertion formats, so it has a lot of options. at its core, the goal is to tell arthur", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "section is designed to be flexible to support a variety of assertion formats, so it has a lot of options. at its core, the goal is to tell arthur how to be able to extract the following information from the assertion : user roles / groups first name last name email user id each field has a corresponding yaml field the defines where to find the information in the saml assertion xml. for example : yamlfirstnameattribute : name : \" employeefirstname \" this configuration tells arthur that it can find the user's first name under the \" employeefirstname \" attribute in the xml assertion. such", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##rstname \" this configuration tells arthur that it can find the user's first name under the \" employeefirstname \" attribute in the xml assertion. such an assertion might look like this : xml < saml2 : attributestatement > < saml2 : attribute name = \" firstname \" > < saml2 : attributevalue > ian < / saml2 : attributevalue > < / saml2 : attribute > < / saml2 : attributestatement > more examples of how to parse attributes out of the saml assertion can be found below. full configuration examples here is an example of a", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attributestatement > more examples of how to parse attributes out of the saml assertion can be found below. full configuration examples here is an example of a full configuration, combining each section described above. yamlversion : v1 kind : saml config : # if your idp hosts its metadata, provide the url to it here metadataurl : \" link to idp metadata goes here \" # if the idp does not host the metadata, provide the xml payload here and comment out metadataurl # metadataxml : # <? xml version = \" 1. 0 \" encoding = \" utf -", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the xml payload here and comment out metadataurl # metadataxml : # <? xml version = \" 1. 0 \" encoding = \" utf - 8 \"? > < md : entitydescriptor... > # < md : idpssodescriptor > #... # < / md : idpssodescriptor > # < / md : entitydescriptor > # this section describes how arthur will parse the saml assertion from your idp # for each required attribute, arthur will use the \" name \" field to match to an xml attribute in the saml assertion assertionatt", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "assertion from your idp # for each required attribute, arthur will use the \" name \" field to match to an xml attribute in the saml assertion assertionattributes : # this roleattribute configuration will use a \" groups \" attribute in the xml assertion which expects the # roles in separate xml attributevalues within the assertion attribute roleattribute : name : \" groups \" useallattributevalues : true firstnameattribute : name : \" employeefirstname \" lastnameattribute : name : \" employeelastname \" emailattribute : name", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##te : name : \" employeefirstname \" lastnameattribute : name : \" employeelastname \" emailattribute : name : \" company _ email \" useridattribute : name : \" companyuserid \" globalroledefs : # here we specify a global role for the idp user group \" idp - admin \" to create and manage rbac in arthur - name : \" idp - admin \" permissions : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete model : - read", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete model : - read - write - delete 4. apply the arthur idp yaml configuration once you have your yaml configuration file ready, you need to add it to your arthur installation. with the arthur admin console open, navigate to the \" use a 3rd party global identity provider \" section and select \" saml \". this will expose a text box for you to paste the yaml config file assembled above. when pasting, make sure whitespace is preserved and the ya", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will expose a text box for you to paste the yaml config file assembled above. when pasting, make sure whitespace is preserved and the yaml document has consistent spacing ( do not mix tabs and spaces ). here is a screenshot of the config section : { note } if your idp enforces signed authorization requests, this config page also provides the ability to upload a certificate and private key for arthur to use when making the requests. click the \" upload a file \" button for the public certificate and private key sections of the config to upload the appropriate files for", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the requests. click the \" upload a file \" button for the public certificate and private key sections of the config to upload the appropriate files for your idp. once you have added your config files, scroll to the bottom and click \" save \" to save the config. then go to the latest version and click \" deploy \" to roll out the change to the cluster. 5. create organization user roles to match the idp user groups in order to complete this section, you will need access to the arthur superadmin user credentials set during your install, or you will need to be able to assume", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "order to complete this section, you will need access to the arthur superadmin user credentials set during your install, or you will need to be able to assume the role defined in the arthur idp config yaml created above in the globalroledefs section. in order to use the api example linked below, you will need a bearer token ( authentication token ) to include with your api request. there are a few options available to retrieve a token : retrieve a saml assertion from your idp and exchange with arthur - most idps will have a method to retrieve a saml assertion for users. some companies make scripts or", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##l assertion from your idp and exchange with arthur - most idps will have a method to retrieve a saml assertion for users. some companies make scripts or apis to do so. if your idp does not have an automated method to retrieve an assertion, use one of the other options below. once you have an assertion, you can exchange it for an arthur access token with the follow api call to arthur : shellcurl - - location - - request post'https : / / < your arthur host > / api / v3 / saml / sso'\\ - - header'content - type : application / x", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": / / < your arthur host > / api / v3 / saml / sso'\\ - - header'content - type : application / x - www - form - urlencoded'\\ - - data - urlencode'samlresponse = < insert url encoded base64 assertion >'retrieve a global role token from your browser cookies - if you sign in to arthur as a user with a global role, the ui will not be fully functional, but it will have a valid access token in the cookies. if you navigate to your browser's developer console and then go to the application storage", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "fully functional, but it will have a valid access token in the cookies. if you navigate to your browser's developer console and then go to the application storage / cookies section, you should see a cookie like authentication. the authentication token is the value of that cookie. use the / login api endpoint with the superadmin user's credentials set during the arthur install ( only available on - prem ). using either of those credentials, we can use the arthur api to define roles in arthur that match the user group names in your idp. see the { ref } creating _ organization _ roles section for an example api request", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to define roles in arthur that match the user group names in your idp. see the { ref } creating _ organization _ roles section for an example api request to create custom roles in arthur. importantly, the role names must uniquely match to a user group in your idp in order for your users to be able to assume those permissions in arthur. therefore, the roles in arthur must be globally unique in the entire arthur installation. 6. test access at this point everything should be configured correctly to sign in to arthur via sso. either navigate to your idp or the arthur homepage to test logging in. 7. cleaning up", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "be configured correctly to sign in to arthur via sso. either navigate to your idp or the arthur homepage to test logging in. 7. cleaning up once users are successfully able to log in to arthur via the idp, you should do the following to ensure proper security best - practices remain enforced : restrict any arthur global roles to only allow access to essential admin functions set the arthur superadmin user password securely, and either store the password in a vault, or discard the password entirely. superadmin shouldn't be used going forward. set up a policy to routinely rotate the superadmin password to keep it", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "discard the password entirely. superadmin shouldn't be used going forward. set up a policy to routinely rotate the superadmin password to keep it secure together, these practices will help ensure the security of your arthur installation, and will give your idp sole control over the platform and who is able to access it. common troubleshooting if after following the steps above, users are not able to log in via the idp try some of these common troubleshooting tips : does the user properly redirected to the idp's log in screen? if not, there is likely a configuration error in the arthur ya", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "tips : does the user properly redirected to the idp's log in screen? if not, there is likely a configuration error in the arthur yaml config with the idp metadata or the url to access it. another problem could be if your idp expects arthur to make signed requests to authenticate users. if that is the case, be sure you have correctly configured arthur's certificate and private key as described above. once the user authenticates with the idp, are they redirected to the arthur homepage? if not, there is likely a configuration error with the idp and the ur", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the idp, are they redirected to the arthur homepage? if not, there is likely a configuration error with the idp and the urls that it uses to communicate with arthur. double - check the acs ( sso ) url is configured correctly for the arthur installation at https : / / < hostname > / api / v3 / saml / sso. a user can see the arthur home page, but can't see any of the model in their organization if a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of the model in their organization if a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to access models ( see { doc }.. /.. / reference / permissions _ by _ endpoint ) or they were not able to correctly assume the role in arthur. double - check the groups in their saml assertion match the role names that have been configured in arthur. a superadmin or global role user with permissions to manage rbac can see a list of roles in the installation by using the following api call. be sure to replace the host and au", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "user with permissions to manage rbac can see a list of roles in the installation by using the following api call. be sure to replace the host and auth token for your installation and user : shellcurl - - location'https : / / < host > / api / v3 / authorization / custom _ roles'\\ - - header'authorization : bearer < insert auth token here >'appendix a : more examples of saml assertion values and how to parse them this section outlines some additional ways to use the assertionattributes section of the arthur idp config yaml format. the below examples", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##se them this section outlines some additional ways to use the assertionattributes section of the arthur idp config yaml format. the below examples include sample saml assertions, then corresponding yaml for how to parse them. basic full example this example shows how to parse a user's information from a saml assertion when each field is its own attribute and the user's groups are each in their own attributevalue. example saml assertion xml : xml < saml2 : attributestatement > < saml : attribute name = \" employeefirstname \" > < saml : attributeval", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "xml : xml < saml2 : attributestatement > < saml : attribute name = \" employeefirstname \" > < saml : attributevalue > john < / saml : attributevalue > < / saml : attribute > < saml : attribute name = \" employeelastname \" > < saml : attributevalue > doe < / saml : attributevalue > < / saml : attribute > < saml : attribute name = \" employeeemail \" > < saml : attributevalue > [ email protected ] < / saml : attributevalue > < / saml", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= \" employeeemail \" > < saml : attributevalue > [ email protected ] < / saml : attributevalue > < / saml : attribute > < saml : attribute name = \" employeeid \" > < saml : attributevalue > 1234567890 < / saml : attributevalue > < / saml : attribute > < saml : attribute name = \" usergroups \" > < saml : attributevalue > group1 < / saml : attributevalue > < saml : attributevalue > group2 < / saml : attributevalue >", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ue > group1 < / saml : attributevalue > < saml : attributevalue > group2 < / saml : attributevalue > < saml : attributevalue > group3 < / saml : attributevalue > < / saml : attribute > < / saml2 : attributestatement > corresponding settings for the arthur idp config yaml assertionattributes for the user information field : yaml assertionattributes : roleattribute : name : \" usergroups \" useallattributevalues : true firstnameattribute", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##butes : roleattribute : name : \" usergroups \" useallattributevalues : true firstnameattribute : name : \" employeefirstname \" lastnameattribute : name : \" employeelastname \" emailattribute : name : \" employeeemail \" useridattribute : name : \" employeeid \" parsing user groups from multiple attribute values this example shows how to parse a user's groups from a saml assertion when each group is its own attributevalue. example saml assertion xml : xml < saml2", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##se a user's groups from a saml assertion when each group is its own attributevalue. example saml assertion xml : xml < saml2 : attributestatement > < saml : attribute name = \" idp _ user _ groups \" > < saml : attributevalue > role1 < / saml : attributevalue > < saml : attributevalue > role2 < / saml : attributevalue > < saml : attributevalue > role3 < / saml : attributevalue > < / saml : attribute >... < / saml2 : attributestate", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##value > role3 < / saml : attributevalue > < / saml : attribute >... < / saml2 : attributestatement > corresponding settings for the arthur idp config yaml assertionattributes for the roleattribute field : yaml assertionattributes : roleattribute : name : \" idp _ user _ groups \" useallattributevalues : true parsing user groups from a string attribute value this example shows how to parse a user's groups from a saml assertion when the groups are in a single string", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "user groups from a string attribute value this example shows how to parse a user's groups from a saml assertion when the groups are in a single string attributevalue. example saml assertion xml : xml < saml2 : attributestatement > < saml : attribute name = \" idp _ user _ groups \" > < saml : attributevalue > role1, role2, role3 < / saml : attributevalue > < / saml : attribute >... < / saml2 : attributestatement > corresponding settings for the arthur idp config yaml assertionattri", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##l : attribute >... < / saml2 : attributestatement > corresponding settings for the arthur idp config yaml assertionattributes for the roleattribute field : yaml assertionattributes : roleattribute : name : \" idp _ user _ groups \" deliminator : \", \" parsing specific fields in a single attribute's attributevalue list this example shows how to parse a user's information from a saml assertion when all fields are in a single assertion attribute's list of attributevalues. example saml assertion xml :", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "user's information from a saml assertion when all fields are in a single assertion attribute's list of attributevalues. example saml assertion xml : xml < saml2 : attributestatement > < saml : attribute name = \" employeeinfo \" > < saml : attributevalue > john < / saml : attributevalue > < saml : attributevalue > doe < / saml : attributevalue > < saml : attributevalue > ( 123 ) 456 - 7890 < / saml : attributevalue > < saml : attributevalue > 42 wallaby way", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##value > ( 123 ) 456 - 7890 < / saml : attributevalue > < saml : attributevalue > 42 wallaby way, sydney < / saml : attributevalue > < saml : attributevalue > [ email protected ] < / saml : attributevalue > < saml : attributevalue > 5678987654 < / saml : attributevalue > < / saml : attribute >... < / saml2 : attributestatement > corresponding settings for the arthur idp config yaml assertionattributes for the", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "... < / saml2 : attributestatement > corresponding settings for the arthur idp config yaml assertionattributes for the user information field : yaml assertionattributes : firstnameattribute : name : \" employeeinfo \" index : 0 lastnameattribute : name : \" employeeinfo \" index : 1 emailattribute : name : \" employeeinfo \" index : 4 useridattribute : name : \" employeeinfo \" index : 5 updated 3 months ago table of contents 1. configure the idp user", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ttribute : name : \" employeeinfo \" index : 5 updated 3 months ago table of contents 1. configure the idp user groups and saml assertion 2. configure the arthur service provider urls in the idp 3. configure arthur to work with your idp configuring the idps metadata url configure the arthur global roles parsing the idp saml assertion full configuration examples 4. apply the arthur idp yaml configuration 5. create organization user roles to match the idp user groups 6. test access 7. cleaning up common troubles", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". apply the arthur idp yaml configuration 5. create organization user roles to match the idp user groups 6. test access 7. cleaning up common troubleshooting does the user properly redirected to the idp's log in screen? once the user authenticates with the idp, are they redirected to the arthur homepage? a user can see the arthur home page, but can't see any of the model in their organization appendix a : more examples of saml assertion values and how to parse them basic full example parsing user groups from multiple attribute values parsing user groups from a string attribute", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "more examples of saml assertion values and how to parse them basic full example parsing user groups from multiple attribute values parsing user groups from a string attribute value parsing specific fields in a single attribute's attributevalue list source : https : / / docs. arthur. ai / docs / saml", "metadata": {"source": "https://docs.arthur.ai/docs/saml", "row": 77, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 78 text : object detection jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by object detectionimage object detection models within arthursuggest editsobject detection models analyze and classify objects within an image by placing a bounding box over the objects. in arthur, these models are listed under the object detection model type. some common examples of object detection are : animal detection with wildlife cameras quality control in manufacturing, detecting defective pieces on the factory line formatted data in arthur object detection models require two columns : image input and bounding box", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "wildlife cameras quality control in manufacturing, detecting defective pieces on the factory line formatted data in arthur object detection models require two columns : image input and bounding box output. when onboarding a reference dataset ( and setting a model schema ), you need to specify the relationship between your prediction and ground truth bounding box columns. many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. formatting bounding boxes when using an object detection model, bounding boxes should be formatted as lists in the form : [ class _ id", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "input attributes. formatting bounding boxes when using an object detection model, bounding boxes should be formatted as lists in the form : [ class _ id, confidence, top _ left _ x, top _ left _ y, width, height ] the first two components of the bounding box list represent the classification being made within the bounding box. the class _ id represents the id of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction ( 0. 0 for completely unconfident and 1. 0 for completely confident ). the next four components of the bound", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the model has in this prediction ( 0. 0 for completely unconfident and 1. 0 for completely confident ). the next four components of the bounding box list represent the location of the bounding box within the image : the top _ left _ x and top _ left _ y represent the x and y pixel coordinates of the top - left corner of the bounding box. these pixel coordinates are calculated from the origin, which is in the top left corner of the image. this means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. the width represents the number of pixels the", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "image. this means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. the width represents the number of pixels the bounding box covers from left to right, and the height represents the number of pixels the bounding box covers from top to bottom. attribute ( image input ) prediction ( bounding boxes ) ground truth ( bounding boxes ) non - input attribute ( numeric or categorical ) image _ 1. jpg45. 3462. 42high school educationimage _ 2. jpg55. 153. 2graduate degree predict function and mapping teams must provide the", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "3462. 42high school educationimage _ 2. jpg55. 153. 2graduate degree predict function and mapping teams must provide the relationship between the prediction and ground truth column to onboard their object detection models. this is defined to help set up your arthur environment to calculate default performance metrics. prediction to ground truth mapping # # single column ground truth output _ mapping = {'pred _ bounding _ box _ column':'gt _ bounding _ box _ column'} # build function for this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ing _ box _ column'} # build function for this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping ) available metrics when onboarding object detection models, several default metrics are available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a object detection model. learn more about these metrics in the performance metrics section.", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( out - of - the - box ) per class when teams onboard a object detection model. learn more about these metrics in the performance metrics section. metricmetric typemapeperformanceinference countingestionaverage confidenceperformance drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. learn more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "learn more about these metrics in the drift and anomaly section. of note, for unstructured data types ( like text and image ), feature drift is calculated for non - input attributes. the actual input to the model ( in this case, image ) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type. psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate driftaver", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate driftaverage raw anomaly scoremultivariate drift note : teams can evaluate drift for inference data at different intervals with our python sdk and query service ( for example, data coming into the model now compared to a month ago ). user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxupdated 3 months ago table of contents formatted data in arthur formatting bounding boxes predict function and mapping available metrics out - of - the - box metrics drift metrics user - defined metrics available enrichments source : https : / / docs. arthur.", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s out - of - the - box metrics drift metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / image - object - detection", "metadata": {"source": "https://docs.arthur.ai/docs/image-object-detection", "row": 78, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 79 text : alerting jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by alertingsuggest editsan alert is a message notifying you that something has occurred with your model. with alerts, arthur scope makes it easy to provide a continuous view of your model by highlighting important changes in model performance. when defining alerts in arthur, there are a few things you need to consider : alert severity two alert severities are available in arthur ; they are warning and critical. teams can set their own severity for alerts.", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "need to consider : alert severity two alert severities are available in arthur ; they are warning and critical. teams can set their own severity for alerts. we typically recommend that teams set two different thresholds for the same value, marking the less severe as warning and the more as critical. understanding alert rules an alert is triggered based on an alert rule, which you define using a metric and a threshold. so to create an alert in arthur, you need to : create a metric in arthur scope a metric in arthur is a function for evaluating model performance. these can be common functions that data scientists or ml teams are familiar with, such", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metric in arthur scope a metric in arthur is a function for evaluating model performance. these can be common functions that data scientists or ml teams are familiar with, such as accuracy or f1 score. or they can be functions specific to a model's use case, such as fairness metrics or user defined metrics. this means that any metric you can create in arthur ( including segmentations, filters, or logical functions ) can be transformed into an alert. for the latter, following the proper steps to create the metric within arthur for your specific model is important. this is because it must first be created for your model to alert on a metric", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the proper steps to create the metric within arthur for your specific model is important. this is because it must first be created for your model to alert on a metric. define threshold & bound for alert after creating your metric, it is time to decide what level of underperformance you would like to be alerted to. this numeric value is called the alert threshold. in arthur, alert threshold values have to be manually set. users must also define whether they want to be alerted when their function exceeds that numeric threshold. this is the upper or lower bound of the alert. define notification timelines for alert ( for streaming models ) for", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "when their function exceeds that numeric threshold. this is the upper or lower bound of the alert. define notification timelines for alert ( for streaming models ) for batch models, alerts are calculated per batch of data. however, teams that are running streaming models need to decide how often they would like alerts to be calculated and how much data. to make these decisions, they have to clarify two values : lookback period : how much data do they want to aggregate in their function? ( do they want to be alerted when the average of just one minute of data has passed the metric threshold, or do they only care if it '", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "function? ( do they want to be alerted when the average of just one minute of data has passed the metric threshold, or do they only care if it's affected the average of a day or week ). alert wait time : how often do you want to be alerted that something is happening? this is how often they would like the alert to be calculated ( and triggered if the function threshold is met ). creating alerts default alerts default data drift alerts for feature and prediction drift are automatically created for every feature once reference and inference data are sent to arthur. these alerts are created with dynamic threshold values specific to your reference", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and prediction drift are automatically created for every feature once reference and inference data are sent to arthur. these alerts are created with dynamic threshold values specific to your reference dataset from the \u201c data _ drift \u201d endpoint with \u201c metric \u201d : \u201c thresholds. \u201d defining custom alert rules in the ui the ui provides a clickable walk - through guide for teams to make common performance, drift, and data - bound alerts. the common practice of alerting on segments ( or filters ) of your data when calculating the metric is also added as an optional step. see below an example of defining an accuracy alert rule for women in the ui", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "filters ) of your data when calculating the metric is also added as an optional step. see below an example of defining an accuracy alert rule for women in the ui to be alerted every hour for the last 24 hours of inferences. defining custom alert rules with the python sdk our predefined alerting structure is not the only way teams can create alerts. alerts can be made for any customizable user defined metric teams create for their model in arthur. teams must first create the user - defined metric, and then they can easily set the alert from their python sdk notebook. alert notifications the latest alerts in", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "must first create the user - defined metric, and then they can easily set the alert from their python sdk notebook. alert notifications the latest alerts in your organization are shown on the homepage of your arthur organization. however, beyond being highlighted in the online arthur ui, alerts can be delivered to teams via email and / or via integrations such as pagerduty and slack. you can learn more about setting up those integrations here. alert endpoint in the api the dedicated alerts endpoint is / models / { model _ id } / alert _ rules. updated 3 months ago table of contents alert severity understanding", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the api the dedicated alerts endpoint is / models / { model _ id } / alert _ rules. updated 3 months ago table of contents alert severity understanding alert rules create a metric in arthur scope define threshold & bound for alert define notification timelines for alert ( for streaming models ) creating alerts default alerts defining custom alert rules in the ui defining custom alert rules with the python sdk alert notifications alert endpoint in the api source : https : / / docs. arthur. ai / docs / creating - alerts", "metadata": {"source": "https://docs.arthur.ai/docs/creating-alerts", "row": 79, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 80 text : airgap kubernetes cluster ( k8s ) install with cli jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwe", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsd", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##binary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functions", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##platform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - re", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##porting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by airgap kubernetes cluster ( k8s ) install with clisuggest editsif you prefer to install programmatically using cli only, follow the steps below. prepare a config. yaml file using the config template deploy the application by running the below kubectl command : shellkubectl kots install arthur", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". yaml file using the config template deploy the application by running the below kubectl command : shellkubectl kots install arthur \\ - - no - port - forward \\ - - namespace arthur \\ - - shared - password [ provide an admin console password ] \\ - - license - file. / license. yaml \\ - - config - values. / config. yaml \\ - - airgap - bundle. / arthur - x. x. x. airgap \\ - - kotsadm - registry [ your private container image repository ] \\ - - ko", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". / arthur - x. x. x. airgap \\ - - kotsadm - registry [ your private container image repository ] \\ - - kotsadm - namespace arthurai \\ - - registry - username [ read - write username ] \\ - - registry - password [ read - write password ] shared - password is the admin console password. installing a specific version of arthur to install a specific version of arthur, you would run the same command as above ( following the same steps to prepare the configuration ), with the inclusion of the - - app - version - label flag. this flag allows you to specify", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "above ( following the same steps to prepare the configuration ), with the inclusion of the - - app - version - label flag. this flag allows you to specify which specific version of arthur you want to install ( e. g., to set up a sandbox environment on the same version as production ). to determine which versions of arthur are available, you can run the following : kubectl kots get versions arthur - n < arthur namespace > updated 3 months ago table of contents installing a specific version of arthur source : https : / / docs. arthur. ai / docs / airgap - kuber", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ago table of contents installing a specific version of arthur source : https : / / docs. arthur. ai / docs / airgap - kubernetes - cluster - k8s - install - with - cli", "metadata": {"source": "https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli", "row": 80, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 81 text : sending historical data jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by sending historical datasuggest editsalthough optional, we've found that a useful step for teams after onboarding is to onboard historical model information to arthur. this allows teams to immediately begin to dig into their model data and explore trends in the data, even before new sending new inferences. to send historical data, we will need to do the following : collect historical data format it's timestamp information send it to your arthur model", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s. to send historical data, we will need to do the following : collect historical data format it's timestamp information send it to your arthur model collect historical data now, we want to collect all the historical data and inferences our model has run on. organizations may store this information in different tables, so you may need to query or merge information from wherever you store data. however, it is important to ensure that this information contains the following : all inputs / outputs expected by arthur model object : during model creation, we explored the format the arthur model object expects to receive information. to review, the arthur model object expects :", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "expected by arthur model object : during model creation, we explored the format the arthur model object expects to receive information. to review, the arthur model object expects : all feature inputs to the model predicted model outputs ( optional ) ground truth ( true label output ), if known ( optional ) any non - input attributes logged to the model if known inference timestamps : this is information about when the model ran the historical predictions. this is especially important to ensure that information logged into arthur is logged in for the correct time so that you can accurately evaluate trends and / or diagnose previous issues. additional inference identification : this is discussed further in", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is logged in for the correct time so that you can accurately evaluate trends and / or diagnose previous issues. additional inference identification : this is discussed further in detail in sending inferences, but this information includes inference identification like partner inference id or batch id ( for batch models ). format timestamps the arthur model object expects timestamps to be in datetime format. below is a quick example of how to format timestamps into the correct format, but here are some references about converting python information into datetime. pythonimport pytz from datetime import datetime # # use lambda function to convert to date time", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "some references about converting python information into datetime. pythonimport pytz from datetime import datetime # # use lambda function to convert to date time def get _ timestamps ( x ) : new _ time = x. split ( '.') [ 0 ] return datetime. strptime ( new _ time,'% y - % m - % d % h : % m : % s') historical _ df ['timestamps'] = historical _ df ['timestamps']. apply ( lambda x : get _ timestamps ( x ) ) send inference", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ps'] = historical _ df ['timestamps']. apply ( lambda x : get _ timestamps ( x ) ) send inferences to arthur now, we can send the inferences to arthur. we must separate the arthur model information from the timestamps to do this. we will see how this works below. although we use a streaming model, we can see a commented - out example of sending historical inferences as a batch. python # # send historical streaming data hist _ df = historical _ df. drop ( columns = ['timestamps'] ) arthur _ model.", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# send historical streaming data hist _ df = historical _ df. drop ( columns = ['timestamps'] ) arthur _ model. send _ inferences ( hist _ df, inference _ timestamps = historical _ df ['timestamps'] ) when sending historical batch data, it is important to remember that you need not only to send historical timestamp information but also unique batch ids. for teams that do not have unique batch ids already stored, a common technique will be to create a unique timestamp based on the frequency that you run batches historically. an", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "have unique batch ids already stored, a common technique will be to create a unique timestamp based on the frequency that you run batches historically. an example below is creating unique batches based on the day. ( i. e., all inferences with the same date will belong to the same batch ). changing this based on your batch frequency ( i. e., if you run data every hour, etc. ) is important. python # # create daily batch times historical _ df ['batch _ id'] = historical _ df ['timestamps']. apply ( lambda x : '", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "batch times historical _ df ['batch _ id'] = historical _ df ['timestamps']. apply ( lambda x :'batch _'+ x. strftime ('% m _ % d _ % y') ) batch _ df = historical _ df. drop ( columns = ['timestamps ','batch _ id'] ) # # send historical batch data arthur _ model. send _ inferences ( batch _ df, batch _ id = historical _ df ['batch _ id'], inference _ timestamps = historical _ df", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( batch _ df, batch _ id = historical _ df ['batch _ id'], inference _ timestamps = historical _ df ['timestamps'] ) key things to keep in mind alerts will backdate : alerts will still trigger any alerts set up on historical data. this can be useful to evaluate when an alert may have occurred in the past. however, it may be best to make notes explore, and then mark and conclude this information in the alerts section. this way, you can gain all the information from backdated alerts but have a cleaned and operable", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and conclude this information in the alerts section. this way, you can gain all the information from backdated alerts but have a cleaned and operable alerting homepage for new alerts your teams will need to act on. ui may show just a window : the ui within arthur is automatically set to show the last month of data. it can be easiest to go into the filter and select \" all time \" to ensure all your data has been sent to the platform. all enabled functionality should be available : you should be ready to explore your data using the arthur ui or query service with all your data and any special enrichments", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". all enabled functionality should be available : you should be ready to explore your data using the arthur ui or query service with all your data and any special enrichments you enabled during arthur model object creation. beyond arthur functionalities, when evaluating your historical data, it is also important to keep in mind that : your model will perform better on data it was trained on : while we encourage onboarding all the historical data you want to your model to view trends, teams often may not realize that this historical data consists of the data they used to train their more recent model. if that is the case, seeing higher performance for inferences included in your", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that this historical data consists of the data they used to train their more recent model. if that is the case, seeing higher performance for inferences included in your model training set is not unusual. we encourage teams to still onboard historical data to visualize trends in their feature set / performance overall, but remember to think critically when seeing high historical accuracy. updated 3 months ago table of contents collect historical data format timestamps send inferences to arthur key things to keep in mind source : https : / / docs. arthur. ai / docs / sending - historical - data", "metadata": {"source": "https://docs.arthur.ai/docs/sending-historical-data", "row": 81, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 82 text : querying explainability jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by querying explainabilitysuggest editsone of the most popular enrichments within arthur, explainability values, can also be queried with arthur's query language. teams looking to query explainability will pull their values from the enriched table in arthur. it's important to note that teams must also ensure that explainability has been enabled for their model ; otherwise, they will be unable to pull this information. tabular explainability for tab", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that teams must also ensure that explainability has been enabled for their model ; otherwise, they will be unable to pull this information. tabular explainability for tabular models, there are three ways to interact with and query explainability : local explainability : explainability is calculated for each inference in a model. global explainability : this aggregates all inferences'explainability values. regional explainability : this is a filtered aggregate of explainability values for a specified cohort ( or region ) of inferences. this is often used for comparison evaluation against the global explainability values or another cohort. local explainability the most common way", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or region ) of inferences. this is often used for comparison evaluation against the global explainability values or another cohort. local explainability the most common way to interact with local explainability is within the ui. however, teams that want to pull a local explanation score within a notebook often use the query below. in this query, we are filtering for a specific inference based on itspartner _ inference _ id, which is the name used for an organization's unique inference id. this is the most common way for teams to look up individual inferences. pythonquery = { \" select \" : [ { \" property \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "id. this is the most common way for teams to look up individual inferences. pythonquery = { \" select \" : [ { \" property \" : \" explainer _ algo \" }, { \" property \" : \" explainer _ predicted _ attribute \" }, { \" property \" : \" explainer _ attribute \" }, { \" property \" : \" explainer _ score \" } ], \" filter \" : [ { \" property \" : \" [ inference _ idpartner _ inference _ id ] \", \" comparator \" : \" eq \", \" value \" : \" < id > [ string", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ idpartner _ inference _ id ] \", \" comparator \" : \" eq \", \" value \" : \" < id > [ string ] \" } ], \" from \" : \" enriched \" } response = arthur _ model. query ( query ) response [ { \" explainer _ algo \" : \" lime \", \" explainer _ attribute \" : \" age \", \" explainer _ predicted _ attribute \" : \" prediction _ 0 \", \" explainer _ score \" : 0. 0016485283063465874 }, { \" explainer _ algo \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" explainer _ score \" : 0. 0016485283063465874 }, { \" explainer _ algo \" : \" lime \", \" explainer _ attribute \" : \" bill _ amt1 \", \" explainer _ predicted _ attribute \" : \" prediction _ 0 \", \" explainer _ score \" : 0. 0032036810960691183 }, { \" explainer _ algo \" : \" lime \", explainer _ attribute \" : \" bill _ amt2 \", \" explainer _ predicted _ attribute \" : \" prediction _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" lime \", explainer _ attribute \" : \" bill _ amt2 \", \" explainer _ predicted _ attribute \" : \" prediction _ 0 \", \" explainer _ score \" : 0. 002008238656596662 }, { \" explainer _ algo \" : \" lime \", \" explainer _ attribute \" : \" age \", \" explainer _ predicted _ attribute \" : \" prediction _ 1 \", \" explainer _ score \" : - 0. 0016485283063465892 }, { \" explainer _ algo \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "explainer _ score \" : - 0. 0016485283063465892 }, { \" explainer _ algo \" : \" lime \", \" explainer _ attribute \" : \" bill _ amt1 \", \" explainer _ predicted _ attribute \" : \" prediction _ 1 \", \" explainer _ score \" : - 0. 0032036810960691126 }, { \" explainer _ algo \" : \" lime \", \" explainer _ attribute \" : \" bill _ amt2 \", \" explainer _ predicted _ attribute \" : \" prediction", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" lime \", \" explainer _ attribute \" : \" bill _ amt2 \", \" explainer _ predicted _ attribute \" : \" prediction _ 1 \", explainer _ score \" : - 0. 0020082386565966667 } ] global explainability global explainability aggregates all the inferences sent to your production model. it is calculated by taking each inference's average value of all feature importance scores. teams can pull this information using the query below : python # # get global explanations explanation _ algo = model. explainability. explanation _ algo predicted _ class _ col =", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "information using the query below : python # # get global explanations explanation _ algo = model. explainability. explanation _ algo predicted _ class _ col = model. get _ positive _ predicted _ class ( ) global _ explanations = pd. dataframe ( model. query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ] } ) ). rename ( columns =", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ] } ) ). rename ( columns = {'explainer _ attribute':'feature ','global _ importance':'global _ importance'} ). sort _ values ( by ='global _ importance ', ascending = false ) feature global _ importance 0 pay _ 0 0. 092898 1 pay _ 2 0. 027737 2 limit _ bal 0. 025445 3 pay _ 3 0. 018831 4 pay _ amt1 0. 013233", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##737 2 limit _ bal 0. 025445 3 pay _ 3 0. 018831 4 pay _ amt1 0. 013233 5 bill _ amt1 0. 012518 6 pay _ amt2 0. 011359 7 pay _ amt3 0. 010639 8 bill _ amt2 0. 008891 9 bill _ amt6 0. 008058 10 pay _ 6 0. 007976 11 pay _ amt6 0. 007970 12 pay _ amt4 0. 007123 13 bill _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "007976 11 pay _ amt6 0. 007970 12 pay _ amt4 0. 007123 13 bill _ amt4 0. 006892 14 bill _ amt5 0. 006786 15 pay _ 5 0. 006765 16 bill _ amt3 0. 006730 17 pay _ 4 0. 006194 18 pay _ amt5 0. 004818 19 age 0. 004392 20 education 0. 002111 21 marriage 0. 001940 22 sex 0. 001188 teams also", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##18 19 age 0. 004392 20 education 0. 002111 21 marriage 0. 001940 22 sex 0. 001188 teams also often choose to plot these values using this plot function : pythonax = global _ explanations. set _ index ('feature'). plot ( kind ='bar') ax. set _ title ( \" global explanations \" ) ax. set _ ylabel ( \" feature importance \" ) regional explainability regional explainability is similar to global explainability, with additional filters applied to define your region or cohort of interest. this can be seen in the example below : query", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ability is similar to global explainability, with additional filters applied to define your region or cohort of interest. this can be seen in the example below : query request : pythonmodel. query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ], \" filter \" : [ { \" property \" : \" age \", \" comparator \" : \" gte \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "explanation _ algo } } ], \" filter \" : [ { \" property \" : \" age \", \" comparator \" : \" gte \", \" value \" : 18 }, { \" property \" : \" age \", \" comparator \" : \" lt \", \" value \" : 40 } ] } ) comparing regional importance to global importance pythondef regional _ compare _ global _ explainability ( model, regional _ filters, title ) : # # get model attributes explanation _ algo = model. explainability. explanation _ algo predicted _ class _ col = model. get _ positive _ predicted _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# get model attributes explanation _ algo = model. explainability. explanation _ algo predicted _ class _ col = model. get _ positive _ predicted _ class ( ) # # get regional explanations from filters regional _ explanations = pd. dataframe ( model. query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ], \" filter \" : regional _ filters } )", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ], \" filter \" : regional _ filters } ) ). rename ( columns = {'explainer _ attribute':'feature ','global _ importance':'regional _ importance'} ). sort _ values ( by ='regional _ importance ', ascending = false ) # # get global explanations global _ explanations = pd. dataframe ( model. query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ] } ) ). rename ( columns = {'explainer _ attribute':'feature ','global _ importance':'global _ importance'} ). sort _ values ( by ='global _ importance ', ascending = false ) # # combine dataframes explanations = global _ explanations", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "importance'} ). sort _ values ( by ='global _ importance ', ascending = false ) # # combine dataframes explanations = global _ explanations. merge ( regional _ explanations, left _ on ='feature ', right _ on ='feature') ax = explanations. set _ index ('feature'). plot ( kind ='bar') ax. set _ title ( title ) ax. set _ ylabel ( \" feature importance \" ) python # # running regional explainability filters = [ { \" property \" : \" age \", \" comparator \" : \" gte \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") python # # running regional explainability filters = [ { \" property \" : \" age \", \" comparator \" : \" gte \", \" value \" : 18 }, { \" property \" : \" age \", \" comparator \" : \" lt \", \" value \" : 40 } ] regional _ compare _ global _ explainability ( arthur _ model, filters, title = \" regional vs global \" ) comparing regional importance to regional importance pythondef regional _ compare _ regional _ explainability ( model, regional _ filters1, regional _ filters2, title ) : # # get model attributes explanation _ al", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "regional _ compare _ regional _ explainability ( model, regional _ filters1, regional _ filters2, title ) : # # get model attributes explanation _ algo = model. explainability. explanation _ algo predicted _ class _ col = model. get _ positive _ predicted _ class ( ) # # get regional explanations from filters regional _ explanations1 = pd. dataframe ( model. query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ], \" filter \" : regional _ filters1 } ) ). rename ( columns = {'explainer _ attribute':'feature ','global _ importance':'regional _ importance1'} ). sort _ values ( by ='regional _ importance1 ', ascending = false ) # # get regional explanations from filters regional _ explanations2 = pd. dataframe ( model", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( by ='regional _ importance1 ', ascending = false ) # # get regional explanations from filters regional _ explanations2 = pd. dataframe ( model. query ( { \" select \" : [ { \" function \" : \" regionalfeatureimportances \", \" alias \" : \" global _ importance \", \" parameters \" : { \" predicted _ attribute _ name \" : predicted _ class _ col, \" explanation _ algorithm \" : explanation _ algo } } ], \" filter \" : regional _ filters2 } ) ). rename ( columns = {'explainer _ attribute':'feature ',", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ], \" filter \" : regional _ filters2 } ) ). rename ( columns = {'explainer _ attribute':'feature ','global _ importance':'regional _ importance'} ). sort _ values ( by ='regional _ importance ', ascending = false ) # # combine dataframes explanations = regional _ explanations1. merge ( regional _ explanations2, left _ on ='feature ', right _ on ='feature') ax = explanations. set _ index ('feature'). plot ( kind ='bar') ax. set _ title ( title ) ax", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' ) ax = explanations. set _ index ('feature'). plot ( kind ='bar') ax. set _ title ( title ) ax. set _ ylabel ( \" feature importance \" ) python # # running regional explainability filters1 = [ { \" property \" : \" sex \", \" comparator \" : \" eq \", \" value \" : 1 } ] filters2 = [ { \" property \" : \" sex \", \" comparator \" : \" eq \", \" value \" : 2 } ] regional _ compare _ regional _ explainability ( arthur _ model, filters1", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "comparator \" : \" eq \", \" value \" : 2 } ] regional _ compare _ regional _ explainability ( arthur _ model, filters1, filters2, title = \" regional vs regional \" ) back to top nlp explainability the nlp _ explanation function can be used to query and filter explanations for tokens in nlp inferences. using this function, the user can filter and order tokens by importance. the following are available optional properties : nlp _ explanation. token - references a token within a specific inference. nlp _ explanation. location - references a token's absolute location within a specific", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "nlp _ explanation. token - references a token within a specific inference. nlp _ explanation. location - references a token's absolute location within a specific inference. nlp _ explanation. value - references a token's explanation value within a specific inference. query request : json { \" select \" : [ { \" function \" : \" nlp _ explanation \", \" alias \" : \" < alias _ name > [ optional ] \", \" parameters \" : { \" attribute _ name \" : \" < text _ input _ attribute _ name > [ string ] \", \" nlp _ predicted _ attribute \" : \" <", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" attribute _ name \" : \" < text _ input _ attribute _ name > [ string ] \", \" nlp _ predicted _ attribute \" : \" < predicted _ attribute _ name > [ string ] \", \" nlp _ explainer \" : \" [ limeshap ] \" } } ], \" filter \" : [ { \" property \" : \" nlp _ explanation. token \", \" comparator \" : \" eq \", \" value \" : \" < token > \" }, { \" property \" : \" nlp _ explanation. location \", \" comparator \" : \" eq \",", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" < token > \" }, { \" property \" : \" nlp _ explanation. location \", \" comparator \" : \" eq \", \" value \" : \" < location > \" } ], \" order _ by \" : [ { \" property \" : \" nlp _ explanation. value \", \" direction \" : \" desc \" } ] } query response : json { \" query _ result \" : [ { \" inference _ id \" : \" < id > [ string ] \", \" nlp _ explanation \" : [ { \" algorithm \" : \" [ limeshap ] \", \" predicted", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" < id > [ string ] \", \" nlp _ explanation \" : [ { \" algorithm \" : \" [ limeshap ] \", \" predicted _ attribute _ name \" : \" < predicted _ attribute _ name > [ string ] \", \" importance _ scores \" : [ { \" attribute _ name \" : \" < input _ attribute _ name > [ string ] \", \" tokens \" : [ { \" token \" : \" < token > [ string ] \", \" position \" : \" < position _ in _ text > [ int ] \", \" value \" : \" < explanation _ score > [ float ]", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" position \" : \" < position _ in _ text > [ int ] \", \" value \" : \" < explanation _ score > [ float ] \" } ] } ] } ] } ] } back to topupdated 3 months ago table of contents tabular explainability local explainability global explainability regional explainability nlp explainability source : https : / / docs. arthur. ai / docs / querying - explainability", "metadata": {"source": "https://docs.arthur.ai/docs/querying-explainability", "row": 82, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 83 text : binary classification jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by binary classificationsuggest editsbinary classification models predict a binary outcome ( i. e., one of two potential classes ). in arthur, these models fall into the classification category and are represented by the multiclass model type. some common examples of tabular binary classification are : is this credit card transaction fraud or not? will a customer click on an ad or not? frequently, these models output both a yes / no answer and a probability for", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "credit card transaction fraud or not? will a customer click on an ad or not? frequently, these models output both a yes / no answer and a probability for each ( i. e., prob _ yes and prob _ no ). these probabilities are then categorized into yes / no based on a threshold. in these cases, during onboarding, teams will supply their classification threshold and continuously track the class probabilities ( i. e., prob _ yes, prob _ no ). formatted data in arthur tabular binary classification models require three things to be specified in their schema : all predicting", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ yes, prob _ no ). formatted data in arthur tabular binary classification models require three things to be specified in their schema : all predicting model attributes ( or features ), predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( numeric or categorical ) attribute ( numeric or categorical ) probability of prediction aprobability of prediction bground truthnon - input attribute ( numeric or cat", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or categorical ) attribute ( numeric or categorical ) probability of prediction aprobability of prediction bground truthnon - input attribute ( numeric or categorical ) high school education34. 5. 95. 05amalegraduate degree44. 1. 86. 14bfemale predict function and mapping these are some examples of common values teams need to onboard for their binary classification models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. there are 3 options for formatting this, depending on your reference dataset. additionally, if teams wish to", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "your arthur environment to calculate default performance metrics. there are 3 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below are common examples of the required runnable predict function ( that outputs two values, the probability of each potential class ). prediction to ground truth mappingexample prediction functionexample prediction function with transformations # # option 1 : single prediction column, single ground truth column # map predictedvalue column to its corresponding groundtruth value. # this tells arthur that the ` pred _ proba", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "column, single ground truth column # map predictedvalue column to its corresponding groundtruth value. # this tells arthur that the ` pred _ proba _ credit _ default ` column represents # the probability that the ground truth column has the value 1 pred _ to _ ground _ truth _ map _ 1 = {'pred _ proba _ credit _ default': 1 } # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 1, ) # # option 2 : multiple prediction columns, single ground truth column # map each predictedvalue attribute to its corresponding groundtruth value. pred _ to _ ground _ truth _ map _ 2 = {'pred _ 0': 0,'pred _ 1': 1 } # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} # building the model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 2, positive _ predicted _ attr ='pred _ 1') # # option 3 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. pred _ to _ ground _ truth _ map _ 3 = {'pred _ 0':'gt _ 0 ','pred _ 1':", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ to _ ground _ truth _ map _ 3 = {'pred _ 0':'gt _ 0 ','pred _ 1':'gt _ 1'} # building the model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map _ 3, positive _ predicted _ attr ='pred _ 1') # example _ entrypoint. py sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) :", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "entrypoint. py sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( x ) # example _ entrypoint. py from utils import pipeline _ transformations sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( pipeline _ transformations ( x ) ) available metrics when onboarding tabular classification models, several default metrics are available to you within the ui. you can", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "pipeline _ transformations ( x ) ) available metrics when onboarding tabular classification models, several default metrics are available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics when teams onboard a binary classification model, the following metrics are automatically available in the ui ( out - of - the - box ). find out more about these metrics in the performance metrics section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformancepre", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##set. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. metricmetric typeaccuracy ratefairnesstrue positive rate ( equal opportunity ) fairnesstrue negative ratefairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##fairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments are able to be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxxxupdated 3 months ago what \u2019 s nextlearn more about how to", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": anomaly detectionhot spotsexplainabilitybias mitigationxxxxupdated 3 months ago what \u2019 s nextlearn more about how to interact with models, including binary classification, in arthurmodel onboardingmodel monitoring metric typestable of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / binary - classification", "metadata": {"source": "https://docs.arthur.ai/docs/binary-classification", "row": 83, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 84 text : deploying on amazon aws eks jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##registering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functions", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityex", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##netes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrest", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by deploying on amazon aws ekssuggest editsthis is a guide with additional steps to help you prepare your existing amazon elastic kubernetes service ( amazon eks ) cluster for installing the arthur platform. ensure the initial steps detailed installing arthur pre - requisites have already been applied to the cluster. configure eks ebs csi driver as of eks 1. 23, the amazon elastic", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "requisites have already been applied to the cluster. configure eks ebs csi driver as of eks 1. 23, the amazon elastic block store ( amazon ebs ) container storage interface ( csi ) driver needs to be installed explicitly. this driver allows eks clusters to manage the lifecycle of ebs volumes for persistent volumes. for more information, see amazon docs. if you are deploying arthur on eks 1. 23 +, you must follow the instructions on this page. verify that the add - on is successfully installed by navigating to aws console \u2192 eks \u2192 cluster \u2192 add -", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "follow the instructions on this page. verify that the add - on is successfully installed by navigating to aws console \u2192 eks \u2192 cluster \u2192 add - ons or by running helm list - a, depending on your installation method. optimizing the aws eks storageclass once the eks ebs csi driver is installed, you can take advantage of the gp3 storageclass type. this storageclass is more cost - effective and performant than the previous gp2 storageclass. apply the below yaml definition to your cluster : yaml { note } apiversion : storage. k8s. io / v1", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##2 storageclass. apply the below yaml definition to your cluster : yaml { note } apiversion : storage. k8s. io / v1 kind : storageclass metadata : annotations : storageclass. kubernetes. io / is - default - class : \" true \" name : gp3 parameters : type : gp3 encrypted : \" true \" # parameter ensures created aws ebs volumes are encrypted using aws managed kms key kmskeyid : < kms key arn > # optional parameter ensures created aws ebs volumes are encrypted using", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "managed kms key kmskeyid : < kms key arn > # optional parameter ensures created aws ebs volumes are encrypted using customer managed kms key provisioner : ebs. csi. aws. com reclaimpolicy : delete volumebindingmode : waitforfirstconsumer allowvolumeexpansion : true ensure there is * * _ only one _ * * default storageclass on the cluster. this is controlled by the ` storageclass. kubernetes. io / is - default - class ` annotation. supported aws service authentication mechanisms", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this is controlled by the ` storageclass. kubernetes. io / is - default - class ` annotation. supported aws service authentication mechanisms if using aws services with arthur, such as s3 or ses, you will need to configure arthur to authenticate with aws. arthur currently supports 3 authentication mechanisms : aws access keys access keys only work with s3. if you want to use access keys, you must provision an iam user and a set of keys. via aws iam, you will need to grant this user read / write access to the s3 storage bucket you", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##m user and a set of keys. via aws iam, you will need to grant this user read / write access to the s3 storage bucket you plan to use with arthur. selecting the access keys option will expand the blob storage section of the config page, where you will be able to enter your access key, secret access key id, and the s3 bucket. irsa we recommend using irsa to authenticate arthur with aws as it is the most secure and the only mechanism supporting ses. using this methodology will require some aws platform work in preparation for arthur. you can follow these aw", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is the most secure and the only mechanism supporting ses. using this methodology will require some aws platform work in preparation for arthur. you can follow these aws docs, which will show you how to do this setup via eksctl or the aws cli, or you can automate this via your internal infrastructure as code. the role you create will need s3 read / write privileges on the bucket you want to use with arthur and permissions to send email via your ses entity. example snippets are as below : sample iam policies we provide some sample iam policy snippets so they", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "via your ses entity. example snippets are as below : sample iam policies we provide some sample iam policy snippets so they can be referenced easily. sample iam policy for s3 access { \" statement \" : [ { \" action \" : [ \" s3 : putobject \", \" s3 : getobject \",... ], \" effect \" : \" allow \", \" resource \" : [ \" arn : aws : s3 : : : < insert - s3 - bucket - name > / * \", \" arn : aws : s3", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": aws : s3 : : : < insert - s3 - bucket - name > / * \", \" arn : aws : s3 : : : < insert - s3 - bucket - name > \" ],.... }, sample iam policy for ses access \" action \" : [ \" ses : sendtemplatedemail \", \" ses : sendemail \", \" ses : sendcustomverificationemail \", \" ses : sendbulktemplatedemail \", \" ses : sendbulkemail \", \" ses : send", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##email \", \" ses : sendbulktemplatedemail \", \" ses : sendbulkemail \", \" ses : sendbounce \" ], \" effect \" : \" allow \", \" resource \" : \" * \", \" sid \" : \" sessendemails \" }, this role will also need to have a trust relationship with the oidc provider of your eks cluster, specifying the arthur service accounts. see the linked docs above for a further explanation. an example snippet of this is : { \" version \" : \" 2012 - 10 - 17 \",", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the linked docs above for a further explanation. an example snippet of this is : { \" version \" : \" 2012 - 10 - 17 \", \" statement \" : [ { \" sid \" : \" \", \" effect \" : \" allow \", \" principal \" : { \" federated \" : \" arn : aws : iam : : 123456789012 : oidc - provider / oidc. eks. us - east - 2. amazonaws. com / id / abdcef...... \" }, \" action \" : \" sts :", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- east - 2. amazonaws. com / id / abdcef...... \" }, \" action \" : \" sts : assumerolewithwebidentity \", \" condition \" : { \" stringequals \" : { \" oidc. eks. us - east - 2. amazonaws. com / id / abdcef : sub \" : [ \" system : serviceaccount : < namespace > : arthurai - < namespace > \", \" system : serviceaccount : < namespace > : arthurai - < namespace > -", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> : arthurai - < namespace > \", \" system : serviceaccount : < namespace > : arthurai - < namespace > - helm - hook \" ] } } } ] } once this is all set up, you can pass this role to arthur via the config page. this sets the role in the arthur service accounts specified above, which enables arthur's pods to authenticate with aws via the role, and the permissions you created. be sure to use the exact formatting shown below : proceed to the blob storage section of the arthur config page to specify the s", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "created. be sure to use the exact formatting shown below : proceed to the blob storage section of the arthur config page to specify the s3 bucket ses to utilize aws ses for arthur - generated emails, you must configure irsa as outlined in the above section. once this is done, navigate to the email configuration section of arthur's config page. select aws ses, then enter the region in which your ses entity is configured. as outlined above, the role associated with the cluster must have permissions on this ses entity. if the ses entity is", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ses entity is configured. as outlined above, the role associated with the cluster must have permissions on this ses entity. if the ses entity is in the same account as your cluster, and you do not need to utilize a different role, such as for cross - account permissions, do not enter a role in the second box. if your ses entity is in another arthur, account, you must set up cross - account privileges between roles. in the account of your ses entity ( account a ), you must create an iam role ( role a ) that has sent email permissions to ses, as", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of your ses entity ( account a ), you must create an iam role ( role a ) that has sent email permissions to ses, as depicted above. role a will also need to have a trust relationship with either the account that your cluster is in ( account b ), the oidc provider on your cluster as depicted above, or the irsa role associated with your cluster. additionally, the irsa role you created above in account b, will also need to be granted sts assume role privileges on the role you are creating in account a. once all of this is set up, enter the role in the account", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "need to be granted sts assume role privileges on the role you are creating in account a. once all of this is set up, enter the role in the account that contains the ses entity ( account a ) that the irsa role should assume to send emails : updated 3 months ago table of contents configure eks ebs csi driver optimizing the aws eks storageclass supported aws service authentication mechanisms aws access keys irsa sample iam policies ses source : https : / / docs. arthur. ai / docs / deploying - on - amazon - aws - eks", "metadata": {"source": "https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks", "row": 84, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 85 text : fundamentals jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by fundamentalsunderstanding the basics of arthur query endpointssuggest editsarthur models are queried using a sql - like wrapper, so a working query's endpoints expect a body that contains the following keys : select ( required ) from ( optional ) subquery ( optional ) filter ( optional ) group _ by ( optional ) order _ by ( optional ) this page is going to go through these endpoints one - by -", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "optional ) filter ( optional ) group _ by ( optional ) order _ by ( optional ) this page is going to go through these endpoints one - by - one to explain what is happening, as well as practice querying some standard information about our model. select the select statement of our query allows us to choose what we want to grab out of our database. typically, when we are grabbing information about our model, we would want to grab one of the \" regular \" properties within the arthur database. these properties include : all of the model's attributes inference _ timestamp received _ timestamp inference _ id partner _ inference", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "within the arthur database. these properties include : all of the model's attributes inference _ timestamp received _ timestamp inference _ id partner _ inference _ id ground _ truth _ timestamp ( if ground truth is included ) batch _ id ( if batch model ) similar to sql, you can also select all of these attributes at once using the * string we can practice running the following query to select inference _ id pythonquery = { \" select \" : [ { \" property \" : \" inference _ id \" } ] } response = arthur _ model. query ( query ) response response [ {'inference _ id '", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" property \" : \" inference _ id \" } ] } response = arthur _ model. query ( query ) response response [ {'inference _ id':'509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b'}, {'inference _ id':'3392ac21 - 3cf9 - 48ad - 9e07 - e0343b76cc50'}, ] we can see that this grabbed all of the inference ids from the inference dataset. it is important to note that the inference dataset", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' }, ] we can see that this grabbed all of the inference ids from the inference dataset. it is important to note that the inference dataset is the default dataset to select from. if we wanted to grab more than one parameter at once in the query, we could do the following : pythonquery = { \" select \" : [ { \" property \" : \" inference _ id \" }, { \" property \" : \" inference _ timestamp \" } ] } response = arthur _ model. query ( query ) response [ {'inference _ id':'509f34eb - 19ae -", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ] } response = arthur _ model. query ( query ) response [ {'inference _ id':'509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b ','inference _ timestamp':'2022 - 11 - 10t15 : 22 : 14. 959z'}, {'inference _ id':'3392ac21 - 3cf9 - 48ad - 9e07 - e0343b76cc50 ','inference _ timestamp':'2022 - 11 -", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ad - 9e07 - e0343b76cc50 ','inference _ timestamp':'2022 - 11 - 10t15 : 23 : 14. 959z'}, ] [UNK] in select functionsteams often ask about running transformations in select functions, such as addition, subtraction, or \" or \" and \" and \" statements. more informationn about creating logic can be found in the transformation functions page. unique model types object detection computer vision models with an output type of object detection have some special fields you can use when querying. bounding boxes are sent using the", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model types object detection computer vision models with an output type of object detection have some special fields you can use when querying. bounding boxes are sent using the following form : [ class _ id, confidence, top _ left _ x, top _ left _ y, width, height ]. while the fields aren't named when sending data, you can access these nested fields when querying. here we can see : pythonquery = { \" select \" : [ { \" property \" : \" inference _ id \" }, { \" property \" : \" objects _ detected \" } ] } arthur _ model. query ( query", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" property \" : \" inference _ id \" }, { \" property \" : \" objects _ detected \" } ] } arthur _ model. query ( query ) response or, to grab each value within the bounding box individually : pythonquery = { \" select \" : [ { \" property \" : \" inference _ id \" }, { \" property \" : \" objects _ detected. class _ id \" } ] } arthur _ model. query ( query ) response [ {'inference _ id':'509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{'inference _ id':'509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b ','class _ id':'0'}, {'inference _ id':'3392ac21 - 3cf9 - 48ad - 9e07 - e0343b76cc50 ','class _ id':'1'}, ] generative text with token likelihoods tokenlikelihoods attributes yield two queryable columns for that attribute with suffixes \u201c _ tokens \u201d and \u201c _ likelihoods", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "text with token likelihoods tokenlikelihoods attributes yield two queryable columns for that attribute with suffixes \u201c _ tokens \u201d and \u201c _ likelihoods \u201d appended to the attribute's name. for example, a model with a tokenlikelihoods attribute named summary _ token _ probs yields two queryable columns : summary _ token _ probs _ tokens and summary _ token _ probs _ likelihoods which represent an array of the selected tokens and an array of their corresponding likelihoods. pythonquery = { \" select \" : [ { \" property \" : \" summary _ token _ probs _", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and an array of their corresponding likelihoods. pythonquery = { \" select \" : [ { \" property \" : \" summary _ token _ probs _ tokens \" }, { \" property \" : \" summary _ token _ probs _ likelihoods \" } ] } response [ { \" summary _ token _ probs _ likelihoods \" : [ 0. 3758265972137451, 0. 6563436985015869, 0. 32000941038131714, 0. 5629857182502747 ], \" summary _ token _ pro", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", 0. 32000941038131714, 0. 5629857182502747 ], \" summary _ token _ probs _ tokens \" : [ \" this \", \" is \", \" a \", \" summary \" ] } ] property types there are two property types within arthur. \" regular \" properties - listed above. these are properties available for any table queried. \" enriched \" properties - you must specify these by name to include them in the response and use the from value enriched : anomaly _ score lime _ importance shap _ importance we'll learn more about querying from", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to include them in the response and use the from value enriched : anomaly _ score lime _ importance shap _ importance we'll learn more about querying from different datasets next. from in the example above, we could pull data from the inference dataset. the inference dataset is the dataset that is most often queried and therefore set as the default. however, it is only one of the datasets from where we can pull data. all of our sources include : inference - the latest raw inference data sent to the platform. this is the default. enriched - every value from the inference data, with additional fields", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sources include : inference - the latest raw inference data sent to the platform. this is the default. enriched - every value from the inference data, with additional fields for anomaly scores and explanations. this data has some insert latency compared to the raw table. reference - the reference data set uploaded for the model. we will talk about the enriched dataset later. so, we can practice pulling an attribute from our reference dataset below pythonquery = { \" select \" : [ { \" property \" : \" age \" } ], \" from \" : \" reference \" } response = arthur _ model. query ( query ) response [ {", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" property \" : \" age \" } ], \" from \" : \" reference \" } response = arthur _ model. query ( query ) response [ {'age': 23 }, {'age': 48 } ] filter the next feature we can add to queries within arthur is filters. these allow us to specify a subset of data we are interested in from a database and grab only them. this filter command works similarly to where in traditional sql query queries. we create this specification by using comparators to create rules within the data. available comparators in arthur numerical comparison eq - filters where the property", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". we create this specification by using comparators to create rules within the data. available comparators in arthur numerical comparison eq - filters where the property field equals the value field. ne - filters where the property field is not equal to the value field. lt - filters where the property field is less than the value field. only valid for number values. gt - filters where the property field is greater than the value field. only valid for number values. lte - filters where the property field is less than or equal to the value field. only valid for number values. gte - filters where the property field is greater than", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "filters where the property field is less than or equal to the value field. only valid for number values. gte - filters where the property field is greater than or equal to the value field. only valid for number values. examplequery = { \" select \" : [ { \" property \" : \" * \" } ], \" filter \" : [ { \" property \" : \" inference _ id \", \" comparator \" : \" eq \", \" value \" : \" 509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b \" } ] } response [", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" 509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b \" } ] } response [ {'inference _ id':'509f34eb - 19ae - 4a18 - b454 - 6517fed4e76b'}, {'age':'22'}, { \" inference _ timestamp \" : \" 2020 - 07 - 22t10 : 01 : 23z \" }, { \" attr1 \" : \" something } ] the numerical comparators can also be used to investigate pythonquery =", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 23z \" }, { \" attr1 \" : \" something } ] the numerical comparators can also be used to investigate pythonquery = { \" select \" : [ { \" property \" : \" * \" }, { \" property \" : \" anomaly _ score \" } ], \" from \" : \" enriched \", \" filter \" : [ { \" property \" : \" inference _ timestamp \", \" comparator \" : \" gte \", \" value \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestam", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" value \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 07 - 22t11 : 00 : 00z \" } ] } response [ { \" inference _ id \" : \" 0001 \", \" attr1 \" : \" something \", \" anomaly _ score \" : 0. 34, \" inference _ timestamp \" : \" 2020 - 07 - 22t10 : 01 : 23z \" }, { \" inference _", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "0. 34, \" inference _ timestamp \" : \" 2020 - 07 - 22t10 : 01 : 23z \" }, { \" inference _ id \" : \" 0002 \", \" attr1 \" : \" something \", \" anomaly _ score \" : 0. 67, \" inference _ timestamp \" : \" 2020 - 07 - 22t10 : 02 : 55z \" } ] in in - filters where the property field is equal to any value in a list of possible values this comparator is typically used in count or rate functions to identify values that are in a set list of expected values.", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value in a list of possible values this comparator is typically used in count or rate functions to identify values that are in a set list of expected values. pythoncategory _ list = ['no college ','some college ','bachelors ','masters ','phd ', null ] query = { \" select \" : [ { \" function \" : \" rate \", \" alias \" : \" oldcatrate \", \" parameters \" : { \" property \" : feature _ of _ interest, \" comparator \" : \" in \", \" value \" : category _ list } } ] }", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" property \" : feature _ of _ interest, \" comparator \" : \" in \", \" value \" : category _ list } } ] } arthur _ model. query ( query ) response [ { \" oldcatrate \" : 0. 89 } ] like like - filters where the property field is like the value field. this filter is only valid for property types of unstructured text. python null values notnull - filters where the property field is not null. the value field should be empty. isnull - filters where the property field is null. the value field should be empty. python # # rate of", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "null. the value field should be empty. isnull - filters where the property field is null. the value field should be empty. python # # rate of null values in feature query = { \" select \" : [ { \" function \" : \" rate \", \" alias \" : \" marriagenullrate \", \" parameters \" : { \" property \" : \" marriage \", \" comparator \" : \" isnull \", \" value \" : \" \" } } ] } response = arthur _ model. query ( query ) response [ { \" marriagenullrate \" : 0. 89 } ] group by the group _", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] } response = arthur _ model. query ( query ) response [ { \" marriagenullrate \" : 0. 89 } ] group by the group _ byendpoint is used in queries to group rows based on one or more columns. it allows for data aggregation by specifying the criteria for grouping and can be used in conjunction with any of the different function types we allow in arthur queries. these function types are further discussed in querying functions, but at a high level, they contain : default performance metrics : all default performance metrics available within arthur transformation functions : functions done on the properties selected aggregation functions : provides", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "high level, they contain : default performance metrics : all default performance metrics available within arthur transformation functions : functions done on the properties selected aggregation functions : provides common data aggregation measures like average, sum, count, quantiles, etc. composing functions : this enables teams to create their own function logic pythonquery = { \" select \" : [ { \" function \" : \" count \", \" alias \" : \" count \" }, { \" property \" : \" age \" } ], \" filter \" : [ { \" property \" :'income ', \" comparator \" : \" gt \", \" value \" :", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" } ], \" filter \" : [ { \" property \" :'income ', \" comparator \" : \" gt \", \" value \" : 30000 } ], \" group _ by \" : [ { \" property \" : \" age \" } ], } response [ { \" count \" : 2500 }, { \" age \" : 25 } ] order by the order _ by endpoint enables teams to order the outputs of their query. within the order _ by command teams will specify what property they would like to sort the output by and in which direction. the options for direction are asc and desc for ascending", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by command teams will specify what property they would like to sort the output by and in which direction. the options for direction are asc and desc for ascending and descending, respectively. pythonquery = { \" select \" : [ { \" property \" : \" age \" }, { \" property \" : \" income \" } ], \" order _ by \" : [ { \" property \" : \" income \", \" direction \" : \" desc \" } } arthur _ model. query ( query ) subquery subqueries are a powerful feature of modern sql. the query endpoint can support subqueries via the sub", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model. query ( query ) subquery subqueries are a powerful feature of modern sql. the query endpoint can support subqueries via the subquery field in the request body. the format of the subquery is exactly the same as the full request body and can even support recursive subqueries! here are some helpful examples that show how to use them. concise queries with subqueries sometimes you may have a calculation that must be aggregated in multiple ways. one option would be to repeat the calculation in each aggregation \u2019 s select, which can lead to lots of repeated json. sub", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "aggregated in multiple ways. one option would be to repeat the calculation in each aggregation \u2019 s select, which can lead to lots of repeated json. subqueries can be used to reduce duplicated expressions. in this example, we use a subquery to square the property home _ value, then aggregate with min, max, and avg without repeating the calculation. query = { \" select \" : [ { \" function \" : \" max \", \" alias \" : \" max \", \" parameters \" : { \" property \" : { \" alias _ ref \" : \" hv _ squared \" } } }, {", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" max \", \" parameters \" : { \" property \" : { \" alias _ ref \" : \" hv _ squared \" } } }, { \" function \" : \" min \", \" alias \" : \" min \", \" parameters \" : { \" property \" : { \" alias _ ref \" : \" hv _ squared \" } } }, { \" function \" : \" avg \", \" alias \" : \" avg \", \" parameters \" : { \" property \" : { \" alias _ ref \" : \" hv _ squared \" } } } ], \" subquery \" : { \"", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { \" property \" : { \" alias _ ref \" : \" hv _ squared \" } } } ], \" subquery \" : { \" select \" : [ { \" function \" : \" multiply \", \" alias \" : \" hv _ squared \", \" parameters \" : { \" left \" : \" home _ value \", \" right \" : \" home _ value \" } } ] } } response [ { \" avg \" : 33413668226. 974968, \" max \" : 17640000000000, \" min \" : 0 } ] subqueries for", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##668226. 974968, \" max \" : 17640000000000, \" min \" : 0 } ] subqueries for grouping subqueries can also be used to perform operations on grouped data. in this example, we get the count of the inferences in each batch in the subquery, then average those counts. python { \" select \" : [ { \" function \" : \" avg \", \" alias \" : \" avg _ count \", \" parameters \" : { \" property \" : { \" alias _ ref \" : \" batch _ count \" } } }, { \"", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##g _ count \", \" parameters \" : { \" property \" : { \" alias _ ref \" : \" batch _ count \" } } }, { \" function \" : \" count \", \" alias \" : \" total _ batches \" } ], \" subquery \" : { \" select \" : [ { \" function \" : \" count \", \" alias \" : \" batch _ count \" }, { \" property \" : \" batch _ id \" } ], \" group _ by \" : [ { \" property \" : \" batch _ id \" } ] } } response [ { \" avg _ count \" :", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" group _ by \" : [ { \" property \" : \" batch _ id \" } ] } } response [ { \" avg _ count \" : 5930. 2558139534885, \" total _ batches \" : 86 } ] updated 3 months ago table of contents select unique model types property types from filter available comparators in arthur group by order by subquery concise queries with subqueries subqueries for grouping source : https : / / docs. arthur. ai / docs / fundamentals", "metadata": {"source": "https://docs.arthur.ai/docs/fundamentals", "row": 85, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 86 text : common queries quickstart jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by common queries quickstartsuggest editsto access information about a model's performance, drift, bias, or other enabled enrichments, write a query object and submit it with the arthur sdk using arthur _ model. query ( query ) for a general overview of this endpoint, including a more thorough description of its rules, power, and customizability, see the fundamentals. in each of the following examples, let", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "endpoint, including a more thorough description of its rules, power, and customizability, see the fundamentals. in each of the following examples, let our model be a binary classifier, and let gt1 and pred1 be the names of our model's ground truth attribute and predicted value. accuracy this is usually the simplest way to check for classifier performance. we can fetch a model's accuracy rate by querying a select on the function accuracyrate using the typical threshold 0. 5. given the following query : pythongt1 ='gt _ isfraud'pred1 ='pred _", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "using the typical threshold 0. 5. given the following query : pythongt1 ='gt _ isfraud'pred1 ='pred _ isfraud'query = { \" select \" : [ { \" function \" : \" accuracyrate \", \" parameters \" : { \" threshold \" : 0. 5, \" ground _ truth _ property \" : gt1, \" predicted _ property \" : pred1 } } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'accuracyrate': 0. 99902694736842", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ model. query ( query ) the query _ result will be : python [ {'accuracyrate': 0. 999026947368421 } ] accuracy by batch to expand the accuracy query by batch, add the batch _ id property to the query's select, and add a group _ by to the query using batch _ id. given the following query : pythongt1 ='gt _ isfraud'pred1 ='pred _ isfraud'query = { \" select \" : [ { \" function \" : \" accuracyrate \", \" parameters \" : { \" threshold \" :", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ isfraud'query = { \" select \" : [ { \" function \" : \" accuracyrate \", \" parameters \" : { \" threshold \" : 0. 5, \" ground _ truth _ property \" : gt1, \" predicted _ property \" : pred1 } }, { \" property \" : \" batch _ id \" } ], \" group _ by \" : [ { \" property \" : \" batch _ id \" } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'accuracyrate': 0. 999704,'batch _", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur _ model. query ( query ) the query _ result will be : python [ {'accuracyrate': 0. 999704,'batch _ id':'newbatch3'}, {'accuracyrate': 0. 999744,'batch _ id':'newbatch0'}, {'accuracyrate': 0. 992952,'batch _ id':'newbatch19'}, {'accuracyrate': 0. 999616,'batch _ id':'newbatch5'}, {'accuracyrate': 0", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' accuracyrate': 0. 999616,'batch _ id':'newbatch5'}, {'accuracyrate': 0. 999144,'batch _ id':'newbatch6'},... ] batch ids querying accuracy by batch includes the batch _ id values in the query result. but to query the batch _ ids on their own, only select and group _ by the batch _ id. given the following query : pythonquery = { \" select \" : [ { \" property \" : \" batch _ id \" } ], \" group _", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". given the following query : pythonquery = { \" select \" : [ { \" property \" : \" batch _ id \" } ], \" group _ by \" : [ { \" property \" : \" batch _ id \" } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'batch _ id':'newbatch19'}, {'batch _ id':'newbatch18'}, {'batch _ id':'newbatch13'}, {'batch _ id':'newbatch12 '", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##18'}, {'batch _ id':'newbatch13'}, {'batch _ id':'newbatch12'}, {'batch _ id':'newbatch16'},... ] accuracy ( single batch ) to query the accuracy for only one batch, add a filter to the query according to the rule batch _ id = = batchname given the following query ( for a specified batch name ) : pythongt1 ='gt _ isfraud'pred1 ='pred _ isfraud'batchname = \" newbatch19 \"", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "pythongt1 ='gt _ isfraud'pred1 ='pred _ isfraud'batchname = \" newbatch19 \" query = { \" select \" : [ { \" function \" : \" accuracyrate \", \" parameters \" : { \" threshold \" : 0. 5, \" ground _ truth _ property \" : gt1, \" predicted _ property \" : pred1 } }, { \" property \" : \" batch _ id \" } ], \" group _ by \" : [ { \" property \" : \" batch _ id \" } ], \" filter \" : [ { \" property \"", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ], \" group _ by \" : [ { \" property \" : \" batch _ id \" } ], \" filter \" : [ { \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : batchname } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'accuracyrate': 0. 992952,'batch _ id':'newbatch19'} ] confusion matrix a confusion matrix counts the number of true positive, true negative, false positive, and false negative classifications", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' :'newbatch19'} ] confusion matrix a confusion matrix counts the number of true positive, true negative, false positive, and false negative classifications ; knowing these values is usually more useful than just accuracy when it is time to improve your model. to query a confusion matrix, we use the confusionmatrix function in our query's select. [UNK] the confusionmatrix function, the ground _ truth _ property and predicted _ property parameters are optional. given the following query : pythonquery = { \" select \" : [ { \" function \" : \" confusionmatrix \", \" parameters \" : { \" threshold \" :", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query : pythonquery = { \" select \" : [ { \" function \" : \" confusionmatrix \", \" parameters \" : { \" threshold \" : 0. 5 } } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'confusionmatrix': {'false _ negative': 4622,'false _ positive': 0,'true _ negative': 4745195,'true _ positive': 183 } } ] confusion matrix ( single batch ) as we did with accuracy, to get a confusion matrix for a single batch we", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' true _ positive': 183 } } ] confusion matrix ( single batch ) as we did with accuracy, to get a confusion matrix for a single batch we add the property batch _ id to the query's select, add a group _ by using batch _ id, and then add a filter according to the rule batch _ id = = batchname given the following query ( for a specified batch name ) : pythonbatchname ='newbatch19'query = { \" select \" : [ { \" function \" : \" confusionmatrix \", \" parameters \" : { \" threshold \" : 0. 5 } },", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" select \" : [ { \" function \" : \" confusionmatrix \", \" parameters \" : { \" threshold \" : 0. 5 } }, { \" property \" : \" batch _ id \" } ], \" group _ by \" : [ { \" property \" : \" batch _ id \" } ], \" filter \" : [ { \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : batchname } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'batch _ id '", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "batchname } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'batch _ id':'newbatch19 ','confusionmatrix': {'false _ negative': 1762,'false _ positive': 0,'true _ negative': 248238,'true _ positive': 0 } } ] confusion matrix ( by group ) instead of querying for metrics and grouping by batch, we can group by other groupings as well. here, we use the model's non - input attribute race so that we can", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and grouping by batch, we can group by other groupings as well. here, we use the model's non - input attribute race so that we can compare model performance across different demographics. to do this, we add the group name race to our query's select and to its group _ by given the following query : pythongroup ='race'query = { \" select \" : [ { \" function \" : \" confusionmatrix \", \" parameters \" : { \" threshold \" : 0. 5 } }, { \" property \" : group } ], \" group _ by \" : [ { \" property \" : group", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" threshold \" : 0. 5 } }, { \" property \" : group } ], \" group _ by \" : [ { \" property \" : group } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'confusionmatrix': {'false _ negative': 1162,'false _ positive': 0,'true _ negative': 1184707,'true _ positive': 44 },'race':'hispanic'}, {'confusionmatrix': {'false _ negative': 1145, '", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' : 44 },'race':'hispanic'}, {'confusionmatrix': {'false _ negative': 1145,'false _ positive': 0,'true _ negative': 1186659,'true _ positive': 49 },'race':'asian'}, {'confusionmatrix': {'false _ negative': 1137,'false _ positive': 0,'true _ negative': 1187500,'true _ positive': 38 },'race':'black'}, {'confusionmatrix': {", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' : 1187500,'true _ positive': 38 },'race':'black'}, {'confusionmatrix': {'false _ negative': 1178,'false _ positive': 0,'true _ negative': 1186329,'true _ positive': 52 },'race':'white'} ] predictions here we aren't querying any metrics - we are just accessing all the predictions that have output by the model. given the following query : pythonpred1 ='pred _ isfraud'query = { \" select", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the predictions that have output by the model. given the following query : pythonpred1 ='pred _ isfraud'query = { \" select \" : [ { \" property \" : pred1 } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'pred _ isfraud _ 1': 0. 005990342859493804 }, {'pred _ isfraud _ 1': 0. 02271116879043313 }, {'pred _ isfraud _", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##d _ isfraud _ 1': 0. 02271116879043313 }, {'pred _ isfraud _ 1': 0. 15305224676085477 }, {'pred _ isfraud _ 1': 0 }, {'pred _ isfraud _ 1': 0. 03280797449330532 },... ] predictions ( average ) to only query the average value across all these predictions ( since querying all predictions and then averaging locally can be slow for production - sized query", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "predictions ( average ) to only query the average value across all these predictions ( since querying all predictions and then averaging locally can be slow for production - sized query results ), we only need to add the avg function to our query's select, with our predicted value pred1 now being a parameter of avg instead of a property we directly select. given the following query : pythonpred1 ='pred _ isfraud'query = { \" select \" : [ { \" function \" : \" avg \", \" parameters \" : { \" property \" : pred1 } } ] } query _ result", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : [ { \" function \" : \" avg \", \" parameters \" : { \" property \" : pred1 } } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'avg': 0. 016030786000398464 } ] predictions ( average over time ) to get the average predictions on each day, we add the function roundtimestamp to our select using a time _ interval of day - this groups the timestamp information according to day instead of options like hour or week. then, we add a", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "select using a time _ interval of day - this groups the timestamp information according to day instead of options like hour or week. then, we add a group _ by to the query using the alias ( day ) specified in the roundtimestamp function. given the following query : pythonpred1 ='pred _ isfraud'query = { \" select \" : [ { \" function \" : \" avg \", \" parameters \" : { \" property \" : pred1 } }, { \" function \" : \" roundtimestamp \", \" alias \" : \" day \", \" parameters \" :", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : pred1 } }, { \" function \" : \" roundtimestamp \", \" alias \" : \" day \", \" parameters \" : { \" property \" : \" inference _ timestamp \", \" time _ interval \" : \" day \" } } ], \" group _ by \" : [ { \" alias \" : \" day \" } ] } query _ result = arthur _ model. query ( query ) the query _ result will be : python [ {'avg': 0. 016030786000359423,'day':'2022 - 07 - 11", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ {'avg': 0. 016030786000359423,'day':'2022 - 07 - 11t00 : 00 : 00z'}, {'avg': 0. 018723459201003300,'day':'2022 - 07 - 12t00 : 00 : 00z'}, {'avg': 0. 014009919280009284,'day':'2022 - 07 - 13t00 : 00 : 00z'}, {'avg '", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##0009284,'day':'2022 - 07 - 13t00 : 00 : 00z'}, {'avg': 0. 016663649020394829,'day':'2022 - 07 - 14t00 : 00 : 00z'}, {'avg': 0. 017791902929210039,'day':'2022 - 07 - 15t00 : 00 : 00z'},... ] updated 3 months ago table of contents accuracy accuracy by batch batch ids accuracy (", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- 15t00 : 00 : 00z'},... ] updated 3 months ago table of contents accuracy accuracy by batch batch ids accuracy ( single batch ) confusion matrix confusion matrix ( single batch ) confusion matrix ( by group ) predictions predictions ( average ) predictions ( average over time ) source : https : / / docs. arthur. ai / docs / query - quickstart", "metadata": {"source": "https://docs.arthur.ai/docs/query-quickstart", "row": 86, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 87 text : bias mitigation jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by bias mitigationpost process technique to determine the fairest thresholdsuggest editsif you are interested in bias mitigation capabilities, we \u2019 re happy to discuss your needs and what approaches would work best for you. within arthur scope, we offer postprocessing methods ; we encourage exploring alternate ( pre - or in - processing ) methods if your data science team has the bandwidth to do so. our currently available postprocessing", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "we encourage exploring alternate ( pre - or in - processing ) methods if your data science team has the bandwidth to do so. our currently available postprocessing method for use is the threshold mitigator. it automatically evaluates for demographic parity, equalized odds, and equal opportunity constraints. enabling bias mitigation will automatically train a mitigation model for all attributes marked as. monitor _ for _ bias = true, for the constraints of demographic parity, equalized odds, and equal opportunity. the onlys of bias mitigation bias mitigation is an enrichment in arthur with a few only's. bias", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ized odds, and equal opportunity. the onlys of bias mitigation bias mitigation is an enrichment in arthur with a few only's. bias mitigation is only available for binary classification models it can only be enabled if at least one model attribute is marked as. monitor _ for _ bias = true it is the only enrichment that is only available in the python sdk. this also means that it is the only enrichment you run in a notebook each time you want to use it. bias mitigation with the python sdk as mentioned above, bias mitigation is only available through our python sdk. here", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "want to use it. bias mitigation with the python sdk as mentioned above, bias mitigation is only available through our python sdk. here is an example notebook that we have put together on how to use the bias mitigation capabilities : bias mitigation notebook on arthur github understanding the algorithm to learn more about the algorithm used for bias mitigation. please refer to the arthur algorithms documentation section. updated 3 months ago what \u2019 s nextlearn about enabling enrichments in the model onboarding sectionenabling enrichmentstable of contents the onlys of bias mitigation bias mitigation with the", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##rn about enabling enrichments in the model onboarding sectionenabling enrichmentstable of contents the onlys of bias mitigation bias mitigation with the python sdk understanding the algorithm source : https : / / docs. arthur. ai / docs / bias - mitigation", "metadata": {"source": "https://docs.arthur.ai/docs/bias-mitigation", "row": 87, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 88 text : welcome to platform administration jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by welcome to platform administrationsuggest editsthe second half of our documentation is dedicated to platform administration resources. platform administration isupdated 3 months ago source : https : / / docs. arthur. ai / docs / welcome - to - platform - administration", "metadata": {"source": "https://docs.arthur.ai/docs/welcome-to-platform-administration", "row": 88, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 89 text : examples jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by examplesget started with arthur scope using one our many examplessuggest editsthroughout our website, documentation, public github, and youtube channel, we are dedicated to providing useful and intuitive examples of how to use the arthur platform. core example groups : we have broken out our core examples resources into four sections : model onboarding notebook tutorials interested in something else? : here are some of our other most commonly referred to references : arthur glossary", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "resources into four sections : model onboarding notebook tutorials interested in something else? : here are some of our other most commonly referred to references : arthur glossary arthur youtube channel [UNK]'t find what you're looking for? if you can't find what you're looking for, please let us know either through the feedback section in the top right corner of this documentation website or submit a page review at the bottom of this page. model onboarding getting started - quickstart guides exampleresource pagequickstart ( tabular ) quickstartquickstart nlpnlp onboardingquickstar", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##start guides exampleresource pagequickstart ( tabular ) quickstartquickstart nlpnlp onboardingquickstart computer visioncv onboardingquickstart generative text ( llm ) generative textonboarding model with apiregistering a model with the api model input / output type model typebatchstreamingtabular binary classificationcredit risk batch examplecredit risk exampletabular multi - class classificationtabular regressionboston housing examplenlp binary classificationnlp multi - class classificationmedical transcript examplenlp regressionnlp - generative text ( ll", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tabular regressionboston housing examplenlp binary classificationnlp multi - class classificationmedical transcript examplenlp regressionnlp - generative text ( llm ) openai examplescv binary classificationsatelite image examplecancer classification examplecv multi - class classificationcv regressioncv object detectionmars rover exampletime series ranked list ( recommender system ) partnerships and integrations nameresource pagespark mlspark mlsagemaker data capturesagemakerlangchainlangchain extra model onboarding tutorials namedescriptionresource page or notebooktesting arthur explainer locallybefore enabling explainability", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##nlangchain extra model onboarding tutorials namedescriptionresource page or notebooktesting arthur explainer locallybefore enabling explainability, teams can test their explainer locally. ensure that it will work when onboarded to arthur. test explainability locallyconnecting arthur model object in notebookteams must first connect to the arthur platform to run any functionality in a notebook. creating a connection to arthuronboarding historical data to arthurwant to backfill your new model on arthur with existing data? onboard historical inferencessending historical datamodel versioningexample of creating mutliple model versions", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##fill your new model on arthur with existing data? onboard historical inferencessending historical datamodel versioningexample of creating mutliple model versionsmodel versioning notebook notebook tutorials metrics and alerting namenotebookcreate custom metrics in a notebookcoming sooncreate custom alerts in a notebookcoming soondeleting alerts in a notebookcoming soon enrichments namedescriptionnotebookbias mitigationrun our bias mitigation enrichment in a notebookbias mitigation notebook querying guides namedescriptionnotebook or resource pagequery guide notebookoverview of querying", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tion enrichment in a notebookbias mitigation notebook querying guides namedescriptionnotebook or resource pagequery guide notebookoverview of querying in a notebookquery jumpstart notebookquerying explainabilityquery multiple levels of explainability in a notebookquerying explainabilityquerying data driftquery data drift comparing different distributionsquery data driftupdated 2 months ago table of contents model onboarding getting started - quickstart guides model input / output type partnerships and integrations extra model onboarding tutorials notebook tutorials metrics and alerting enrichments querying guides source : https :", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model input / output type partnerships and integrations extra model onboarding tutorials notebook tutorials metrics and alerting enrichments querying guides source : https : / / docs. arthur. ai / docs / examples", "metadata": {"source": "https://docs.arthur.ai/docs/examples", "row": 89, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 90 text : time series onboarding jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by time series onboardingsuggest editsthis page walks through the basics of setting up a time series input model and onboarding it to arthur scope to monitor performance. getting started the first step is to import functions from the arthurai package and establish a connection with arthur scope. python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url =", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \" ) registering a time series model each time series model is created with a name and with input _ type = inputtype. timeseries. here, we register a time series model : pythonarthur _ model = arthur. model ( name = \" recsysquickstart \", input _ type = inputtype. timeseries, model", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##arthur _ model = arthur. model ( name = \" recsysquickstart \", input _ type = inputtype. timeseries, model _ type = outputtype. rankedlist ) formatting reference / inference data column names can contain only alphanumeric and underscore characters. time series data can be uploaded to arthur either in a dataframe or a json file. typically, a json file is a more natural formatting for time series data. for a time series model tracking credit card balance over time as input, the reference data might look like this : json { \" reference _ data \" :", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". for a time series model tracking credit card balance over time as input, the reference data might look like this : json { \" reference _ data \" : { \" credit _ card _ balance \" : [ { \" timestamp \" : \" 2023 - 10 - 05t00 : 00 : 00z \", \" value \" : 3004. 18 }, { \" timestamp \" : \" 2023 - 10 - 06t00 : 00 : 00z \", \" value \" : 150. 19 } ], \" id \" : \" 6euqxgjai11qr0genggvgh", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" value \" : 150. 19 } ], \" id \" : \" 6euqxgjai11qr0genggvgh \", \" account _ id \" : \" 8klqsgjil78qr4gljkklsy \" },... / / more inferences here } data requirements arthur requires that all times will be present in a given series according to a regular interval ( eg. one value each day ). there is an upper bound of 500 timestamps in a single time series inference. reviewing the model schema before you register your model with", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "each day ). there is an upper bound of 500 timestamps in a single time series inference. reviewing the model schema before you register your model with arthur by calling arthur _ model. save ( ), you can call arthur _ model. review ( ) on the model schema to check that your data was parsed correctly in your call to arthur _ model. build ( ). for a time series model, the model schema should look like this : python name stage value _ type categorical is _ unique 0 time _ series _ attr pipeline _ input time _ series false true 1 non _ input _ 1 non _", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "stage value _ type categorical is _ unique 0 time _ series _ attr pipeline _ input time _ series false true 1 non _ input _ 1 non _ input _ data float false false... finishing onboarding once you have finished formatting your reference data and your model schema looks correct using arthur _ model. review ( ), you are finished registering your model and its attributes - so you are ready to complete onboarding your model. see this guide for further details on how to save your model, send inferences, and get performance results from arthur. these steps are the same for time series models as for models of any", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "on how to save your model, send inferences, and get performance results from arthur. these steps are the same for time series models as for models of any inputtype and outputtype. updated about 2 months ago table of contents getting started registering a time series model formatting reference / inference data data requirements reviewing the model schema finishing onboarding source : https : / / docs. arthur. ai / docs / ranked - list - output - onboarding", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-output-onboarding", "row": 90, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 91 text : user - defined metrics jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by user - defined metricstrack and communicate the unique ways your internal team or external stakeholders define performancesuggest editsbeyond arthur's ui capabilities and apis, arthur's python sdk has a built - in query functionality. this sql - like query structure allows teams the ability to turn any functions they create into alert - able metrics within the arthur platform. pieces of user - defined metrics there are three pieces to define", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the ability to turn any functions they create into alert - able metrics within the arthur platform. pieces of user - defined metrics there are three pieces to define when creating user - defined metrics : metric name : the name refers to how the user identifies or calls the metric. metric type : the arthur platform has four unique metric types. these are : metric typeperformancedatadriftdataboundmodeloutput typically, arthur infers the type of metric from the function provided. however, for more advanced metrics that arthur cannot infer or for teams that want to ensure a specific metric type, teams should define the", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the function provided. however, for more advanced metrics that arthur cannot infer or for teams that want to ensure a specific metric type, teams should define the specific metric type. arthur query function : finally, the most important aspect of defining a metric is creating the mathematical function the metric will track. these functions are built with the arthur query structure. more about building out this arthur query function can be found below : building an arthur query function a more in - depth querying guide can be found in the query section. however, there are some key things to keep in mind : the query must be built with the information contained within arthur (", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can be found in the query section. however, there are some key things to keep in mind : the query must be built with the information contained within arthur ( per inference ). additional information beyond model inputs ( features ) and outputs ( predictions ) can be added to arthur as non - input attributes. the query should return a single value. for example, the query should not return a row for each inference, time - series data, or score for multiple attributes. the query should generally not include filters unless it is to define a very specific segment you wish to track ( and not a further filter ). this is because filters can easily be", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "generally not include filters unless it is to define a very specific segment you wish to track ( and not a further filter ). this is because filters can easily be defined when evaluating a metric. keeping the metric definition general allows different segmentations to be easily applied. the query may include parameters, which are denoted by { { param _ name } } template values in the query definition and have corresponding entries in the parameters field on the metric definition. custom data drift metrics of note for teams looking to create custom metrics for data drift, data drift uses a special query structure. so, these metrics would need to be defined with this", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for teams looking to create custom metrics for data drift, data drift uses a special query structure. so, these metrics would need to be defined with this special structure. teams also need to add in is _ data _ drift = true to their definition. defining metrics examples using python sdk we typically recommend that teams create custom metrics within a notebook with our python sdk. this is because teams can easily craft and validate their queries in a notebook before onboarding them to arthur. follow this notebook example to learn more : coming soon using api you can define a custom metric for your model by sending a post request to", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ing them to arthur. follow this notebook example to learn more : coming soon using api you can define a custom metric for your model by sending a post request to the dedicated metrics endpoint at / models / { model _ id } / metrics. check out the api reference guide for the full specification. updated 3 months ago table of contents pieces of user - defined metrics building an arthur query function defining metrics examples using python sdk using api source : https : / / docs. arthur. ai / docs / custom", "metadata": {"source": "https://docs.arthur.ai/docs/custom", "row": 91, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 92 text : oidc jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by oidcsuggest [UNK] configurations are only supported in on - prem arthur installations this page provides a walk - through for how to configure your arthur installation to work with an oidc compatible idp. in order to complete this guide, you need administrator access to your idp and access to your arthur installation's admin console configuration. additionally, you will either need access to the arthur superadmin user or be able to", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##p and access to your arthur installation's admin console configuration. additionally, you will either need access to the arthur superadmin user or be able to assume a role in your idp to give yourself rbac management permissions in arthur. this guide will walk through the following steps : configure the idp user groups and oidc token claims configure the idp oidc settings configure arthur to work with your idp apply the arthur idp yaml configuration create organization user roles to match the idp user groups test access cleaning up 1. configure the idp", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apply the arthur idp yaml configuration create organization user roles to match the idp user groups test access cleaning up 1. configure the idp user groups and oidc token claims in order to properly map users to permissions, arthur requires a claim in your oidc json web token ( jwt ) that contains information about the group memberships of the user. each group in the idp should correspond to a role in arthur's { doc } custom _ rbac permission system. this process can vary depending on your idp, but most idp's should have a user grouping mechanism, and", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} custom _ rbac permission system. this process can vary depending on your idp, but most idp's should have a user grouping mechanism, and a mechanism to configure attributes in the jwt claims. for example using okta, admins can configure the jwt claims to include group information under their account's security - > api - > default - > claims then the \" add claim \" button. from the popup, give the claim a name, \" groups \" in this example, set the \" include in token type \" to \" access token \", select \" value type \" as", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the claim a name, \" groups \" in this example, set the \" include in token type \" to \" access token \", select \" value type \" as \" groups \", and include a \" matches regex \" filter to select the groups to include : retrieving an example oidc token is idp - specific and may involve completing the sign - in flow via a script or api client. an alternative can be to use a 3rd party like https : / / oidcdebugger. com, but that will require whitelisting https : / / oidcdebugger. com / debug", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "oidcdebugger. com, but that will require whitelisting https : / / oidcdebugger. com / debug as a valid redirect url for your arthur sso app ( this could be enabled temporarily for debugging, then removed ). here is an example jwt after setting the group claims field : json { \" iss \" : \" https : / / dev. okta. com / oauth2 / default \", \" aud \" : \" api : / / default \", \" scp \" : [ \" openid \" ], \" groups", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##2 / default \", \" aud \" : \" api : / / default \", \" scp \" : [ \" openid \" ], \" groups \" : [ \" idp - admin \", \" org - 1 - model - owner \", \" org - 2 - model - owner \" ], \" firstname \" : \" john \", \" lastname \" : \" doe \", \" login \" : \" [ email protected ] \" } as the example token shows, the user's groups in the idp are populated as a string list in the \" groups \" field in the token. arthur will use", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "example token shows, the user's groups in the idp are populated as a string list in the \" groups \" field in the token. arthur will use this list of groups to match the user to the corresponding roles in arthur by name. 2. configure the idp oidc settings in order for your idp to speak to arthur, it needs to know where to find it. enter the following url in your idp's configuration to whitelist arthur's callback endpoint ( sign - in redirect url ) : https : / / < your arthur host > / login /", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##list arthur's callback endpoint ( sign - in redirect url ) : https : / / < your arthur host > / login / callback. additionally, the idp will need to know what oidc protocol to speak with arthur. today, arthur supports two protocol flows : implicit pkce both flows are intended to be used with \" single page applications \" or spas. follow the configuration for your idp that matches one of those two flows with spas. additionally, note the following settings from your idp in order to use in the arthur configuration below : client id resource id ( if available", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "flows with spas. additionally, note the following settings from your idp in order to use in the arthur configuration below : client id resource id ( if available ) oidc flow ( pkce or implicit ) audience ( value that is set in the token by the idp ) 3. configure arthur to work with your idp next, arthur needs to know how to handshake with your idp. to do that, arthur requires the following information : your idp's discovery url, typically in the format < idp path > /. well - known / openid - configuration url that", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": your idp's discovery url, typically in the format < idp path > /. well - known / openid - configuration url that contains the relevant endpoints for your idp. { note } if this page isn't accessible to arthur due to cors or other restrictions, the values can be provided manually. one or more idp administrator user groups that will be paired to global custom roles in arthur ( see here for a description of what these are for ) an understanding of your oidc token claims ( values ) and how to parse user information out of it the four configuration values captured above from", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are for ) an understanding of your oidc token claims ( values ) and how to parse user information out of it the four configuration values captured above from your idp with that information available, it is possible to fill out arthur's idp configuration yaml. the next subsections explain each subsection of the arthur yaml configuration, and is followed by some complete examples further down. configuring the idps discovery url almost all oidc idp's have accessible discovery urls, but some do not provide cors support for them, so their contents need to be filled out manually. to", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "idp's have accessible discovery urls, but some do not provide cors support for them, so their contents need to be filled out manually. to support either option, arthur has two configurations that can be used, discoverybaseurl or endpointoverrides. if your idp has cors restrictions, see appendix b for additional setup that is required. yaml # use this option if your idp has an accessible discovery url # important : don't include the /. well - known / openid - configuration suffix!! # for example, if the full url is https : / / < host", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "t include the /. well - known / openid - configuration suffix!! # for example, if the full url is https : / / < host > / oauth2 / default /. well - known / openid - configuration # only specify : https : / / < host > / oauth2 / default discoverybaseurl : \" https : / / < host > / oauth2 / default \" # use this option if your idp has cors restrictions on the discovery url, otherwise comment this out. # fill in the values manually from the discovery endpoint's contents endpoint", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##p has cors restrictions on the discovery url, otherwise comment this out. # fill in the values manually from the discovery endpoint's contents endpointoverrides : issuer : \" issuer string for the idp \" authorization _ endpoint : \" url ending in / authorize \" token _ endpoint : \" url ending in / token \" jwks _ uri : \" url ending in / keys \" userinfo _ endpoint : \" url ending in / userinfo \" # note not all idps will have the following endpoints, fill in as many as you can end", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" url ending in / userinfo \" # note not all idps will have the following endpoints, fill in as many as you can end _ session _ endpoint : \" url ending in / logout \" device _ authorization _ endpoint : \" url ending in / devicecode \" revocation _ endpoint : \" url ending in / revoke \" introspection _ endpoint : \" url ending in / introspect \" registration _ endpoint : \" url ending in / clients \" configure the arthur global roles arthur has the ability to create roles for the cluster administrators during", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ endpoint : \" url ending in / clients \" configure the arthur global roles arthur has the ability to create roles for the cluster administrators during the configuration of the idp. these roles are often needed by admins to configure rbac and create organizations for other users in the system. see { ref } creating _ global _ roles _ in _ arthur _ config for a deep dive on how to use global roles. [UNK] roles in the uiglobal roles only provide access for the arthur api, and therefore, cannot be used to grant universal access to workflows in the arthur ui.", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "uiglobal roles only provide access for the arthur api, and therefore, cannot be used to grant universal access to workflows in the arthur ui. a user that belongs to a group that maps to a global role will not be able to see all organizations in the ui, nor enter and read / write data in a specific organization unless they are bound to a role within that organization through an organization - scoped role. this section of the yaml config is under the globalroledefs field. it accepts a list of role definitions that will be created when the configuration is applied. the names of the roles in this", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "under the globalroledefs field. it accepts a list of role definitions that will be created when the configuration is applied. the names of the roles in this section must match the user groups in your idp in order to be able to assume them in arthur. yaml globalroledefs : # here we can specify a list to define multiple global roles - name : \" idp - admin \" # change this name to match the cluster administrator group name in your idp permissions : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete model :", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##p permissions : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete model : - read - write - delete parsing the idp jwt claims in order for arthur to communicate with your idp, it needs to understand the format of the jwt claims your idp uses. this section of the config falls under the accesstokenvalidation yaml field. this section is designed to be flexible to support a variety of claim formats, so it has a lot of options. at its core, the goal is to tell arthur how", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is designed to be flexible to support a variety of claim formats, so it has a lot of options. at its core, the goal is to tell arthur how to be able to extract the following information from the claims : user roles / groups first name last name email user id each field has a corresponding yaml configuration that defines where to find the information in the jwt claims. for example : yamlclaimmapping : firstname : firstname this configuration tells arthur that it can find the user's first name under the \" firstname \" claim in the jwt. such a token might look like this : json { \"", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can find the user's first name under the \" firstname \" claim in the jwt. such a token might look like this : json { \" iss \" : \" https : / / dev. okta. com / oauth2 / default \", \" aud \" : \" api : / / default \", \" scp \" : [ \" openid \" ], \" groups \" : [ \" idp - admin \", \" org - 1 - model - owner \", \" org - 2 - model - owner \" ], \" firstname \" : \" john \", \" lastname \" :", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "1 - model - owner \", \" org - 2 - model - owner \" ], \" firstname \" : \" john \", \" lastname \" : \" doe \", \" login \" : \" [ email protected ] \" } more examples of how to parse user information out of the jwt claims can be found below. full configuration examples here is an example of a full configuration, combining each section described above. yamlversion : v2 kind : oidc config : # discovery url without the /. well - known / openid - configuration suffix discoverybaseurl : https : / / example", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##c config : # discovery url without the /. well - known / openid - configuration suffix discoverybaseurl : https : / / example. com / oauth2 / default # either \" implicit \" or \" pkce \" flowtype : pkce # client id from your idp for the arthur sso application clientid : \" client id string \" # optional : resource id from your idp for the arthur sso application if required by the idp resourceid : \" \" authorizescopes : - openid # required for oidc # use this section to define global roles #", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by the idp resourceid : \" \" authorizescopes : - openid # required for oidc # use this section to define global roles # one example role would be to give the cluster admin permissions to create organizations and manage custom roles globalroledefs : - name : \" iam - admin \" # change this to match the user group name in your idp for administrators permissions : custom _ roles : - read - write - delete organization _ global : - read - write organization : - read - delete # this section describes how to parse the user information out of the jwt returned", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "organization _ global : - read - write organization : - read - delete # this section describes how to parse the user information out of the jwt returned from the idp # this is used by arthur to understand who the user is and what their roles are accesstokenvalidation : type : jwt # only jwt is supported today # fields in the token arthur will use to extract the authentication information claimmapping : roles : groups # this is telling arthur to look in the \" groups \" claim to find the list of user's roles userid : employeeid username : login firstname : firstname", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to look in the \" groups \" claim to find the list of user's roles userid : employeeid username : login firstname : firstname lastname : lastname email : login # one or more audiences to validate, this should match your idp's configuration audience : - api : / / default # optional override signature algo # signaturealgo : rs256 here is an additional descriptions of the fields that need to be set in the config yaml above : discoverybaseurl : this is the base url for your identity provider. your idp should have a /.", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the config yaml above : discoverybaseurl : this is the base url for your identity provider. your idp should have a /. well - known / openid - configuration endpoint and the discoverybaseurl is simply that url minus the /. well - known / openid - configuration part. flowtype : we support both implicit and pkce flows. consult with your team to decide which oidc flow type is right for your organization. clientid : when you create the application integration in your idp, a client id will be provided to you. paste that value into this field.", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "organization. clientid : when you create the application integration in your idp, a client id will be provided to you. paste that value into this field. resourceid : this is optional. if your idp gives you a resource id when creating your application integration, paste the value here. claimmapping : we extract various pieces of authentication information from the provided jwt access token. however, there is no common standard for how these pieces of information should be formatted in the token. for us to extract this information from the token, we need you to explicitly tell arthur where this information is stored in the token. for example,", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the token. for us to extract this information from the token, we need you to explicitly tell arthur where this information is stored in the token. for example, a username could be stored in a field called username or login or email or userid. in order to get this user information, a mapping needs to be provided for the following items roles : this is the field for where either a single authorization role or a list of authorization roles will be specified. note that this is not where you paste a list of roles, this is the name of a field in the jwt where the user's roles are specified. for", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this is not where you paste a list of roles, this is the name of a field in the jwt where the user's roles are specified. for help with role definitions, see { doc } custom _ rbac. ( required ) userid : this is the field for a unique identifier for the user ; this is frequently the same as username and / or email. ( optional, omit if empty ) username : this is the field for the user's unique username ; this is frequently the same as username and / or email. ( optional, omit if empty ) firstname : this", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "user's unique username ; this is frequently the same as username and / or email. ( optional, omit if empty ) firstname : this is the field for the user's first name. ( optional, omit if empty ) lastname : this is the field for the user's last name. ( optional, omit if empty ) email : this is the field for the user's email. ( optional, omit if empty ) audience : this is part of the jwt standard. the aud field for any jwt you create must be a value in this list. for example in the", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": this is part of the jwt standard. the aud field for any jwt you create must be a value in this list. for example in the above configuration, any token that has an aud field that is not set to api : / / defaults, the token will be automatically rejected by arthur. if you are having trouble finding this value, it is frequently the same as your resourceid. remember to format this as a list, not a single value. { note } if your idp has cors restrictions see [ appendix b ] ( # appendix - b - setup - for - idps - with - co", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ note } if your idp has cors restrictions see [ appendix b ] ( # appendix - b - setup - for - idps - with - cors - restrictions ) below for a workaround. 4. apply the arthur idp yaml configuration once you have your yaml configuration file ready, you need to add it to your arthur installation. with the arthur admin console open, navigate to the \" use a 3rd party global identity provider \" section and select \" oidc \". this will expose a text box for you to paste the yaml config file assembled above. when pasting, make sure whites", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "oidc \". this will expose a text box for you to paste the yaml config file assembled above. when pasting, make sure whitespace is preserved and the yaml document has consistent spacing ( do not mix tabs and spaces ). here is a screenshot of the config section : once you have added your config files, scroll to the bottom and click \" save \" to save the config. then go to the latest version and click \" deploy \" to roll out the change to the cluster. 5. create organization user roles to match the idp user groups in order to", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "latest version and click \" deploy \" to roll out the change to the cluster. 5. create organization user roles to match the idp user groups in order to complete this section, you will need access to the arthur superadmin user credentials set during your install, or you will need to be able to assume the role defined in the arthur idp config yaml created above in the globalroledefs section. in order to use the api example linked below, you will need a bearer token ( authentication token ) to include with your api request. there are a few options available to retrieve a token : retrieve a global role token", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will need a bearer token ( authentication token ) to include with your api request. there are a few options available to retrieve a token : retrieve a global role token directly from your idp - most idps will have a method to retrieve tokens for users. some companies make scripts or apis that allow retrieving a token. if your idp does not have an automated method to retrieve a token, you can try setting up a tool like https : / / oidcdebugger. com ( this may involve adding https : / / oidcdebugger. com as an allowed url in your idp", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##cdebugger. com ( this may involve adding https : / / oidcdebugger. com as an allowed url in your idp settings ). retrieve a global role token from your browser cookies - if you sign in to arthur as a user with a global role, the ui will not be fully functional, but it will have a valid access token in the cookies. if you navigate to your browser's developer console and then go to the application storage / cookies section, you should see a cookie like arthurauth0, which is your authentication token. note : if your user has a large number of groups", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ cookies section, you should see a cookie like arthurauth0, which is your authentication token. note : if your user has a large number of groups, there may be multiple cookies of the form arthurauthn. in this case your token was too large to fit in the browser cookie, so it had to be split. you can assemble the full token by concatenating the values of the arthurauthn cookies in order. use the / login api endpoint with the superadmin user's credentials set during the arthur install ( only available on - prem ). using either of those credentials, we", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "api endpoint with the superadmin user's credentials set during the arthur install ( only available on - prem ). using either of those credentials, we can use the arthur api to define roles in arthur that match the user group names in your idp. see the { ref } creating _ organization _ roles section for an example api request to create custom roles in arthur. importantly, the role names must uniquely match to a user group in your idp in order for your users to be able to assume those permissions in arthur. therefore, the roles in arthur must be globally unique in the entire arthur installation. 6. test access", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "users to be able to assume those permissions in arthur. therefore, the roles in arthur must be globally unique in the entire arthur installation. 6. test access at this point everything should be configured correctly to sign in to arthur via sso. either navigate to your idp or the arthur homepage to test logging in. 7. cleaning up once users are successfully able to log in to arthur via the idp, you should do the following to ensure proper security best - practices remain enforced : restrict any arthur global roles to only allow access to essential admin functions set the arthur superadmin user password securely, and either store the password", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "remain enforced : restrict any arthur global roles to only allow access to essential admin functions set the arthur superadmin user password securely, and either store the password in a vault, or discard the password entirely. superadmin shouldn't be used going forward. set up a policy to routinely rotate the superadmin password to keep it secure together, these practices will help ensure the security of your arthur installation, and will give your idp sole control over the platform and who is able to access it. common troubleshooting if after following the steps above, users are not able to log in via the idp try some of", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "who is able to access it. common troubleshooting if after following the steps above, users are not able to log in via the idp try some of these common troubleshooting tips : does the user properly redirected to the idp's log in screen? if not, there is likely a configuration error in the arthur yaml config with the idp discovery url. double check that the url entered resolved correctly when you append /. well - known / openid - configuration to the end of it. the full url should be viewable in your browser or via a rest client. once", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "well - known / openid - configuration to the end of it. the full url should be viewable in your browser or via a rest client. once the user authenticates with the idp, are they redirected to the arthur homepage? if not, there is likely a configuration error with the idp and the urls that it uses to communicate with arthur. double - check the redirect ( whitelisted ) url is configured correctly for the arthur installation at https : / / < hostname > / login / callback. a user can see the arthur home page, but can't see", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur installation at https : / / < hostname > / login / callback. a user can see the arthur home page, but can't see any of the model in their organization if a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to access models ( see { doc }.. /.. / reference / permissions _ by _ endpoint ) or they were not able to correctly assume the role in arthur. double - check the groups in their jwt claims match the role names that have been configured in arthur. a superadmin or global", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the role in arthur. double - check the groups in their jwt claims match the role names that have been configured in arthur. a superadmin or global role user with permissions to manage rbac can see a list of roles in the installation by using the following api call. be sure to replace the host and auth token for your installation and user : shellcurl - - location'https : / / < host > / api / v3 / authorization / custom _ roles'\\ - - header'authorization : bearer < insert auth token here >'appendix a : more examples of jwt claims and how to par", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ roles'\\ - - header'authorization : bearer < insert auth token here >'appendix a : more examples of jwt claims and how to parse them this section outlines some additional ways to use the accesstokenvalidation section of the arthur idp config yaml format. the below examples include sample jwts, then corresponding yaml for how to parse them. basic full example this example shows how to parse a user's information from jwt claims in a typical format. example parsed jwt claims json : json { \" iss \" : \" https : / / dev", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "information from jwt claims in a typical format. example parsed jwt claims json : json { \" iss \" : \" https : / / dev. okta. com / oauth2 / default \", \" aud \" : \" api : / / default \", \" scp \" : [ \" openid \" ], \" groups \" : [ \" idp - admin \", \" org - 1 - model - owner \", \" org - 2 - model - owner \" ], \" firstname \" : \" john \", \" lastname \" : \" doe \", \" login \" :", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- 2 - model - owner \" ], \" firstname \" : \" john \", \" lastname \" : \" doe \", \" login \" : \" [ email protected ] \", \" employeeid \" : \" 1234567890 \" } corresponding settings for the arthur idp config yaml accesstokenvalidation for the user information field : yamlaccesstokenvalidation : type : jwt claimmapping : roles : groups userid : employeeid username : login firstname : firstname lastname : lastname email : login minimal example this example shows how", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": groups userid : employeeid username : login firstname : firstname lastname : lastname email : login minimal example this example shows how to parse a user's information from jwt claims when many fields are missing. example parsed jwt claims json : json { \" iss \" : \" https : / / dev. okta. com / oauth2 / default \", \" aud \" : \" api : / / default \", \" scp \" : [ \" openid \" ], \" groups \" : [ \" idp - admin \", \" org - 1", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "default \", \" scp \" : [ \" openid \" ], \" groups \" : [ \" idp - admin \", \" org - 1 - model - owner \", \" org - 2 - model - owner \" ], \" user \" : \" [ email protected ] \" } corresponding settings for the arthur idp config yaml accesstokenvalidation for the user information field : yamlaccesstokenvalidation : type : jwt claimmapping : roles : groups userid : user username : user firstname : \" \" lastname : \" \" email : user appendix b", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "jwt claimmapping : roles : groups userid : user username : user firstname : \" \" lastname : \" \" email : user appendix b : setup for idps with cors restrictions completing this will require access to the kubernetes cluster arthur is running in, and the ability to create ingress resources in that cluster. if your oidc identity provider does not support cors ( common with microsoft azure ad ), you will need to proxy requests via the arthur backend. the following examples show how this can be done with a cluster using the nginx ingress controller. this first example yaml", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "via the arthur backend. the following examples show how this can be done with a cluster using the nginx ingress controller. this first example yaml configures a route on nginx that will proxy oidc connections to your idp. you'll need to replace the < idp host > and < arthur host > placeholders, then apply it to your cluster with kubectl apply - n < namespace > - f file. yaml. there should be two places to fill in each variable below. yamlapiversion : v1 kind : service metadata : name : external -", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "yaml. there should be two places to fill in each variable below. yamlapiversion : v1 kind : service metadata : name : external - idp spec : type : externalname externalname : \" < idp host > \" - - - apiversion : networking. k8s. io / v1 kind : ingress metadata : name : external - idp annotations : kubernetes. io / ingress. class : nginx nginx. ingress. kubernetes. io / rewrite - target : / $ 2 nginx. ingress. kuber", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##x nginx. ingress. kubernetes. io / rewrite - target : / $ 2 nginx. ingress. kubernetes. io / backend - protocol : \" https \" # important nginx. ingress. kubernetes. io / upstream - vhost : \" < idp host > \" spec : rules : - host : \" < arthur host > \" http : paths : - backend : service : name : external - idp port : number : 443 path : / oidc ( / $ ) (. * ) pathtype : prefix t", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": name : external - idp port : number : 443 path : / oidc ( / $ ) (. * ) pathtype : prefix tls : - hosts : - \" < arthur host > \" secretname : kotsadm - tls after you've applied the above configuration to your cluster, you should be able to visit your idp's /. well - known / openid - configuration endpoint at the following url : https : / / < arthur host > / oidc / < your idp's. well - known endpoint path >. once that is accessible, we", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ / < arthur host > / oidc / < your idp's. well - known endpoint path >. once that is accessible, we need to modify the oidc yaml configuration file. fill in the following example with the correct values in the endpointoverrides section. note, the issuer and authorization _ endpoint fields should match what is in your idp's /. well - known spec. the rest of the values should use the same path as shown in the idp's /. well - known spec, but with the value of < arthur host > / oidc", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "use the same path as shown in the idp's /. well - known spec, but with the value of < arthur host > / oidc / substituted for the host of the idp. the following example shows a proper cors config for an idp at the https : / / xxxx. okta. com address. yamlversion : v2 kind : oidc config : discoverybaseurl : https : / / xxxx. okta. com / oauth2 / default # if your idp has cors restrictions with the metadata url, # specify this", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "xxxx. okta. com / oauth2 / default # if your idp has cors restrictions with the metadata url, # specify this block to prevent using the metadata endpoint to look them up endpointoverrides : # these first two match the idp's. well - known spec issuer : \" https : / / xxxx. okta. com / oauth2 / default \" authorization _ endpoint : \" https : / / xxxx. okta. com / oauth2 / default / authorize \" # notice the following are all modified to add the < arthur", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ / xxxx. okta. com / oauth2 / default / authorize \" # notice the following are all modified to add the < arthur host > / oidc prefix in the url token _ endpoint : \" https : / / < arthur host / oidc / oauth2 / default / tokens \" jwks _ uri : \" https : / / < arthur host / oidc / oauth2 / default / keys \" userinfo _ endpoint : \" https : / / < arthur host / oidc / oauth2 / default / user", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "default / keys \" userinfo _ endpoint : \" https : / / < arthur host / oidc / oauth2 / default / user _ info \" end _ session _ endpoint : \" https : / / < arthur host / oidc / oauth2 / default / logout \" # the rest of this file is unchanged from the examples above... once you have modified this yaml file accordingly, follow the steps above to save it to your installation. updated 3 months ago table of contents 1. configure the idp user groups and oidc token claims 2. con", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "it to your installation. updated 3 months ago table of contents 1. configure the idp user groups and oidc token claims 2. configure the idp oidc settings 3. configure arthur to work with your idp configuring the idps discovery url configure the arthur global roles parsing the idp jwt claims full configuration examples 4. apply the arthur idp yaml configuration 5. create organization user roles to match the idp user groups 6. test access 7. cleaning up common troubleshooting does the user properly redirected to the", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "create organization user roles to match the idp user groups 6. test access 7. cleaning up common troubleshooting does the user properly redirected to the idp's log in screen? once the user authenticates with the idp, are they redirected to the arthur homepage? a user can see the arthur home page, but can't see any of the model in their organization appendix a : more examples of jwt claims and how to parse them basic full example minimal example appendix b : setup for idps with cors restrictions source : https : / / docs. arthur. ai / docs", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "them basic full example minimal example appendix b : setup for idps with cors restrictions source : https : / / docs. arthur. ai / docs / okta", "metadata": {"source": "https://docs.arthur.ai/docs/okta", "row": 92, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 93 text : tabular jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by tabularsuggest editstabular input models are a type of machine learning model that operates on tabular data, which is data that is arranged in a table or spreadsheet format. these models are used to make predictions or classifications based on the input data, which may include features such as numerical or categorical variables. tabular input models are commonly used in fields such as finance, healthcare, and marketing, where data is often structured in a tab", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "numerical or categorical variables. tabular input models are commonly used in fields such as finance, healthcare, and marketing, where data is often structured in a tabular format. updated 3 months ago source : https : / / docs. arthur. ai / docs / tabular", "metadata": {"source": "https://docs.arthur.ai/docs/tabular", "row": 93, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 94 text : integrations jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by integrationssuggest editscurrently there are two available integrations for single - sign - on. find these in the single sign on ( sso ) integrations page. updated 3 months ago source : https : / / docs. arthur. ai / docs / integrations", "metadata": {"source": "https://docs.arthur.ai/docs/integrations", "row": 94, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 95 text : glossary jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by glossarysuggest editsthe following definitions are specific to the arthur platform, though in most cases apply to ml more broadly. arthur inference container class for inferences uploaded to the arthur platform. an inference comprises input features, prediction values, and ( optionally ) ground truth values and any non - input data. example : pythonground _ truth = { \" consumer credit score \" : 652. 0 } inference = arthur _ model. get _ inference", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- input data. example : pythonground _ truth = { \" consumer credit score \" : 652. 0 } inference = arthur _ model. get _ inference ( external _ id ) inference. update ( ground _ truth ) related terms : inference, arthurmodel arthur model a model object sends and retrieves data pertinent to a deployed ml system. the arthurmodel object is separate from the underlying model trained and makes predictions ; it serves as a wrapper for the underlying model to access arthur platform functionality. an arthurmodel contains at least aname, an inputtype and a outputtype. examples : pythonarthur _", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "underlying model to access arthur platform functionality. an arthurmodel contains at least aname, an inputtype and a outputtype. examples : pythonarthur _ model = connection. model ( name = \" new _ model \", input _ type = inputtype. tabular, model _ type = outputtype. regression ) pythonarthur _ model = connection. get ( model _ id ) arthur _ model. send _ inference (... ) arthur model group arthur model groups are an organizational construct the arthur platform uses to track different versions of an arthur model. every arthur model is a version of one model group, and a model", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "groups are an organizational construct the arthur platform uses to track different versions of an arthur model. every arthur model is a version of one model group, and a model group will always have at least one arthur model. the model group for an arthur model can only be specified during onboarding, and once the arthur model is saved, its group cannot be changed. if an arthur model is created without specifying a model group, a new model group will be created automatically with the new model as its single version. when adding a model to a model group, the model is assigned a unique, incrementing version sequence number ( starting at 1 )", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "as its single version. when adding a model to a model group, the model is assigned a unique, incrementing version sequence number ( starting at 1 ) corresponding to the order in which it was added to the model group. additionally, you can provide a version label to store a custom version string label along with the version sequence number. example : python # retrieve the first version of a model arthur _ model _ v1 = connection. get ( model _ id ) model _ group = arthur _ model _ v1. model _ group # create the new version of the model arthur _ model _ v2 = connection. model ( name", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "group = arthur _ model _ v1. model _ group # create the new version of the model arthur _ model _ v2 = connection. model ( name = \" model _ v2 \", input _ type = inputtype. tabular, model _ type = outputtype. regression ) # add the new model to the model group model _ group. add _ version ( arthur _ model _ v2, label = \" 2. 0. 1 \" ) arthur _ model _ v2. save ( ) related terms : version label, version sequence number attribute a variable associated with a model., be input, prediction, ground truth", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ v2. save ( ) related terms : version label, version sequence number attribute a variable associated with a model., be input, prediction, ground truth or ancillary information ( these groupings are known as stages in the arthur platform ). it can be categorical or continuous. example : the attribute age is an input to the model, whereas the attribute creditworthy is the target for the model. synonyms : variable, { predictor, input }, { ouput, target }, prediction. related terms : input, stage, prediction, ground truth bias while bias is an overloaded term in stats & ml,", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ouput, target }, prediction. related terms : input, stage, prediction, ground truth bias while bias is an overloaded term in stats & ml, we refer specifically to where the model's outcomes have the potential to lead to discriminatory outcomes. example : this credit approval model tends to lead to biased outcomes : men are approved for loans at a rate 50 % higher than women are. related terms : bias detection, bias mitigation, disparate impact bias detection the detection and quantification of { ref } algorithmic bias < bias _ monitoring > in an ml system, typically as evaluated on a model '", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "bias detection the detection and quantification of { ref } algorithmic bias < bias _ monitoring > in an ml system, typically as evaluated on a model's outputs ( predictions ) across different definitions of a sensitive attribute. many definitions of algorithmic bias have been proposed, including group and individual fairness definitions. group fairness definitions are often defined by comparing group - conditional statistics about the model's predictions. in the below definitions, the group membership feature is indicated by g, and a particular group membership value is simplified by g. example : common metrics for group fairness include demographic parity, equalized odds, and equality of opportunity.", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a particular group membership value is simplified by g. example : common metrics for group fairness include demographic parity, equalized odds, and equality of opportunity. related terms : bias mitigation demographic parity a fairness metric that compares group - conditional selection rates. the quantity being compared is : p ( y ^ = 1 g = g ) there is not necessarily a normative ideal relationship between the selection rates for each group : in some situations, such as allocating resources, it may be important to minimize the disparity in selection rates across groups ; in others, metrics based on group - conditional accuracy may be more relevant.", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "it may be important to minimize the disparity in selection rates across groups ; in others, metrics based on group - conditional accuracy may be more relevant. however, even in the latter case, understanding group - conditional selection rates, especially when compared against the original training data, can be useful contextualization for the model and its task as a whole. related term : disparate impact equal opportunity a fairness metric that compares group - conditional true positive rates. the quantity being compared is : p ( y ^ = 1 y = 1, g = g ) for all groups, a true positive rate closer to 1 is better. equal", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "compared is : p ( y ^ = 1 y = 1, g = g ) for all groups, a true positive rate closer to 1 is better. equalized odds a fairness metric that incorporates both group - conditional true positive rates and false positive rates, equivalently, true positive and negative rates. there are a variety of implementations due to the fact that some quadrants of the confusion matrix are complements of one another ) ; here is one possible quantity to compare across groups : p ( y ^ = 1 y = 1, g = g ) + p ( y ^ = 0 y = 0, g = g ) in this implementation", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "p ( y ^ = 1 y = 1, g = g ) + p ( y ^ = 0 y = 0, g = g ) in this implementation, this quantity should be as close to 2 as possible for all groups. bias mitigation automated techniques to mitigate bias in a discriminatory model. can be characterized by where the technique sits in the model lifecycle : pre - processing : techniques that analyze datasets and often modify / resample training datasets to make the learned classifier less discriminatory. in - processing : techniques for training a fairness - aware classifier ( or regressor", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "datasets to make the learned classifier less discriminatory. in - processing : techniques for training a fairness - aware classifier ( or regressor ) that explicitly trades off optimizing for accuracy and maintaining fairness across sensitive groups. post - processing : techniques that only adjust the output predictions from a discriminatory classifier without modifying the training data or the classifier. related terms : bias detection binary classification a modeling task where the target variable belongs to a discrete set with two possible outcomes. example : this binary classifier will predict whether or not a person is likely to default on their credit card. related terms : output type", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with two possible outcomes. example : this binary classifier will predict whether or not a person is likely to default on their credit card. related terms : output type, classification, multilabel classification categorical attribute an attribute whose value is taken from a discrete set of possibilities. example : a person's blood type is a categorical attribute : it can only be a, b, ab, or o. synonyms : discrete attribute related terms : attribute, continuous, classification continuous attribute an attribute whose value is taken from an ordered continuum can be bounded or unbounded. example : a person's height, weight, income, iq", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attribute an attribute whose value is taken from an ordered continuum can be bounded or unbounded. example : a person's height, weight, income, iq can all be through of as continuous attributes. synonyms : numeric attribute related terms : attribute, continuous, regression classification a modeling task where the target variable belongs to a discrete set with a fixed number of possible outcomes. example : this classification model will determine whether an input image is of a cat, a dog, or fish. related terms : output type, binary classification, multilabel classification data drift refers to the problem arising when, after a trained model is deployed, changes", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "fish. related terms : output type, binary classification, multilabel classification data drift refers to the problem arising when, after a trained model is deployed, changes in the external world lead to degradation of model performance and the model becoming stale. detecting data drift will provide a le ing indicator of data stability and integrity. data drift can be quantified with respect to a specific reference set ( e. g., the model's training data ) or, more generally, the model's temporal shifts in a variable with respect to past time windows. your project can { ref } query data drift metrics through the arthur api < data", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model's temporal shifts in a variable with respect to past time windows. your project can { ref } query data drift metrics through the arthur api < data _ drift >. this section will provide an overview o the available data drift metrics in arthur's query service. related terms : ou arthur'stribution multivariate arthur also offers a multivariate anomaly score, which you can configure via the steps detailed in enabling enrichments. see anomaly detection for an explanation of how these scores are used and arthur algorithms for how they're calculated. disparate impact legal terminology originally from fair lending case law", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detection for an explanation of how these scores are used and arthur algorithms for how they're calculated. disparate impact legal terminology originally from fair lending case law. this constraint is strictly harder than dispara e treatment and asserts that model outcomes must not be discriminatory across protected groups. that is, the outcome of a decision process should not be substantially higher ( or lower ) for one group of a protected class over another. while there does not exist a single threshold for establishing the presence or absence of disparate impact, the so - called \" 80 % rule \" is commonly referenced. however, harm certain subgroups of a", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "establishing the presence or absence of disparate impact, the so - called \" 80 % rule \" is commonly referenced. however, harm certain subgroups of a population differentially r, we strongly recommend against adopting this rule - of - thumb, as these analyses should be grounded in use - case - specific analysis and the legal framework pertinent to a given industry. example : even though the model didn't take gender as input, it still results in disparate impact when we compare outcomes for males and females. related terms : bias, disparate treatment disparate treatment legal terminology originally from fair lending case law. di", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "when we compare outcomes for males and females. related terms : bias, disparate treatment disparate treatment legal terminology originally from fair lending case law. disparate treatment asserts that you are not allowed to consider protected variables ( e. g., race, age, gender ) when approving or denying a credit card loan application. in practical terms, a data scientist cannot include these attributes as inputs to a credit decision model. adherence to disparate treatment is not a sufficient condition for actually achieving a fair model ( see proxy and bias detedefinitionstion ). \" fairness through \" unawareness \" is not good enough", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sufficient condition for actually achieving a fair model ( see proxy and bias detedefinitionstion ). \" fairness through \" unawareness \" is not good enough. related terms : bias, disparate impact enrichment generally used to describe data or metrics added to raw data after ingestion. arthur provides various enrichments such as anomaly detection and explainability. see entity enrichments for details about using enrichments within arthur. feature an individual attribute that is an input to a model example : the credit scoring model has features like \u201c home _ value \u201d, \u201c zip _ code \u201d, \u201c height \". ground truth the true label or", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model example : the credit scoring model has features like \u201c home _ value \u201d, \u201c zip _ code \u201d, \u201c height \". ground truth the true label or target variable ( y ) corresponds to inputs ( x ) for a dataset. examples : pred = sklearn _ model. predict _ proba ( x ) arthur _ model. send _ inference ( model _ pipeline _ input = x, predicted _ values = { 1 : pred, 0 : 1 - pred } ) related terms : prediction image data imagery data is commonly used for computer vision models. related terms : attribute, output type, stage inference one row", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "pred } ) related terms : prediction image data imagery data is commonly used for computer vision models. related terms : attribute, output type, stage inference one row of a dataset. inference refers to passing a single input into a model and the model's prediction. data associated with that inference might include ( 1 ) the input to the model, ( 2 ) the model's prediction and ( 3 ) the corresponding ground truth. with respect to the arthur platform, the term inference denotes any and all of those related components of data for a single input & prediction. related terms : arthurinference, stage input a single data instance upon", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inference denotes any and all of those related components of data for a single input & prediction. related terms : arthurinference, stage input a single data instance upon which a model can calculate an output prediction. the input consists of all relevant features together. example : the input features for the credit scoring model consist of \u201c home _ value \u201d, \u201c zip _ code \u201d, \u201c height \". related terms : feature, model input type for an arthurmodel, this field declares what kind of input datatype will be flowing into the system. allowable values are defined in the inputtype enum : tabular image nlp example : python", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "kind of input datatype will be flowing into the system. allowable values are defined in the inputtype enum : tabular image nlp example : pythonarthur _ model = connection. model ( name = \" new _ model \", input _ type = inputtype. tabular, model _ type = outputtype. regression ) related terms : output type, tabular data, nlp data model health score on the ui dashboard, you will see a model health score between 0 - 100 for each of your models. this score averages over a 30 - day window of the following normalized metrics : performance, drift, and", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "between 0 - 100 for each of your models. this score averages over a 30 - day window of the following normalized metrics : performance, drift, and ingestion. performance : regression : 1 - normalized mae classification : f1 score drift 1 - average anomaly score ingestion the variance of normalized periods between ingestion events the variance of normalized volume differences between ingestion events you can also extract the health score via an api call. model onboarding model onboarding refers to the process of defining an arthurmodel, preparing it with the necessary reference dataset, passing it through a validation check, and saving", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model onboarding refers to the process of defining an arthurmodel, preparing it with the necessary reference dataset, passing it through a validation check, and saving it to the arthur system. once your model is onboarded onto arthur, you can use the arthur system to track the model and view all its performance and analytics in your online arthur dashboard. related terms : arthurmodel, reference dataset multiclass classification a modeling task where each input is associated with one label from a fixed set of possible labels. often this is a binary classifier ( the output is either 0 or 1 ), but the output can also have more than 2", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "fixed set of possible labels. often this is a binary classifier ( the output is either 0 or 1 ), but the output can also have more than 2 possible labels. example : this nlp model applies the most relevant tag to news articles. the model is trained on example articles which are tagged with a topic like congress. related terms : multilabel clasification, output type, multilabel classification a modeling task where each input is associated with two or more labels from a fixed set of possible labels. example : this nlp model applies relevant tags to news articles. the model is trained on example articles which are tagged with", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "from a fixed set of possible labels. example : this nlp model applies relevant tags to news articles. the model is trained on example articles which are tagged with multiple topics like politics, elections, congress. related terms : output type, multiclass clasification nlp data unstructured text sequences are commonly used for natural language processing models. related terms : attribute, output type, stage non - input attribute a non - input attribute is an attribute that an arthurmodel will track that does not actually enter the model as an input. common non - input attributes are protected class attributes such as age, race, or sex. by the", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "track that does not actually enter the model as an input. common non - input attributes are protected class attributes such as age, race, or sex. by the model ending such non - input attributes to arthur, you can track model performance based on these groups in your data to evaluate model bias and fairness. related terms : attribute, bias object detection the outputtype is for computer vision models to detect an object within an image and output a box that bounds the object. this bounding box is used to identify where the object resides in the image. related terms : image out of distribution detection refers to the challenge of detecting when an input ( or", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "box is used to identify where the object resides in the image. related terms : image out of distribution detection refers to the challenge of detecting when an input ( or set of inputs ) is substantially different from the distribution of a larger set of reference inferences. this term commonly arises in data drift, where we want to detect if new inputs differ from the training data ( and distribution thereof ) for a particular model. ood detection is a relevant challenge for tabular data and unstructured data such as images and sequences. related terms : { ref } glossary _ data _ drift output type for an arthurmodel, this field declares what", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ructured data such as images and sequences. related terms : { ref } glossary _ data _ drift output type for an arthurmodel, this field declares what kind of output predictions will flow out of the system. allowable values are defined in the outputtype enum : regression appropriate for continuous - valued targets multiclass appropriate for both binary classifiers and multiclass classifiers multilabel appropriate for multilabel classifiers objectdetection only available for computer vision models example : pythonarthur _ model = connection. model ( name = \" new _ model \", input _ type = inputtype. tabular, output _ type", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": pythonarthur _ model = connection. model ( name = \" new _ model \", input _ type = inputtype. tabular, output _ type = outputtype. regression ) related terms : input type prediction the output prediction ( y _ hat ) of a trained model for any input. related terms : ground truth protected attribute an attribute of an inference that is considered sensitive with respect to model bias. common examples include race, age, and gender. the term \" protected \" comes from the civil rights act of 1964. synonyms : sensitive attribute related terms : bias, proxy proxy an input attribute in a model ( or combination thereof", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" comes from the civil rights act of 1964. synonyms : sensitive attribute related terms : bias, proxy proxy an input attribute in a model ( or combination thereof ) is highly correlated with a protected attribute such as race, age, or gender. the presence of proxies in a dataset makes it difficult to rely only on [ disparate treatment ] as a standard for fair ml. example : in most us cities, zip code is a strong proxy for race. therefore, one must be cautious when using zip code as an input to a model. related terms : bias, disparate impact, disparate treatment reference the", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", one must be cautious when using zip code as an input to a model. related terms : bias, disparate impact, disparate treatment reference the dataset is used as a baseline reference for an arthur model. a reference dataset must include a sample of the input features a model receives. a reference dataset can optionally include a sample of model outputs, ground truth values, and other non - input attributes as metadata. the reference dataset for a model is used to compute drift : the distribution of input features in the reference dataset makes up the baseline against which future inferences are compared to compute anomaly scores. related", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "used to compute drift : the distribution of input features in the reference dataset makes up the baseline against which future inferences are compared to compute anomaly scores. related terms : inference regression a modeling task ( or model ) where the target variable is a continuous variable. example : this regression model predicts what the stock price of $ appl will be tomorrow. related terms : output type sensitive attribute see protected attribute stage the arthur platform uses taxonomy to delineate how attributes contribute to the model computations. allowable values are defined in the stage enum : modelpipelineinput : input to the entire model pipeline. this will most commonly", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model computations. allowable values are defined in the stage enum : modelpipelineinput : input to the entire model pipeline. this will most commonly be the stage used to represent all model inputs. will contain base input features familiar to the data scientist : categorical and continuous columns of a tabular dataset. predictfunctioninput : potential alternative input source, representing direct input into the model's predi t ( ) method. therefore, data in the specific models have already undergone all relevant transformations, including scaling, one - hot encoding, or embedding. predictedvalue : the predictions coming out of the", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "specific models have already undergone all relevant transformations, including scaling, one - hot encoding, or embedding. predictedvalue : the predictions coming out of the model. groundtruth : the ground truth ( or target ) attribute or a model. must be one - hot for classifiers groundtruthclass : the ground truth class for classification models, not one - hot encoded noninput : ancillary data that can be associated with each inference but not necessarily a direct input t the model. for example, sensitive attributes like age, sex, or race might not be direct model inputs, but will be useful to associate with each", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "input t the model. for example, sensitive attributes like age, sex, or race might not be direct model inputs, but will be useful to associate with each prediction. tabular data the data type for model inputs where the data can be thought of as a table ( or spreadsheet ) composed o rows and columns. each column represents an input attribute for the model, and each row represents a separate record that composes the training data. in supervised learning, exactly one of the columns acts as the target. example : this credit scoring model is trained on tabular data. the input attributes are income, country, and age and the", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the columns acts as the target. example : this credit scoring model is trained on tabular data. the input attributes are income, country, and age and the target is fico score. related terms : attribute, output type, stage token likelihood the token likelihood is a number between 0 and 1 that quantifies the model \u2019 s level of surprise that this token was the next predicted token of the sentence. if a token has a low likelihood ( close to 0 ), the model is more unsure about selecting this token. while a likelihood close to 1 indicates that the model is very confident in predicting this token. version label a version label", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model is more unsure about selecting this token. while a likelihood close to 1 indicates that the model is very confident in predicting this token. version label a version label is a string representing a custom version of your arthur model within its a thur model group. version labels are not required, and the platform will default to using the version sequence number when not provided. example : python # retrieve the model group model _ group = connection. get _ model _ group ( model _ group _ id ) # create the new version of the model arthur _ model _ v2 = connection. model ( name = \" model _ v2 \", input _", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "id ) # create the new version of the model arthur _ model _ v2 = connection. model ( name = \" model _ v2 \", input _ type = inputtype. tabular, model _ type = outputtype. regression ) # add the new model to the model group model _ group. add _ version ( arthur _ model _ v2, label = \" 2. 0. 1 \" ) label = arthur _ model _ v2. version _ label arthur _ model _ v2. save ( ) # label = = \" 2. 0. 1 \" related terms : arthur model, arthur model group, version sequence", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ model _ v2. save ( ) # label = = \" 2. 0. 1 \" related terms : arthur model, arthur model group, version sequence number version sequence number a version sequence number is a unique, auto - incrementing ( starting at 1 ) integer assigned to arthur models in an a thur model group. this number uniquely represents an arthur model \u2019 s version with the model group. if a version label is not provided, the platform will show the version sequence number instead. example : python # retrieve the first version of a model arthur _ model _ v1 = connection. get ( model _ id ) num", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "sequence number instead. example : python # retrieve the first version of a model arthur _ model _ v1 = connection. get ( model _ id ) num = arthur _ model _ v1. version _ sequence _ num # num = = 1 # retrieve the second version of a model model _ group = arthur _ model _ v1. model _ group arthur _ model _ v2 = model _ group. get _ version ( sequence _ num = 2 ) num = arthur _ model _ v2. version _ sequence _ num # num = = 2 related terms : arthur model, arthur model group,", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "num = arthur _ model _ v2. version _ sequence _ num # num = = 2 related terms : arthur model, arthur model group, version labelupdated 3 months ago table of contents arthur inference arthur model arthur model group attribute bias bias detection demographic parity equal opportunity equalized odds bias mitigation binary classification categorical attribute continuous attribute classification data drift multivariate disparate impact disparate treatment enrichment feature ground truth image data inference input input type model health score model onboarding multiclass classification multilabel classification nlp data non - input attribute object detection out of distribution detection output type prediction protected attribute", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "type model health score model onboarding multiclass classification multilabel classification nlp data non - input attribute object detection out of distribution detection output type prediction protected attribute proxy reference regression sensitive attribute stage tabular data token likelihood version label version sequence number source : https : / / docs. arthur. ai / docs / glossary", "metadata": {"source": "https://docs.arthur.ai/docs/glossary", "row": 95, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 96 text : langchain jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by langchainsuggest editsthis guide walks through how to use the arthurcallbackhandler, an integration that allows you to send llm inferences to the arthur platform through langchain. register your arthurmodel you can skip this step if your generative text model is already registered with arthur. if you do not have a model currently onboarded to arthur, follow the steps on our generative text onboarding guide you will", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is already registered with arthur. if you do not have a model currently onboarded to arthur, follow the steps on our generative text onboarding guide you will need your model's id registered to the platform before you create an arthurcallbackhandler with your langchain llm. create your langchain llm with the arthurcallbackhandler first, get your arthur login info and the id of your registered arthurmodel : pythonarthur _ url = \" < https : / / app. arthur. ai \" > arthur _ login = \" your - arthur - login - username", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ url = \" < https : / / app. arthur. ai \" > arthur _ login = \" your - arthur - login - username - here \" arthur _ model _ id = \" your - arthur - model - id - here \" next, we create a langchain chatopenai llm with your arthur credential info passed to the arthurcallbackhandler note that we are also configuring the llm with the useful streamingstdoutcallbackhandler from langchain, which returns responses as a token - by - token stream instead of returning the entire result at once", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "streamingstdoutcallbackhandler from langchain, which returns responses as a token - by - token stream instead of returning the entire result at once - this typically makes for a better ux for development & testing. pythonfrom langchain. callbacks import arthurcallbackhandler from langchain. callbacks. streaming _ stdout import streamingstdoutcallbackhandler from langchain. chat _ models import chatopenai chatgpt = chatopenai ( streaming = true, temperature = 0. 1, callbacks = [ streamingstdoutcallbackhandler ( )", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "chatgpt = chatopenai ( streaming = true, temperature = 0. 1, callbacks = [ streamingstdoutcallbackhandler ( ), arthurcallbackhandler. from _ credentials ( arthur _ model _ id, arthur _ url = arthur _ url, arthur _ login = arthur _ login ) ] ) you can now use the llm in your langchain application, with its input text, output text, and other monitored attributes recorded to the arthur platform for each inference. note that the attributes from each llm response will only be saved to arthur if you have registered those attributes", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "monitored attributes recorded to the arthur platform for each inference. note that the attributes from each llm response will only be saved to arthur if you have registered those attributes with your arthurmodel. for more information on registered generative text models with additional attributes besides input text & output text, visit the generative text onboarding guide. run the model to log inferences to the arthur platform here we define a run function that executes a loop for a chat between a user and an llm until the user types q to quit note that this function is not required to use the arthurcallbackhandler - it is just meant as a", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "llm until the user types q to quit note that this function is not required to use the arthurcallbackhandler - it is just meant as a quick demonstration of how to use a langchain llm pythonfrom langchain. schema import humanmessage def run ( llm ) : history = [ ] while true : user _ input = input ( \" \\ n > > > input > > > \\ n > > > : \" ) if user _ input = = \" q \" : break history. append ( humanmessage ( content = user _ input ) ) history. append", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "if user _ input = = \" q \" : break history. append ( humanmessage ( content = user _ input ) ) history. append ( llm ( history ) ) each subsequent user < > llm back - and - forth response will be logged as its own inference in the arthur platform. for example, here is an execution of this run function with the chatgpt llm. pythonrun ( chatgpt ) textinput > > > > > > : what is a callback handler? > > > a callback handler, also known as a callback function or callback method, is", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> > > : what is a callback handler? > > > a callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. it is commonly used in programming languages that support event - driven or asynchronous programming paradigms. the purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. when the event is triggered,", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "occurs. instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. when the event is triggered, the callback function is invoked, allowing the program to respond accordingly. callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event - driven architectures. they provide a flexible and modular way to handle events and decouple different components of a system. input > > > > > > : what do i need to do to get the full benefits of this > > >", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##uple different components of a system. input > > > > > > : what do i need to do to get the full benefits of this > > > to get the full benefits of using a callback handler, you should consider the following : 1. understand the event or condition : identify the specific event or condition that you want to respond to with a callback handler. this could be user input, network requests, or any other asynchronous operation. 2. define the callback function : create a function that will be executed when the event or condition occurs. this function should contain the desired behavior or actions you want", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". define the callback function : create a function that will be executed when the event or condition occurs. this function should contain the desired behavior or actions you want to take in response to the event. 3. register the callback function : depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. this ensures that the callback function is invoked when the event occurs. 4. handle the callback : implement the necessary logic within the callback function to handle the event or condition. this could involve updating the user interface, processing data, making further", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##back : implement the necessary logic within the callback function to handle the event or condition. this could involve updating the user interface, processing data, making further requests, or triggering other actions. 5. consider error handling : it's important to handle any potential errors or exceptions that may occur within the callback function. this ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior. 6. maintain code readability and modularity : as your codebase grows, it's crucial to keep your callback handlers organized and maintainable. consider using design patterns or architectural principles to structure your code in", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "codebase grows, it's crucial to keep your callback handlers organized and maintainable. consider using design patterns or architectural principles to structure your code in a modular and scalable way. by following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event - driven programming, improved responsiveness, and modular code design. input > > > > > > : q updated 3 months ago table of contents register your arthurmodel create your langchain llm with the arthurcallbackhandler run the model to log inferences to the arthur platform source : https :", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##model create your langchain llm with the arthurcallbackhandler run the model to log inferences to the arthur platform source : https : / / docs. arthur. ai / docs / langchain", "metadata": {"source": "https://docs.arthur.ai/docs/langchain", "row": 96, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 97 text : administration jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by administrationsuggest editsby default, the installer creates a new organization, \" my organization, \" for convenience. you can also create new organizations using the api with the superadmin user. full instructions for creating new users and organizations can be found organizations and users. to access the ui for the default organization dashboard, visit thehttps : / / your _ arthur _ domain from your web browser. login with admin username and superse", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "default organization dashboard, visit thehttps : / / your _ arthur _ domain from your web browser. login with admin username and supersecret password. make sure to change the password as soon as possible. refer to the quickstart guide to start onboarding your models. admin console the admin console can be made available via the ingress controller on port 443 by creating a subdomain dns record that starts with admin. ( e. g., admin. arthur. mydomain. com ). this eliminates the port 8800 egress requirement for vm", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( e. g., admin. arthur. mydomain. com ). this eliminates the port 8800 egress requirement for vm installation on the firewall. we recommend that you rotate your admin console password often. you can reset the password using this command : shellkubectl kots reset - password - n < namespace > update admin password for embedded postgres the'postgres'admin user manages the embedded postgres database. if you would like to update the password for this admin user, you can execute the following commands on the primary database pod : shell", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "postgres database. if you would like to update the password for this admin user, you can execute the following commands on the primary database pod : shellkubectl exec - it database - master - 0 - - psql - u postgres password for user postgres : < type _ current _ secret > psql ( 11. 13 ) type \" help \" for help. postgres = # alter role postgres with password'< insert _ new _ secret >'; postgres = # \\ q $ updated 3 months ago table of contents admin console update admin password", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' < insert _ new _ secret >'; postgres = # \\ q $ updated 3 months ago table of contents admin console update admin password for embedded postgres source : https : / / docs. arthur. ai / docs / administration", "metadata": {"source": "https://docs.arthur.ai/docs/administration", "row": 97, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 98 text : regression jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by regressionsuggest editsregression models predict a numeric outcome. in arthur, these models are listed under the regression model type. some common examples of text regression are : what is the predicted review score for written restaurant reviews? predict house price from description text formatted data in arthur tabular regression models require two columns : text input and numeric output. when onboarding a reference dataset ( and setting a model schema ), you need to", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "regression models require two columns : text input and numeric output. when onboarding a reference dataset ( and setting a model schema ), you need to specify a target column for each inference's ground truth. many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( numeric or categorical ) attribute ( numeric or categorical ) prediction ( numeric ) ground truth ( numeric ) non - input attribute ( numeric or categorical ) 45graduate degree45. 3462. 42female", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") ground truth ( numeric ) non - input attribute ( numeric or categorical ) 45graduate degree45. 3462. 42female22bachelor's degree55. 153. 2male predict function and mapping these are some examples of common values teams need to onboard for their regression models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth mappingexample prediction function # # single column ground truth output _ mapping = {'prediction _ column':'gt _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping ) # # example prediction function for binary classification def predict ( x ) : return model. predict ( x ) available metrics when onboarding", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= output _ mapping ) # # example prediction function for binary classification def predict ( x ) : return model. predict ( x ) available metrics when onboarding tabular regression models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a regression model. find out more about these metrics in the performance metrics section. metricmetric typeroot mean squared error", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") per class when teams onboard a regression model. find out more about these metrics in the performance metrics section. metricmetric typeroot mean squared errorperformancemean absolute errorperformancer squaredperformanceinference countingestionaverage predictioningestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. note : teams are able to evaluate drift for inference data", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "out of the box for comparison. find out more about these metrics in the drift and anomaly section. note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift user - defined metrics whether your team uses a different performance metric, wants", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##diction driftprediction driftmultivariate driftmultivariate drift user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "spotsexplainabilitybias mitigationxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / regression", "metadata": {"source": "https://docs.arthur.ai/docs/regression", "row": 98, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 99 text : release notes jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearcharthur platform 3. 12. 0 23 days ago by readme apirelease notes for arthur platform", "metadata": {"source": "https://docs.arthur.ai/changelog", "row": 99, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearcharthur platform 3. 12. 0 23 days ago by readme apirelease notes for arthur platform 3. 12. 0arthur platform 3. 11. 0 about 2 months ago by readme apirelease notes for arthur platform 3. 11. 0arthur platform 3. 10. 0 about 2 months ago by readme apirelease notes for arthur platform 3. 10. 0arthur platform 3. 9. 0 4 months ago by readme apirelease notes for arthur platform 3. 9. 0arthur platform 3. 8. 0 5 months ago", "metadata": {"source": "https://docs.arthur.ai/changelog", "row": 99, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "9. 0 4 months ago by readme apirelease notes for arthur platform 3. 9. 0arthur platform 3. 8. 0 5 months ago by readme apirelease notes for arthur platform 3. 8. 0arthur platform 3. 7. 0 7 months ago by readme apirelease notes for arthur platform 3. 7. 0addedmay 2023 10 months ago by haley massanew featureswelcome to arthur about 1 year ago by haley massawelcome to the developer hub and documentation for arthur. source : https : / / docs. arthur. ai / changelog", "metadata": {"source": "https://docs.arthur.ai/changelog", "row": 99, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 100 text : email jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by emailsuggest editsone of the most popular alert notification techniques is through email. this notification technique can easily be set up in the organization or model - level ui. organization level they are typically used when teams want to manage organization - wide email alert notification access over multiple models quickly. teams can go into the organization's setting page and configure which users will receive email alerts for each model that has alerts enabled. model level if you", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "into the organization's setting page and configure which users will receive email alerts for each model that has alerts enabled. model level if you do not want to look at alerts at an organizational level, you can also set up email alerts per model within an individual model's alerts tab. here you can navigate to manage the alert rules of the model. within this management center, you can select and edit the notification channels for this model. this is where emails are entered and saved for users that wish to receive alerts. resulting alert after configuring your alert email settings, teams are ready to", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is where emails are entered and saved for users that wish to receive alerts. resulting alert after configuring your alert email settings, teams are ready to receive alerts for these models to their designated emails. an example of an email alert notification sent by arthur configuring email alert notifications on sso for users in sso environments to subscribe other users up to email alerts notifications, configure your email domain whitelist through the kots admin console to be able to send email alert notifications. in the kots admin console, check \u201c show other advanced options \u201d navigate to the \u201c", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "admin console to be able to send email alert notifications. in the kots admin console, check \u201c show other advanced options \u201d navigate to the \u201c other advanced options \u201d section under \" other advanced options, \" set a comma - separated list of domains in the \u201c email domain whitelist \u201d field. an example of the email domain whitelist within the admin consoleupdated 3 months ago table of contents organization level model level resulting alert configuring email alert notifications on sso source : https : / / docs. arthur. ai / docs / email", "metadata": {"source": "https://docs.arthur.ai/docs/email", "row": 100, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 101 text : ranked list ( recommender systems ) jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser -", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ssionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explain", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstra", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityextern", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##installation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess control", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##es cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestorin", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##sarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by ranked list ( recommender systems ) suggest editsranked list output data is typically used in recommender systems, which is a type of machine learning model that generates suggestions about \u201c relevant \u201d ranked items based on some input data. an example of a recommender system using ranked list data is a model that recommends relevant movies to a viewer based on metadata generated from their watch history. formatted data in arthur ranked list output models require the", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list data is a model that recommends relevant movies to a viewer based on metadata generated from their watch history. formatted data in arthur ranked list output models require the following data formatting : json [ { / / first recommended item \" item _ id \" : \" item1 \", \" score \" : 0. 324, \" label \" : \" apple \" }, { / / second recommended item \" item _ id \" : \" item2 \", \" score \" : 0. 024, \" label \" : \" banana \" ] in this formatting, the score must be a float value, whereas the label and item _ id", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". 024, \" label \" : \" banana \" ] in this formatting, the score must be a float value, whereas the label and item _ id must be string values. the label is an optional, readable version of item _ id and score is an optional score / probability for a given item. if one of these optional metadata fields are specified in one inference, it must be specified for all of them. arthur expects the list of ranked items to be sorted in rank order, such that the highest ranked item is first. each ranked list output model in arthur can have max 1000 total unique recommended items in its reference dataset", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "rank order, such that the highest ranked item is first. each ranked list output model in arthur can have max 1000 total unique recommended items in its reference dataset. additionally, each ranked list output model can have max 100 recommendations per inference / ground truth. recommender systems ground truth the ground truth for ranked list output models is an array of strings representing the items that have been determined \u201c relevant \u201d for a given inference. available metrics when onboarding recommender system models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) when teams onboard a ranked list model. find out more about these metrics in the performance metrics section. metricmetric typeprecision at kperformancerecall at kperformancendcg at kperformancemean reciprocal rankperformanceranked list aucperformanceinference countingestion drift metrics", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ancendcg at kperformancemean reciprocal rankperformanceranked list aucperformanceinference countingestion drift metrics in the arthur platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ).", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). psifeature drifttime series driftfeature driftprediction driftprediction drift user - defined metrics whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. updated about 2 months ago table of contents formatted data in arthur available metrics", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "creating metrics with data in arthur in the user - defined metrics section. updated about 2 months ago table of contents formatted data in arthur available metrics source : https : / / docs. arthur. ai / docs / ranked - list - recommender - systems", "metadata": {"source": "https://docs.arthur.ai/docs/ranked-list-recommender-systems", "row": 101, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 102 text : image jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by imagesuggest editsimage input models are a type of machine learning model that operates on image data, such as digital photographs, medical scans, and satellite imagery. these models are designed to recognize patterns and features within images and can perform tasks such as object detection, image segmentation, and image classification. image input models can be built using a variety of techniques, including convolutional neural networks, which are specifically designed to work with image data.", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". image input models can be built using a variety of techniques, including convolutional neural networks, which are specifically designed to work with image data. sending images to arthur arthur takes in the raw images as input. this means that instead of taking in images as matrixes of pixel values, we take the images in with [. gif,. jpeg,. png,. tiff ] format. after sending these values to arthur, they are stored in an aws s3 bucket. updated 3 months ago what \u2019 s nextbinary classificationtable of contents sending images to arthur source : https : / / doc", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "an aws s3 bucket. updated 3 months ago what \u2019 s nextbinary classificationtable of contents sending images to arthur source : https : / / docs. arthur. ai / docs / image", "metadata": {"source": "https://docs.arthur.ai/docs/image", "row": 102, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 103 text : pre - requisites jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by pre - requisitessuggest editsthe following configurations should be applied to the platform to use arthur's backup and restore capabilities : arthur must be configured to use external object storage, specifically, aws s3 the access to external storage must be configured using irsa annotations in order to use irsa annotations, the cluster must be deployed using amazon eks if the above are not true / possible for your deployment", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##otations in order to use irsa annotations, the cluster must be deployed using amazon eks if the above are not true / possible for your deployment, please reach out to arthur support. configuring velero the only component that needs to be installed separately from arthur to perform backup and restores is velero. instructions are provided below for setting up velero to store backups in s3 using irsa. the general overview of the installation is as follows : setup permissions for velero install velero confirm velero is installed and configured correctly configure the backup storage destination", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is as follows : setup permissions for velero install velero confirm velero is installed and configured correctly configure the backup storage destination to point to s3 velero permissions generate the below policy which will grant velero the necessary permissions : textshell $ cat > velero - policy. json < < eof { \" version \" : \" 2012 - 10 - 17 \", \" statement \" : [ { \" effect \" : \" allow \", \" action \" : [ \" ec2 : describevolumes \", \" ec2 : describesnapshots \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" allow \", \" action \" : [ \" ec2 : describevolumes \", \" ec2 : describesnapshots \", \" ec2 : createtags \", \" ec2 : createvolume \", \" ec2 : createsnapshot \", \" ec2 : deletesnapshot \" ], \" resource \" : \" * \" }, { \" effect \" : \" allow \", \" action \" : [ \" s3 : getobject \", \" s3 : deleteobject \", \" s3 : putobject \", \" s3 :", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" s3 : getobject \", \" s3 : deleteobject \", \" s3 : putobject \", \" s3 : abortmultipartupload \", \" s3 : listmultipartuploadparts \" ], \" resource \" : [ \" arn : aws : s3 : : : $ { bucket } / * \" ] }, { \" effect \" : \" allow \", \" action \" : [ \" s3 : listbucket \" ], \" resource \" : [ \" arn : aws : s3 : : : $ { bucket", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": [ \" s3 : listbucket \" ], \" resource \" : [ \" arn : aws : s3 : : : $ { bucket } \" ] }, { \" effect \" : \" allow \", \" action \" : [ \" kms : creategrant * \", \" kms : reencrypt * \", \" kms : generatedatakey * \", \" kms : encrypt * \", \" kms : describekey * \", \" kms : decrypt * \" ], \" resource \" : \" * \" } ] } eof $ aws", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": describekey * \", \" kms : decrypt * \" ], \" resource \" : \" * \" } ] } eof $ aws iam create - policy \\ - - policy - name velero - perms \\ - - policy - document file : / / velero - policy. json attach this iam policy to the iam role that the arthur service account ( irsa ) assumes shell $ aws iam attach - role - policy \\ - - role - name < irsa - role - name > \\ - - policy - arn < policy - arn > [UNK] data at", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- policy \\ - - role - name < irsa - role - name > \\ - - policy - arn < policy - arn > [UNK] data at restarthur highly recommends that your ebs volumes are encrypted with kms. in addition to giving velero the permission for kms, please make sure that the iam roles assumed by arthur service account also have access to kms so the restored encrypted volumes can be re - attached. if you're using separate kms keys on the cluster you backed - up and the cluster you're restoring to, the ebs volume snapshots", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "if you're using separate kms keys on the cluster you backed - up and the cluster you're restoring to, the ebs volume snapshots must be copied with the new kms key so the new cluster can work with the snapshots. install velero velero can be installed on the kubernetes cluster using helm. generate a velero - values file as follows ( taken from the official source with defaults removed for brevity ) : yaml $ cat > velero - values. yaml < < eof resources : requests : cpu : 500m memory : 128mi", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ity ) : yaml $ cat > velero - values. yaml < < eof resources : requests : cpu : 500m memory : 128mi limits : cpu : 1000m memory : 512mi initcontainers : - name : velero - plugin - for - aws image : velero / velero - plugin - for - aws : v1. 6. 1 imagepullpolicy : ifnotpresent volumemounts : - mountpath : / target name : plugins podsecuritycontext : runasnonroot : true runasuser :", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ent volumemounts : - mountpath : / target name : plugins podsecuritycontext : runasnonroot : true runasuser : 1000 runasgroup : 1000 upgradecrds : true cleanupcrds : false configuration : # cloud provider being used ( e. g. aws, azure, gcp ). provider : aws backupstoragelocation : # name is the name of the backup storage location where backups should be stored. name : < insert - bsl - name - here > provider : aws # bucket is the name of the bucket to store backups in. required.", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "name : < insert - bsl - name - here > provider : aws # bucket is the name of the bucket to store backups in. required. bucket : < insert - s3 - bucket - name - here > config : region : us - east - 2 volumesnapshotlocation : # name is the name of the volume snapshot location where snapshots are being taken. required. name : < insert - vsl - name - here > config : region : us - east - 2 # these are server - level settings passed as cli flags to the ` velero server `", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "> config : region : us - east - 2 # these are server - level settings passed as cli flags to the ` velero server ` command. loglevel : debug namespace : < insert - velero - namespace - here > rbac : # whether to create the velero role and role binding to give all permissions to the namespace to velero. create : true # whether to create the cluster role binding to give administrator permissions to velero clusteradministrator : true # name of the clusterrole. clusteradministratorname : cluster", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to give administrator permissions to velero clusteradministrator : true # name of the clusterrole. clusteradministratorname : cluster - admin # information about the kubernetes service account velero uses. serviceaccount : server : create : true name : velero annotations : eks. amazonaws. com / sts - regional - endpoints : \" true \" eks. amazonaws. com / role - arn : < insert - iam - role - arn - here > labels : credentials : usesecret : false backupsenabled", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ role - arn : < insert - iam - role - arn - here > labels : credentials : usesecret : false backupsenabled : true snapshotsenabled : true deploynodeagent : false eof install the velero helm chart with the above values file : shell { note } $ velero _ namespace = < insert - velero - namespace - here > $ helm install velero vmware - tanzu / velero \\ - - create - namespace \\ - - namespace $ velero _ namespace \\ - - version 3.", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- tanzu / velero \\ - - create - namespace \\ - - namespace $ velero _ namespace \\ - - version 3. 2. 0 \\ - f velero - values. yaml arthur recommends installing velero in a different namespace from arthur, so velero can be managed separately from arthur. [UNK] to install veleroarthur recommends installing velero in a different namespace from arthur, so velero can be managed separately from arthur. verify velero installation to confirm that velero is installed and configured correctly : open the kots admin interface and", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can be managed separately from arthur. verify velero installation to confirm that velero is installed and configured correctly : open the kots admin interface and navigate to the \" snapshots \" tab click the \" check for velero \" button ( see the screenshot below ) validate the backup storage location the backup storage location is a velero resource that points to the s3 bucket where backups will be stored. use kubectl to validate connectivity / access to aws s3, which should say \" available \". shell $ velero _ namespace = < insert - velero -", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "connectivity / access to aws s3, which should say \" available \". shell $ velero _ namespace = < insert - velero - namespace - here > $ kubectl get backupstoragelocation - n $ velero _ namespace please do not proceed until the backup storage location is available. configuring clickhouse - backup configuring clickhouse - backup to store backups in remote storage ( e. g., s3 ) can be done in the admin console. once your cluster is set up for backup and restore, you should see the \" enable ol", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "., s3 ) can be done in the admin console. once your cluster is set up for backup and restore, you should see the \" enable olap database backup capabilities \" option in the \" olap database \" section. ensure that : the configuration that points to the bucket is correct the bucket name the bucket region the serviceaccount is the same serviceaccount that you've configured with the irsa annotation ( if you are not sure, enter the default value ) the iam role that you are using for the irsa annotation has the appropriate permissions to read / write / list from", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "enter the default value ) the iam role that you are using for the irsa annotation has the appropriate permissions to read / write / list from the s3 bucket the s3 path is where you want to be storing backups updated 3 months ago table of contents configuring velero velero permissions install velero verify velero installation validate the backup storage location configuring clickhouse - backup source : https : / / docs. arthur. ai / docs / pre - requisites", "metadata": {"source": "https://docs.arthur.ai/docs/pre-requisites", "row": 103, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 104 text : performance metrics jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by performance metricsmonitor and communicate model performancesuggest editswhether a performance alert has been triggered requiring the ml team's attention or an external stakeholder has requested a performance breakdown for a specific segment, arthur provides a single location to report on and explore model performance across an organization. performance metrics in arthur out - of - the - box it can be overwhelming for external stakeholders to balance too many technical definitions for model accuracy, primarily if", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". performance metrics in arthur out - of - the - box it can be overwhelming for external stakeholders to balance too many technical definitions for model accuracy, primarily if each team in an organization provides different baseline metrics. onboarded model schema automatically infers performance metrics of interest for your models, easily standardizing top performance metrics across the organization. each model types out - of - the - box performance metrics are defined for them in their model types section. user - defined metrics different stakeholders define performance differently and accommodate all measures of performance utilizing our ability to curate performance metrics by data slices and user - defined custom metrics", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "defined metrics different stakeholders define performance differently and accommodate all measures of performance utilizing our ability to curate performance metrics by data slices and user - defined custom metrics. performance metric ui guide performance metrics for specific models can be found within that model's overview tab. time display the default option for seeing performance charts in the ui is in time series mode. these charts provide an average of each metric overtime at the time interval specified at the top of the chart. snapshot mode for teams that are not interested in viewing their metrics over time, they can select snapshot mode in the top corner. this mode creates bar graphs of the", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for teams that are not interested in viewing their metrics over time, they can select snapshot mode in the top corner. this mode creates bar graphs of the average metric over the specified range from the global filters ( discussed more below ). segment sections of interest as referenced above, users can also segment data of interest. global filters global filters are available in the top corner of the ui. teams can apply time or attribute - based filter rules to generate representative charts for those groups. note : global filter are applied globally. global filters do not apply just to the performance graph or overview tab. these filters are applied globally across the ui. this", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "note : global filter are applied globally. global filters do not apply just to the performance graph or overview tab. these filters are applied globally across the ui. this means that they can be used to explore the inferences in the inference deep dive for example. however, it is important to remember that the filters are applied before navigating to other tabs. require ground truth one key thing to keep in mind is that many performance metrics require ground truth ( or labels ). teams that have a lag between prediction and ground truth should look into drift and anomaly metrics within arthur. updated 3 months ago what \u2019 s nextset alert", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "teams that have a lag between prediction and ground truth should look into drift and anomaly metrics within arthur. updated 3 months ago what \u2019 s nextset alerts on your performance metrics, or learn more about different metrics within arthur. alertingdrift and anomalyfairness metricsuser defined metricstable of contents performance metrics in arthur out - of - the - box user - defined metrics performance metric ui guide time display segment sections of interest global filters require ground truth source : https : / / docs. arthur. ai / docs / performance - metrics", "metadata": {"source": "https://docs.arthur.ai/docs/performance-metrics", "row": 104, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 105 text : versioning jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by versioningsuggest editsversioning is crucial for production systems in machine learning as it allows for the following : seamless updates and rollbacks of machine learning models enables performance monitoring and a / b testing, facilitating data - driven decisions for model improvements. and ensures reproducibility and auditability of deployed models, meeting compliance requirements and providing transparency in machine learning systems. model groups = = model tasks we can think of versions relating to a particular model", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ability of deployed models, meeting compliance requirements and providing transparency in machine learning systems. model groups = = model tasks we can think of versions relating to a particular model task ( or use case ). for example, a \u201c real time transaction fraud model \u201d will be a years - long project with much evolution. many small and large changes may occur over the years, from retraining with new data to building completely new architectures. when you represent these new versions in arthur scope, they go into a model group. within arthur, a model group refers to a group of versions of the models that are used for the same model task. creating", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "they go into a model group. within arthur, a model group refers to a group of versions of the models that are used for the same model task. creating a new model version with the python sdk new model versions within arthur are completely new arthurmodel objects. this means that building out a new model version consists of building a new arthurmodel object and linking it to your existing arthurmodel object through its arthur model group. that may seem a bit confusing, so we'll break the steps down below : step 0 : have an initial model on arthur to put the first version of your model onto arthur, you do not", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "we'll break the steps down below : step 0 : have an initial model on arthur to put the first version of your model onto arthur, you do not need to specify its version. you just need to follow the creating arthur model object section of the documentation. get the model group for arthurmodel you want to version once you have been running that version of your model on arthur for a while, you may decide that it is time to onboard a new model version. to ensure that this new version is connected with the old version, you will use the model _ group _ id. here is a code example of how to grab the", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ensure that this new version is connected with the old version, you will use the model _ group _ id. here is a code example of how to grab the model _ group _ id from a running arthurmodel object. python # get the model group for v1 ( we will save v2 in the same model group ) model _ group = connection. get _ model _ group ( arthur _ model _ v1 ) build a new arthurmodel object for this version build a new arthur model object for this version, following the same steps as creating arthur model object. just make sure that you do not save the model object until you", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a new arthur model object for this version, following the same steps as creating arthur model object. just make sure that you do not save the model object until you do the next step. python # register v2 of the model with arthur arthur _ model _ v2 = connection. model ( display _ name = \" magic _ model \", # the model name can be the same or different input _ type = inputtype. tabular, output _ type = outputtype. multiclass ) # creating mapping from predictions to ground truth pred _ to _ ground _ truth _ map = { } for i, name in enumerate (", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##class ) # creating mapping from predictions to ground truth pred _ to _ ground _ truth _ map = { } for i, name in enumerate ( pred _ df _ 2. columns ) : pred _ to _ ground _ truth _ map [ name ] = i arthur _ model _ v2. build ( ref _ df _ 2, ground _ truth _ column ='label ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map ) arthur _ model _ v2. get _ attribute ('label'). set ( categorical = true", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##d _ to _ ground _ truth _ map ) arthur _ model _ v2. get _ attribute ('label'). set ( categorical = true, categories = list ( ref _ df _ 2 ['label']. unique ( ) ) ) arthur _ model _ v2. review ( ) save your new version the connection between your old arthurmodel object and the new arthurmodel object happens when you save the model. here we can see how we are assigning our new model object the same model _ group _ id as our original arthurmodel object. we can also provide a version _ label which", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "we are assigning our new model object the same model _ group _ id as our original arthurmodel object. we can also provide a version _ label which will represent what this new version is in the platform. when we finish linking all that, we can save the model to the platform. python # add v2 of the model to the same model group as v1 arthur _ model _ v2. model _ group _ id = model _ group. id # assign a version label to v2 of the model arthur _ model _ v2. version _ label = \" v2 \" # save v2 of the model to arthur", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "version label to v2 of the model arthur _ model _ v2. version _ label = \" v2 \" # save v2 of the model to arthur model _ id = arthur _ model _ v2. save ( ) update inference sending techniques with all that done, your new arthurmodel object is now on the arthur platform. however, you are not done just yet. you need to ensure that any system you set up for sending inferences to your old model ( or want to set up for sending inferences to your new model ) is implemented to begin effectively monitoring. to do this, teams should follow the techniques listed in", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or want to set up for sending inferences to your new model ) is implemented to begin effectively monitoring. to do this, teams should follow the techniques listed in sending inferences. know that for model versioning, teams often follow a few different patterns : replacing old inference connection to new model id : if you are switching over completely to this new model version, replacing the old arthurmodel id connection with this new version's model id can be the easiest. setting up validation in production pipelines : many teams use versioning before completely switching which model is being served. teams may run different types of validation tests in production ( such as", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "in production pipelines : many teams use versioning before completely switching which model is being served. teams may run different types of validation tests in production ( such as a / b testing, shadow deployments, or canary tests ). these are all possible within arthur, you will just need to create different arthurmodel objects for each test and set up inference sending to the platform for them to be effective. updated 3 months ago table of contents model groups = = model tasks creating a new model version with the python sdk step 0 : have an initial model on arthur get the model group for arthurmodel you want to version build a new", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "new model version with the python sdk step 0 : have an initial model on arthur get the model group for arthurmodel you want to version build a new arthurmodel object for this version save your new version update inference sending techniques source : https : / / docs. arthur. ai / docs / versioning", "metadata": {"source": "https://docs.arthur.ai/docs/versioning", "row": 105, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 106 text : troubleshooting explainability jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by troubleshooting explainabilitysuggest editstroubleshooting attributeerror when loading predict function while this can be an issue with any model type, it is common to see when using sk - learn objects that take in custom user functions. we will use tfidfvectorizer as an example, which is a commonly used vectorizer for nlp models, that often utilizes custom user functions. a tfidfvectorizer", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ctorizer as an example, which is a commonly used vectorizer for nlp models, that often utilizes custom user functions. a tfidfvectorizer accepts a user defined tokenize function, which is used to split a text string into tokens. problem say this code was used to create your model. python # make _ model. py def tokenize ( text ) : # tokenize and lemmatize doc = nlp ( txt ) tokens = [ ] for token in doc : if not token. is _ stop and not token. is _ punct \\ and not token. is _ space and", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= [ ] for token in doc : if not token. is _ stop and not token. is _ punct \\ and not token. is _ space and token. lemma _! ='- pron -': tokens. append ( token. lemma _ ) return tokens def make _ model ( ) : # here we pass a custom function to an sklearn object vectorizer = tfidfvectorizer ( tokenizer = tokenize ) vectorizer. fit ( x _ train ) model = logisticregression ( ) model. fit ( vectorizer. transform ( x _ train )", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") vectorizer. fit ( x _ train ) model = logisticregression ( ) model. fit ( vectorizer. transform ( x _ train ) ) pipeline = make _ pipeline ( vectorizer, model ) joblib. dump ( pipeline,'model. pkl') if _ _ name _ _ = = \" _ _ main _ _ \" : make _ model ( ) now you create this entrypoint file to enable explainability : python # entrypoint. py model = joblib. load ( \". / model. pkl \" ) def predict ( fv ) : return model. predict _", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". py model = joblib. load ( \". / model. pkl \" ) def predict ( fv ) : return model. predict _ proba ( fv ) now when the sdk imports entrypoint to test the function, the following error gets thrown : attributeerror : module'_ _ main _ _'has no attribute'tokenize'what happens is that python failed to serialize the custom function, only the reference to how it was imported. which in this case, it was just top level in the model creation script ( hence _ _ main _ _. tokenize in the error ).", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "imported. which in this case, it was just top level in the model creation script ( hence _ _ main _ _. tokenize in the error ). this function doesn't exist in entrypoint, and so the error is thrown. solution to solve, you need to pull out tokenize into its own module, that can be imported from both create _ model. py and also in entrypoint. py. python # model _ utils. py def tokenize ( text ) : # tokenize and lemmatize doc = nlp ( txt ) tokens = [ ] for token in doc :", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "tokenize ( text ) : # tokenize and lemmatize doc = nlp ( txt ) tokens = [ ] for token in doc : if not token. is _ stop and not token. is _ punct \\ and not token. is _ space and token. lemma _! ='- pron -': tokens. append ( token. lemma _ ) return tokens python # create _ model. py from model _ utils import tokenize def make _ model ( ) : # here we pass a custom function to an sklearn object vectorizer = tfidfve", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "utils import tokenize def make _ model ( ) : # here we pass a custom function to an sklearn object vectorizer = tfidfvectorizer ( tokenizer = tokenize ) vectorizer. fit ( x _ train ) model = logisticregression ( ) model. fit ( vectorizer. transform ( x _ train ) ) pipeline = make _ pipeline ( vectorizer, model ) joblib. dump ( pipeline,'model. pkl') if _ _ name _ _ = = \" _ _ main _ _ \" : make _ model ( ) python # entrypoint. py from", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##l') if _ _ name _ _ = = \" _ _ main _ _ \" : make _ model ( ) python # entrypoint. py from model _ utils import tokenize model = joblib. load ( \". / model. pkl \" ) def predict ( fv ) : return model. predict _ proba ( fv ) now, when python serializes the model, it stores the reference as model _ utils. tokenize, which is also imported within entrypoint. py and therefore no error is thrown. now everything will work, but both model _ utils. py and", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "which is also imported within entrypoint. py and therefore no error is thrown. now everything will work, but both model _ utils. py and entrypoint. py must be included in the directory passed to enable _ explainability ( ). updated 3 months ago table of contents troubleshooting attributeerror when loading predict function source : https : / / docs. arthur. ai / docs / troubleshooting - explainability", "metadata": {"source": "https://docs.arthur.ai/docs/troubleshooting-explainability", "row": 106, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 107 text : registering a model with the api jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by registering a model with the apithis page describes the process of registering models through standard rest api calls. suggest editsintroduction as an api - first solution, arthur scope allows the entire model onboarding process to be fully automated, either through our sdk, or through standard rest api calls. this allows arthur to be integrated with basically any ml platform, workflow management or automation software. this is possible because every step of the onboard", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "rest api calls. this allows arthur to be integrated with basically any ml platform, workflow management or automation software. this is possible because every step of the onboarding process can be achieved through api calls. in this page, we will describe the process of onboarding a model using standard api calls. we will also be providing hints about how the sdk can be leveraged to help with some of these steps, for python - based automation environments. the main steps required to onboard a model are : create the model schema definition and set basic metadata save the model upload reference dataset for model manage model enrichments ( bias monitoring,", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model are : create the model schema definition and set basic metadata save the model upload reference dataset for model manage model enrichments ( bias monitoring, explainability, hotspots, etc ) create model alert rules ( optional ) create the model schema definition and basic metadata to onboard a model, arthur scope needs information about the attributes ( input, non - input, prediction and ground truth ), as well as some basic metadata about the model. when onboarding a model through the api, this information is sent in json format, as in the example below : json { \" display _ name \" : \" credit risk", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model through the api, this information is sent in json format, as in the example below : json { \" display _ name \" : \" credit risk \", \" partner _ model _ id \" : \" creditriskmodel _ fg _ 20230523115857 \", \" description \" : \" credit risk model created through rest api \", \" input _ type \" : \" tabular \", \" output _ type \" : \" multiclass \", \" attributes \" : [ { \" name \" : \" limit _ bal \", \" value _ type \" : \" integer \", \" stage \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" attributes \" : [ { \" name \" : \" limit _ bal \", \" value _ type \" : \" integer \", \" stage \" : \" pipeline _ input \", \" position \" : 0, \" categorical \" : false, \" min _ range \" : 10000, \" max _ range \" : 1000000, \" monitor _ for _ bias \" : false, \" is _ unique \" : true, \" is _ positive _ predicted _ attribute \" : false },... ], \" tags \" : [ \" tabular \", \" classification \", \" credit \" ], \" is _ batch", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": false },... ], \" tags \" : [ \" tabular \", \" classification \", \" credit \" ], \" is _ batch \" : true, \" version _ label \" : \" credit risk v1 \" } ps : most of the attributes were removed from this code block, for readability. please refer to the documentation for more details on which metadata parameters are required. do keep in mind that the partner _ model _ id attribute must be absolutely unique, even across different versions of the model, or for models that were deleted and re - created. the recommendation is to add a timestamp to the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "unique, even across different versions of the model, or for models that were deleted and re - created. the recommendation is to add a timestamp to the id, in order to avoid potential duplication. when preparing the model schema, the bulk of the work will go to the task of preparing the list of attributes. as we can see from the example above, the model attributes are sent as a list inside the json payload. this list must include all input, non - input ( optionally ), prediction, and ground truth attributes. attributes cannot be added once the model is saved, so this list must be complete at", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "non - input ( optionally ), prediction, and ground truth attributes. attributes cannot be added once the model is saved, so this list must be complete at model saving time. every attribute has several parameters that can be set. this is a more complete list of parameters from the api documentation : json { \" name \" : \" string \", \" label \" : \" string \", \" value _ type \" : \" boolean \", \" stage \" : \" ground _ truth \", \" position \" : 0, \" categorical \" : false, \" categories \" : [ ], \" min _ range \" : 0, \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "truth \", \" position \" : 0, \" categorical \" : false, \" categories \" : [ ], \" min _ range \" : 0, \" max _ range \" : 0, \" monitor _ for _ bias \" : false, \" bins \" : [ ], \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false, \" attribute _ link \" : \" string \", \" gt _ class _ link \" : \" string \", \" implicit \" : false } do note that some of these parameters only apply to certain types of attributes. a brief description of each attribute can be", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "string \", \" implicit \" : false } do note that some of these parameters only apply to certain types of attributes. a brief description of each attribute can be found in the table below : attributedescriptionnamename of the attribute. must be unique within the model. labela friendly label that can be set for attribute names that are encoded. value _ typetype of data managed by the attribute ( integer, float, etc ). a list of supported value types can be found here. stagedetermines whether this attribute is input, non - input, prediction or ground truth. positionthis is an incremental counter that", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "found here. stagedetermines whether this attribute is input, non - input, prediction or ground truth. positionthis is an incremental counter that should start with 0 and increase by 1 for each attribute ( with the exception of prediction and ground truth attributes, that must start at zero ). categoricalset it to true if the attribute has a limited number of potential values. categorieslist all the categories ( potential values ) for the attribute. only relevant if categorical is set to true. min _ rangefor non - categorical, numerical attributes. lowest numerical value this attribute should receive ( ps : this will not be enforced", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is set to true. min _ rangefor non - categorical, numerical attributes. lowest numerical value this attribute should receive ( ps : this will not be enforced as a threshold limit ). max _ rangefor non - categorical, numerical attributes. highest numerical value this attribute should receive ( ps : this will not be enforced as a threshold limit ). monitor _ for _ biastrue or false. determines whether or not this attribute should be monitored for bias. binsfor non - categorical, numerical attributes that are being monitored for bias. describes the bins ( or buckets ) that arthur should use to group inferences as", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "non - categorical, numerical attributes that are being monitored for bias. describes the bins ( or buckets ) that arthur should use to group inferences as it checks for bias. is _ uniqueused to determined if the values of the attribute are unique ( ps : this will not be enforced through unique constraint validation ). is _ positive _ predicted _ attributeonly used for the predicted attribute of a binary classification model ( where there's a'positive'and'negative'prediction. attribute _ linkused to associate prediction attributes to their corresponding ground truth attributes. gt _ class _ linkused for single ground truth class models, where", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' prediction. attribute _ linkused to associate prediction attributes to their corresponding ground truth attributes. gt _ class _ linkused for single ground truth class models, where the prediction attribute is associated with the corresponding string value of the ground truth class. next, let's examine some attributes to see how they can be configured. json { \" name \" : \" limit _ bal \", \" value _ type \" : \" integer \", \" stage \" : \" pipeline _ input \", \" position \" : 0, \" categorical \" : false, \" min _ range \" : 10000, \" max _ range \" : 1000000", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" position \" : 0, \" categorical \" : false, \" min _ range \" : 10000, \" max _ range \" : 1000000, \" monitor _ for _ bias \" : false, \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false }, this is an integer input attribute, non - categorical, that can range between 10, 000 and 1, 000, 000. it will not be monitored for bias. json { \" name \" : \" age \", \" value _ type \" : \" integer \", \" stage \" : \" pipeline _ input \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "bias. json { \" name \" : \" age \", \" value _ type \" : \" integer \", \" stage \" : \" pipeline _ input \", \" position \" : 1, \" monitor _ for _ bias \" : true, \" bins \" : [ { \" continuous _ start \" : 0, \" continuous _ end \" : 35 }, { \" continuous _ start \" : 35, \" continuous _ end \" : 55 }, { \" continuous _ start \" : 55, \" continuous _ end \" : 100 } ], \" categorical \" : false, \" min _ range \" : 21, \" max _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : 55, \" continuous _ end \" : 100 } ], \" categorical \" : false, \" min _ range \" : 21, \" max _ range \" : 79, \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false }, this is an integer input attribute, non - categorical, being monitored for bias. because this attribute is non - categorical, we need to specify the bins for bias monitoring, so that arthur knows how to group the inferences. json { \" name \" : \" sex \", \" value _ type \" : \" integer \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that arthur knows how to group the inferences. json { \" name \" : \" sex \", \" value _ type \" : \" integer \", \" stage \" : \" pipeline _ input \", \" position \" : 2, \" monitor _ for _ bias \" : true, \" categorical \" : true, \" categories \" : [ { \" value \" : \" 1 \", \" label \" : \" male \" }, { \" value \" : \" 2 \", \" label \" : \" female \" } ], \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false }, this", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" label \" : \" female \" } ], \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false }, this is an integer input, categorical attribute. it will be monitored for bias ; in this case, arthur will used the defined categories to group inferences for bias monitoring. json { \" name \" : \" unit _ code \", \" value _ type \" : \" string \", \" stage \" : \" pipeline _ input \", \" position \" : 3, \" categorical \" : true, \" monitor _ for _ bias \" : false, \" categories \" : [ {", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "input \", \" position \" : 3, \" categorical \" : true, \" monitor _ for _ bias \" : false, \" categories \" : [ { \" value \" : \" dw00 \" }, { \" value \" : \" a800 \" }, { \" value \" : \" yz00 \" }, { \" value \" : \" m 00 \" }, { \" value \" : \" h 00 \" }, { \" value \" : \" h100 \" }, { \" value \" : \" n 00 \" }, { \" value \" : \" t800 \" }, { \" value \" :", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##100 \" }, { \" value \" : \" n 00 \" }, { \" value \" : \" t800 \" }, { \" value \" : \" a 00 \" }, { \" value \" : \" gx00 \" }, { \" value \" : \" rg00 \" }, { \" value \" : \" jl00 \" }, { \" value \" : \" tc00 \" }, { \" value \" : \" r 00 \" }, { \" value \" : \" lv00 \" }, { \" value \" : \" e 00 \" }, { \" value \" : \" t 00 \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" value \" : \" lv00 \" }, { \" value \" : \" e 00 \" }, { \" value \" : \" t 00 \" } ], \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false }, this is a string input. all string input attributes should be categorical. json { \" name \" : \" risk _ amt \", \" value _ type \" : \" float \", \" stage \" : \" non _ input _ data \", \" position \" : 4, \" categorical \" : false, \" min _ range \" : 10152", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "stage \" : \" non _ input _ data \", \" position \" : 4, \" categorical \" : false, \" min _ range \" : 10152. 23, \" max _ range \" : 999990. 0, \" monitor _ for _ bias \" : false, \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : false }, this is a float, non - input attribute. it is non - categorical and it will not be monitored for bias. json { \" name \" : \" prediction _ 1 \", \" value _ type \" : \" float \", \" stage", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will not be monitored for bias. json { \" name \" : \" prediction _ 1 \", \" value _ type \" : \" float \", \" stage \" : \" predicted _ value \", \" position \" : 1, \" categorical \" : false, \" min _ range \" : 0, \" max _ range \" : 1, \" monitor _ for _ bias \" : false, \" is _ unique \" : false, \" is _ positive _ predicted _ attribute \" : true, \" attribute _ link \" : \" ground _ truth _ 1 \" }, { \" name \" : \" ground _ truth _ 1 \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": true, \" attribute _ link \" : \" ground _ truth _ 1 \" }, { \" name \" : \" ground _ truth _ 1 \", \" value _ type \" : \" integer \", \" stage \" : \" ground _ truth \", \" position \" : 1, \" categorical \" : true, \" categories \" : [ { \" value \" : \" 0 \" }, { \" value \" : \" 1 \" } ], \" is _ unique \" : false, \" monitor _ for _ bias \" : false, \" is _ positive _ predicted _ attribute \" : false, \" attribute _ link \" : \" prediction", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "false, \" monitor _ for _ bias \" : false, \" is _ positive _ predicted _ attribute \" : false, \" attribute _ link \" : \" prediction _ 1 \" } this is a pair of matching prediction and ground truth attributes. they must be explicitly correlated for arthur to be able to calculate performance metrics. regression models will have one prediction and one ground truth attributes, while multiclass models will have many. for every prediction attribute declared, a corresponding ground truth attribute must also be provided, even if this model is not expected to receive ground truth data at all. the position of these elements should match as well. in this case", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "also be provided, even if this model is not expected to receive ground truth data at all. the position of these elements should match as well. in this case, the prediction _ 1 is the'positive'prediction made by the model, so we will mark it as such. arthur expects the prediction attribute to always be a float ( the probability of this class ), while the ground truth should be either an integer ( 0 or 1 ) or a string. [UNK] arthur sdk can be used to help with the process of mapping model attributes, especially for models with dozens or hundreds of attributes. the sdk provides a build ( ) function", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can be used to help with the process of mapping model attributes, especially for models with dozens or hundreds of attributes. the sdk provides a build ( ) function that will create a dataframe with the model attributes, based on the reference data provided. this dataframe can then be converted to json and used on a direct rest api call. consider leveraging the arthur sdk to do the basic mapping of attributes, and then apply your specific logic to ensure the proper value types, ranges, categories, etc. save the model with the model schema and metadata in place, saving the model is a simple call to : post { {", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", ranges, categories, etc. save the model with the model schema and metadata in place, saving the model is a simple call to : post { { hostname } } / api / v3 / models documentation : https : / / docs. arthur. ai / api - documentation / v3 - api - docs. html # tag / models / paths / ~ 1models / post ps : this will require credentials with the model owner role, or a custom role allowed to create models. if the call is successful, the return message will include the model id. this model id will be required for all subsequent", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a custom role allowed to create models. if the call is successful, the return message will include the model id. this model id will be required for all subsequent calls, so make sure to save it to an environment variable. a number of backend operations are executed at model saving time, including creating the database tables for the model, along with kafka topics and other components. this process should only take a few seconds. at this time, the model is saved and it has the necessary infrastructure to receive data. however, since no reference dataset is available, drift and anomaly scores will not be calculated. also, none of the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "has the necessary infrastructure to receive data. however, since no reference dataset is available, drift and anomaly scores will not be calculated. also, none of the other enrichments will have been enabled. upload reference dataset for model the reference dataset is required to calculate drift metrics, as well as to train the anomaly score model that arthur will create for each model being monitored. when models are saved through the arthur sdk, several tasks happen automatically behind the scenes : the sdk will save the reference data to a parquet or json file, and will upload it to arthur once the model is done saving. those", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the sdk will save the reference data to a parquet or json file, and will upload it to arthur once the model is done saving. those steps must be executed explicitly when saving the model through direct api calls. the reference data must contain all input, non - input, prediction, and ground truth attributes. it must also include the column headers : it should also be representative of all expected values for the inferences ; otherwise, the drift scores might be misleading. for instance, if the reference dataset is only comprised of records for customers between 20 and 50 years old, any inference data about 80 - year - old", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". for instance, if the reference dataset is only comprised of records for customers between 20 and 50 years old, any inference data about 80 - year - old customers will receive a high drift score. setting the reference data is done in 2 steps : upload the reference data close the reference data upload the reference data the parquet or json file can be uploaded to the following api endpoint : post { { hostname } } / api / v3 / models / { { model _ id } } / reference _ data the file should be attached as reference _ data. this is a python example for that call : payload =", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ { model _ id } } / reference _ data the file should be attached as reference _ data. this is a python example for that call : payload = { } files = [ ('reference _ data ', ('dataset. parquet ', open ('/ users /... / dataset. parquet ','rb'),'application / octet - stream') ) ] headers = {'arthur - organization - id':'{ { organization _ id } } ','authorization':'{ { access _ token } }'} response = requests. request (", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' :'{ { organization _ id } } ','authorization':'{ { access _ token } }'} response = requests. request ( \" post \", url, headers = headers, data = payload, files = files ) ps : the arthur - organization - id header is required for environments with multiple organizations. the return message will include the number of records that were successfully uploaded. note this number, as you will need it for the next call. close the reference data arthur will wait until the reference data is closed before doing the backend processing ( which includes training the anomaly score model and other tasks", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "call. close the reference data arthur will wait until the reference data is closed before doing the backend processing ( which includes training the anomaly score model and other tasks ). the reference data can not be modified once it's closed, so ensure the proper data is in place before executing the next call. reference data can be closed through a patch call to the same endpoint as before : patch { { hostname } } / api / v3 / models / { { model _ id } } / reference _ data the body must include the number of records uploaded, which must match the number of successful records uploaded in the previous step :", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ id } } / reference _ data the body must include the number of records uploaded, which must match the number of successful records uploaded in the previous step : json { \" status \" : \" uploaded \", \" total _ record _ count \" : 30000 } in this case, the reference data uploaded had 30, 000 records. manage model enrichments arthur provides a set of standard enrichments that are available for all model types. other enrichments will be specific to certain model types. in this section, we will review the available api endpoints for the different enrichment capabilities. main documentation page : https : / / docs.", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model types. in this section, we will review the available api endpoints for the different enrichment capabilities. main documentation page : https : / / docs. arthur. ai / api - documentation / v3 - api - docs. html # tag / enrichments [UNK] in mind that most enrichments will only be applied to new inferences. make sure to have all enrichments enabled before the model gets populated with data. retrieving a list of current enrichments the following api endpoint can be used to fetch a list of enrichments configured for a model : get { { hostname } } / api / v3 /", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "following api endpoint can be used to fetch a list of enrichments configured for a model : get { { hostname } } / api / v3 / models / { { model _ id } } / enrichments the return message ( for a fully configured model ) will look like this : json { \" anomaly _ detection \" : { \" enabled \" : true, \" config \" : { } }, \" bias _ mitigation \" : { \" enabled \" : false }, \" explainability \" : { \" enabled \" : true, \" config \" : { \" sdk _ version \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : false }, \" explainability \" : { \" enabled \" : true, \" config \" : { \" sdk _ version \" : \" 3. 25. 0 \", \" python _ version \" : \" 3. 8 \", \" explanation _ algo \" : \" shap \", \" model _ server _ cpu \" : \" 1000m \", \" model _ server _ memory \" : \" 1gi \", \" explanation _ nsamples \" : 2000, \" shap _ expected _ values \" : [ 0. 8227832963996526, 0. 1772167036", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 2000, \" shap _ expected _ values \" : [ 0. 8227832963996526, 0. 17721670360034752 ], \" inference _ consumer _ cpu \" : \" 100m \", \" inference _ consumer _ memory \" : \" 256mi \", \" model _ server _ max _ replicas \" : 2, \" inference _ consumer _ score _ percent \" : 1, \" streaming _ explainability _ enabled \" : true, \" user _ predict _ function _ import _ path \" : \" entrypoint \", \" inference _ consumer _ thread _ pool _ size \"", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : true, \" user _ predict _ function _ import _ path \" : \" entrypoint \", \" inference _ consumer _ thread _ pool _ size \" : 2 } }, \" hotspots \" : { \" enabled \" : true, \" config \" : { } } } in this example, we can see that this model has anomaly detection, explainability and hotspots enabled, while bias mitigation is currently disabled. it is possible to update the configuration for all enrichments with a single call. the url would be the same as described above, only using the patch method instead of get. the", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the configuration for all enrichments with a single call. the url would be the same as described above, only using the patch method instead of get. the json block above would be sent as part of the form - multipart request. do keep in mind that explainability requires additional assets to be attached ( this will be covered further down in this page ). anomaly detection anomaly detection is automatically enabled for every model other than time series input models, once the reference data is uploaded ( and closed ). because of this, it does not need to be explicitly enabled after the model is saved. it can be disabled and re -", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is uploaded ( and closed ). because of this, it does not need to be explicitly enabled after the model is saved. it can be disabled and re - enabled at any time ( at which point, the anomaly score model will be re - trained from the reference data ). this is the api endpoint to check the status of the anomaly detection enrichment : get { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / anomaly _ detection the return payload will look like this : json { \" enabled \" : true, \" config \" : { } } to", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s / anomaly _ detection the return payload will look like this : json { \" enabled \" : true, \" config \" : { } } to modify the status ( enable / disable ), the same endpoint can be used, with the patch method : patch { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / anomaly _ detection the request body will contain the desired status for the enrichment : json { \" enabled \" : false } in this case, anomaly detection is being disabled for this model. bias mitigation to check the status of bias", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "json { \" enabled \" : false } in this case, anomaly detection is being disabled for this model. bias mitigation to check the status of bias mitigation, use this api endpoint : get { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / bias _ mitigation the return payload will be similar to this : json { \" enabled \" : false } to update the status, use the same endpoint, with the patch method : patch { { hostname } } / api / v3 / models / { { model _ id }", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "use the same endpoint, with the patch method : patch { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / bias _ mitigation the request body will contain the desired status for this enrichment : json { \" enabled \" : false } hotspots hotspots is a capability that aims to identify and surface regions of underperformance in the model. it is currently only available for tabular - classification models. hotspots will not be automatically enabled once the model is saved. it can, however, be enabled at any time after that, and", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- classification models. hotspots will not be automatically enabled once the model is saved. it can, however, be enabled at any time after that, and it does not require any additional data or assets. to check the status of hotspots, use this api endpoint : get { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / hotspots the return payload will be similar to this : json { \" enabled \" : true, \" config \" : { } } to update the status, use the same endpoint, with the patch method", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##son { \" enabled \" : true, \" config \" : { } } to update the status, use the same endpoint, with the patch method : patch { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / hotspots the request body will contain the desired status for this enrichment : json { \" enabled \" : false } explainability with explainability enabled, arthur is able to provide explanations for each inference. this data is also used for global explainability ( feature importance, etc ). this capability requires arthur to be able to generate predictions on demand", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for each inference. this data is also used for global explainability ( feature importance, etc ). this capability requires arthur to be able to generate predictions on demand, which means arthur needs a working model that can be called at any time. because of that, there are a number of assets required to enable explainability, including the model assets, and a python function that can call the model's predict function and return the probability arrays. also, some explainability assets need to be generated by the arthur sdk, which makes the sdk a required part of the explainability process. [UNK] uses lime or shap, which are industry", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "be generated by the arthur sdk, which makes the sdk a required part of the explainability process. [UNK] uses lime or shap, which are industry accepted algorithms, to produce explanations. this means that the explainability assets will be python - based. these algorithms will be packaged in an explainer object, that must be created with the arthur sdk. this explainer is uploaded as part of the call to enable explainability. in order to enable explainability for a model, the following assets and files are required : user _ project. zip file this is a zip file that contains all required model assets, and an'entry", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model, the following assets and files are required : user _ project. zip file this is a zip file that contains all required model assets, and an'entrypoint'file the model assets include anything required to create a prediction :. pkl files, tokenizers, etc. arthur must be able to generate a prediction from the inference data received. the entrypoint file is a python file that will run the predict ( ) function. this is the appropriate place for any data manipulation that might be required for the input data : transformations, scaling, one - hot encoding, etc. this file must contain a predict ( ) function and", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data manipulation that might be required for the input data : transformations, scaling, one - hot encoding, etc. this file must contain a predict ( ) function and return an array of probabilities. the model assets must be in the root of the zip file ( not in a subfolder ). this is important, as it will fail to generate predictions otherwise. user _ requirements _ file. txt ( requirements. txt file ) this file contains a list of python requirements to run the model. make sure it includes all pre - reqs to your model, or the entrypoint file will fail to load ( with '", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "requirements to run the model. make sure it includes all pre - reqs to your model, or the entrypoint file will fail to load ( with'package not found'exceptions ) explainer. pkl this is the explainer object created by arthur sdk. it will include the lime or shap algorithm to produce feature importance data. config this is a json block with additional settings. it's the same as can be seen in the get / enrichments example above. preparing the user _ project zip file move all required model assets to a folder. keep in mind that arthur will automatically install the packages", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ enrichments example above. preparing the user _ project zip file move all required model assets to a folder. keep in mind that arthur will automatically install the packages listed in the requirements. txt file, from a public or private repository. in that folder, create the entrypoint. py file. this file must contain a predict ( ) function, which arthur will call at runtime. the predict ( ) function will received a 2 - d numpy array, where each item represents a perturbation of the original attribute input data. the default number of perturbations is 5000 ( it is a configurable", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "each item represents a perturbation of the original attribute input data. the default number of perturbations is 5000 ( it is a configurable parameter ). so the entrypoint file must be able to process an array of input elements. the expected return is a 2 - d numpy array with the probability scores. the size of this array should match the size of the input data. this is an example of entrypoint file : pythonimport joblib import os from pathlib import path model _ path = os. path. join ( path ( _ _ file _ _ ). resolve ( ). parents [", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "import os from pathlib import path model _ path = os. path. join ( path ( _ _ file _ _ ). resolve ( ). parents [ 0 ], \" credit _ model. pkl \" ) sk _ model = joblib. load ( model _ path ) def predict ( x ) : return sk _ model. predict _ proba ( x ) [ :, 1 ] in this case, the entrypoint file loads the model and uses the predict ( ) function to wrap the predict _ proba ( ) function from the model. more complicated cases might require data transformation, one - hot encoding, etc.", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( ) function to wrap the predict _ proba ( ) function from the model. more complicated cases might require data transformation, one - hot encoding, etc. keep in mind that the input of the predict ( ) function will always be an array of input elements, and arthur expects to receive as return an array of probabilities. next, prepare the requirements. txt file with all the necessary packages. finally, zip the folder, ensuring that the entrypoint. py and requirements. txt files are at the root. creating the explainer. pkl file the easiest way to create this file is by using the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and requirements. txt files are at the root. creating the explainer. pkl file the easiest way to create this file is by using the arthur sdk. depending on the environment, it might be easier to use the enable _ explainability ( ) function of the sdk. some customer environments, however, will prefer to run these functions as pure rest calls, without having to load the arthur sdk. so the explainability assets can be prepared in advance and then uploaded through direct rest calls. the basic steps to create the explainer object are : connect to the arthur instance load the model definition from arthur prepare the (", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "then uploaded through direct rest calls. the basic steps to create the explainer object are : connect to the arthur instance load the model definition from arthur prepare the ( unzipped ) user project folder, with the entrypoint and requirements files load reference data ( can be a small subset ) create a packager object using the sdk extract the explainer from the packager save the explainer to a. pkl file in the user project folder save the model ( regressor / classifier / etc ) to a. pkl file in the user project folder this code example walks through those steps : pythonconnection =", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##or / classifier / etc ) to a. pkl file in the user project folder this code example walks through those steps : pythonconnection = arthurai ( url = url, login = login, password = password ) arthur _ model = connection. get _ model ( model _ id ) import os project _ dir = os. path. join ( os. getcwd ( ) + \" / explain \" ) from arthurai. explainability. explanation _ packager import explanationpackager packager = explanationpackager ( arthur _ model, df = df _ reference, project _ directory =", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". explanation _ packager import explanationpackager packager = explanationpackager ( arthur _ model, df = df _ reference, project _ directory = project _ dir, user _ predict _ function _ import _ path ='entrypoint ', streaming _ explainability _ enabled = true, requirements _ file = \" requirements. txt \", explanation _ algo ='lime') packager. create ( ) arthur _ explainer = packager. explainer # test the explainer by producing 1 explanation # the input for the explainer is a list of input values - - not a dataframe # remove non -", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "# test the explainer by producing 1 explanation # the input for the explainer is a list of input values - - not a dataframe # remove non - input, prediction and ground truth attributes from this list def get _ sample ( num _ samples ) : sample _ inf = df _ reference. sample ( num _ samples ) del sample _ inf ['pred'] del sample _ inf ['gt'] return sample _ inf. values. tolist ( ) sample _ inf = get _ sample ( 1 ) explanation = arthur _ explainer. explain _ tabular _ lime (", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". values. tolist ( ) sample _ inf = get _ sample ( 1 ) explanation = arthur _ explainer. explain _ tabular _ lime ( sample _ inf ) print ( explanation ) # save model to. pkl file import dill as pickle with open ( \". / explain / model. pkl \", \" wb \" ) as file : pickle. dump ( regressor, file ) # save explainer to. pkl file with open ( \". / explain / explainer. pkl \", \" wb \" ) as file : pickle. dump ( arthur", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##kl file with open ( \". / explain / explainer. pkl \", \" wb \" ) as file : pickle. dump ( arthur _ explainer, file ) currently, the project folder will contain the model. pkl file, the explainer, the entrypoint, and the requirements. file. these are all the assets required to enable explainability. enabling explainability this is the api endpoint to enable explainability : patch { { hostname } } / api / v3 / models / { { model _ id } } / enrichments / explainability this is a multipart / form", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##name } } / api / v3 / models / { { model _ id } } / enrichments / explainability this is a multipart / form - data call that must include the elements described above. the following python code is an example of how this call can be configured : pythonheaders = {'arthur - organization - id':'{ { organization _ id } } ','authorization':'{ { access _ token } }'} files = [ ('user _ project. zip ', ('explain. zip ', open ( '. / explain. zip ','rb')", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "= [ ('user _ project. zip ', ('explain. zip ', open ( '. / explain. zip ','rb'),'application / zip') ), ('user _ requirements _ file. txt ', ('requirements. txt ', open ( '. / explain / requirements. txt ','rb'),'text / plain') ), ('explainer. pkl ', ('explainer. pkl ', open ( '. / explain / explainer. pkl ','rb'), '", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", ('explainer. pkl ', open ( '. / explain / explainer. pkl ','rb'),'application / octet - stream') ) ] payload = {'config':'{ \" enabled \" : true, \" config \" : { \" python _ version \" : \" 3. 8 \", \" sdk _ version \" : \" 3. 25. 0 \", \" streaming _ explainability _ enabled \" : true, \" user _ predict _ function _ import _ path \" : \" entrypoint \", \" shap _ expected _ values", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "explainability _ enabled \" : true, \" user _ predict _ function _ import _ path \" : \" entrypoint \", \" shap _ expected _ values \" : [ 0 ], \" model _ server _ cpu \" : \" 2 \", \" model _ server _ memory \" : \" 1500mi \", \" model _ server _ max _ replicas \" : 30, \" explanation _ nsamples \" : 2000, \" explanation _ algo \" : \" lime \", \" inference _ consumer _ cpu \" : \" 500m \", \" inference _ consumer _ memory \" : \" 512mi \", \" inference _ consumer _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" inference _ consumer _ cpu \" : \" 500m \", \" inference _ consumer _ memory \" : \" 512mi \", \" inference _ consumer _ score _ percent \" : 1, \" inference _ consumer _ thread _ pool _ size \" : 5 } }'} exp _ response = session. request ( \" patch \", f'{ url } / models / { model _ id } / enrichments / explainability ', headers = headers, data = payload, files = files ) the expected response is an \" ok \" string. in the backend, arthur will provision a model server, which", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", data = payload, files = files ) the expected response is an \" ok \" string. in the backend, arthur will provision a model server, which will be able to provide explanations on demand. this process should take a few minutes to complete. appendix : sending inferences once the model is saved to arthur and all enrichments are enabled, everything should be ready to receive and process inference data. inferences can be sent to arthur in several different ways : individually or in batches ; in json format or as parquet, and when using the arthur sdk, directly as pandas dataframes. when sending inference data", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "batches ; in json format or as parquet, and when using the arthur sdk, directly as pandas dataframes. when sending inference data, the following information is required : inference timestamp inference id inference data ( all input values ) non - input data ( optional, but it cannot be uploaded after the fact ) prediction attributes optionally, ground truth data can also be sent ( when available ). this is a json payload example sending 2 inferences : jsonpayload = json. dumps ( [ { \" partner _ inference _ id \" : \" inf _ \" + str", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": jsonpayload = json. dumps ( [ { \" partner _ inference _ id \" : \" inf _ \" + str ( uuid. uuid4 ( ) ), \" inference _ timestamp \" : \" 2023 - 06 - 07t12 : 00 : 13. 656449z \", \" inference _ data \" : { \" input _ 1 \" : 1000, \" input _ 2 \" : \" abdc1234 \", \" input _ 3 \" : 9999, \" pred \" : 0. 85 }, \" ground _ truth _ timestamp \" :", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##34 \", \" input _ 3 \" : 9999, \" pred \" : 0. 85 }, \" ground _ truth _ timestamp \" : \" 2023 - 06 - 07t12 : 00 : 13. 656449z \", \" ground _ truth _ data \" : { \" gt \" : 1 } }, { \" partner _ inference _ id \" : \" inf _ \" + str ( uuid. uuid4 ( ) ), \" inference _ timestamp \" : \" 2023 - 05 - 07t12 : 01 : 13. 656449z \", \" inference _", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" inference _ timestamp \" : \" 2023 - 05 - 07t12 : 01 : 13. 656449z \", \" inference _ data \" : { \" input _ 1 \" : 2000, \" input _ 2 \" : \" abdc12345 \", \" input _ 3 \" : 8888, \" pred \" : 0. 65 }, \" ground _ truth _ timestamp \" : \" 2022 - 05 - 07t12 : 01 : 13. 656449z \", \" ground _ truth _ data \" : { \" gt \" : 0 } } ] ) ps :", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##12 : 01 : 13. 656449z \", \" ground _ truth _ data \" : { \" gt \" : 0 } } ] ) ps : for batch models, the batch _ id parameter is required. in this example, ground truth data is being sent along with inference data. arthur supports uploading ground truth data at any moment after receiving the inference data. this is the api endpoint that can be used to send inferences in json format : post { { hostname } } / api / v3 / models / { { model _ id } } / inferences sending inferences as parquet files a different", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ hostname } } / api / v3 / models / { { model _ id } } / inferences sending inferences as parquet files a different api endpoint is available to process inferences in parquet files : post { { hostname } } / api / v3 / models / { { model _ id } } / inferences / file the parquet file must contain the headers, as well as the timestamps and inference ids : in this case, the parquet file does not contain ground truth data. the file must be attached to the request as inference _ data, as shown in the following", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this case, the parquet file does not contain ground truth data. the file must be attached to the request as inference _ data, as shown in the following example : pythonfiles = [ ('inference _ data ', ('data. parquet ', open ( '. / data. parquet ','rb'),'application / octet - stream') ) ] uploading ground truth data when uploading ground truth data, only the ground truth timestamp, ground truth attributes, and inference ids are required. the inference ids must match the existing inferences : the endpoint to up", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "truth timestamp, ground truth attributes, and inference ids are required. the inference ids must match the existing inferences : the endpoint to upload ground truth data is the same one used to upload inference data : post { { hostname } } / api / v3 / models / { { model _ id } } / inferences / file the difference will be that the file should be sent as ground _ truth _ data, instead of inference _ data : pythonfiles = [ ('ground _ truth _ data ', ('gt. parquet ', open ( '. / gt. parquet", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": pythonfiles = [ ('ground _ truth _ data ', ('gt. parquet ', open ( '. / gt. parquet ','rb'),'application / octet - stream') ) ] updated 2 months ago table of contents introduction create the model schema definition and basic metadata save the model upload reference dataset for model appendix : sending inferences source : https : / / docs. arthur. ai / docs / registering - a - model - with - the - api", "metadata": {"source": "https://docs.arthur.ai/docs/registering-a-model-with-the-api", "row": 107, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 108 text : full directory of arthur permissions jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##duct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##object detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainability", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstrans", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternali", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controlde", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##thur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by full directory of arthur permissionssuggest editsthese can be used to specify permissions in the custom role mapping configuration json under \" resource \" and \" action \". ex : * \" resource \" * \" action \" * \" action \" * \" action \" org and user related permissions organization _ global read write organization read delete custom _ roles read write delete system _ config write user read write delete", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and user related permissions organization _ global read write organization read delete custom _ roles read write delete system _ config write user read write delete user _ invite write organization _ metrics read model related permissions model read write delete tag read write delete enrichment _ config read write metric _ query read write delete raw _ data read write ground _ truth write query execute alert related permissions alert read resolve notify insight read resolve alert _ rule read write delete alert _ notification _ config read write delete alert _ summary notify alert _ summary _ config read write delete alert _", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "write delete alert _ notification _ config read write delete alert _ summary notify alert _ summary _ config read write delete alert _ summary _ subscriber read write delete updated about 2 months ago table of contents org and user related permissions model related permissions alert related permissions source : https : / / docs. arthur. ai / docs / full - directory - of - arthur - permissions", "metadata": {"source": "https://docs.arthur.ai/docs/full-directory-of-arthur-permissions", "row": 108, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 109 text : default evaluation functions jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by default evaluation functionssuggest editsarthur provides common default metrics for all model input / output types. a list of each default metric available can be found within each model types page. this page will provide an overview of all of them to show how to query arthur. regression all regression evaluation metrics will follow the below request body structure. query request : json { \" select \" : [ { \" function \" : \" [ rmsemaers", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "evaluation metrics will follow the below request body structure. query request : json { \" select \" : [ { \" function \" : \" [ rmsemaersquared ] \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" ground _ truth _ property \" : \" < attribute _ name > [ string ] \", \" predicted _ property \" : \" < attribute _ name > [ string ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < evaluation _ value", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < evaluation _ value > [ float ] \" } ] } rmse get the rmse between a prediction attribute and a ground truth attribute. sample request : json { \" select \" : [ { \" function \" : \" rmse \", \" alias \" : \" error \", \" parameters \" : { \" ground _ truth _ property \" : \" fico _ actual \", \" predicted _ property \" : \" fico _ predicted \" } } ] } sample response : json { \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" fico _ actual \", \" predicted _ property \" : \" fico _ predicted \" } } ] } sample response : json { \" query _ result \" : [ { \" error \" : 0. 76 } ] } back to top mae get the mean absolute error between a prediction and ground truth attributes. this function takes an optional parameter aggregation that allows swapping the aggregation from \" avg \" to either \" min \" or \" max \". this can be helpful if you're looking for extremes, such as the lowest or highest absolute error. additionally, this function supports optional params normalizationmax and normal", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "be helpful if you're looking for extremes, such as the lowest or highest absolute error. additionally, this function supports optional params normalizationmax and normalizationmin that accept numbers and will perform min / max normalization on the values before aggregation if both params are provided. query request : json { \" select \" : [ { \" function \" : \" mae \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" predicted _ property \" : \" < predicted _ property _ name > [ string ] \", \" ground _ truth _ property \" : \" < ground", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { \" predicted _ property \" : \" < predicted _ property _ name > [ string ] \", \" ground _ truth _ property \" : \" < ground _ truth _ property _ name > [ string ] \", \" aggregation \" : \" [ avgminmax ] ( default avg, optional ) \", \" normalizationmin \" : \" < value > [ optional number ] \", \" normalizationmax \" : \" < value > [ optional number ] \" } } ] } sample request : json { \" select \" : [ { \" function \" : \" mae \", \" alias \" : \" error \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ] } sample request : json { \" select \" : [ { \" function \" : \" mae \", \" alias \" : \" error \", \" parameters \" : { \" ground _ truth _ property \" : \" fico _ actual \", \" predicted _ property \" : \" fico _ predicted \" } } ] } sample response : json { \" query _ result \" : [ { \" error \" : 0. 76 } ] } back to top r squared get the r squared value between a prediction and ground truth attributes. sample request : json { \" select \" : [ { \" function \" : \" rsqua", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "get the r squared value between a prediction and ground truth attributes. sample request : json { \" select \" : [ { \" function \" : \" rsquared \", \" alias \" : \" rsq \", \" parameters \" : { \" ground _ truth _ property \" : \" fico _ actual \", \" predicted _ property \" : \" fico _ predicted \" } } ] } sample response : json { \" query _ result \" : [ { \" rsq \" : 0. 94 } ] } back to top binary classification when using binary classification evaluation functions with a multiclass model, outputs will be calculated assuming a", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##q \" : 0. 94 } ] } back to top binary classification when using binary classification evaluation functions with a multiclass model, outputs will be calculated assuming a one vs. all approach. confusion matrix calculates the confusion matrix for a classification model. for binary classifiers, users must specify a probability threshold to count a prediction as a positive class. query request : json { \" select \" : [ { \" function \" : \" confusionmatrix \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" threshold \" : \" < value [ float ] > [ required only for", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" < alias _ name > [ optional string ] \", \" parameters \" : { \" threshold \" : \" < value [ float ] > [ required only for binary classifiers ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : { \" true _ positive \" : \" < count > [ int ] \", \" false _ positive \" : \" < count > [ int ] \", \" true _ negative \" : \" < count > [ int ] \", \" false _ negative \" : \" < count > [ int ] \" }", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" true _ negative \" : \" < count > [ int ] \", \" false _ negative \" : \" < count > [ int ] \" } } ] } sample request : calculate the confusion matrix for a binary classifier with a threshold of 0. 5 ( the standard threshold for confusion matrix ). json { \" select \" : [ { \" function \" : \" confusionmatrix \", \" parameters \" : { \" threshold \" : 0. 5 } } ] } sample response : json { \" query _ result \" : [ { \" confusionmatrix \" : { \" true _ positive \" : 100480", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] } sample response : json { \" query _ result \" : [ { \" confusionmatrix \" : { \" true _ positive \" : 100480, \" false _ positive \" : 100076, \" true _ negative \" : 100302, \" false _ negative \" : 99142 } } ] } back to top confusion matrix rate calculates the confusion matrix rates for a classification model. for binary classifiers, users must specify a probability threshold to count a prediction as a positive class. query request : json { \" select \" : [ { \" function \" : \" confusionmatrixrate \", \" alias", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a prediction as a positive class. query request : json { \" select \" : [ { \" function \" : \" confusionmatrixrate \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" threshold \" : \" < value [ float ] > [ required only for binary classifiers ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : { \" true _ positive _ rate \" : \" < rate > [ float ] \", \" false _ positive _ rate \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ name > \" : { \" true _ positive _ rate \" : \" < rate > [ float ] \", \" false _ positive _ rate \" : \" < rate > [ float ] \", \" true _ negative _ rate \" : \" < rate > [ float ] \", \" false _ negative _ rate \" : \" < rate > [ float ] \", \" accuracy _ rate \" : \" < rate > [ float ] \" } } ] } sample request : calculate the confusion matrix for a binary classifier with a threshold of 0. 5 ( the standard threshold for confusion matrix ). json { \" select \" : [", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the confusion matrix for a binary classifier with a threshold of 0. 5 ( the standard threshold for confusion matrix ). json { \" select \" : [ { \" function \" : \" confusionmatrixrate \", \" parameters \" : { \" threshold \" : 0. 5 } } ] } response : json { \" query _ result \" : [ { \" confusionmatrixrate \" : { \" true _ positive _ rate \" : 0. 5033513340213003, \" false _ positive _ rate \" : 0. 49943606583557076, \" true _ negative _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##213003, \" false _ positive _ rate \" : 0. 49943606583557076, \" true _ negative _ rate \" : 0. 5005639341644292, \" false _ negative _ rate \" : 0. 4966486659786997 } } ] } back to top confusion matrix variants if you only want a specific metric derived from a confusion matrix, you can use one of the following functions : truepositiverate falsepositiverate truenegativerate falsenegativerate accuracyrate balancedaccuracyrate f1 sensitivity specificity", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "functions : truepositiverate falsepositiverate truenegativerate falsenegativerate accuracyrate balancedaccuracyrate f1 sensitivity specificity precision recall for example, to return the truepositiverate : json { \" select \" : [ { \" function \" : \" truepositiverate \", \" parameters \" : { \" threshold \" : 0. 5, \" ground _ truth _ property \" : \" class _ a \", \" predicted _ property \" : \" ground _ truth _ a \" } } ] } response : json { \" query _ result \" : [ { \" trueposit", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ property \" : \" ground _ truth _ a \" } } ] } response : json { \" query _ result \" : [ { \" truepositiverate \" : 0. 5033513340213003 } ] } back to top auc the area under the roc curve can also be computed for binary classifiers. sample query : json { \" select \" : [ { \" function \" : \" auc \", \" parameters \" : { \" ground _ truth _ property \" : \" class _ a \", \" predicted _ property \" : \" ground _ truth _ a \" } } ] } response", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" ground _ truth _ property \" : \" class _ a \", \" predicted _ property \" : \" ground _ truth _ a \" } } ] } response : json { \" query _ result \" : [ { \" auc \" : 0. 9192331426352897 } ] } multi - class classification multi - class accuracy rate calculates the global accuracy rate. query request : json { \" select \" : [ { \" function \" : \" accuracyratemulticlass \", \" alias \" : \" < alias _ name > [ optional string ] \" } ] } query response : json", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" accuracyratemulticlass \", \" alias \" : \" < alias _ name > [ optional string ] \" } ] } query response : json { \" query _ result \" : [ { \" accuracyratemulticlass \" : \" < rate > [ float ] \" } ] } example : json { \" select \" : [ { \" function \" : \" accuracyratemulticlass \" } ] } response : json { \" query _ result \" : [ { \" accuracyratemulticlass \" : 0. 785 } ] } back to top multi - class confusion matrix calculates the confusion", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : [ { \" accuracyratemulticlass \" : 0. 785 } ] } back to top multi - class confusion matrix calculates the confusion matrix for a multi - class model with regard to a single class. the predicted attribute and ground truth attribute must be passed as parameters. query request : json { \" select \" : [ { \" function \" : \" confusionmatrixmulticlass \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" predicted _ property \" : \" < predicted _ property _ name > \", \" ground _ truth _ property", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "optional string ] \", \" parameters \" : { \" predicted _ property \" : \" < predicted _ property _ name > \", \" ground _ truth _ property \" : \" < ground _ truth _ property _ name > \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : { \" true _ positive \" : \" < count > [ int ] \", \" false _ positive \" : \" < count > [ int ] \", \" true _ negative \" : \" < count > [ int ] \", \" false _ negative \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" < count > [ int ] \", \" true _ negative \" : \" < count > [ int ] \", \" false _ negative \" : \" < count > [ int ] \" } } ] } example : json { \" select \" : [ { \" function \" : \" confusionmatrixmulticlass \", \" parameters \" : { \" predicted _ property \" : \" predicted _ class _ a \", \" ground _ truth _ property \" : \" gt _ predicted _ class _ a \" } } ] } response : json { \" query _ result \" : [ { \" confusionmatrix \" : {", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "gt _ predicted _ class _ a \" } } ] } response : json { \" query _ result \" : [ { \" confusionmatrix \" : { \" true _ positive \" : 100480, \" false _ positive \" : 100076, \" true _ negative \" : 100302, \" false _ negative \" : 99142 } } ] } back to top multi - class confusion matrix rate calculates the confusion matrix rates for a multi - class classification model in regards to a single predicted class. query request : json { \" select \" : [ { \" function \" : \" confusionmatrixratemult", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model in regards to a single predicted class. query request : json { \" select \" : [ { \" function \" : \" confusionmatrixratemulticlass \", \" alias \" : \" < alias _ name > [ optional string ] \", \" parameters \" : { \" predicted _ property \" : \" predicted _ class _ a \", \" ground _ truth _ property \" : \" gt _ predicted _ class _ a \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : { \" true _ positive _ rate \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##son { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : { \" true _ positive _ rate \" : \" < rate > [ float ] \", \" false _ positive _ rate \" : \" < rate > [ float ] \", \" true _ negative _ rate \" : \" < rate > [ float ] \", \" false _ negative _ rate \" : \" < rate > [ float ] \", \" accuracy _ rate \" : \" < rate > [ float ] \", \" balanced _ accuracy _ rate \" : \" < rate > [ float ] \", \" precision \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" < rate > [ float ] \", \" balanced _ accuracy _ rate \" : \" < rate > [ float ] \", \" precision \" : \" < rate > [ float ] \", \" f1 \" : \" < rate > [ float ] \" } } ] } example calculating the confusion matrix rates : json { \" select \" : [ { \" function \" : \" confusionmatrixratemulticlass \", \" parameters \" : { \" predicted _ property \" : \" predicted _ class _ a \", \" ground _ truth _ property \" : \" gt _ predicted _ class _ a \" } } ] } response", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" predicted _ class _ a \", \" ground _ truth _ property \" : \" gt _ predicted _ class _ a \" } } ] } response : json { \" query _ result \" : [ { \" confusionmatrixratemulticlass \" : { \" true _ positive _ rate \" : 0. 6831683168316832, \" false _ positive _ rate \" : 0. 015653220951234198, \" true _ negative _ rate \" : 0. 9843467790487658, \" false _ negative _ rate \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##4198, \" true _ negative _ rate \" : 0. 9843467790487658, \" false _ negative _ rate \" : 0. 31683168316831684, \" accuracy _ rate \" : 0. 9378818737270875, \" balanced _ accuracy _ rate \" : 0. 8337575479402245, \" precision \" : 0. 8884120171673819, \" f1 \" : 0. 7723880597014925 } } ] } back to top if you", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##4120171673819, \" f1 \" : 0. 7723880597014925 } } ] } back to top if you only want a specific value from the confusion matrix rate function, you can use one of the following functions : truepositiveratemulticlass falsepositiveratemulticlass truenegativeratemulticlass falsenegativeratemulticlass for example, to return the truepositiverate : json { \" select \" : [ { \" function \" : \" truepositiveratemulticlass \", \" parameters \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##iverate : json { \" select \" : [ { \" function \" : \" truepositiveratemulticlass \", \" parameters \" : { \" predicted _ property \" : \" predicted _ class _ a \", \" ground _ truth _ property \" : \" gt _ predicted _ class _ a \" } } ] } response : json { \" query _ result \" : [ { \" truepositiverate \" : 0. 5033513340213003 } ] } back to top multi - class f1 calculates the components needed to compute a f1 score for a multi - class model. in", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##213003 } ] } back to top multi - class f1 calculates the components needed to compute a f1 score for a multi - class model. in this example, the model has 3 classes : class - 1, class - 2, class - 3 and the corresponding ground truth labels class - 1 - gt, class - 2 - gt, class - 3 - gt. query request : json { \" select \" : [ { \" function \" : \" count \", \" alias \" : \" count \" }, { \" function \" : \" confusionmatrixratemulticlass \", \" alias \" : \" class -", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" alias \" : \" count \" }, { \" function \" : \" confusionmatrixratemulticlass \", \" alias \" : \" class - 1 \", \" parameters \" : { \" predicted _ property \" : \" class - 1 \", \" ground _ truth _ property \" : \" class - 1 - gt \" } }, { \" function \" : \" countif \", \" alias \" : \" class - 1 - gt \", \" parameters \" : { \" property \" : \" multiclass _ model _ ground _ truth _ class \", \" comparator \" : \" eq \", \" value \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" property \" : \" multiclass _ model _ ground _ truth _ class \", \" comparator \" : \" eq \", \" value \" : \" class - 1 - gt \" }, \" stage \" : \" ground _ truth \" }, { \" function \" : \" confusionmatrixratemulticlass \", \" alias \" : \" class - 2 \", \" parameters \" : { \" predicted _ property \" : \" class - 2 \", \" ground _ truth _ property \" : \" class - 2 - gt \" } }, { \" function \" : \" countif \", \" alias \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" ground _ truth _ property \" : \" class - 2 - gt \" } }, { \" function \" : \" countif \", \" alias \" : \" class - 2 - gt \", \" parameters \" : { \" property \" : \" multiclass _ model _ ground _ truth _ class \", \" comparator \" : \" eq \", \" value \" : \" class - 2 - gt \" }, \" stage \" : \" ground _ truth \" }, { \" function \" : \" confusionmatrixratemulticlass \", \" alias \" : \" class - 3 \", \" parameters \" : {", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", { \" function \" : \" confusionmatrixratemulticlass \", \" alias \" : \" class - 3 \", \" parameters \" : { \" predicted _ property \" : \" class - 3 \", \" ground _ truth _ property \" : \" class - 3 - gt \" } }, { \" function \" : \" countif \", \" alias \" : \" class - 3 - gt \", \" parameters \" : { \" property \" : \" multiclass _ model _ ground _ truth _ class \", \" comparator \" : \" eq \", \" value \" : \" class - 3 - gt \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ model _ ground _ truth _ class \", \" comparator \" : \" eq \", \" value \" : \" class - 3 - gt \" }, \" stage \" : \" ground _ truth \" } ] } query response : json { \" query _ result \" : [ { \" count \" : 7044794, \" class - 1 - gt \" : 2540963, \" class - 2 - gt \" : 2263918, \" class - 3 - gt \" : 2239913, \" class - 1 \" : { \" true _ positive _ rate \" : 0. 4318", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" class - 3 - gt \" : 2239913, \" class - 1 \" : { \" true _ positive _ rate \" : 0. 4318807475748368, \" false _ positive _ rate \" : 0. 3060401245073361, \" true _ negative _ rate \" : 0. 6939598754926639, \" false _ negative _ rate \" : 0. 5681192524251633, \" accuracy _ rate \" : 0. 5994314383074935, \" balanced _ accuracy _ rate \" : 0", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##24251633, \" accuracy _ rate \" : 0. 5994314383074935, \" balanced _ accuracy _ rate \" : 0. 5629203115337503, \" precision \" : 0. 4432575070302042, \" f1 \" : 0. 437495178612114 }, \" class - 2 \" : { \" true _ positive _ rate \" : 0. 42177322676881407, \" false _ positive _ rate \" : 0. 3514795196528837, \" true _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##2676881407, \" false _ positive _ rate \" : 0. 3514795196528837, \" true _ negative _ rate \" : 0. 6485204803471163, \" false _ negative _ rate \" : 0. 578226773231186, \" accuracy _ rate \" : 0. 5756528863725469, \" balanced _ accuracy _ rate \" : 0. 5351468535579652, \" precision \" : 0. 3623427088234848, \" f1 \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 0. 5351468535579652, \" precision \" : 0. 3623427088234848, \" f1 \" : 0. 38980575845890253 }, \" class - 3 \" : { \" true _ positive _ rate \" : 0. 26144274353512836, \" false _ positive _ rate \" : 0. 2805894672521546, \" true _ negative _ rate \" : 0. 7194105327478454, \" false _ negative _ rate \" : 0. 73855725", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "negative _ rate \" : 0. 7194105327478454, \" false _ negative _ rate \" : 0. 7385572564648716, \" accuracy _ rate \" : 0. 5737983254017079, \" balanced _ accuracy _ rate \" : 0. 4904266381414869, \" precision \" : 0. 3028268576818381, \" f1 \" : 0. 2806172238153916 } } ] } with this result, you can calculate the weighted f1 score by multiplying each classes", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 0. 2806172238153916 } } ] } with this result, you can calculate the weighted f1 score by multiplying each classes's f1 score by the count of the ground truth and dividing by the total count. in this example, that would be ( class - 1. f1 * class - 1 - gt + class - 2. f1 * class - 2 - gt + class - 3. f1 * class - 3 - gt ) / count and with numbers : ( 0. 437495178612114 * 2540963 + 0. 389805758458", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and with numbers : ( 0. 437495178612114 * 2540963 + 0. 38980575845890253 * 2263918 + 0. 2806172238153916 * 2239913 ) / 7044794 = 0. 3722898785 back to top object detection objects detected for multiclass, multilabel, and regression models, querying model performance works the same for arthur computer vision models as more tabular and nlp models. but object detection computer vision has some special fields you can use when querying.", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the same for arthur computer vision models as more tabular and nlp models. but object detection computer vision has some special fields you can use when querying. example query fetching all bounding box fields : json { \" select \" : [ { \" property \" : \" inference _ id \" }, { \" property \" : \" objects _ detected \" } ] } the response will have 1 object per bounding box. json { \" query _ result \" : [ { \" inference _ id \" : \" 1 \", \" objects _ detected. class _ id \" : 0, \" objects _ detected. confidence \" : 0", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" inference _ id \" : \" 1 \", \" objects _ detected. class _ id \" : 0, \" objects _ detected. confidence \" : 0. 6, \" objects _ detected. top _ left _ x \" : 23, \" objects _ detected. top _ left _ y \" : 45, \" objects _ detected. width \" : 20, \" objects _ detected. height \" : 30 }, { \" inference _ id \" : \" 1 \", \" objects _ detected. class _ id \" : 1, \" objects _ detected. confidence \" : 0. 6, \" objects _ detected. top _ left _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "objects _ detected. class _ id \" : 1, \" objects _ detected. confidence \" : 0. 6, \" objects _ detected. top _ left _ x \" : 23, \" objects _ detected. top _ left _ y \" : 45, \" objects _ detected. width \" : 20, \" objects _ detected. height \" : 30 }, { \" inference _ id \" : 2, \"... \" : \"... \" } ] } you can also specify only a single nested field : json { \" select \" : [ { \" property \" : \" inference _ id \" }, { \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can also specify only a single nested field : json { \" select \" : [ { \" property \" : \" inference _ id \" }, { \" property \" : \" objects _ detected. class _ id \" }, { \" property \" : \" objects _ detected. confidence \" } ] } the response will have 1 object per bounding box. json { \" query _ result \" : [ { \" inference _ id \" : \" 1 \", \" objects _ detected. class _ id \" : 0, \" objects _ detected. confidence \" : 0. 6 }, { \" inference _ id \" : \" 1 \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detected. class _ id \" : 0, \" objects _ detected. confidence \" : 0. 6 }, { \" inference _ id \" : \" 1 \", \" objects _ detected. class _ id \" : 1, \" objects _ detected. confidence \" : 0. 6 }, { \" inference _ id \" : 2, \"... \" : \"... \" } ] } when supplying the bounding box specific fields in filters, group bys, or order bys the columns must also be supplied in the select clause in order for the query to succeed. mean average precision calculates mean average precision for an", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or order bys the columns must also be supplied in the select clause in order for the query to succeed. mean average precision calculates mean average precision for an object detection model. this is used as measure of accuracy for object detection models. threshold determines the minimum iou value to be considered a match for a label. predicted _ property and ground _ truth _ property are optional parameters and should be the names of the predicted and ground truth attributes for the model. they default to \" objects _ detected \" and \" label \" respectively if nothing is specified for these parameters. query request : json { \" select \" : [ { \" function \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "objects _ detected \" and \" label \" respectively if nothing is specified for these parameters. query request : json { \" select \" : [ { \" function \" : \" meanaverageprecision \", \" alias \" : \" < alias _ name > [ optional ] \", \" parameters \" : { \" threshold \" : \" < threshold > [ float ] \", \" predicted _ property \" : \" < predicted _ property > [ str ] \", \" ground _ truth _ property \" : \" < ground _ truth _ property > [ str ] \" } } ] } query response : json { \" query _ result \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ property \" : \" < ground _ truth _ property > [ str ] \" } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < result > [ float ] \" } ] } example : json { \" select \" : [ { \" function \" : \" meanaverageprecision \", \" parameters \" : { \" threshold \" : 0. 5, \" predicted _ property \" : \" objects _ detected \", \" ground _ truth _ property \" : \" label \" } } ] } query response : json { \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ property \" : \" objects _ detected \", \" ground _ truth _ property \" : \" label \" } } ] } query response : json { \" query _ result \" : [ { \" meanaverageprecision \" : 0. 78 } ] } generative text token likelihood tokenlikelihoods attributes yield two queryable columns for that attribute with suffixes \u201c _ tokens \u201d and \u201c _ likelihoods \u201d appended to the attribute's name. for example, a model with a tokenlikelihoods attribute named summary _ token _ probs yields two queryable columns : summary _ token _ probs _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "example, a model with a tokenlikelihoods attribute named summary _ token _ probs yields two queryable columns : summary _ token _ probs _ tokens and summary _ token _ probs _ likelihoods which represent an array of the selected tokens and an array of their corresponding likelihoods. pythonquery = { \" select \" : [ { \" property \" : \" summary _ token _ probs _ tokens \" }, { \" property \" : \" summary _ token _ probs _ likelihoods \" } ] } response [ { \" summary _ token _ probs _ likelihoods \" : [ 0. 375", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "summary _ token _ probs _ likelihoods \" } ] } response [ { \" summary _ token _ probs _ likelihoods \" : [ 0. 3758265972137451, 0. 6563436985015869, 0. 32000941038131714, 0. 5629857182502747 ], \" summary _ token _ probs _ tokens \" : [ \" this \", \" is \", \" a \", \" summary \" ] } ] bias bias mitigation calculates mitigated predictions based on conditional thresholds, returning", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" is \", \" a \", \" summary \" ] } ] bias bias mitigation calculates mitigated predictions based on conditional thresholds, returning 0 / 1 for each inference. this function returns null for inferences that don't match any of the provided conditions. query request : json { \" select \" : [ { \" function \" : \" biasmitigatedpredictions \", \" alias \" : \" < alias _ name > [ optional ] \", \" parameters \" : { \" predicted _ property \" : \" < predicted _ property > [ str ] \", \" thresholds \" : [ {", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" parameters \" : { \" predicted _ property \" : \" < predicted _ property > [ str ] \", \" thresholds \" : [ { \" conditions \" : { \" property \" : \" < attribute _ name > [ string or nested ] \", \" comparator \" : \" < comparator > [ string ] optional : default'eq'\", \" value \" : \" < string or number to compare with property > \" }, \" threshold \" : \" < threshold > [ float ] \" } ] } } ] } query response : json { \" query _ result \" : [ {", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" threshold \" : \" < threshold > [ float ] \" } ] } } ] } query response : json { \" query _ result \" : [ { \" < function _ name / alias _ name > \" : \" < result > [ int ] \" } ] } example : json { \" select \" : [ { \" function \" : \" biasmitigatedpredictions \", \" parameters \" : { \" predicted _ property \" : \" prediction _ 1 \", \" thresholds \" : [ { \" conditions \" : [ { \" property \" : \" sex \", \" value \" : 1 } ], \" threshold", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" thresholds \" : [ { \" conditions \" : [ { \" property \" : \" sex \", \" value \" : 1 } ], \" threshold \" : 0. 4 }, { \" conditions \" : [ { \" property \" : \" sex \", \" value \" : 2 } ], \" threshold \" : 0. 6 } ] } } ] } response : json { \" query _ result \" : [ { \" sex \" : 1, \" biasmitigatedpredictions \" : 1 }, { \" sex \" : 2, \" biasmitigatedpredictions \" : 0 }, {", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##mitigatedpredictions \" : 1 }, { \" sex \" : 2, \" biasmitigatedpredictions \" : 0 }, { \" sex \" : 1, \" biasmitigatedpredictions \" : 0 } ] } ranked list outputs performance metrics precision at k precision is an indicator of the efficiency of a supervised machine learning model. if one model gets all the relevant items by recommending fewer items than another model, it has a higher precision. for item recommendation models, precision at k measures the fraction of all relevant items among top - k recommended items. query request json { \" select \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "precision. for item recommendation models, precision at k measures the fraction of all relevant items among top - k recommended items. query request json { \" select \" : [ { \" function \" : \" precisionatk \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \", \" k \" : 5 } } ] } query response json { \" query _ result \" : [ { \" precisionatk \" : 0. 26666666666666666 } ] } recall at k recall is an indicator of the effectiveness of", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ { \" precisionatk \" : 0. 26666666666666666 } ] } recall at k recall is an indicator of the effectiveness of a supervised machine learning model. the model which correctly identifies more of the positive instances gets a higher recall value. in case of recommendations, the recall at k is measured as the fraction of all relevant items that were recovered in top k recommendations. query request json { \" select \" : [ { \" function \" : \" recallatk \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "recallatk \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \", \" k \" : 5 } } ] } query response json { \" query _ result \" : [ { \" recallatk \" : 0. 26666666666666666 } ] } mean average precision at k ( map @ k ) the map @ k metric is the most commonly used metric for evaluating recommender systems. it calculates the precision at every location 1 through k where there is a relevant item. average precision is calculated per inference", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "used metric for evaluating recommender systems. it calculates the precision at every location 1 through k where there is a relevant item. average precision is calculated per inference, then the per inference values are averaged across a group of inferences to create mean average precision. query request json { \" select \" : [ { \" function \" : \" mapatk \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \", \" k \" : 5 } } ] } query response json { \" query _ result \" : [ { \" map", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" relevant _ items \", \" k \" : 5 } } ] } query response json { \" query _ result \" : [ { \" mapatk \" : 0. 26666666666666666 } ] } normalized discounted cumulative gain at k ( ndcg @ k ) ndcg measures the overall reward at all positions that hold a relevant item. the reward is an inverse log of the position ( i. e. higher ranks for relevant items would lead to better reward, as desired ). similar to map @ k, this metric calculates a value per inference, then is aggregated", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for relevant items would lead to better reward, as desired ). similar to map @ k, this metric calculates a value per inference, then is aggregated across inferences using an average. query request json { \" select \" : [ { \" function \" : \" ndcgatk \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \", \" k \" : 5 } } ] } query response json { \" query _ result \" : [ { \" ndcgatk \" : 0. 26666666666", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "5 } } ] } query response json { \" query _ result \" : [ { \" ndcgatk \" : 0. 26666666666666666 } ] } auc in the case of ranked list metrics, auc measures the likelihood that a random relevant item is ranked higher than a random irrelevant item. higher the likelihood of this happening implies a higher auc score meaning a better recommendation system. we calculate this likelihood empirically based on the ranks given by the algorithm to all items \u2014 out of all possible pairs of type ( relevant - item, non - relevant - item ), auc is a proportion", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "given by the algorithm to all items \u2014 out of all possible pairs of type ( relevant - item, non - relevant - item ), auc is a proportion of pairs where the relevant item was ranked higher than the irrelevant item from that pair. this metric is calculated per - inference, then aggregated as an average over a group of inferences. query request json { \" select \" : [ { \" function \" : \" rankedlistauc \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \" } } ] } query response", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \" } } ] } query response json { \" query _ result \" : [ { \" rankedlistauc \" : 0. 26666666666666666 } ] } mean reciprocal rank ( mrr ) mean reciprocal rank quantifies the rank of the first relevant item found in the recommendation list. it takes the reciprocal of this \u201c first relevant item rank \u201d, meaning that if the first item is relevant ( i. e. the ideal case ) then mrr will be 1, otherwise it", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "first relevant item rank \u201d, meaning that if the first item is relevant ( i. e. the ideal case ) then mrr will be 1, otherwise it will be lower. query request json { \" select \" : [ { \" function \" : \" meanreciprocalrank \", \" parameters \" : { \" predicted _ property \" : \" predicted _ items \", \" ground _ truth _ property \" : \" relevant _ items \" } } ] } query response json { \" query _ result \" : [ { \" meanreciprocalrank \" : 0. 26666666666666666 } ]", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "json { \" query _ result \" : [ { \" meanreciprocalrank \" : 0. 26666666666666666 } ] } back to topupdated 3 months ago table of contents regression rmse mae r squared binary classification confusion matrix confusion matrix rate confusion matrix variants auc multi - class classification multi - class accuracy rate multi - class confusion matrix multi - class confusion matrix rate multi - class f1 object detection objects detected mean average precision generative text token likelihood bias bias mitigation ranked list outputs performance metrics source : https : / / docs. arthur. ai / docs / querying", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "text token likelihood bias bias mitigation ranked list outputs performance metrics source : https : / / docs. arthur. ai / docs / querying - performance", "metadata": {"source": "https://docs.arthur.ai/docs/querying-performance", "row": 109, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 110 text : multiclass classification jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by multiclass classificationsuggest editsmulticlass classification models predict one class from more than two potential classes. in arthur, these models fall into the category of classification and are represented by the multiclass model type. some common examples of tabular multiclass classification are : what breed of dog is in this photo? what part of the car is damaged in this photo? similar to binary classification, these models frequently output not only the predicted class but also", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is in this photo? what part of the car is damaged in this photo? similar to binary classification, these models frequently output not only the predicted class but also a probability for each class predicted. the highest probability class is then the predicted output. in these cases, a threshold does not need to be provided to arthur and it will automatically track the highest probability class as the predicted output. formatted data in arthur tabular binary classification models require three things to be specified in their schema : all predicting model attributes ( or features ), predicted probability of outputs, and a column for the inference's true label ( or ground truth ).", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##hema : all predicting model attributes ( or features ), predicted probability of outputs, and a column for the inference's true label ( or ground truth ). many teams also choose to onboard metadata for the model ( i. e. any information you want to track about your inferences ) as non - input attributes. attribute ( numeric or categorical ) attribute ( numeric or categorical ) probability of prediction aprobability of prediction bprobability of prediction cground truthnon - input attribute ( numeric or categorical ) high school education34. 5. 90. 05. 05amalegraduate degree44. 1", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "truthnon - input attribute ( numeric or categorical ) high school education34. 5. 90. 05. 05amalegraduate degree44. 1. 46. 14. 40bfemalegraduate degree33. 5. 16. 17. 71cfemale predict function and mapping these are some examples of common values teams need to onboard for their multi - class classification models. the relationship between the prediction and ground truth column must be defined to help set up your arthur environment to calculate default performance metrics. there are 2 options for formatting this, depending on your reference dataset. additionally, if teams wish to", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "your arthur environment to calculate default performance metrics. there are 2 options for formatting this, depending on your reference dataset. additionally, if teams wish to enable explainability, they must provide a few assets required for explainability. below is an example of the runnable predict function, which outputs a single numeric prediction. prediction to ground truth mappingexample prediction function # # option 1 : multiple prediction columns, single ground truth column # map each predictedvalue attribute to its corresponding groundtruth value. output _ mapping _ 1 = {'pred _ class _ one _ column':'one ','pre", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to its corresponding groundtruth value. output _ mapping _ 1 = {'pred _ class _ one _ column':'one ','pred _ class _ two _ column':'two ','pred _ class _ three _ column':'three'} # build arthur model with this technique arthur _ model. build ( reference _ data, ground _ truth _ column ='ground _ truth ', pred _ to _ ground _ truth _ map = output _ mapping _ 1 ) # # option 2 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding ground", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "truth _ map = output _ mapping _ 1 ) # # option 2 : multiple prediction and ground truth columns # map each predictedvalue attribute to its corresponding groundtruth attribute. output _ mapping _ 2 = {'pred _ class _ one _ column':'gt _ class _ one _ column ','pred _ class _ two _ column':'gt _ class _ two _ column ','pred _ class _ three _ column':'gt _ class _ three _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "' gt _ class _ three _ column'} # build arthur model with this technique arthur _ model. build ( reference _ data, pred _ to _ ground _ truth _ map = output _ mapping _ 2 ) # # example prediction function for classification def predict ( x ) : return model. predict _ proba ( x ) available metrics when onboarding multiclass classification models, you have a number of default metrics available to you within the ui. you can learn more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "more about each specific metric in the metrics section of the documentation. out - of - the - box metrics the following metrics are automatically available in the ui ( out - of - the - box ) per class when teams onboard a multiclass classification model. find out more about these metrics in the performance metrics section. metricmetric typeaccuracy rateperformancebalanced accuracy rateperformanceaucperformancerecallperformanceprecisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rate", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##precisionperformancespecificity ( tnr ) performancef1performancefalse positive rateperformancefalse negative rateperformanceinference countingestioninference count by classingestion drift metrics in the platform, drift metrics are calculated compared to a reference dataset. so, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. find out more about these metrics in the drift and anomaly section. note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "about these metrics in the drift and anomaly section. note : teams are able to evaluate drift for inference data at different intervals with our python sdk and query service ( for example data coming into the model now, compared to a month ago ). psifeature driftkl divergencefeature driftjs divergencefeature drifthellinger distancefeature drifthypothesis testfeature driftprediction driftprediction driftmultivariate driftmultivariate drift fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##iate driftmultivariate drift fairness metrics as further described in the fairness metrics section of the documentation, fairness metrics are available for any tabular arthur attributes manually selected to monitor for bias. metricmetric typeaccuracy ratefairnesstrue positive rate ( equal opportunity ) fairnesstrue negative ratefairnessfalse positive ratefairnessfalse negative ratefairness user - defined metrics whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders ( like product or business metrics ). learn more about creating metrics with data in arthur in the user - defined metrics section. available enrichments the following enrichments can be enabled for this model type : anomaly detectionhot spotsexplainabilitybias mitigationxxxupdated 3 months ago table of contents formatted data in arthur predict function and mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : /", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "mapping available metrics out - of - the - box metrics drift metrics fairness metrics user - defined metrics available enrichments source : https : / / docs. arthur. ai / docs / multi - class - classification", "metadata": {"source": "https://docs.arthur.ai/docs/multi-class-classification", "row": 110, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 111 text : quickstart jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by quickstartsuggest editsfrom a python environment with the arthurai package installed, this quickstart code will : make binary classification predictions on a small dataset onboard the model with reference data to arthur log batches of model inference data with arthur get performance results for our model imports the arthurai package can be pip - installed from the terminal, along with numpy and pandas : bashpip install arthurai numpy pandas", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthurai package can be pip - installed from the terminal, along with numpy and pandas : bashpip install arthurai numpy pandas then you can import the functionality we'll use from the arthurai package like this : python # arthur imports from arthurai import arthurai from arthurai. common. constants import inputtype, outputtype, stage from arthurai. util import generate _ timestamps # other libraries used in this example import numpy as np import pandas as pd model predictions we write out samples from a titanic survival prediction dataset explicitly in python, giving the age of each", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "numpy as np import pandas as pd model predictions we write out samples from a titanic survival prediction dataset explicitly in python, giving the age of each passenger, the cost of their ticket, the passenger class of their ticket, and the ground - truth label of whether they survived. our model's outputs are given by a predict function using only the age variable. we split the data into reference _ data for onboarding the model inference _ data for in - production inferences the model processes python # define titanic sample data titanic _ data = pd. dataframe ( { \" age \" : [ 19. 0, 37. 0", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s the model processes python # define titanic sample data titanic _ data = pd. dataframe ( { \" age \" : [ 19. 0, 37. 0, 65. 0, 30. 0, 22. 0, 24. 0, 16. 0, 40. 0, 58. 0, 32. 0 ], \" fare \" : [ 8. 05, 29. 7, 7. 75, 7. 8958, 7. 75, 49. 5042, 86. 5, 7. 8958, 153. 4625, 7. 8958 ], \" passenger _ class \" : [ 3, 1,", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", 86. 5, 7. 8958, 153. 4625, 7. 8958 ], \" passenger _ class \" : [ 3, 1, 3, 3, 3, 1, 1, 3, 1, 3 ], \" survived \" : [ 1, 0, 0, 0, 1, 1, 1, 0, 1, 0 ] } ) # split into reference and inference data reference _ data, inference _ data = titanic _ data [ : 6 ]. copy ( ), titanic _ data [ 6 : ]. copy ( ) # predict the probability of titanic survival as inverse percentile of age def", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "6 ]. copy ( ), titanic _ data [ 6 : ]. copy ( ) # predict the probability of titanic survival as inverse percentile of age def predict ( age ) : nearest _ age _ index = np. argmin ( np. abs ( np. sort ( reference _ data ['age'] ) - age ) ) return 1 - ( nearest _ age _ index / ( len ( reference _ data ) - 1 ) ) # reference _ data and inference _ data contain the model's inputs and outputs reference _ data ['pred _ survived'] = reference _ data ['age'].", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inference _ data contain the model's inputs and outputs reference _ data ['pred _ survived'] = reference _ data ['age']. apply ( predict ) inference _ data ['pred _ survived'] = inference _ data ['age']. apply ( predict ) onboarding this code will only run once you enter a valid username. first we connect to the arthur api and create an arthur _ model with some high - level metadata : a classification model operating on tabular data with the name \" titanicquickstart \". python # connect to arthur arthur = arthurai ( url =", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a classification model operating on tabular data with the name \" titanicquickstart \". python # connect to arthur arthur = arthurai ( url = \" https : / / app. arthur. ai \", login = \" < your _ username _ or _ email > \", password = os. environ ['arthur _ password'] ) # register the model type with arthur arthur _ model = arthur. model ( display _ name = \" example : titanic quickstart \", input _ type = inputtype. tabular, output _ type = outputtype. multiclass ) next, we infer", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": titanic quickstart \", input _ type = inputtype. tabular, output _ type = outputtype. multiclass ) next, we infer the model schema from thereference _ data, specifying which attributes are in which { ref } stage < basic _ concepts _ attributes _ and _ stages >. additionally, we configure extra settings for the passenger _ class attribute. then we save the model to the platform. python # map predictedvalue attribute to its corresponding groundtruth attribute value. # this tells arthur that the ` pred _ survived ` column represents # the probability that the ground truth column", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attribute to its corresponding groundtruth attribute value. # this tells arthur that the ` pred _ survived ` column represents # the probability that the ground truth column has the value 1 pred _ to _ ground _ truth _ map = {'pred _ survived': 1 } # build arthur _ model schema on the reference dataset, # specifying which attribute represents ground truth # and which attributes are noninputdata. # arthur will monitor noninputdata attributes even though they are not model inputs. arthur _ model. build ( reference _ data, ground _ truth _ column ='survived ', pre", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##data attributes even though they are not model inputs. arthur _ model. build ( reference _ data, ground _ truth _ column ='survived ', pred _ to _ ground _ truth _ map = pred _ to _ ground _ truth _ map, non _ input _ columns = ['fare ','passenger _ class'] ) # configure the ` passenger _ class ` attribute # 1. turn on bias monitoring for the attribute. # 2. specify that the passenger _ class attribute has possible values [ 1, 2, 3 ], # since that information was not present in reference _ data ( only", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". specify that the passenger _ class attribute has possible values [ 1, 2, 3 ], # since that information was not present in reference _ data ( only values 1 and 3 are present ). arthur _ model. get _ attribute ( name ='passenger _ class'). set ( monitor _ for _ bias = true, categories = [ 1, 2, 3 ] ) # onboard the model to arthur arthur _ model. save ( ) sending inferences here we send inferences from inference _ data to arthur. we'll oversample inference _ data and use arthur's utility function to generate some fake timestamps", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inferences from inference _ data to arthur. we'll oversample inference _ data and use arthur's utility function to generate some fake timestamps as though the inferences were made over the last five days. python # sample the inference dataset with predictions inferences = inference _ data. sample ( 100, replace = true ) # generate mock timestamps over the last five days timestamps = generate _ timestamps ( len ( inferences ), duration ='5d') # send the inferences to arthur arthur _ model. send _ inferences ( inferences, inference _ timestamps =", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "duration ='5d') # send the inferences to arthur arthur _ model. send _ inferences ( inferences, inference _ timestamps = timestamps ) inferences usually become available for analysis in seconds, but it can take up to a few minutes. you can wait until they're ready for your analysis like this : python # wait until some inferences land in arthur arthur _ model. await _ inferences ( ) performance results with our model onboarded and inferences sent, we can get performance results from arthur. view your model in your arthur dashboard, or use the code below to fetch the overall accuracy", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed and inferences sent, we can get performance results from arthur. view your model in your arthur dashboard, or use the code below to fetch the overall accuracy rate : python # query overall model accuracy query = { \" select \" : [ { \" function \" : \" accuracyrate \" } ] } query _ result = arthur _ model. query ( query ) print ( query _ result ) you should see [ {'accuracyrate': 0. 8 } ] or a similar value depending on the random sampling of your inference set. updated 3 months ago what \u2019 s nextlearn more about important terms with the core concepts in arthur page", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "value depending on the random sampling of your inference set. updated 3 months ago what \u2019 s nextlearn more about important terms with the core concepts in arthur page, try out in - depth examples in our arthur github sandbox, or start your in - depth onboarding walkthrough with the data preparation for arthur page. core concepts in arthurarthur sandbox github repositorydata preparation for arthurtable of contents imports model predictions onboarding sending inferences performance results source : https : / / docs. arthur. ai / docs / quickstart", "metadata": {"source": "https://docs.arthur.ai/docs/quickstart", "row": 111, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 112 text : grouped inference queries jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by grouped inference queriessuggest editsinitial analyses that treat inferences as independent of one another can provide tremendous value. but over time, models often make multiple predictions about the same real - world entities. no matter what you're predicting, it can be helpful to compare the inputs and outputs of your model on an entity - by - entity basis. for example, let's say that your model makes predictions about whether customers will make", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "inputs and outputs of your model on an entity - by - entity basis. for example, let's say that your model makes predictions about whether customers will make a purchase in the next 30 days. you might have the following attributes : customer _ id : a non - input attribute will _ purchase _ pred : the prediction attribute : whether a customer will make a purchase in the next 30 days will _ purchase _ gt : the ground truth attribute : whether a customer actually did make a purchase within 30 days recent _ purchase _ count : an input attribute with the total number of purchases the customer made in the last 90 days newsletter _ subscribe", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a purchase within 30 days recent _ purchase _ count : an input attribute with the total number of purchases the customer made in the last 90 days newsletter _ subscriber : an input attribute depicting whether the customer subscribes to the deals newsletter your model might be run on the full universe of customer ids at some regular interval. with arthur's powerful query api, you can follow inferences for each customer id through time and answer questions like : how does recent _ purchase _ count tend to change for each customer, from the first to last time inference is conducted? what is the per - customer variance of recent _ purchase _ count across", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "count tend to change for each customer, from the first to last time inference is conducted? what is the per - customer variance of recent _ purchase _ count across time? how many customers changed their newsletter subscription status, from one month ago to today? what is the distribution of the lifetimes of customer ids? example queries we'll walk through some example queries for these entity - by - entity comparisons, exploring the sample case outlined above. per - customer variance we can look at how consistent recent _ purchase _ count is for each customer across time. we'll compute the variance in recent _ purchase _ count for each customer", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can look at how consistent recent _ purchase _ count is for each customer across time. we'll compute the variance in recent _ purchase _ count for each customer across all their inferences, and then roll those individual variances up into a distribution. json { \" select \" : [ { \" function \" : \" distribution \", \" alias \" : \" recent _ purchase _ count _ variance _ distribution \", \" parameters \" : { \" property \" : { \" nested _ function \" : { \" function \" : \" variance \", \" parameters \" : { \" property \" : \" recent _ purchase _ count \" } } }", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ function \" : { \" function \" : \" variance \", \" parameters \" : { \" property \" : \" recent _ purchase _ count \" } } }, \" num _ bins \" : 20 } } ], \" subquery \" : { \" select \" : [ { \" property \" : \" recent _ purchase _ count \" }, { \" property \" : \" customer _ id \" } ], \" group _ by \" : [ { \" property \" : \" customer _ id \" } ] } } change across batches if our model is a batch model, we might want to compare the values for each customer", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" customer _ id \" } ] } } change across batches if our model is a batch model, we might want to compare the values for each customer between two difference batches. we'll again look at the distribution of change in the recent _ purchase _ count, but this time look at the difference for each customer between two specific batches. json { \" select \" : [ { \" function \" : \" distribution \", \" alias \" : \" recent _ purchase _ count _ difference _ distribution \", \" parameters \" : { \" property \" : { \" nested _ function \" : { \" function \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "purchase _ count _ difference _ distribution \", \" parameters \" : { \" property \" : { \" nested _ function \" : { \" function \" : \" subtract \", \" parameters \" : { \" left \" : \" batch1 _ recent _ purchase _ count \", \" right \" : \" batch2 _ recent _ purchase _ count \" } } }, \" num _ bins \" : 20 } } ], \" subquery \" : { \" select \" : [ { \" property \" : \" customer _ id \" }, { \" property \" : \" batch1 _ recent _ purchase _ count \" }", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "select \" : [ { \" property \" : \" customer _ id \" }, { \" property \" : \" batch1 _ recent _ purchase _ count \" }, { \" property \" : \" batch2 _ recent _ purchase _ count \" } ], \" subquery \" : { \" select \" : [ { \" property \" : \" customer _ id \" }, { \" function \" : \" anyif \", \" parameters \" : { \" result \" : \" recent _ purchase _ count \", \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ purchase _ count \", \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" batch1 \" }, \" alias \" : \" batch1 _ recent _ purchase _ count \" }, { \" function \" : \" anyif \", \" parameters \" : { \" result \" : \" recent _ purchase _ count \", \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" batch2 \" }, \" alias \" : \" batch2 _ recent _ purchase _ count \" } ],", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" eq \", \" value \" : \" batch2 \" }, \" alias \" : \" batch2 _ recent _ purchase _ count \" } ], \" group _ by \" : [ { \" property \" : \" customer _ id \" } ] }, \" where \" : [ { \" property \" : \" batch1 _ recent _ purchase _ count \", \" comparator \" : \" notnull \" }, { \" property \" : \" batch2 _ recent _ purchase _ count \", \" comparator \" : \" notnull \" } ] } } change across first to last inference per customer we can", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "recent _ purchase _ count \", \" comparator \" : \" notnull \" } ] } } change across first to last inference per customer we can again compare the difference between two absolute points, but instead of comparing fixed batches compute it for the earliest and latest inference for each customer : json { \" select \" : [ { \" function \" : \" distribution \", \" alias \" : \" recent _ purchase _ count _ difference _ distribution \", \" parameters \" : { \" property \" : { \" nested _ function \" : { \" function \" : \" subtract \", \" parameters \" : { \" left", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": { \" property \" : { \" nested _ function \" : { \" function \" : \" subtract \", \" parameters \" : { \" left \" : \" newest _ recent _ purchase _ count \", \" right \" : \" oldest _ recent _ purchase _ count \" } } }, \" num _ bins \" : 20 } } ], \" subquery \" : { \" select \" : [ { \" property \" : \" customer _ id \" }, { \" function \" : \" argmax \", \" parameters \" : { \" argument \" : \" inference _ timestamp \", \" value", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "}, { \" function \" : \" argmax \", \" parameters \" : { \" argument \" : \" inference _ timestamp \", \" value \" : \" recent _ purchase _ count \" }, \" alias \" : \" newest _ recent _ purchase _ count \" }, { \" function \" : \" argmin \", \" parameters \" : { \" argument \" : \" inference _ timestamp \", \" value \" : \" recent _ purchase _ count \" }, \" alias \" : \" oldest _ recent _ purchase _ count \" } ], \" group _ by \" : [ { \" property \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" }, \" alias \" : \" oldest _ recent _ purchase _ count \" } ], \" group _ by \" : [ { \" property \" : \" customer _ id \" } ] } } change in categorical variables we can also look at change in categorical variables on an entity - by - entity basis. let's look at the distribution of customers who remained subscribed, remained unsubscribed, newly subscribed, or newly unsubscribed from one batch to the next. json { \" select \" : [ { \" alias \" : \" batch1 _ not _ subscribe", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##bscribed from one batch to the next. json { \" select \" : [ { \" alias \" : \" batch1 _ not _ subscribed \", \" function \" : \" equals \", \" parameters \" : { \" left \" : \" batch1 _ newsletter _ subscriber \", \" right \" : 0 } }, { \" alias \" : \" batch1 _ is _ subscribed \", \" function \" : \" equals \", \" parameters \" : { \" left \" : \" batch1 _ newsletter _ subscriber \", \" right \" : 1 } }, { \" alias \" :", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "parameters \" : { \" left \" : \" batch1 _ newsletter _ subscriber \", \" right \" : 1 } }, { \" alias \" : \" batch2 _ not _ subscribed \", \" function \" : \" equals \", \" parameters \" : { \" left \" : \" batch2 _ newsletter _ subscriber \", \" right \" : 0 } }, { \" alias \" : \" batch2 _ is _ subscribed \", \" function \" : \" equals \", \" parameters \" : { \" left \" : \" batch2 _ newsletter _ subscriber \", \" right \" : 1", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "function \" : \" equals \", \" parameters \" : { \" left \" : \" batch2 _ newsletter _ subscriber \", \" right \" : 1 } }, { \" alias \" : \" stayed _ unsubscribed _ count \", \" function \" : \" and \", \" parameters \" : { \" left \" : { \" alias _ ref \" : \" batch1 _ not _ subscribed \" }, \" right \" : { \" alias _ ref \" : \" batch2 _ not _ subscribed \" } } }, { \" alias \" : \" did _ subscribe _ count \",", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ ref \" : \" batch2 _ not _ subscribed \" } } }, { \" alias \" : \" did _ subscribe _ count \", \" function \" : \" and \", \" parameters \" : { \" left \" : { \" alias _ ref \" : \" batch1 _ not _ subscribed \" }, \" right \" : { \" alias _ ref \" : \" batch2 _ is _ subscribed \" } } }, { \" alias \" : \" stayed _ subscribed _ count \", \" function \" : \" and \", \" parameters \" : { \" left \" : { \" alias", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" stayed _ subscribed _ count \", \" function \" : \" and \", \" parameters \" : { \" left \" : { \" alias _ ref \" : \" batch1 _ is _ subscribed \" }, \" right \" : { \" alias _ ref \" : \" batch2 _ is _ subscribed \" } } }, { \" alias \" : \" did _ unsubscribe _ count \", \" function \" : \" and \", \" parameters \" : { \" left \" : { \" alias _ ref \" : \" batch1 _ is _ subscribed \" }, \" right", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\", \" parameters \" : { \" left \" : { \" alias _ ref \" : \" batch1 _ is _ subscribed \" }, \" right \" : { \" alias _ ref \" : \" batch2 _ not _ subscribed \" } } } ], \" subquery \" : { \" select \" : [ { \" property \" : \" customer _ id \" }, { \" property \" : \" batch1 _ newsletter _ subscriber \" }, { \" property \" : \" batch2 _ newsletter _ subscriber \" } ], \" subquery \" : { \" select \" : [ {", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", { \" property \" : \" batch2 _ newsletter _ subscriber \" } ], \" subquery \" : { \" select \" : [ { \" property \" : \" customer _ id \" }, { \" function \" : \" anyif \", \" parameters \" : { \" result \" : \" newsletter _ subscriber \", \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" batch1 \" }, \" alias \" : \" batch1 _ newsletter _ subscriber \" }, { \" function \" : \" anyif \", \"", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "batch1 \" }, \" alias \" : \" batch1 _ newsletter _ subscriber \" }, { \" function \" : \" anyif \", \" parameters \" : { \" result \" : \" newsletter _ subscriber \", \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" batch2 \" }, \" alias \" : \" batch2 _ newsletter _ subscriber \" } ], \" group _ by \" : [ { \" property \" : \" customer _ id \" } ] }, \" where \" : [ { \" property \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" group _ by \" : [ { \" property \" : \" customer _ id \" } ] }, \" where \" : [ { \" property \" : \" batch1 _ newsletter _ subscriber \", \" comparator \" : \" notnull \" }, { \" property \" : \" batch2 _ newsletter _ subscriber \", \" comparator \" : \" notnull \" } ] } } updated 3 months ago table of contents example queries per - customer variance change across batches change across first to last inference per customer change in categorical variables source : https : / / docs. arthur.", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "per - customer variance change across batches change across first to last inference per customer change in categorical variables source : https : / / docs. arthur. ai / docs / grouped - inference - queries", "metadata": {"source": "https://docs.arthur.ai/docs/grouped-inference-queries", "row": 112, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 113 text : audit log jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by audit logsuggest editsthe arthur platform has the ability to produce an audit log of all calls to sensitive endpoints that include models, organizations, rbac, and uploading / modifying data. event format each event in the audit log has the following fields : fieldtypenotesevent _ categorystringa description of the overarching category for this event. see the table below for a breakdown of the various categories. event _ typestringa", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "categorystringa description of the overarching category for this event. see the table below for a breakdown of the various categories. event _ typestringan explanation of what kind of event occurred within the event _ category. see the table below for a breakdown of the various types. event _ idstringa unique id for this event, currently in uuid format but this may change in the future. timestamp [ string, int ] a timestamp in either unix epoch millisecond integer format or rfc 3339 string format, depending on the point of integration. organization _ id [ string, null ] a", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "either unix epoch millisecond integer format or rfc 3339 string format, depending on the point of integration. organization _ id [ string, null ] a string uuid of the organization if there is one associated with the event. model _ id [ string, null ] a string uuid of the model if there is one associated with the event. user _ id [ string, null ] a string id of the user if there is one associated with the event. user _ type [ string, null ] a string description of the kind of user if there is one associated with the event. this can be one of : service - account", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "type [ string, null ] a string description of the kind of user if there is one associated with the event. this can be one of : service - account, arthur - managed, or idp - managed. http _ path [ string, null ] a string http path of the request that triggered the event if one exists. http _ method [ string, null ] a string http method of the request that triggered the event if one exists. http _ status _ code [ int, null ] an integer http status code of the request that triggered the event if one exists. logged endpoints when enabled, audit logging will track all requests made", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", null ] an integer http status code of the request that triggered the event if one exists. logged endpoints when enabled, audit logging will track all requests made to the following endpoints and set the event category and event type respectively in the audit log events. endpointmethodevent categoryevent type / organizationspostevents. arthur. ai / organizationcreated / organizations / { organization _ id } deleteevents. arthur. ai / organizationdeleted / modelspostevents. arthur. ai / modelcreated / models / { model _ id } putevents. arthur. ai / modelupdate", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "/ modelspostevents. arthur. ai / modelcreated / models / { model _ id } putevents. arthur. ai / modelupdated / models / { model _ id } deleteevents. arthur. ai / modeldeleted / alerts / { alert _ id } / notificationspostevents. arthur. ai / alertcreated / models / { model _ id } / inferencespostevents. arthur. ai / ingestioninference _ data _ received / models / { model _ id } / inferencespatchevents. arthur. ai / ingestionground", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ingestioninference _ data _ received / models / { model _ id } / inferencespatchevents. arthur. ai / ingestionground _ truth _ data _ received / models / { model _ id } / inferences / filepostevents. arthur. ai / ingestioninference _ data _ received / models / { model _ id } / reference _ datapostevents. arthur. ai / ingestionreference _ data _ received / models / { model _ id } / batches / { batch _ id } patchevents. arthur. ai / ingestioninference _", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "received / models / { model _ id } / batches / { batch _ id } patchevents. arthur. ai / ingestioninference _ data _ batch _ completed / models / { model _ id } / reference _ datapatchevents. arthur. ai / ingestionreference _ data _ upload _ completed / models / { model _ id } / metricspostevents. arthur. ai / metricscreated / models / { model _ id } / metrics / { metric _ id } putevents. arthur. ai / metricsupdated / models / { model _", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "{ model _ id } / metrics / { metric _ id } putevents. arthur. ai / metricsupdated / models / { model _ id } / metrics / { metric _ id } deleteevents. arthur. ai / metricsdeleted / authorization / custom _ rolespostevents. arthur. ai / rbacupdated / authorization / custom _ rolesdeleteevents. arthur. ai / rbacupdated a more thorough description of these endpoints is available at our api documentation. integration with eventbridge the on - prem installation provides support for shipping the audit log to aw", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "more thorough description of these endpoints is available at our api documentation. integration with eventbridge the on - prem installation provides support for shipping the audit log to aws eventbridge. to configure this, you will need the following : bus name : required. the name of the eventbridge bus. this should not be the full arn of the bus. region : required. this is the aws region where your eventbridge bus is located. source : optional. this value will be added to the eventbridge events \" source \" for all events. this defaults to \" arthur - audit - log \". detail type :", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this value will be added to the eventbridge events \" source \" for all events. this defaults to \" arthur - audit - log \". detail type : optional. this value will be added to the eventbridge events \" detail - type \" for all events. this defaults to \" events. arthur. ai. \" an example of the events that are written to eventbridge looks like the following ( this was captured via an eventbridge to cloudwatch log group rule and target ) : json { \" version \" : \" 0 \", \" id \" : \" b87f2a3a - 6be1 - e", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") : json { \" version \" : \" 0 \", \" id \" : \" b87f2a3a - 6be1 - e1d9 - bc94 - 720d60e0a9d8 \", \" detail - type \" : \" events. arthur. ai \", \" source \" : \" arthur - audit - log \", \" account \" : \" 1234567890 \", \" time \" : \" 2022 - 07 - 21t22 : 07 : 00z \", \" region \" : \" us - east - 2 \", \" resources \" : [ ]", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##2 - 07 - 21t22 : 07 : 00z \", \" region \" : \" us - east - 2 \", \" resources \" : [ ], \" detail \" : { \" event _ type \" : \" created \", \" event _ category \" : \" events. arthur. ai / model \", \" event _ id \" : \" da2ec82d - f581 - 4e72 - bb66 - fc82504f2a7e \", \" timestamp \" : \" 2022 - 07 - 21t22 : 06 : 59. 683 + 0000 \"", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##2a7e \", \" timestamp \" : \" 2022 - 07 - 21t22 : 06 : 59. 683 + 0000 \", \" organization _ id \" : \" d579359a - 7259 - 4397 - a08b - 3e36c212350f \", \" model _ id \" : \" a950c9ad - 6a1e - 4042 - 8e47 - 461d13072da5 \", \" user _ id \" : \" df3fe374 - 26d7 - 4bd8 -", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##d13072da5 \", \" user _ id \" : \" df3fe374 - 26d7 - 4bd8 - bf62 - e04a6e078e2b \", \" user _ type \" : \" arthur - managed \", \" http _ path \" : \" / api / v3 / models \", \" http _ method \" : \" post \", \" http _ status _ code \" : 200 } } configuration the eventbridge integration can be enabled on the admin console config page by : checking \" show other advanced options \" under the other", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "200 } } configuration the eventbridge integration can be enabled on the admin console config page by : checking \" show other advanced options \" under the other advanced options section after that is checked, a new section will appear called \" audit logging \" check \" enable audit log \" next, a choice of persistence methods appears. choose \" aws eventbridge \" fill out the \" bus name, \" \" region, \" \" event source, \" and \" detail type \" fields that appear. click \" save config \" and deploy the updated version required iam permissions to send events to aws eventbridge, the arthur iam", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "appear. click \" save config \" and deploy the updated version required iam permissions to send events to aws eventbridge, the arthur iam credentials or role will require the events : putevents permission. here is an example policy that grants that permission on an eventbridge bus called arthur - events in the us - east - 2 region, in the 0123456789 aws account. json { \" statement \" : [ { \" action \" : \" events : putevents \", \" effect \" : \" allow \", \" resource \" : \" arn : aws : events : us", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" events : putevents \", \" effect \" : \" allow \", \" resource \" : \" arn : aws : events : us - east - 2 : 0123456789 : event - bus / arthur - events \", \" sid \" : \" \" } ], \" version \" : \" 2012 - 10 - 17 \" } updated 3 months ago table of contents event format logged endpoints integration with eventbridge configuration required iam permissions source : https : / / docs. arthur. ai / docs / audit - log", "metadata": {"source": "https://docs.arthur.ai/docs/audit-log", "row": 113, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 114 text : online kubernetes cluster ( k8s ) install jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfair", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##iclass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsas", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##time series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functions", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##l directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by online kubernetes cluster ( k8s ) installsuggest editsmake sure your k8s cluster is ready for arthur platform installation by following the kubernetes preparation guide. download installation files go to the download portal using the url and the password provided by arthur. click the \" download license \" button to download your license in the yaml file. setup for installation make sure you", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "url and the password provided by arthur. click the \" download license \" button to download your license in the yaml file. setup for installation make sure you're in the correct kubectl environment context before running the installer. install the kots kubectl extension on your local machine : shellcurl https : / / kots. io / install bash start installation run the admin console installer and login on to your browser at localhost : 8800 via the provided port forwarding tunnel : shellkubectl kots install arthur for namespace - scoped installs, follow this", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##t : 8800 via the provided port forwarding tunnel : shellkubectl kots install arthur for namespace - scoped installs, follow this { doc } guide < k8s _ install _ namespace _ scoped >. when you need to re - create the tunnel to admin console, run : shellkubectl kots admin - console - - namespace < your _ name _ space > upload your license file. on the following screen, click on the link to install arthur from the internet. configure arthur. review the preflight checks to make sure that your machine meets", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "following screen, click on the link to install arthur from the internet. configure arthur. review the preflight checks to make sure that your machine meets the minimum requirements before you proceed with the installation. verify installation monitor the admin console dashboard for the application status to become ready. to see the progress of the deployment, monitor the deployment status with thekubectl cli : shellkubectl get deployment, statefulset, pod - n < yournamespace > if anything is showing pending, it is likely you need to add more / bigger nodes to your cluster. customize installation configure graphs", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##namespace > if anything is showing pending, it is likely you need to add more / bigger nodes to your cluster. customize installation configure graphs on admin console by clicking on the configure graphs button and providing your prometheus endpoint ( e. g., http : / / kube - prometheus - stack - prometheus. monitoring. svc. cluster. local : 9090 ). updated 3 months ago table of contents download installation files setup for installation start installation verify installation customize installation source : https : / / docs. arthur. ai / docs / online -", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "contents download installation files setup for installation start installation verify installation customize installation source : https : / / docs. arthur. ai / docs / online - kubernetes - cluster - k8s - install", "metadata": {"source": "https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install", "row": 114, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 115 text : querying data drift jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by querying data driftsuggest editsquerying drift in python the basic format of a drift query using the python sdk involves specifying that the query _ type parameter has the value'drift': pythonquery = {... } arthur _ model. query ( query, query _ type ='drift') data drift endpoint data drift has a dedicated endpoint at query data drift. returns the data drift metric between a", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", query _ type ='drift') data drift endpoint data drift has a dedicated endpoint at query data drift. returns the data drift metric between a base dataset with a target dataset. this endpoint can support up to 100 properties in one request. num _ bins - specifies the granularity of bucketing for continuous distributions and will be ignored if the attribute is categorical. metric - specify one metric among { ref } the data drift metrics arthur offers < glossary _ data _ drift >. filter - optional blocks specific to either reference or inference set to specify which data should be used in the data", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "arthur offers < glossary _ data _ drift >. filter - optional blocks specific to either reference or inference set to specify which data should be used in the data drift calculation. group _ by - global and applies to both the base and target data. rollup - optional parameter aggregating the calculated data drift value by the supported time dimension. for hypothesistest, the returned value is transformed as - log _ 10 ( p _ value ) to maintain directional parity with the other data drift metrics. a lower p _ value is more significant and implies data drift, reflected in a higher - log _ 10 ( p _ value ).", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data drift metrics. a lower p _ value is more significant and implies data drift, reflected in a higher - log _ 10 ( p _ value ). further mathematical details are in the glossary query request : json { \" properties \" : [ \" < attribute1 _ name > [ string ] \", \" < attribute2 _ name > [ string ] \", \" < attribute3 _ name > [ string ] \" ], \" num _ bins \" : \" < num _ bins > [ int ] \", \" metric \" : \" [ psikldivergencejsdivergencehellingerdistance", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" < num _ bins > [ int ] \", \" metric \" : \" [ psikldivergencejsdivergencehellingerdistancehypothesistest ] \", \" base \" : { \" source \" : \" [ inferencereference ] \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" target \" : { \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" target \" : { \" source \" : \" [ inferencereferenceground _ truth ] \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" group _ by [ optional ] \" : [ { \" property \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ threshold _ value > [ stringintfloat ] \" } ] }, \" group _ by [ optional ] \" : [ { \" property \" : \" < group _ by _ attribute _ name > [ string ] \" } ], \" rollup [ optional ] \" : \" minutehourdaymonthyearbatch _ id \" } query response : json { \" query _ result \" : [ { \" < attribute1 _ name > \" : \" < attribute1 _ data _ drift > [ float ] \", \" < attribute2 _ name > \" : \" < attribute2 _ data _ drift > [ float ]", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##1 _ data _ drift > [ float ] \", \" < attribute2 _ name > \" : \" < attribute2 _ data _ drift > [ float ] \", \" < attribute3 _ name > \" : \" < attribute3 _ data _ drift > [ float ] \", \" < group _ by _ attribute _ name > \" : \" < group _ by _ attribute _ value > [ stringintnull ] \", \" rollup \" : \" < rollup _ attribute _ value > [ stringnull ] \" } ] } example : reference vs. inference sample request : calculate data drift for males, grouped by country", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ attribute _ value > [ stringnull ] \" } ] } example : reference vs. inference sample request : calculate data drift for males, grouped by country, rolled up by hour. json { \" properties \" : [ \" age \" ], \" num _ bins \" : 10, \" metric \" : \" psi \", \" base \" : { \" source \" : \" reference \", \" filter \" : [ { \" property \" : \" gender \", \" comparator \" : \" eq \", \" value \" : \" male \" } ] }, \" target \" : { \" source \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" comparator \" : \" eq \", \" value \" : \" male \" } ] }, \" target \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" gender \", \" comparator \" : \" eq \", \" value \" : \" male \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" gte \", \" value \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestamp \", \" com", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 07 - 23t10 : 00 : 00z \" } ] }, \" group _ by \" : [ { \" property \" : \" country \" } ], \" rollup \" : \" hour \" } sample response : json { \" query _ result \" : [ { \" age \" : 2. 3, \" country \" : \" canada \", \" rollup \" : \" 2020 - 07", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "query _ result \" : [ { \" age \" : 2. 3, \" country \" : \" canada \", \" rollup \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" age \" : 2. 4, \" country \" : \" united states \", \" rollup \" : \" 2020 - 07 - 22t10 : 00 : 00z \" } ] } example : inference vs. inference sample request : compare data drift between two batches, with no grouping, filters, or rollups. json { \" properties \" : [ \" age \" ], \" num _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "between two batches, with no grouping, filters, or rollups. json { \" properties \" : [ \" age \" ], \" num _ bins \" : 10, \" metric \" : \" psi \", \" base \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" 5 \" } ] }, \" target \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" batch _ id \", \" com", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", \" target \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" 6 \" } ] } } sample response : json { \" query _ result \" : [ { \" age \" : 2. 3 } ] } back to top example : reference vs. ground truth sample request : calculate data drift for individual ground truth class prediction probabilities, rolled up by hour. json { \" properties \" : [ \" gt _ 1 \" ], \" num _ bins", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "class prediction probabilities, rolled up by hour. json { \" properties \" : [ \" gt _ 1 \" ], \" num _ bins \" : 10, \" metric \" : \" psi \", \" base \" : { \" source \" : \" reference \" }, \" target \" : { \" source \" : \" ground _ truth \", \" filter \" : [ { \" property \" : \" ground _ truth _ timestamp \", \" comparator \" : \" gte \", \" value \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##tor \" : \" gte \", \" value \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \" : \" ground _ truth _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 07 - 23t10 : 00 : 00z \" } ] }, \" rollup \" : \" hour \" } sample response : json { \" query _ result \" : [ { \" gt _ 1 \" : 0. 03, \" rollup \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ { \" gt _ 1 \" : 0. 03, \" rollup \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" gt _ 1 \" : 0. 4, \" rollup \" : \" 2020 - 07 - 22t11 : 00 : 00z \" } ] } back to top data drift psi bucket table values this metric has a dedicated endpoint at query psi bucket table. returns the psi scores by bucket using the reference set data. this query for this endpoint omits the need for metric and takes in a single property but otherwise is identical to the data drift endpoint", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "reference set data. this query for this endpoint omits the need for metric and takes in a single property but otherwise is identical to the data drift endpoint note when using this endpoint with categorical features, the bucket _ min and bucket _ max fields will not be returned in the response. instead, the bucket field will contain the category name. query request : json { \" property \" : \" < attribute _ name > [ string ] \", \" num _ bins \" : \" < num _ bins > [ int ] \", \" base \" : { \" source \" : \" [ inferencereference ]", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "bins \" : \" < num _ bins > [ int ] \", \" base \" : { \" source \" : \" [ inferencereference ] \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" target \" : { \" source \" : \" [ inferencereference ] \", \" filter [ optional ] \" : [ { \" property", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} ] }, \" target \" : { \" source \" : \" [ inferencereference ] \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" group _ by [ optional ] \" : [ { \" property \" : \" < group _ by _ attribute _ name > [ string ] \" } ], \" rollup [ optional ] \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "[ { \" property \" : \" < group _ by _ attribute _ name > [ string ] \" } ], \" rollup [ optional ] \" : \" minutehourdaymonthyearbatch _ id \" } query response : json { \" query _ result \" : [ { \" bucket \" : \" string \", \" rollup \" : \" stringnull \", \" group _ by _ property _ 1 \" : \" stringnull \", \" base _ bucket _ max \" : \" number \", \" base _ bucket _ min \" : \" number \", \" base _ count _ per _ bucket \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ bucket _ max \" : \" number \", \" base _ bucket _ min \" : \" number \", \" base _ count _ per _ bucket \" : \" number \", \" base _ ln _ probability _ per _ bucket \" : \" number \", \" base _ probability _ per _ bucket \" : \" number \", \" base _ total \" : \" number \", \" target _ bucket _ max \" : \" number \", \" target _ bucket _ min \" : \" number \", \" target _ count _ per _ bucket \" : \" number \", \" target _ ln _ probability _ per _ bucket \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": \" number \", \" target _ count _ per _ bucket \" : \" number \", \" target _ ln _ probability _ per _ bucket \" : \" number \", \" target _ probability _ per _ bucket \" : \" number \", \" target _ total \" : \" number \", \" probability _ difference \" : \" number \", \" ln _ probability _ difference \" : \" number \", \" psi \" : \" number \" } ] } sample request : calculate data drift bucket components for males, grouped by country, rolled up by hour. json { \" property \" : \" age \", \" num _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "calculate data drift bucket components for males, grouped by country, rolled up by hour. json { \" property \" : \" age \", \" num _ bins \" : 2, \" base \" : { \" source \" : \" reference \", \" filter \" : [ { \" property \" : \" gender \", \" comparator \" : \" eq \", \" value \" : \" male \" } ] }, \" target \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" gender \", \" comparator \" : \" eq \", \" value \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" inference \", \" filter \" : [ { \" property \" : \" gender \", \" comparator \" : \" eq \", \" value \" : \" male \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" gte \", \" value \" : \" 2020 - 07 - 22t10 : 00 : 00z \" }, { \" property \" : \" inference _ timestamp \", \" comparator \" : \" lt \", \" value \" : \" 2020 - 07 - 23t10 : 00 : 00z \" } ] }, \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "comparator \" : \" lt \", \" value \" : \" 2020 - 07 - 23t10 : 00 : 00z \" } ] }, \" group _ by \" : [ { \" property \" : \" country \" } ], \" rollup \" : \" hour \" } sample response : json { \" query _ result \" : [ { \" bucket \" : \" bucket _ 1 \", \" rollup \" : \" 2020 - 01 - 01t00 : 00 : 00z \", \" country \" : \" canada \", \" base _ bucket _ max \" : 0. 9999971182990", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "00 : 00z \", \" country \" : \" canada \", \" base _ bucket _ max \" : 0. 9999971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" : 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base _ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318807485 }, { \" bucket \" : \" bucket _ 2 \", \" rollup \" : \" 2020 - 01 - 01t00 : 00", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##807485 }, { \" bucket \" : \" bucket _ 2 \", \" rollup \" : \" 2020 - 01 - 01t00 : 00 : 00z \", \" country \" : \" united states \", \" base _ bucket _ max \" : 0. 9999971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" : 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base _ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318807485 }, { \" bucket \" : \" bucket _ 1 \", \" rollup \" : \" 2020 - 01 - 01t01 : 00 : 00z \", \" country \" : \" canada \", \" base _ bucket _ max \" : 0. 9999971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##7, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" : 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base _ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318807485 }, { \" bucket \" : \" bucket _ 2 \", \" rollup \" : \" 2020 - 01 - 01t01 : 00 : 00z \", \" country \" : \" united states \", \" base _ bucket _ max \" : 0. 999997118299017", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "00z \", \" country \" : \" united states \", \" base _ bucket _ max \" : 0. 9999971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" : 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base _ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0.", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##1762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318807485 } ] } sample request : compare data drift bucket components between two batches, with no grouping, no filters, and no rollups. j", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##7485 } ] } sample request : compare data drift bucket components between two batches, with no grouping, no filters, and no rollups. json { \" property \" : \" age \", \" num _ bins \" : 10, \" base \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" 5 \" } ] }, \" target \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" batch", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" 5 \" } ] }, \" target \" : { \" source \" : \" inference \", \" filter \" : [ { \" property \" : \" batch _ id \", \" comparator \" : \" eq \", \" value \" : \" 6 \" } ] } } sample response : json { \" query _ result \" : [ { \" bucket \" : \" bucket _ 1 \", \" base _ bucket _ max \" : 0. 9999971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##1182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" : 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base _ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318807485 }, { \" bucket \" : \" bucket _ 2 \", \" base _ bucket _ max \" : 0. 9999971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##9971182990177, \" base _ bucket _ min \" : 0. 5009102069226075, \" base _ count _ per _ bucket \" : 4988, \" base _ ln _ probability _ per _ bucket \" : - 0. 6955500651756032, \" base _ probability _ per _ bucket \" : 0. 4988, \" base _ total \" : 10000, \" target _ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 500910", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ bucket _ max \" : 0. 9999971182990177, \" target _ bucket _ min \" : 0. 5009102069226075, \" target _ count _ per _ bucket \" : 2487, \" target _ ln _ probability _ per _ bucket \" : - 0. 6701670131762315, \" target _ probability _ per _ bucket \" : 0. 5116231228142357, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699,", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##57, \" target _ total \" : 4861, \" probability _ difference \" : - 0. 012823122814235699, \" ln _ probability _ difference \" : - 0. 025383051999371742, \" psi \" : 0. 00032548999318807485 } ] } back to top data drift for classification outputs for classification outputs, one may want to examine drift among a collection of different classes, i. e., the system of outputs, instead of the drift of the probability predictions of a single class. the query", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "among a collection of different classes, i. e., the system of outputs, instead of the drift of the probability predictions of a single class. the query uses one of \" predicted _ classes \" : [ \" * \" ] or \" ground _ truth _ classes \" : [ \" * \" ] but otherwise is identical to a standard data drift query. rather than using the star operator to select all prediction or ground truth classes, respectively, in a model, a list of string classes can be provided for looking at the drift of a subset of multiclass outputs. predicted _ classes - specifies which prediction classes to use for predictedclass data drift", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "classes can be provided for looking at the drift of a subset of multiclass outputs. predicted _ classes - specifies which prediction classes to use for predictedclass data drift. ground _ truth _ classes - specifies which prediction classes to use for groundtruthclass data drift. properties can be included in the same query as long as the target sourcecorresponds to the classification output tag. for example, one can query drift on input attributes and predictedclass in the same query with target source of inference ; one can query drift on individual ground truth labels and groundtruthclass in the same query with target source of ground _ truth. query request", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "source of inference ; one can query drift on individual ground truth labels and groundtruthclass in the same query with target source of ground _ truth. query request : json { \" properties [ optional ] \" : [ \" < attribute1 _ name > [ string ] \", \" < attribute2 _ name > [ string ] \", \" < attribute3 _ name > [ string ] \" ], \" [ predicted _ classesground _ truth _ classes ] \" : [ \" < class0 _ name > [ string ] \" \" < class1 _ name > [ string ] \" ], \" num _ bins \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "< class0 _ name > [ string ] \" \" < class1 _ name > [ string ] \" ], \" num _ bins \" : \" < num _ bins > [ int ] \", \" metric \" : \" [ psikldivergencejsdivergencehellingerdistancehypothesistest ] \", \" base \" : { \" source \" : \" [ inferencereference ] \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ]", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" target \" : { \" source \" : \" [ inferencereferenceground _ truth ] \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" group _ by [ optional ] \" : [ { \" property \" : \" < group _ by _ attribute _ name > [ string ] \" } ], \" rollup [ optional ] \" : \" minutehourdaymonthyearbatch _ id \" } query response : json { \" query _ result \" : [ { \" < attribute1 _ name > \" : \" < attribute1", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ch _ id \" } query response : json { \" query _ result \" : [ { \" < attribute1 _ name > \" : \" < attribute1 _ data _ drift > [ float ] \", \" < attribute2 _ name > \" : \" < attribute2 _ data _ drift > [ float ] \", \" < attribute3 _ name > \" : \" < attribute3 _ data _ drift > [ float ] \", \" [ predictedclassgroundtruthclass ] \" : \" < classification _ data _ drift > [ float ] \", \" < group _ by _ attribute _ name > \" : \" < group _", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "] \" : \" < classification _ data _ drift > [ float ] \", \" < group _ by _ attribute _ name > \" : \" < group _ by _ attribute _ value > [ stringintnull ] \", \" rollup \" : \" < rollup _ attribute _ value > [ stringnull ] \" } ] } sample request : calculate data drift on all prediction classes. json { \" predicted _ classes \" : [ \" * \" ], \" num _ bins \" : 20, \" base \" : { \" source \" : \" reference \" }, \" target \" : { \" source \" :", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "num _ bins \" : 20, \" base \" : { \" source \" : \" reference \" }, \" target \" : { \" source \" : \" inference \" }, \" metric \" : \" psi \" } sample response : json { \" query _ result \" : [ { \" predictedclass \" : 0. 021 } ] } sample request : calculate data drift on ground truth using the first and third ground truth classes. json { \" predicted _ classes \" : [ \" gt _ 1 \", \" gt _ 3 \" ], \" num _ bins \" : 20, \" base \" : { \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : [ \" gt _ 1 \", \" gt _ 3 \" ], \" num _ bins \" : 20, \" base \" : { \" source \" : \" reference \" }, \" target \" : { \" source \" : \" ground _ truth \" }, \" metric \" : \" psi \" } sample response : json { \" query _ result \" : [ { \" groundtruthclass \" : 0. 021 } ] } back to top automated data drift thresholds what is a sufficiently high data drift value to suggest that the target data has actually drifted from the base data? for hypothesistest, we can", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "data drift thresholds what is a sufficiently high data drift value to suggest that the target data has actually drifted from the base data? for hypothesistest, we can reverse engineer - log _ 10 ( p _ value ) and plug in the conventional. 05 alpha level to establish a lower bound of - log _ 10 (. 05 ). for the other data drift metrics, pining a constant is insufficient. we abstract this away for the user and allow queries to obtain automatically generated data drift thresholds ( lower bounds ) based on a model's data. these thresholds can be used in alerting. for more information, see", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "generated data drift thresholds ( lower bounds ) based on a model's data. these thresholds can be used in alerting. for more information, see : automating data drift thresholding in machine learning systems. the query uses the \" metric \" : \" thresholds \" and does not require nor use \" target \" and \" rollup \" fields but otherwise is identical to a standard data drift query. query request : json { \" properties \" : [ \" < attribute1 _ name > [ string ] \", \" < attribute2 _ name > [ string ] \", \" < attribute3 _ name > [ string ]", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attribute1 _ name > [ string ] \", \" < attribute2 _ name > [ string ] \", \" < attribute3 _ name > [ string ] \" ], \" num _ bins \" : \" < num _ bins > [ int ] \", \" metric \" : \" thresholds \", \" base \" : { \" source \" : \" reference \", \" filter [ optional ] \" : [ { \" property \" : \" < filter _ attribute _ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "_ name > [ string ] \", \" comparator \" : \" < comparator > [ string ] \", \" value \" : \" < filter _ threshold _ value > [ stringintfloat ] \" } ] }, \" group _ by [ optional ] \" : [ { \" property \" : \" < group _ by _ attribute _ name > [ string ] \" } ] } query response : json { \" query _ result \" : [ { \" < attribute1 _ name > \" : { \" hellingerdistance \" : \" < threshold > [ float ] \", \" jsdivergence \" : \"", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "attribute1 _ name > \" : { \" hellingerdistance \" : \" < threshold > [ float ] \", \" jsdivergence \" : \" < threshold > [ float ] \", \" kldivergence \" : \" < threshold > [ float ] \", \" psi \" : \" < threshold > [ float ] \" }, \" < attribute2 _ name > \" : { \" hellingerdistance \" : \" < threshold > [ float ] \", \" jsdivergence \" : \" < threshold > [ float ] \", \" kldivergence \" : \" < threshold > [ float ] \",", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" jsdivergence \" : \" < threshold > [ float ] \", \" kldivergence \" : \" < threshold > [ float ] \", \" psi \" : \" < threshold > [ float ] \" } } ] } sample request : json { \" properties \" : [ \" age \" ], \" num _ bins \" : 20, \" base \" : { \" source \" : \" reference \" }, \" metric \" : \" thresholds \" } sample response : json { \" query _ result \" : [ { \" age \" : { \" hellingerdistance \" : 0. 000417", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} sample response : json { \" query _ result \" : [ { \" age \" : { \" hellingerdistance \" : 0. 00041737395239735647, \" jsdivergence \" : 2. 959228131592643, \" kldivergence \" : 0. 001893866910388703, \" psi \" : 0. 0018945640055550161 } } ] } back to topupdated 3 months ago table of contents querying drift in python data drift endpoint data drift psi bucket table", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##61 } } ] } back to topupdated 3 months ago table of contents querying drift in python data drift endpoint data drift psi bucket table values data drift for classification outputs automated data drift thresholds source : https : / / docs. arthur. ai / docs / querying - data - drift", "metadata": {"source": "https://docs.arthur.ai/docs/querying-data-drift", "row": 115, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 116 text : alert summary reports jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by alert summary reportssuggest editsan alert summary is a way for teams to send alert reports about different models in their organization to members of their internal team or external stakeholders interested in alert governance. what is sent out with an alert summary? an alert summary report contains information about the status of alerts triggered for a particular model. what needs to be configured? an alert summary report in arthur is an aggregated report that can be shared with members of your", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s triggered for a particular model. what needs to be configured? an alert summary report in arthur is an aggregated report that can be shared with members of your organization on some regular cadence. the report can be configured to control the following : name of your alert summary report : teams can create as many unique alert summary configurations as they want. each alert configuration will have a unique id, which will be used when maintaining the report ( keeping up - to - date subscriber lists, maintaining models to be alerted on, etc ). which arthur models are included : list of arthur model uuids who gets sent the report : known as", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", maintaining models to be alerted on, etc ). which arthur models are included : list of arthur model uuids who gets sent the report : known as a subscriber, this contains email addresses of users who would like to receive the report frequency of sent reports : there are currently two options : daily and weekly. for weekly, teams can also configure what day of the week they would like to receive the report. time of day to send report : do you want to receive the report on monday mornings at a specific time? set up the time of day you would like your report to be sent this can be configured", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "you want to receive the report on monday mornings at a specific time? set up the time of day you would like your report to be sent this can be configured through the create alert summary configuration. this will return : alert summary configuration id : this unique id is used when maintaining the report ( keeping up - to - date subscriber lists, maintaining models to be alerted on, etc. ). maintaining your alert summary report after creating a specific alert summary report, teams can maintain these reports to keep them current. a few common api calls and actions that teams will make include : adding and deleting alert summary subscribers : teams will use", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "these reports to keep them current. a few common api calls and actions that teams will make include : adding and deleting alert summary subscribers : teams will use these api calls to add new subscribers and remove subscribers from specific alert summary configurations. updating alert summary configuration : another piece of alert summary maintenance is being able to make changes to an existing configuration. one of the most common reasons teams make changes is to add in a newly onboarded model to the group's alert summary. this can be done with the update alert summary configuration api call. pulling reports on your alert summary report : the level of meta - governance often needed in governance", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". this can be done with the update alert summary configuration api call. pulling reports on your alert summary report : the level of meta - governance often needed in governance systems. teams can call the arthur api to pull reports on their alert summary configurations, such as lists of alert summaries past, get subscribers,, and subscriber notification configurations. updated 3 months ago table of contents what is sent out with an alert summary? what needs to be configured? maintaining your alert summary report source : https : / / docs. arthur. ai / docs / alert - summary - 3", "metadata": {"source": "https://docs.arthur.ai/docs/alert-summary-3", "row": 116, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 117 text : servicenow jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamples", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##i and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalerting", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshoot", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscom", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational database", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "- prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access control", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformapp", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by servicenowservice now integration guidesuggest editswith the arthur + servicenow integration, you can set up email automation to notify on - call teams in servicenow of alerts arthur triggers. to set up this integration, follow these steps : step 1 : set up your email integration in servicenow an inbound email action in servicenow can be configured to receive arthur alerts and create incidents in response to those alerts. please see", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "integration in servicenow an inbound email action in servicenow can be configured to receive arthur alerts and create incidents in response to those alerts. please see the servicenow email actions guide for details on how to do this. once you have set up an email action to handle incoming arthur alerts and generate incidents from those alerts, retrieve the servicenow instance email address for step 2. step 2 : configure your integration in arthur to configure the servicenow integration for a model in arthur, you can send a post request to the / alert _ notification _ configurations. model _ id - uuid of the", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "servicenow integration for a model in arthur, you can send a post request to the / alert _ notification _ configurations. model _ id - uuid of the model this alert notification configuration belongs to. type - type of notification to send. in this case, \" servicenow \". destination - the integration email address obtained in step 1. enabled - whether or not the notification configuration is enabled. it defaults to true. example query request : json { \" model _ id \" : \" < model _ id > [ string ] \", \" type \" : \" [ servicenow ] \", \" destination \" : \" < [ email", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" : \" < model _ id > [ string ] \", \" type \" : \" [ servicenow ] \", \" destination \" : \" < [ email protected ] > [ string ] \", } for more information on alert notifications, please see the notifications section of the alerting page. step 3 : start monitoring! your integration is now ready to use! when an alert is triggered in arthur for this model, an incident will be created in your servicenow instance. updated 3 months ago table of contents step 1 : set up your email integration in servicenow step 2 : configure your integration in arthur step 3", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". updated 3 months ago table of contents step 1 : set up your email integration in servicenow step 2 : configure your integration in arthur step 3 : start monitoring! source : https : / / docs. arthur. ai / docs / servicenow", "metadata": {"source": "https://docs.arthur.ai/docs/servicenow", "row": 117, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 118 text : arthur scope faq jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformex", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsal", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroub", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##viewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by arthur scope faqsuggest edits1. can i use arthur scope without using the python sdk? yes! the arthur scope platform is api - first. you can use our arthur api to onboard models, send predictions, and query metrics and insights. 2. does arthur need a copy of my model? arthur doesn \u2019 t generally need access to your actual model but only captures the inputs to the model and the predictions it makes.", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "need a copy of my model? arthur doesn \u2019 t generally need access to your actual model but only captures the inputs to the model and the predictions it makes. this means that you can even use arthur to monitor models you cannot access, such as models hosted by third - party services. to enable explainability, arthur does need access to your model. when enabling explainability, you will need to provide access to the modelspredict function. 3. what if my data is proprietary? can i still use arthur? yes! arthur offers on - premises installation for customers with data security requirements. by integrating arthur into your business's on -", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "? can i still use arthur? yes! arthur offers on - premises installation for customers with data security requirements. by integrating arthur into your business's on - premises stack, you can be confident that all security requirements are met while still getting the benefits of the computation and analytics arthur provides. 4. what if i don \u2019 t have ground truth labels for my data? or what if i will have the ground truth labels in the future, but they are not available yet? you don't need ground truth labels to log your model's inferences with arthur. if your ground truth labels become available after your model's inferences", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "don't need ground truth labels to log your model's inferences with arthur. if your ground truth labels become available after your model's inferences, whether seconds later or years later, arthur can link these new ground truth values to your model's past predictions, linking the new values by id to their corresponding inferences already in the arthur system. in the meantime, arthur \u2019 s data drift metrics can offer leading indicators of model underperformance to keep you covered if your ground truth labels are delayed or never become available. 5. i got an error using the sdk. what do i do? if the", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "you covered if your ground truth labels are delayed or never become available. 5. i got an error using the sdk. what do i do? if the error message says \" an internal exception occurred, please report to arthur \" that means there was a problem on our side. please email the arthur support team at [ email protected ] to let us know what happened. otherwise, the error message should provide helpful instructions for how to resolve the issue. if you don \u2019 t find the error message actionable, please let arthur know so we can improve it. 6. do i have to type my credentials in every time i use the sd", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the error message actionable, please let arthur know so we can improve it. 6. do i have to type my credentials in every time i use the sdk? no! instead of manually entering them, you can specify an arthur _ endpoint _ url and arthur _ api _ key environment variable to be used to create the arthurai connection object. 7. what are streaming and batch models? streaming and batch are two model types with different patterns of ingesting data to send to arthur. a streaming model processes data as a stream of individual inferences : data is logged with arthur directly as individual inferences when the data flows", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to send to arthur. a streaming model processes data as a stream of individual inferences : data is logged with arthur directly as individual inferences when the data flows into the model. a batch model processes data as a sequence of grouped inferences, which are usually grouped over time : data is logged with arthur as a group of inferences as the model processes the batch. 8. which drift metric should i use? population stability index ( psi ) is typically a good default drift metric. in some cases, one wants a drift metric with a certain property, e. g., using a drift metric with the unit nats for interpretability", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". in some cases, one wants a drift metric with a certain property, e. g., using a drift metric with the unit nats for interpretability or using a drift metric bounded between 0 and 1 so that drift values don't increase arbitrarily for outliers. in these cases, other metrics may be preferable to psi. for a review of the data drift metrics arthur offers and their properties, see the data drift section of our glossary. furthermore, see our blog post for an overview of data on how arthur automates the choice of thresholding for drift metrics. updated about 2 months", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ary. furthermore, see our blog post for an overview of data on how arthur automates the choice of thresholding for drift metrics. updated about 2 months ago table of contents 1. can i use arthur scope without using the python sdk? 2. does arthur need a copy of my model? 3. what if my data is proprietary? can i still use arthur? 4. what if i don \u2019 t have ground truth labels for my data? or what if i will have the ground truth labels in the future, but they are not available yet? 5. i got an error using the sdk. what do i do", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will have the ground truth labels in the future, but they are not available yet? 5. i got an error using the sdk. what do i do? 6. do i have to type my credentials in every time i use the sdk? 7. what are streaming and batch models? 8. which drift metric should i use? source : https : / / docs. arthur. ai / docs / arthur - faq", "metadata": {"source": "https://docs.arthur.ai/docs/arthur-faq", "row": 118, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 119 text : backup and restore jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by backup and restoresuggest editswarnings [UNK] tested on aws, as writtenthese instructions have been tested as written for an aws deployment. if you find they do not work for your use - case, please reach out to arthur support before modifying them. we cannot guarantee reliable operation if these instructions are not followed exactly as written. [UNK] no network connection between backup / restore environmentswhen restoring into a new cluster, you must ensure", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "reliable operation if these instructions are not followed exactly as written. [UNK] no network connection between backup / restore environmentswhen restoring into a new cluster, you must ensure that the new cluster is unable to communicate with any services or data store in the old cluster. if you took a backup on cluster apple, and performed a restore into cluster banana, cluster banana must point to its own rds instance, clickhouse database, and kafka store ( note : it is ok if clusters share the s3 bucket ). to ensure this, you must re - configure via the admin interface when restoring into a new cluster. failure", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "share the s3 bucket ). to ensure this, you must re - configure via the admin interface when restoring into a new cluster. failure to do this will cause data corruption on both clusters that is unrecoverable. [UNK] everything at the same timeif you are either manually taking a backup or scheduling a backup, you must take a backup of the full platform. you cannot use a clickhouse snapshot taken at midnight with an rds snapshot taken at 0400 am ( or any other time ). all backup operations must be performed at the same time, and when restoring, the data you are using", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "taken at 0400 am ( or any other time ). all backup operations must be performed at the same time, and when restoring, the data you are using must all belong to the same backup operation. this is to ensure data consistency across the different data stores. ignoring this will cause data corruption. overview the overall backup and restore process for the arthur platform is as follows : backing up the arthur platform take a backup of clickhouse data take a backup of kubernetes deployment state and persistent volumes enrichments infrastructure model servers data pipeline services enrichment / delete enrichment workflows kafka deployment state and ebs volumes ( using e", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "state and persistent volumes enrichments infrastructure model servers data pipeline services enrichment / delete enrichment workflows kafka deployment state and ebs volumes ( using ebs snapshots ) take a backup of rds postgres restore the arthur platform restore rds postgres update configuration and install the platform restore clickhouse data restore the kafka deployment state and persistent volumes restore enrichments infrastructure restore workflows smoke tests and validation overview - clickhouse - backup the arthur platform stores inference data, data built from the enrichments pipeline, reference and ground truth data in clickhouse. clickhouse is an open - source olap database", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "platform stores inference data, data built from the enrichments pipeline, reference and ground truth data in clickhouse. clickhouse is an open - source olap database which enables sql - like query execution, replication, sharding and many additional features. to backup clickhouse, the arthur platform uses a tool called clickhouse - backup. clickhouse - backup is a sidecar - container included on the clickhouse pods and is responsible for taking backups, performing restores, and coordinating with remote storage ( in this case s3 ) to store and retrieve backups. clickhouse - backup uses built - in functionality of clickhouse to take", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "coordinating with remote storage ( in this case s3 ) to store and retrieve backups. clickhouse - backup uses built - in functionality of clickhouse to take backups and perform restores. overview - velero the arthur platform uses velero, which is an industry - standard, battle - tested tool for backing up kubernetes resources including persistent volumes. arthur uses velero to backup necessary namespaced kubernetes resources, as well as the ebs volume snapshot backups for each persistentvolumes claimed by the statefulsets ( eg : via pvcs ). backup data (", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "ebs volume snapshot backups for each persistentvolumes claimed by the statefulsets ( eg : via pvcs ). backup data ( not including ebs volume snapshots ) is stored in an s3 bucket which is accessible via a serviceaccount that is provisioned for the backup and restore agent. backups and restores are managed by velero using kubernetes custom resource definitions ( crds ), which are consumed by the velero backup controller. velero has a feature which also allows backups to be scheduled, using a cron - like configuration. it also", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the velero backup controller. velero has a feature which also allows backups to be scheduled, using a cron - like configuration. it also provides servicemonitors which expose metrics via prometheus, so that operators can monitor backup and restore status and set up alerts for when backup or restore operations fail. overview - arthur ( argo ) workflows the arthur platform uses argo workflows as a workflow orchestration engine for running certain jobs. argo installs a handful of custom resource definitions ( crds ) which enable the argo workflow services to schedule, execute and update these jobs", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "jobs. argo installs a handful of custom resource definitions ( crds ) which enable the argo workflow services to schedule, execute and update these jobs. workflows are dynamically managed, meaning that their definitions are not stored in the arthur installer script. the backup and restore operation accounts for this by treating restoration of workflows on a case - by - case basis, as follows : enrichments and delete enrichments workflows these workflows are created to stand - up and tear - down infrastructure necessary for processing enrichments data ( eg : kafka topics, pods which manage the data pipeline", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are created to stand - up and tear - down infrastructure necessary for processing enrichments data ( eg : kafka topics, pods which manage the data pipeline for enrichments, etc. ) these workflows are idempotent and safe to recover therefore, these workflows are backed up and restored just like any other kubernetes resource during the backup stage batch workflows these workflows are created to manage batch jobs, which are used by clients when uploading large data files to models ( inferences and / or ground truths ). these workflows are sometimes safe to recover therefore, these workflow", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "when uploading large data files to models ( inferences and / or ground truths ). these workflows are sometimes safe to recover therefore, these workflows are restored selectively based on what state they were in when the backup was taken workflows for which arthur received all the data from the client are resumed by manually re - submitting them ( this is done via an administrative http endpoint that needs to be invoked manually ) workflows for which arthur did not receive all the data from the client will need to be re - submitted. operators restoring the cluster will need to reach out to affected clients to communicate that their batch", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "receive all the data from the client will need to be re - submitted. operators restoring the cluster will need to reach out to affected clients to communicate that their batch workflows should be re - submitted. reference and cron workflows reference workflows are created for monitoring the upload of reference datasets to s3 reference datasets that were in - flight during a backup will need to be re - uploaded via the sdk. cron workflows are scheduled workflows which perform some regular processing ( eg : triggering alerts for non - batch inferences ) cron workflows are meant to", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are scheduled workflows which perform some regular processing ( eg : triggering alerts for non - batch inferences ) cron workflows are meant to be run on a regular schedule. it is safe to wait for the next workflow to be triggered, and therefore, these workflows are not backed up nor restored. overview - s3 the arthur platform uses aws s3 object storage for storing inference data, reference data, as well as data and trained models for the enrichments pipeline. arthur recommends enabling cross - region replication on the aws s3 buckets, so that objects are available in the rare event of", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "for the enrichments pipeline. arthur recommends enabling cross - region replication on the aws s3 buckets, so that objects are available in the rare event of an aws region outage. the arthur backup solution does not manage consistency with the s3 bucket and other backup data. the data in s3 is only used in conjunction with data that is stored in postgres ( eg : model definitions ), so it's ok if there's data in s3 that isn't represented in postgres. therefore, the s3 bucket for a cluster will always reflect the most up - to - date state,", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##3 that isn't represented in postgres. therefore, the s3 bucket for a cluster will always reflect the most up - to - date state, regardless of when a backup was taken. updated 3 months ago table of contents warnings overview overview - clickhouse - backup overview - velero overview - arthur ( argo ) workflows overview - s3 source : https : / / docs. arthur. ai / docs / backup - and - restore", "metadata": {"source": "https://docs.arthur.ai/docs/backup-and-restore", "row": 119, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 120 text : welcome to arthur scope! jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchdiscardsubmit suggested editswelcome to arthurwelcome to arthur scope", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchdiscardsubmit suggested editswelcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfair", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##iclass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsas", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##time series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functions", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##gap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##l directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by suggestas your team's data science operations center, arthur helps enterprise teams monitor, measure and optimize ai performance at scale. source : https : / / docs. arthur. ai / edit / getting - started", "metadata": {"source": "https://docs.arthur.ai/edit/getting-started", "row": 120, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 121 text : fairness metrics jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by fairness metricsmonitor and track fairness metrics in production to take action on underperformance in sensitive segments. suggest editsenabling fairness sections a dedicated fairness section within the ui enables teams to track fairness performance between groups easily. the ui does not infer these trackable groups and must be explicitly defined. for attributes to show up ( and be tracked ) within the fairness section of the dashboard, they must be enabled for bias monitoring. this", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "must be explicitly defined. for attributes to show up ( and be tracked ) within the fairness section of the dashboard, they must be enabled for bias monitoring. this can be done with the python sdk and is further explained in enabling enrichments under bias mitigation. tracking fairness in the ui marked sensitive attributes are tracked in the fairness section in the model's overview tab. this section operationalizes systematic comparisons for critical groups. metrics : easily compare different accuracy rates between groups by selecting from standard fairness metrics within our drop - down selection. these metrics include : metricdescriptionaccuracy ratethe proportion of correctly", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by selecting from standard fairness metrics within our drop - down selection. these metrics include : metricdescriptionaccuracy ratethe proportion of correctly classified instances out of the total number of instancestrue positive ratethe proportion of actual positive instances correctly identified by a machine learning model out of the total number of actual positive instancestrue negative ratethe proportion of actual negative instances correctly identified by a machine learning model out of the total number of actual negative instancesfalse positive ratethe proportion of actual negative instances that are incorrectly classified as positive by a machine learning model out of the total number of actual negative instancesfalse negative", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "positive ratethe proportion of actual negative instances that are incorrectly classified as positive by a machine learning model out of the total number of actual negative instancesfalse negative ratethe proportion of actual positive instances incorrectly classified as negative by a machine learning model out of the total number of positive instances baseline : within arthur, the fairness section allows comparisons within groups of different attributes of interest. the baseline group is the group that all other groups in that attribute will be compared against. this selection is made by clicking on the set baseline button, and the selected group can be seen in the fairness table under the baseline column for each comparison. fairness threshold toggle :", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "by clicking on the set baseline button, and the selected group can be seen in the fairness table under the baseline column for each comparison. fairness threshold toggle : a fairness threshold is the acceptable rate of disparate performance. an appropriate fairness threshold heavily depends on the team and use case, so arthur does not apply strict parameters. instead, teams can toggle the threshold to their model's acceptable disparate performance rate. fairness status : based on the threshold provided, a visual representation of whether a group's performance rate has passed the allowed threshold is provided. a green check represents that all groups'performance for the rate selected is", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "visual representation of whether a group's performance rate has passed the allowed threshold is provided. a green check represents that all groups'performance for the rate selected is within the threshold compared to the baseline group. on the other hand, red exclamation points mean that one or more comparison groups have points beyond the specified fairness threshold. tracking attribute disparity by default, attribute disparity rates are tracked over time. using snap shot mode while tracking performance over time is incredibly helpful for debugging or evaluating when disparate impact occurred, another popular way teams use the fairness section in reporting is with snapshot mode. snapshot", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "helpful for debugging or evaluating when disparate impact occurred, another popular way teams use the fairness section in reporting is with snapshot mode. snapshot mode is a toggle at the top of the ui that converts all charts from time series to average bar charts ( for the time range selected in the global filters ). this allows teams to easily create shareable charts for reports on the average impact rate between different groups. querying fairness in a notebook beyond the metrics enabled in the arthur ui, arthur can query additional fairness metrics in a notebook. using the bias metrics submodule, teams can call demographic _", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "enabled in the arthur ui, arthur can query additional fairness metrics in a notebook. using the bias metrics submodule, teams can call demographic _ parity, group _ confusion _ matrices, or group _ positivity _ rates to be calculated on a specified attribute. pythonarthur _ model. bias. metrics. demographic _ parity ('< attr _ name >') arthur _ model. bias. metrics. group _ confusion _ matrices ('< attr _ name >') a description of these bias metrics : metricdescriptiondemographic parityget group - conditional po", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "('< attr _ name >') a description of these bias metrics : metricdescriptiondemographic parityget group - conditional positivity rates for all inferences, with the option to filter for a batch _ id or a particular chunk of time. group confusion matricesget group - conditional confusion matrices for all inferences, with the option to filter for a batch _ id or a particular chunk of time. group positivity ratesget group - conditional positivity rates for all inferences, with the option to filter for a batch _ id or a particular chunk of time. available model types since fairness", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "conditional positivity rates for all inferences, with the option to filter for a batch _ id or a particular chunk of time. available model types since fairness metrics are calculated with accuracy rates, they are only available for classification models within arthur. additionally, since fairness metrics are a different visual way of tracking accuracy between sensitive groups, they require ground truth labels. updated 3 months ago table of contents enabling fairness sections tracking fairness in the ui tracking attribute disparity using snap shot mode querying fairness in a notebook available model types source : https : / / docs. arthur. ai / docs / fairness - metrics", "metadata": {"source": "https://docs.arthur.ai/docs/fairness-metrics", "row": 121, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 122 text : model metric definitions jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by model metric definitionsunderstanding model metrics monitored with arthursuggest editsperformance accuracy rate accuracy is the most common metric for classification tasks. accuracy is the measure of how many predictions were correct out of all the predictions made. accuracy rate = ( # correct predictions ) / ( total # predictions ) we can also think of accuracy in terms of common confusion matrix rates accuracy rate = ( tp + tn ) / ( tp + tn + f", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "predictions ) we can also think of accuracy in terms of common confusion matrix rates accuracy rate = ( tp + tn ) / ( tp + tn + fp + fn ) auc the area under the curve ( auc ) is a metric that measures the performance of a classification model by calculating the area under the receiver operating characteristic ( roc ) curve. it provides a single score that summarizes the trade - off between the true positive rate and false positive rate across different thresholds for defining positive cases. average prediction this is a metric used by regression models. it returns the average prediction your model has output over the average token", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "different thresholds for defining positive cases. average prediction this is a metric used by regression models. it returns the average prediction your model has output over the average token likelihood the token likelihood is a number between 0 and 1 that quantifies the model \u2019 s level of surprise that this token was the next predicted token of the sentence. in token sequence models, this metric quantifies the average in arthur, this metric is available for token sequence models. average sequence length this metric is available for token sequence models. this is the average count of tokens for each inference ingested by arthur. balanced accuracy rate the balanced accuracy rate is a metric", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "available for token sequence models. this is the average count of tokens for each inference ingested by arthur. balanced accuracy rate the balanced accuracy rate is a metric used for classification tasks. balanced accuracy rate = ( sensitivity + specificity ) / 2 confusion matrix rates a confusion matrix is a table that summarizes the performance of a classification model by comparing its predicted outputs to the true outputs across different classes. it contains information about the number of true positives, true negatives, false positives, and false negatives, which can be used to calculate various metrics for evaluating model performance. false negative rate the false negative rate is a", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "false positives, and false negatives, which can be used to calculate various metrics for evaluating model performance. false negative rate the false negative rate is a metric that measures the proportion of actual positive cases that are incorrectly predicted as negative by a classification model. a high false negative rate indicates that the model is missing a significant number of positive cases, which can have serious consequences in some applications. false positive rate the false positive rate is a metric that measures the proportion of negative cases that are incorrectly predicted as positive by a classification model. a high false positive rate indicates that the model is producing a large number of false alarms, which can be", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "that are incorrectly predicted as positive by a classification model. a high false positive rate indicates that the model is producing a large number of false alarms, which can be costly or lead to unnecessary actions in some applications. true negative rate the true negative rate, also known as specificity, is a metric that measures the proportion of actual negative cases that are correctly identified as negative by a classification model. a high true negative rate indicates that the model is able to effectively identify negative cases, which is important in applications where avoiding false positives is critical. true positive rate the true positive rate, also known as sensitivity or recall, is a metric that measures", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is important in applications where avoiding false positives is critical. true positive rate the true positive rate, also known as sensitivity or recall, is a metric that measures the proportion of actual positive cases that are correctly identified as positive by a classification model. a high true positive rate indicates that the model is able to effectively identify positive cases, which is especially important in applications where detecting all positive cases is critical. f1 the f1 score is a single metric that combines precision and recall, two performance metrics used in classification tasks. it ranges from 0 to 1, with higher values indicating better model performance in balancing precision and recall. f1 = ( precision *", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "metrics used in classification tasks. it ranges from 0 to 1, with higher values indicating better model performance in balancing precision and recall. f1 = ( precision * recall ) / ( precision + recall ) f1 = tp / ( tp + 0. 5 * ( fp + fn ) ) likelihood stability this metric is available for token sequence models. mean average error ( mae ) mean absolute error ( mae ) is a metric that measures the average magnitude of the differences between the predicted and actual values of a numerical variable in a regression model. it is calculated as the average absolute difference between the predicted and actual values and is one of", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the predicted and actual values of a numerical variable in a regression model. it is calculated as the average absolute difference between the predicted and actual values and is one of the most popular measures of the model's predictive accuracy. mean average precision in object detection, mean average precision ( map ) is a commonly used metric that measures the precision and recall of a model in detecting objects of different classes in an image. it takes into account the localization accuracy of the predicted bounding boxes and is often used to compare the performance of different object detection models. recall recall, also known as sensitivity or true positive rate, is a metric that measures the", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and is often used to compare the performance of different object detection models. recall recall, also known as sensitivity or true positive rate, is a metric that measures the proportion of true positive cases among all actual positive cases in a classification model. it provides information about the model's ability to detect positive cases and is especially important in applications where detecting all positive cases is critical. rsme the root mean squared error ( rmse ) is a metric that measures the average magnitude of the differences between the predicted and actual values of a numerical variable in a regression model. it is calculated as the square root of the average squared difference between the predicted and actual", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "between the predicted and actual values of a numerical variable in a regression model. it is calculated as the square root of the average squared difference between the predicted and actual values and is often used as a measure of the model's predictive accuracy. r squared r - squared, also known as the coefficient of determination, is a statistical metric that measures the proportion of variation in the dependent variable that is explained by the independent variable ( s ) in a regression model. it ranges from 0 to 1, with higher values indicating a better model fit to the data. inference count available for all model types. this is the number of predictions that have been", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to 1, with higher values indicating a better model fit to the data. inference count available for all model types. this is the number of predictions that have been sent to arthur. inference count = # of predictions inference count by class for classification models, this metric counts the number of predictions ingested by arthur per each class label. overall accuracy rate the overall accuracy rate is a metric that measures the proportion of correctly classified cases in a classification model, across all classes. it provides a general sense of how well the model is performing, but it may not capture the performance of individual classes or the cost of misclassification errors. precision precision is", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "general sense of how well the model is performing, but it may not capture the performance of individual classes or the cost of misclassification errors. precision precision is a metric that measures the proportion of true positive cases among all predicted positive cases in a classification model. it provides information about the model's ability to minimize false positives and is especially important in applications where avoiding false positives is critical. precision = tp / ( tp + fp ) data drift background p and q we establish some mathematical housekeeping for the below metrics. let p be the reference distribution and q be the target distribution. binning the underlying reference", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and q we establish some mathematical housekeeping for the below metrics. let p be the reference distribution and q be the target distribution. binning the underlying reference and target sets can approximate these probability distributions. generally, p is an older dataset, and q is a new dataset of interest. we'd like to quantify how far the distributions differ to see if the reference set has gone stale, and algorithms trained on it should not be used to perform inferences on the target dataset. entropy let h ( p ) be the entropy of distribution p. it is interpreted as the expected ( i. e., average )", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the target dataset. entropy let h ( p ) be the entropy of distribution p. it is interpreted as the expected ( i. e., average ) number of bits ( if log base 2 ) or nats ( if log base e ) required to encode information of a datapoint from the distribution p. arthur applications use log base e, so interpretation will be in nats. kl divergence let d ( pq ) be the kullback - leibler ( kl ) divergence from p to q. it is interpreted as the nats of information we expect to lose using q instead of p for modeling", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##bler ( kl ) divergence from p to q. it is interpreted as the nats of information we expect to lose using q instead of p for modeling data x, discretized over probability space k. kl divergence is not symmetrical, i. e., d ( pq ) does not equal d ( qp ) and should not be used as a distance metric. population stability index ( psi ) let psi ( p, q ) be the population stability index ( psi ) between p and q. it is interpreted as the roundtrip loss of na s of information we expect to lose from p to q and then", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "index ( psi ) between p and q. it is interpreted as the roundtrip loss of na s of information we expect to lose from p to q and then from q returning back to p, and vice versa. psi smooths out kl divergence since the return \" trip information loss is included, and this metric is popular in financial applications. js divergence let jsd ( p, q ) be the jensen - shannon ( js ) divergence between p and q. it smooths out kl divergence using a mixture of the base and target distributions and is interpreted as the entropy of the mixture m = ( p", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "q. it smooths out kl divergence using a mixture of the base and target distributions and is interpreted as the entropy of the mixture m = ( p + q ) _ / 2 minus the mixture of the entropies of the individual distributions. hellinger distance let he ( p, q ) be the hellinger distance between p and q. it is interpreted as the euclidean norm of the difference of the square root distributions of p and q. hypothesis test hypothesis testing uses different tests depending on whether a feature is categorical or continuous. for categorical features, let the equation below be the chi - squared test statistic for p", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "different tests depending on whether a feature is categorical or continuous. for categorical features, let the equation below be the chi - squared test statistic for p and q, with k being the number of categories of the feature, i. e., k - 1 are the degrees of freedom. let npk and nqk be the count of feature occurrences being k, with 1 < = k < = k, for p and q, respectively. the chi - squared test statistic summarizes the standardized differences of expected counts between p and q. for continuous features, let ks ( p, q ) be the kolm", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "test statistic summarizes the standardized differences of expected counts between p and q. for continuous features, let ks ( p, q ) be the kolmogorov - smirnov test statistic for p and q. let fp and fq be the empirical cumulative density for p and q, respectively. the kolmogorov - smirnov test is a nonparametric, i. e., distribution - free test that compares the empirical cumulative density functions of p and q. the returned test statistic is then compared to cutoffs for significance. a higher test statistic indicates more data drift. we've", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of p and q. the returned test statistic is then compared to cutoffs for significance. a higher test statistic indicates more data drift. we've abstracted the calculations away within our query endpoint. for hypothesistest, the returned value is transformed as - log _ 10 ( p _ value ) to maintain directional parity with the other data drift metrics. a lower p _ value is more significant and implies data drift, reflected in a higher - log _ 10 ( p _ value ). fairness demographic parity demographic parity is a fairness metric that measures whether the proportion of positive outcomes is the same across different demographic groups", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( p _ value ). fairness demographic parity demographic parity is a fairness metric that measures whether the proportion of positive outcomes is the same across different demographic groups in a classification model. it aims to ensure that the model is not systematically biased towards or against certain groups based on demographic characteristics like race or gender. equalized odds equalized odds is a fairness metric that measures the true positive and false positive rates for a given group, such as a protected group defined by a demographic characteristic like race or gender, in a classification model. it ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or une", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "like race or gender, in a classification model. it ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or unequal treatment issues in both true positive and false positive rates. equal opportunity equal opportunity is a fairness metric that measures the true positive rate for a given group, such as a protected group defined by a demographic characteristic like race or gender, in a classification model. it ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or unequal treatment issues. updated 3 months ago table of contents performance accuracy rate auc average prediction average token likelihood average sequence", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "can help identify and address potential discrimination or unequal treatment issues. updated 3 months ago table of contents performance accuracy rate auc average prediction average token likelihood average sequence length balanced accuracy rate confusion matrix rates f1 likelihood stability mean average error ( mae ) mean average precision recall rsme r squared inference count inference count by class overall accuracy rate precision data drift background kl divergence population stability index ( psi ) js divergence hellinger distance hypothesis test fairness demographic parity equalized odds equal opportunity source : https : / / docs. arthur. ai / docs / model - metric - definitions", "metadata": {"source": "https://docs.arthur.ai/docs/model-metric-definitions", "row": 122, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 123 text : on - prem deployment requirements jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by on - prem deployment requirementssuggest editsgeneral a dns hostname tls private key & certificate smtp server ( starttls supported ) the minimum compute resource requirements in this documentation is for running a few small models in a non - production environment. your production deployment will likely use more compute resources to achieve higher availability, performance and scalability. arthur \u2019 s horizontally elastic architecture allows high throughput processing in both streaming and batch", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "will likely use more compute resources to achieve higher availability, performance and scalability. arthur \u2019 s horizontally elastic architecture allows high throughput processing in both streaming and batch. the platform's auto - scaler mechanism self - manages resource utilization in optimized and cost - effective fashion. it automatically scales up and down based on compute resource requests by the platform activities as well as the lag observed in the data pipeline queue within the limits of the allocated hardware. this works best in a cloud infrastructure with a managed kubernetes service that enables arthur to also auto - scale the provisioned hardware ( e. g. aws eks", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a cloud infrastructure with a managed kubernetes service that enables arthur to also auto - scale the provisioned hardware ( e. g. aws eks, azure ask ). storage volumes used for arthur deployment should be encrypted with a data key using industry - standard data encryption ( e. g. aes - 256 ). this applies to the mounted disk volumes as well as the externalized storage, such as the s3 object storage and the relational database if any. kubernetes install kubectl - ing workstation : linux or macos kubernetes : 1. 25 to 1.", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "if any. kubernetes install kubectl - ing workstation : linux or macos kubernetes : 1. 25 to 1. 27 runtime : containerd or docker namespace storage class minimum node group resource 16 cpus 32 gb ram storage with at least 3000 iops ( > 100gb recommended ) permissions when arthur platform is installed, kubernetes rbac resources are created to allow the admin console to manage the application. the kubectl - ing user who installs arthur must have the wildcard privileges in the cluster. refer to this documentation for the clusterrole and", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the kubectl - ing user who installs arthur must have the wildcard privileges in the cluster. refer to this documentation for the clusterrole and clusterrolebinding that will be created for the admin console. components prometheus ingress controller ( nginx or ambassador ) kubernetes metrics server velero with restic ( optional for managed backup and restore feature ) for airgapped installation only : an existing private container registry existing private python registries ( pypi, anaconda ) - only required for the model explanation feature vm install minimum server resource 16 cpus 32 gb", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "private python registries ( pypi, anaconda ) - only required for the model explanation feature vm install minimum server resource 16 cpus 32 gb ram storage with at least 3000 iops ( > 100gb recommended ) supported operating systems the latest versions of the following linux operating systems are supported. ubuntu rhel please do the following before running the installer on your vm for a smoother deployment experience : if selinux is enabled, set it to the permissive mode make sure the vm doesn't have any container runtime pre - installed, such as docker or containerd firewall configurations", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to the permissive mode make sure the vm doesn't have any container runtime pre - installed, such as docker or containerd firewall configurations ingress the tcp port 443 is the only entry point that arthur exposes. egress the platform requires access to any integrations ( e. g. smtp, idp ) as well as the components you externalize ( e. g. postgres, s3 ). for airgap installation your private container and python registries must be accessible. ( requirements _ for _ online _ installation ) = for online installation access to container images and deployment", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##p installation your private container and python registries must be accessible. ( requirements _ for _ online _ installation ) = for online installation access to container images and deployment manifest files from the below public registries are required. hostexisting clusterembedded clusterdocker hubrequiredrequiredproxy. replicated. comrequiredrequiredreplicated. apprequiredrequiredk8s. kurl. shnot requiredrequiredamazonaws. comnot requiredrequiredupdated about 2 months ago table of contents general kubernetes install minimum node group", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##quiredamazonaws. comnot requiredrequiredupdated about 2 months ago table of contents general kubernetes install minimum node group resource permissions components vm install minimum server resource supported operating systems firewall configurations ingress egress source : https : / / docs. arthur. ai / docs / on - prem - deployment - requirements", "metadata": {"source": "https://docs.arthur.ai/docs/on-prem-deployment-requirements", "row": 123, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 124 text : assets required for explainability jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platform", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ct documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metric", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytro", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransform", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##overviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##lation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefa", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by assets required for explainabilitysuggest editsarthur can automatically calculate explanations ( feature importances ) for every prediction your model makes. to make this possible, we package up your model in a way that allows us to call it's predict function, which allows us to calculate explanations. we require a few things from your end : a python script that wraps your models predict function for image models, a second function, load _ image is", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "explanations. we require a few things from your end : a python script that wraps your models predict function for image models, a second function, load _ image is also required ( see cv explainability ). a directory containing the above file, along with any serialized model files, and other supporting code a requirements. txt with the dependencies to support the above this guide will set everything up and then use the sdk to enable explainability. setting up project directory project structure here is an example of what your project directory might look like. - - model _ folder / - - data / - - training _ data. csv - -", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "is an example of what your project directory might look like. - - model _ folder / - - data / - - training _ data. csv - - testing _ data. csv - - requirements. txt - - model _ entrypoint. py - - utils. py - - serialized _ model. pkl requirements file your project requirements and dependencies can be stored in any format you like, such as the typical requirements. txt file, or another form of dependency management. this should contain all packages your model and predict function need to run. [UNK] do not need to include the arthurai package in", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "or another form of dependency management. this should contain all packages your model and predict function need to run. [UNK] do not need to include the arthurai package in this requirements file. we supply that # example _ requirements. txt pandas = = 0. 24. 2 numpy = = 1. 16. 4 scikit - learn = = 0. 21. 3 torch = = 1. 3. 1 torchvision = = 0. 4. 2 it is advised to pin the specific versions your model requires. if no version is pinned, we will use the latest version. this can cause issues if the latest version is incompatible", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to pin the specific versions your model requires. if no version is pinned, we will use the latest version. this can cause issues if the latest version is incompatible with the version used to build your model. prediction function we need to be able to send new inferences to your model to get predictions and generate explanations. for us to have access to your model, you need to create an entrypoint file that defines a predict ( ) method. the exact name of the file isn't strict, so long as you specify the correct name when you enable explainability ( see below ). the only thing that does matter is that this file implements", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "t strict, so long as you specify the correct name when you enable explainability ( see below ). the only thing that does matter is that this file implements a predict ( ) method. in most cases, if you have a previously trained model, this predict ( ) method will likely just invoke the prediction from your trained model. python # example _ entrypoint. py sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( x ) this predict method can be as simple or complicated as you need", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "\" ) def predict ( x ) : return sk _ model. predict _ proba ( x ) this predict method can be as simple or complicated as you need, so long as you can go from raw input data to a model output prediction. specifically, in the case of a binary classifier, we expect a 2 - d array where the first column indicates probability _ 0 for each input, and the second column indicates probability _ 1 for each input. in the case of a multiclass classifier with n possible labels, we expect an n - d array where column i corresponds to the predicted probability that each input belongs to class i.", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "multiclass classifier with n possible labels, we expect an n - d array where column i corresponds to the predicted probability that each input belongs to class i. preprocessing for prediction commonly, a fair amount of feature processing and transformation will need to happen before invoking your actual model. predict ( ). this might include normalizations, rescaling, one - hot encoding, embedding, etc. whatever those transformations are, you can make them a part of this predict ( ) method. alternatively, you can wrap all those transformations into a helper function. python # example _ entrypoint. py from utils", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "this predict ( ) method. alternatively, you can wrap all those transformations into a helper function. python # example _ entrypoint. py from utils import pipeline _ transformations sk _ model = joblib. load ( \". / serialized _ model. pkl \" ) def predict ( x ) : return sk _ model. predict _ proba ( pipeline _ transformations ( x ) ) enabling explainability enabling explainability can be done using the sdk function arthur _ model. enable _ explainability, which takes as input a sample of your model's data ( to train the explainer ), and which takes as input", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model. enable _ explainability, which takes as input a sample of your model's data ( to train the explainer ), and which takes as input the files that contain your model's predict function and necessary environment. pythonarthur _ model. enable _ explainability ( df = x _ train. head ( 50 ), project _ directory = \" / path / to / model _ folder / \", requirements _ file = \" requirements. txt \", user _ predict _ function _ import _ path = \" model _ entrypoint \", ignore _ dirs = [ \" folder _ to _ ignore \" ] #", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "user _ predict _ function _ import _ path = \" model _ entrypoint \", ignore _ dirs = [ \" folder _ to _ ignore \" ] # optionally exclude directories within the project folder from being bundled with predict function ) the above provides a simple example. for a list of all configuration options and details around them, see the explainability section in enabling enrichments. notes about the above example : joblib is a python library allowing you to reconstruct your model from a serialized pickle file. x _ train is your trained model data frame. user _ predict _ function _ import _ path is the python path to", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "model from a serialized pickle file. x _ train is your trained model data frame. user _ predict _ function _ import _ path is the python path to import the entry point file as if you imported it into the python program running enable _ explainability. configuration requirements when going from disabled to enabled, you will need to include the required configuration settings. once the explainability enrichment has been enabled, you can update the non - required configuration settings without re - supplying the required fields. you must not pass in any config settings when disabling the explainability enrichment. configuration settingrequireddescriptiondfxthe data", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". you must not pass in any config settings when disabling the explainability enrichment. configuration settingrequireddescriptiondfxthe dataframe passed to the explainer. it should be similar to, or a subset of, the training data. typically small, ~ 50 - 100 rows. project _ directoryxthe path to the directory containing your predict function, requirements file, model file, and any other resources needed to support the predict function. user _ predict _ function _ import _ pathxthe name of the file containing the predict function. do not include. py extension. used to import the predict function", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "predict _ function _ import _ pathxthe name of the file containing the predict function. do not include. py extension. used to import the predict function. requirements _ filexthe name of the file containing pip requirements for the predict function. python _ versionxthe python version to use when executing the predict function. this is automatically set to the current python version when usingmodel. enable _ explainability ( ). sdk _ versionxthe arthurai version used to make the enable request. this is automatically set to the currently installed sdk version when using themodel. enable _ explainability ( ). explanation _", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "to make the enable request. this is automatically set to the currently installed sdk version when using themodel. enable _ explainability ( ). explanation _ algothe explanation algorithm to use. valid options are'lime'or'shap '. the default value of'lime '. explanation _ nsamplesthe number of perturbed samples used to generate the explanation. the result will be calculated more quickly for a smaller number of samples but may be less robust. it is recommended to use at least 100 samples. the default value of 2000. inference _ consumer _ score _ percentthe number between 0. 0 and 1", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". it is recommended to use at least 100 samples. the default value of 2000. inference _ consumer _ score _ percentthe number between 0. 0 and 1. 0 sets the percent of inferences for which to compute an explanation score. only applicable when streaming _ explainability _ enabled is set to true. the default value of 1. 0 ( all inferences explained ). streaming _ explainability _ enabledif true, every inference will have an explanation generated for it. if false, explanations are available on - demand only. ignore _ dirslist of paths to directories within project _ directory that will not be bundled and included with", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "false, explanations are available on - demand only. ignore _ dirslist of paths to directories within project _ directory that will not be bundled and included with the predict function. use to prevent including irrelevant code or files in larger directories. cv explainability [UNK] is currently available as an enrichment for classification, multi - labeling, and regression cv models, but not object detection cv models. in your model _ entrypoint. py for multiclass image models, in addition to the predict ( ) function, there is a second function which is required : load _ image ( ). this function should take in a string, which is a", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the predict ( ) function, there is a second function which is required : load _ image ( ). this function should take in a string, which is a path to an image file. the function should return the image in a numpy array. any image processing, such as converting to greyscale, should also happen in this function. this is because lime ( the explanation algorithm used behind the scenes ) will create variations of this array to generate explanations. however, any transformation resulting in a non - numpy array should happen in the predict function, such as converting to a tensor. no image resizing is required. as", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "transformation resulting in a non - numpy array should happen in the predict function, such as converting to a tensor. no image resizing is required. as part of onboarding an image model, pixel _ height and pixel _ width are set as metadata on the model. when ingesting, arthur will automatically resize the image to the configured size and pass this resized image path to the load _ image function. below is a full example file for an image model, with both load _ image and predict defined. imports and class definitions are omitted for brevity. python # example _ entrypoint. py import..", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with both load _ image and predict defined. imports and class definitions are omitted for brevity. python # example _ entrypoint. py import... class mednet ( nn. module ) :... # load model using custom user defined class net = mednet ( ) path = pathlib. path ( _ _ file _ _ ). parent. absolute ( ) net. load _ state _ dict ( torch. load ( f'{ path } / pretrained _ model') ) # helper function for transforming image def quantize ( np _ array ) : return np _ array +", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "} / pretrained _ model') ) # helper function for transforming image def quantize ( np _ array ) : return np _ array + ( np. random. random ( np _ array. shape ) / 256 ) def load _ image ( image _ path ) : \" \" \" takes in single image path, and returns single image in format predict expects \" \" \" return quantize ( np. array ( image. open ( image _ path ). convert ('rgb') ) / 256 ) def predict ( images _ in ) : \" \" \" takes in numpy array of images, and", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "convert ('rgb') ) / 256 ) def predict ( images _ in ) : \" \" \" takes in numpy array of images, and returns predictions in numpy array. can handle both single image in ` numpy ` array, or multiple images. \" \" \" batch _ size, pixdim1, pixdim2, channels = images _ in. shape raw _ tensor = torch. from _ numpy ( images _ in ) processed _ images = torch. reshape ( raw _ tensor, ( batch _ size, channels, pixdim1, pixdim2 )", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") processed _ images = torch. reshape ( raw _ tensor, ( batch _ size, channels, pixdim1, pixdim2 ) ). float ( ) net. eval ( ) with torch. no _ grad ( ) : return net ( processed _ images ). numpy ( ) [UNK] on enabling explainability for cv modelsexplainability for cv, at least for cv models, should be configured with 4 cpus and 4 gb ram ( default 1 ) to avoid long explanation times ( which could break the ui ). it \u2019 s done per model when enabling explainability in the notebook", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "gb ram ( default 1 ) to avoid long explanation times ( which could break the ui ). it \u2019 s done per model when enabling explainability in the notebook. this enabling explainability configuration can be seen here : pythonarthur _ model. enable _ explainability ( project _ directory = project _ dir, user _ predict _ function _ import _ path ='entrypoint ', streaming _ explainability _ enabled = false, requirements _ file = \" requirements. txt \", explanation _ algo ='lime ', explanation _ nsamples = 2000, model _ server _ num _ cpu = \" 4 \", model", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ", explanation _ algo ='lime ', explanation _ nsamples = 2000, model _ server _ num _ cpu = \" 4 \", model _ server _ memory = \" 4gi \" ) nlp explainability enabling explainability for nlp models follows the same process for tabular models [UNK] important choice for nlp explainability is the text _ demiliter parameter, since this delimiter determines how tokens will be perturbed when generating explanations. here is an example entrypoint. py file which loads our nlp model and defines a predict function that the explainer will use : pythonmodel _", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "here is an example entrypoint. py file which loads our nlp model and defines a predict function that the explainer will use : pythonmodel _ path = os. path. join ( os. path. dirname ( _ _ file _ _ ), \" model. pkl \" ) model = joblib. load ( model _ path ) def predict ( fvs ) : # our model expects a list of strings, no nesting # if we receive nested lists, unnest them if not isinstance ( fvs [ 0 ], str ) : fvs = [ fv [ 0 ]", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ed lists, unnest them if not isinstance ( fvs [ 0 ], str ) : fvs = [ fv [ 0 ] for fv in fvs ] return model. predict _ proba ( fvs ) updated 3 months ago table of contents setting up project directory project structure requirements file prediction function preprocessing for prediction enabling explainability configuration requirements cv explainability nlp explainability source : https : / / docs. arthur. ai / docs / assets - required - for - explainability", "metadata": {"source": "https://docs.arthur.ai/docs/assets-required-for-explainability", "row": 124, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 125 text : monitoring best practices jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamp", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##api and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalert", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##ranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubles", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functions", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##creating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##on - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platform", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "##s by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by monitoring best practicessuggest editsthe arthur scope product is used to monitor machine learning models. it runs on kubernetes and is able to scale on - demand. there are several components that should be monitored so the platform stays healthy. some recommended best practices for monitoring the various scope components are as follows : kubernetes pods cpu / memory utilization pods are the smallest building blocks in kubernetes. it's always advised to", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "components are as follows : kubernetes pods cpu / memory utilization pods are the smallest building blocks in kubernetes. it's always advised to ensure pods have sufficient resources ( cpu and memory ) available for them to run. number of restarts pods getting restarted frequently is a sign of an buggy code or bad configuration. pods in pending / unknown / unavailable / clbo state pods not in a ready state is a sign of hardware degradation or connectivity failures to external systems. persistent volumes iops ensure the storage backing persistent volumes have enough throughput provisioned and there is no throttling being experienced. available disk space ensure", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "systems. persistent volumes iops ensure the storage backing persistent volumes have enough throughput provisioned and there is no throttling being experienced. available disk space ensure attached persistent volumes have enough disk space. volumeattachment errors ensure there are no volumeattachment errors observed in persistent volumes. this is particularly critical in multi - az deployments. nodes sufficient nodes in each az ensure there are required number of * * nodes per az for each deployment. max nodes per cluster monitoring the total number of nodes a cluster is scaled to ensures performance and costs are in optimal. datastores meta database ( external ) disk space ensure there is enough", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the total number of nodes a cluster is scaled to ensures performance and costs are in optimal. datastores meta database ( external ) disk space ensure there is enough disk space for the database. iops monitor for any throttling of performance for the database disk and adjust iops accordingly. cpu monitor for any throttling of performance for the database cpu and adjust it accordingly. olap database replication lag the olap database is usually deployed in a 3 node setup, which are synced via replication. a lag happens when data is not consistent across all nodes. delayed / rejected inserts this usually happens when a large number of", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "are synced via replication. a lag happens when data is not consistent across all nodes. delayed / rejected inserts this usually happens when a large number of inserts are sent too quickly. this can lead to data loss or corruption. zookeeper exceptions these should generally not happen and is sometimes an indication of bad hardware. messaging middleware kafka consumer lag producers write data and consumers read data from the messaging middleware. if consumers are not able to keep up with the producers, it will lead to a lag which can mean poor performance for the platform. under replicated partitions follower replicas get data from leader replica", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the producers, it will lead to a lag which can mean poor performance for the platform. under replicated partitions follower replicas get data from leader replicas using replication. due to resource exhaustion or leader failure, it is possible the follower replicas don \u2019 t keep up with the leader replicas. kafka connect connector failures these failures mean data is not being written to data stores, which can lead to data loss. task failures these failures mean data is not matching the configurations, which can lead to data loss / corruption. zookeeper outstanding requests this is the number of requests waiting to be processed by zookeeper. workflow", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "the configurations, which can lead to data loss / corruption. zookeeper outstanding requests this is the number of requests waiting to be processed by zookeeper. workflow scheduler failed steps this usually implies a bad configuration or being unable to communicate with external systems. failed workflows failed steps or bad configurations could lead to failed workflows. queued workflows workflows being queued could mean there is a lack of resources on the cluster. microservices rate of 4xx / 5xx http response status bad http status codes could happen due to various reasons ( bugs, pod restarts, invalid creds, access", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "of 4xx / 5xx http response status bad http status codes could happen due to various reasons ( bugs, pod restarts, invalid creds, access etc. ). response times elevated response times can happen due to various reasons ( bugs, pod restarts etc. ). updated about 2 months ago table of contents kubernetes datastores messaging middleware workflow scheduler microservices source : https : / / docs. arthur. ai / docs / monitoring - best - practices", "metadata": {"source": "https://docs.arthur.ai/docs/monitoring-best-practices", "row": 125, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 126 text : overview jump to contentproduct documentationapi and python sdk referencerelease notesv3. 6. 0v3. 7. 0v3. 8. 0v3. 9. 0v3. 10. 0v3. 11. 0v3. 12. 0schedule a demoschedule a demomoon ( dark mode ) sun ( light mode ) v3. 12. 0product documentationapi and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarth", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "and python sdk referencerelease notessearchloading \u2026 welcome to arthurwelcome to arthur scope! pages in the arthur scope platformexamplesarthur sdkarthur apimodel typesmodel input / output typestabularbinary classificationmulticlass classificationregressiontextbinary classificationmulticlass classificationregressiontoken sequence ( llm ) imagebinary classificationmulticlass classificationregressionobject detectionranked list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmana", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "list ( recommender systems ) time seriescore conceptsmetricsperformance metricsdata drift metricsfairness metricsuser - defined metricsalertingmanaging alertsalert summary reportsenrichmentsanomaly detectionbias mitigationexplainabilityhot spotstoken likelihoodversioningmodel onboardingquickstartcv onboardingnlp onboardinggenerative textranked list outputs onboardingtime series onboardingregistering a model with the apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "apidata preparation for arthurcreating arthur model objectregistering model attributes manuallyenabling enrichmentsassets required for explainabilitytroubleshooting explainabilitysending inferencessending historical datasending ground truthintegrations and examplesalerting servicesemailpagerdutyservicenowdata pipelinessagemakerml platformslangchainsingle sign on ( sso ) oidcsamlspark mlarthur query guideoverviewcreating queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscompo", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "queriesfundamentalscommon queries quickstartquerying functionsdefault evaluation functionsaggregation functionstransformation functionscomposing advanced functionsenrichments + data driftquerying data driftquerying explainabilityadvanced walk throughsgrouped inference queriesresourcesarthur scope faqglossarymodel metric definitionsarthur algorithmsplatform administrationwelcome to platform administrationinstallation overviewon - prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseins", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "prem deployment requirementsplatform readiness for existing cluster installsvirtual machine installationconfiguring for high availabilityexternalizing the relational databaseinstalling kubernetesinstalling arthur pre - requisitesdeploying on amazon aws ekskubernetes cluster ( k8s ) install with namespace scope privilegesonline kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcus", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ") installairgap kubernetes cluster ( k8s ) install with cliplatform access controlaccess controldefault access controlcustom rbacintegrationsongoing platform maintenancewhat does ongoing maintenance look like? audit logadministrationorganizations and usersupgradingmonitoring best practicesplatform resourcesconfig templateexporting platform configurationsfull directory of arthur permissionsarthur permissions by standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappend", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "standard rolesarthur permissions by endpointbackup and restorepre - requisitesbacking up the arthur platformrestoring the arthur platformappendixpowered by overviewsuggest editsthe arthur query service is a feature within arthur that enables teams to interact with and analyze their data within arthur scope. built with a sql - like wrapper, the functionality can be used to create metric functions, pull data, create graphs for custom reports, and much more. interacting with arthur query service python sdk one of the most common ways teams can query is through the python sdk. building a queryfrom arthurai import", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "interacting with arthur query service python sdk one of the most common ways teams can query is through the python sdk. building a queryfrom arthurai import arthurai # # create connection to model of interest url = # # arthur url access _ key = # # arthur access key connection = arthurai ( url = url, access _ key = access _ key, verify _ ssl = false ) model _ id = # # model id arthur _ model. connection. get _ model ( model _ id ) # # build query function query _ function = { \" select \" : [ # #....", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ". connection. get _ model ( model _ id ) # # build query function query _ function = { \" select \" : [ # #..... ] } arthur _ model. query ( query _ function ) api api calls that take in a query expect a json - formatted query. json { \" select \" : [ { \" property \" : \" * \" } ], \" from \" : \" inference \" } quick common examples some of the most common use cases for querying include : pulling custom data : teams often use the query service to pull data that they are interested in experimenting with / reporting on further", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "common use cases for querying include : pulling custom data : teams often use the query service to pull data that they are interested in experimenting with / reporting on further with custom graphs / reports. creating metrics to track and alert within arthur : user - defined metrics are created based on the arthur query language. teams can also set alerts based on these metrics in the ui, python sdk, or api after creating a metric. comparing cohorts : finally, another common workflow of the query service is to pull cohorts of data to compare in a notebook. this can be done for any function, but some of", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": "common workflow of the query service is to pull cohorts of data to compare in a notebook. this can be done for any function, but some of the most common are performance, drift, or regional explainability. updated 3 months ago table of contents interacting with arthur query service python sdk api quick common examples source : https : / / docs. arthur. ai / docs / overview", "metadata": {"source": "https://docs.arthur.ai/docs/overview", "row": 126, "content_type": "arthur_scope_docs"}, "type": "Document"}
{"page_content": ": 127 text : add scorer configurations - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar add scorer configurations # many scorers included in the bench package have optional parameters that provide flexibility for users to match scorers with their use case. please visit our sdk documentation to view the optional configurations avaiable for each scorer. as an example, in the quickstart, we showed how to use the exact _ match scorer. by default, the exact _ match scorer is case sensitive. this means, the scorer returns", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##start, we showed how to use the exact _ match scorer. by default, the exact _ match scorer is case sensitive. this means, the scorer returns the value of 1. 0 only when the candidate output matches the content and the capitalization of the reference output. if we want to ignore capitalization differences, we can add a configuration to the exact _ match scorer. creating the test suite # instantiate a test suite with a name, scorer, input text, and reference outputs. for our use case, instead of invoking the scorer using the string representation ( which corresponds to the default config ), we will explicitly", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "reference outputs. for our use case, instead of invoking the scorer using the string representation ( which corresponds to the default config ), we will explicitly call the scorer and add optional configurations. from arthur _ bench. run. testsuite import testsuite from arthur _ bench. scoring import exactmatch suite = testsuite ( name ='bench _ quickstart ', scoring _ method = exactmatch ( case _ sensitive = false ), input _ text _ list = [ \" what year was fdr elected? \", \" what is the opposite of down? \" ], reference _ output _ list", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ text _ list = [ \" what year was fdr elected? \", \" what is the opposite of down? \" ], reference _ output _ list = [ \" 1932 \", \" up \" ] ) running the test # to create a test run, we need to specify the candidate responses. run = suite. run ('quickstart _ run ', candidate _ output _ list = [ \" 1932 \", \" up \" ] ) print ( run. test _ cases ) > > > [ testcaseoutput ( output ='1932 ', score = 1. 0 ), testcaseoutput ( output =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ cases ) > > > [ testcaseoutput ( output ='1932 ', score = 1. 0 ), testcaseoutput ( output ='up ', score = 1. 0 ) ] we have now logged the results for both test cases as 1. 0 even though the capitalization doesn \u2019 t match the reference. this is non - default behavior for which we needed to configure the scorer while creating the test suite. additional resources # we also support creating custom scorers that provide even more flexibility. please view the guide here to learn how custom scorers can be created. next custom scoring previous compare", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "also support creating custom scorers that provide even more flexibility. please view the guide here to learn how custom scorers can be created. next custom scoring previous compare generation settings copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page add scorer configurations creating the test suite running the test additional resources source : https : / / bench. readthedocs. io / en / latest / add _ scorer _ config. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/add_scorer_config.html", "row": 127, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 128 text : code evaluation - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark /", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar code evaluation # basic usage # code evaluation refers to the process of checking whether llm - written code passes unit tests to use a code evaluation scoring method, instantiate the scorer with the unit tests you want to attach to the suite, and proceed with test suite creation / test case running as usual. here we show the basic usage for the pythonunittesting code evaluation scorer. see the data requirements and example walkthrough", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "test case running as usual. here we show the basic usage for the pythonunittesting code evaluation scorer. see the data requirements and example walkthrough sections below for more details on preparing unit tests and candidate solutions. from arthur _ bench. run. testsuite import testsuite from arthur _ bench. scoring import pythonunittesting # create scorer from unit _ tests : list [ str ] python _ scorer = pythonunittesting ( unit _ tests = unit _ tests ) # create test suite # we explain how to prepare the data for python _ unit _ test _ df below python _ suite = testsuit", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "unit _ tests ) # create test suite # we explain how to prepare the data for python _ unit _ test _ df below python _ suite = testsuite ( \" python _ testsuite \", python _ scorer, reference _ data = python _ unit _ test _ df ) data requirements # unit tests # unit tests must be compatible with the code _ eval evaluator metric from huggingface, which is what the pythonunittesting scorer uses under the hood. format each unit test is expected to invoke the candidate function by name and assert its output the general format of the unit test expected by bench is", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "hood. format each unit test is expected to invoke the candidate function by name and assert its output the general format of the unit test expected by bench is as follows ( the name check is not required ) def check ( candidate ) : assert candidate ( test _ input _ 0 ) = test _ output _ 0 assert candidate ( test _ input _ 1 ) = test _ output _ 1 assert candidate ( test _ input _ 2 ) = test _ output _ 2 #... check ( candidate _ function _ name ) for example, here is the unit test for the greatest _ common _ divisor task from the humaneval data", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". check ( candidate _ function _ name ) for example, here is the unit test for the greatest _ common _ divisor task from the humaneval dataset : def check ( candidate ) : assert candidate ( 3, 7 ) = = 1 assert candidate ( 10, 15 ) = = 5 assert candidate ( 49, 14 ) = = 7 assert candidate ( 144, 60 ) = = 12 check ( greatest _ common _ divisor ) provide unit tests as strings unit tests can be passed to the pythonunittesting scorer as a list of strings, which is likely the simpler option if you are loading tests from a benchmark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "tests can be passed to the pythonunittesting scorer as a list of strings, which is likely the simpler option if you are loading tests from a benchmark dataset ( e. g. humaneval as we do in the example below ) : # create scorer from unit _ test : list [ str ] python _ scorer = pythonunittesting ( unit _ tests = unit _ tests ) provide unit tests as directory alternatively you can load unit tests from a directory to evaluate your candidate solutions. given a directory of unit test scripts : unit _ test _ dir _ name : - unit _ test _ 0. py - unit _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "your candidate solutions. given a directory of unit test scripts : unit _ test _ dir _ name : - unit _ test _ 0. py - unit _ test _ 1. py... the pythonunittesting scorer can be created just from that directory name : python _ scorer = pythonunittesting ( unit _ test _ dir = unit _ test _ dir _ name ) solutions # candidate solutions will only be evaluated to be correct if they contain : a function to call ( in the humaneval dataset, this is called the entry _ point ) any necessary imports this is correct : import math def greatest _ common _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "to call ( in the humaneval dataset, this is called the entry _ point ) any necessary imports this is correct : import math def greatest _ common _ divisor ( a : int, b : int ) - > int : return math. gcd ( a, b ) this will be scored as incorrect due to missing the math import def greatest _ common _ divisor ( a : int, b : int ) - > int : return math. gcd ( a, b ) this will be scored as incorrect due to missing a function entrypoint for the unit test to invoke : import math return math.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( a, b ) this will be scored as incorrect due to missing a function entrypoint for the unit test to invoke : import math return math. gcd ( a, b ) input prompts & reference outputs # input prompts and reference outputs ( aka canonical / golden solutions ) have no requirements in bench. these components are only for your own analysis, and are not used by the scorers under the hood in code evaluation. example walkthrough # here is some example code that you can use to generate and compare python coding solutions using openai \u2019 s gpt - 3. 5 and anthropic \u2019", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "is some example code that you can use to generate and compare python coding solutions using openai \u2019 s gpt - 3. 5 and anthropic \u2019 s claude - 2 on the humaneval dataset from huggingface environment setup # first we set environment variables for openai _ api _ key and anthropic _ api _ key before running our generation code pip install openai anthropic export \" openai _ api _ key \" = \" sk -... \" export \" anthropic _ api _ key \" = \" sk - ant -... \" data preparation # our dataset is the", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ".. \" export \" anthropic _ api _ key \" = \" sk - ant -... \" data preparation # our dataset is the humaneval dataset from huggingface loaded into a pandas dataframe from datasets import load _ dataset import pandas as pd humaneval _ code _ dataset = load _ dataset ( \" openai _ humaneval \" ) humaneval _ df = pd. dataframe ( humaneval _ code _ dataset [ \" test \" ] ) humaneval _ df _ sample = humaneval _ df. sample ( 20, random _ state = 278", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ dataset [ \" test \" ] ) humaneval _ df _ sample = humaneval _ df. sample ( 20, random _ state = 278487 ) prepare unit tests # we prepare the unit tests to invoke each candidate function using the test and entry _ point fields of the humaneval dataset : unit _ tests = [ f'\\ n { humaneval _ df _ sample. test. values [ i ] } \\ ncheck ( { humaneval _ df _ sample. entry _ point. values [ i ] } )'for i in range ( len ( humaneval _ df", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "{ humaneval _ df _ sample. entry _ point. values [ i ] } )'for i in range ( len ( humaneval _ df _ sample ) ) ] generate solutions # from langchain. chat _ models import chatopenai, chatanthropic gpt35 = chatopenai ( ) claude = chatanthropic ( ) prompt _ template = \" \" \" you are a bot that gives answers to coding tasks only. if the task is a coding task, give an expert python solution. if the task is unrelated, give the response \" i don't know. \" always mark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "the task is a coding task, give an expert python solution. if the task is unrelated, give the response \" i don't know. \" always mark the beginning and end of your solution with ` ` ` python markdown markers. without these markers, the code cannot be extracted. therefore the markers are required. = = = < text > = = = solution : \" \" \" # used to extract the portion of an llm response which is python code extract _ python = lambda x : x. replace ('python \\ n ','' ). replace ('` ` ` ','' ). replace (", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "= lambda x : x. replace ('python \\ n ','' ). replace ('` ` ` ','' ). replace ('def ','def') def get _ solutions ( model ) : filled _ prompt _ templates = [ prompt _ template. replace ( \" < text > \", humaneval _ df _ sample. prompt. values [ i ] ) for i in range ( len ( humaneval _ df _ sample ) ) ] return [ extract _ python ( model. predict ( x ) ) for x in filled _ prompt _ templates ] gpt35 _ solutions", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "sample ) ) ] return [ extract _ python ( model. predict ( x ) ) for x in filled _ prompt _ templates ] gpt35 _ solutions = get _ solutions ( gpt35 ) claude _ solutions = get _ solutions ( claude ) create and run test suite now that you have generated solutions for each model, we can create a test suite and a run for each llm from arthur _ bench. run. testsuite import testsuite from arthur _ bench. scoring import pythonunittesting python _ scorer = pythonunittesting ( unit _ tests = unit _ tests ) python _ suite = testsuite", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. scoring import pythonunittesting python _ scorer = pythonunittesting ( unit _ tests = unit _ tests ) python _ suite = testsuite ( \" humaneval _ testsuite \", python _ scorer, input _ text _ list = list ( humaneval _ df _ sample. prompt. values ), reference _ output _ list = list ( humaneval _ df _ sample. canonical _ solution. values ), ) python _ suite. run ( \" gpt - 3. 5 - turbo \", candidate _ output _ list = gpt35 _ solutions ) python _ suite. run ( \" claude", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( \" gpt - 3. 5 - turbo \", candidate _ output _ list = gpt35 _ solutions ) python _ suite. run ( \" claude - 2 \", candidate _ output _ list = claude _ solutions ) best practices # prompt templating for code extraction # evaluation becomes more straightforward if you can easily extract the part of an llm response which is its actual code solution. the simplest way to do that seems to be including an instruction in your prompt or system message that specifies to place code in between \u201c ` ` ` python markers \u201d in markdown, as we did in the example above. function signature # performance", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "system message that specifies to place code in between \u201c ` ` ` python markers \u201d in markdown, as we did in the example above. function signature # performance tends to improve on coding when your task description contains an explicit function signature that you want the solution to adhere to, as well as including example input / output behavior in its docstring. as an example, here is the input prompt for the greatest _ common _ divisor coding task from humaneval : def greatest _ common _ divisor ( a : int, b : int ) - > int : \" \" \" return a greatest common divisor of two integers", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ common _ divisor ( a : int, b : int ) - > int : \" \" \" return a greatest common divisor of two integers a and b > > > greatest _ common _ divisor ( 3, 5 ) 1 > > > greatest _ common _ divisor ( 25, 15 ) 5 \" \" \" note that the humaneval dataset prompts all contain docstrings like this one next python api reference previous custom scoring copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page code evaluation basic usage data requirements unit tests solutions input prompt", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page code evaluation basic usage data requirements unit tests solutions input prompts & reference outputs example walkthrough environment setup data preparation prepare unit tests generate solutions best practices prompt templating for code extraction function signature source : https : / / bench. readthedocs. io / en / latest / code _ evaluation. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/code_evaluation.html", "row": 128, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 129 text : compare generation settings - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar compare generation settings # in this guide we compare llm - generated answers to questions using different temperature settings. higher temperature improves creativity and diversity of answers, but increases the likelihood that responses veer into nonsense. we use a custom scorer that compares each llm temperature setting based on how many typos each response contains environment setup # in this guide, we use the openai api and use the pyspellchecker", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "based on how many typos each response contains environment setup # in this guide, we use the openai api and use the pyspellchecker package for a custom scorer pip install openai pyspellchecker export openai _ api _ key = \" sk -... \" data preparation # we write out some basic questions which we will use to test how much temperature impacts the responses inputs = [ \" what planet are we on? \", \" what time is it? \", \" what day is it? \", \" what is love? \" ] llm response generation # we use different temperature settings", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "what time is it? \", \" what day is it? \", \" what is love? \" ] llm response generation # we use different temperature settings to generate three different lists of responses : from langchain. chat _ models import chatopenai chatgpt _ zero _ temp = chatopenai ( temperature = 0. 0, max _ tokens = 100 ) chatgpt _ low _ temp = chatopenai ( temperature = 0. 5, max _ tokens = 100 ) chatgpt _ med _ temp = chatopenai ( temperature = 1. 2, max _ tokens =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", max _ tokens = 100 ) chatgpt _ med _ temp = chatopenai ( temperature = 1. 2, max _ tokens = 100 ) chatgpt _ high _ temp = chatopenai ( temperature = 1. 9, max _ tokens = 100 ) baseline _ responses = [ chatgpt _ zero _ temp. predict ( x ) for x in inputs ] low _ temp _ responses = [ chatgpt _ low _ temp. predict ( x ) for x in inputs ] med _ temp _ responses = [ chatgpt _ med _ temp. predict (", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ temp. predict ( x ) for x in inputs ] med _ temp _ responses = [ chatgpt _ med _ temp. predict ( x ) for x in inputs ] high _ temp _ responses = [ chatgpt _ high _ temp. predict ( x ) for x in inputs ] create test suite # for this test suite, we want to measure how corrupted the responses get as we increase the generation temperature. let \u2019 s define a quick custom scorer that uses the pyspellchecker package to scan for typos in the response, and we will then see how much the typo", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "custom scorer that uses the pyspellchecker package to scan for typos in the response, and we will then see how much the typo score changes between the low, medium, and high temperature model generations. from arthur _ bench. run. testsuite import testsuite from arthur _ bench. scoring import scorer from spellchecker import spellchecker import string from typing import list, optional class customspellingscore ( scorer ) : \" \" \" custom scoring which scores each llm response with the formula 1 / ( 2 ^ number of typos ) this gives a typo - free response a score", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "custom scoring which scores each llm response with the formula 1 / ( 2 ^ number of typos ) this gives a typo - free response a score of 1, and each additional typo further decreases the score \" \" \" def _ _ init _ _ ( self ) : self. spell _ checker = spellchecker ( ) @ staticmethod def name ( ) - > str : return \" spell _ checker \" @ staticmethod def requires _ reference ( ) - > bool : return false def run _ batch ( self, candidate _ batch : list [ str ], reference _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "def requires _ reference ( ) - > bool : return false def run _ batch ( self, candidate _ batch : list [ str ], reference _ batch : optional [ list [ str ] ] = none, input _ text _ batch : optional [ list [ str ] ] = none, context _ batch : optional [ list [ str ] ] = none ) - > list [ float ] : res = [ ] for s in candidate _ batch : # remove punctuation s =''. join ( ch for ch in s if ch not in string. punctuation ) # get number of typo", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "remove punctuation s =''. join ( ch for ch in s if ch not in string. punctuation ) # get number of typos in s num _ typos = len ( self. spell _ checker. unknown ( s. split ( ) ) ) # custom score is 1 / ( 2 ^ num _ typos ) res. append ( 1. 0 / ( 2 * * num _ typos ) ) return res my _ suite = testsuite ( \" test - spelling \", customspellingscore ( ), input _ text _ list = inputs,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ") return res my _ suite = testsuite ( \" test - spelling \", customspellingscore ( ), input _ text _ list = inputs, reference _ output _ list = baseline _ responses ) run test suite # my _ suite. run ( \" low _ temp _ responses \", candidate _ output _ list = low _ temp _ responses ) my _ suite. run ( \" med _ temp _ responses \", candidate _ output _ list = med _ temp _ responses ) my _ suite. run ( \" high _ temp _ responses \", candidate _ output _ list = high _ temp _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ temp _ responses ) my _ suite. run ( \" high _ temp _ responses \", candidate _ output _ list = high _ temp _ responses ) view results # run bench from your command line to visualize the run results comparing the different temperature settings. next add scorer configurations previous compare prompts copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page compare generation settings environment setup data preparation llm response generation create test suite run test suite view results source : https : / / bench. readthedocs. io / en / latest / compare _ generation _ settings", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "create test suite run test suite view results source : https : / / bench. readthedocs. io / en / latest / compare _ generation _ settings. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_generation_settings.html", "row": 129, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 130 text : compare llm providers - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. ad", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light /", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar compare llm providers # in this guide we compare llms answers at summarizing text. environment setup # in this guide, we use the openai api and the cohere api pip install openai cohere export openai _ api _ key = \" sk -... \" export cohere _ api _ key = \"... \" data preparation # we load a publically available congressional", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "= \" sk -... \" export cohere _ api _ key = \"... \" data preparation # we load a publically available congressional bill summarization dataset from huggingface. import pandas as pd from datasets import load _ dataset billsum = load _ dataset ( \" billsum \", split = \" ca _ test \" ) billsum _ df = pd. dataframe ( billsum ). sample ( 10, random _ state = 278487 ) llm response generation # we use openai and cohere to generate summaries of these bills : from lang", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "random _ state = 278487 ) llm response generation # we use openai and cohere to generate summaries of these bills : from langchain. llms import openai, cohere from langchain. chains import llmchain from langchain. prompts import prompttemplate gpt3 = openai ( temperature = 0. 0, max _ tokens = 100 ) command = cohere ( temperature = 0. 0, max _ tokens = 100 ) prompt _ template = prompttemplate ( input _ variables = [ \" text \" ], template = \" \" \"", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "0, max _ tokens = 100 ) prompt _ template = prompttemplate ( input _ variables = [ \" text \" ], template = \" \" \" you are an expert summarizer of text. a good summary captures the most important information in the text and doesnt focus too much on small details. text : { text } summary : \" \" \" ) gpt3 _ chain = llmchain ( llm = gpt3, prompt = prompt _ template ) command _ chain = llmchain ( llm = command, prompt = prompt _ template ) # generate summaries with truncated text gpt", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "template ) command _ chain = llmchain ( llm = command, prompt = prompt _ template ) # generate summaries with truncated text gpt3 _ summaries = [ gpt3 _ chain. run ( bill [ : 3000 ] ) for bill in billsum _ df. text ] command _ summaries = [ command _ chain. run ( bill [ : 3000 ] ) for bill in billsum _ df. text ] create test suite # for this test suite, we want to compare gpt - 3 against command. we will use the summaryquality scoring metric to a / b test", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "suite # for this test suite, we want to compare gpt - 3 against command. we will use the summaryquality scoring metric to a / b test each set of candidate responses against the reference summaries from the dataset from arthur _ bench. run. testsuite import testsuite my _ suite = testsuite ( \" congressional _ bills \", \" summary _ quality \", input _ text _ list = list ( billsum _ df. text ), reference _ output _ list = list ( billsum _ df. summary ) ) run test suite # my _ suite. run ( \" gpt", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "), reference _ output _ list = list ( billsum _ df. summary ) ) run test suite # my _ suite. run ( \" gpt3 _ summaries \", candidate _ output _ list = gpt3 _ summaries ) my _ suite. run ( \" command _ summaries \", candidate _ output _ list = command _ summaries ) view results # run bench from your command line to visualize the run results comparing the different temperature settings. next compare prompts previous creating test suites copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "different temperature settings. next compare prompts previous creating test suites copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page compare llm providers environment setup data preparation llm response generation create test suite run test suite view results source : https : / / bench. readthedocs. io / en / latest / compare _ llm _ providers. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_llm_providers.html", "row": 130, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 131 text : compare prompts - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar compare prompts # in this guide we compare prompts environment setup # in this guide, we use the openai api pip install openai export openai _ api _ key = \" sk -... \" data preparation # we load a publically available congressional bill summarization dataset from huggingface. we also prepare an example bill with its summary to include in a prompt as an example response. import pandas", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bill summarization dataset from huggingface. we also prepare an example bill with its summary to include in a prompt as an example response. import pandas as pd from datasets import load _ dataset billsum = load _ dataset ( \" billsum \" ) billsum _ df = pd. dataframe ( billsum [ \" ca _ test \" ] ). sample ( 10, random _ state = 278487 ) example _ bill = billsum [ \" test \" ] [ 6 ] [ \" text \" ] example _ bill _ summary = billsum [ \" test \" ] [ 6 ] [ \" summary", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "[ \" test \" ] [ 6 ] [ \" text \" ] example _ bill _ summary = billsum [ \" test \" ] [ 6 ] [ \" summary \" ] llm response generation # we use two different prompt templates to generate responses from langchain. chat _ models import chatopenai from langchain. chains import llmchain from langchain. prompts import prompttemplate gpt35 = chatopenai ( temperature = 0. 0, max _ tokens = 100 ) prompt0 _ template = prompttemplate ( input _ variables = [ \" text \" ], template = \" \" \"", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", max _ tokens = 100 ) prompt0 _ template = prompttemplate ( input _ variables = [ \" text \" ], template = \" \" \" text : { text } summary : \" \" \" ) prompt1 _ template = prompttemplate ( input _ variables = [ \" text \", \" example _ bill \", \" example _ bill _ summary \" ], template = \" \" \" you are an expert summarizer of legal text. a good summary captures the most important information in the text and doesnt focus too much on small details. make sure to use your expert legal knowledge in summarizing. = =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "the most important information in the text and doesnt focus too much on small details. make sure to use your expert legal knowledge in summarizing. = = = text : { example _ bill } summary : { example _ bill _ summary } = = = text : { text } summary : \" \" \" ) prompt0 _ chain = llmchain ( llm = gpt35, prompt = prompt0 _ template ) prompt1 _ chain = llmchain ( llm = gpt35, prompt = prompt1 _ template ) # generate summaries with truncated text prompt0 _ summaries = [ prompt", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( llm = gpt35, prompt = prompt1 _ template ) # generate summaries with truncated text prompt0 _ summaries = [ prompt0 _ chain. run ( bill [ : 3000 ] ) for bill in billsum _ df. text ] prompt1 _ summaries = [ prompt1 _ chain ( { \" text \" : bill [ : 3000 ], \" example _ bill \" : example _ bill, \" example _ bill _ summary \" : example _ bill _ summary } ) [ \" text \" ] for bill in billsum _ df. text ] create test suite # for this test suite", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": example _ bill _ summary } ) [ \" text \" ] for bill in billsum _ df. text ] create test suite # for this test suite, we will use bertscore to measure how much the candidate summaries approach the reference summaries by upgrading our prompt with task - specific detail and an example. from arthur _ bench. run. testsuite import testsuite my _ suite = testsuite ( \" congressional _ bills _ to _ reference \", \" bertscore \", input _ text _ list = list ( billsum _ df. text ), reference _ output _ list =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "reference \", \" bertscore \", input _ text _ list = list ( billsum _ df. text ), reference _ output _ list = list ( billsum _ df. summary ) ) run test suite # my _ suite. run ( \" prompt0 _ summaries \", candidate _ output _ list = prompt0 _ summaries ) my _ suite. run ( \" prompt1 _ summaries \", candidate _ output _ list = prompt1 _ summaries ) view results # run bench from your command line to visualize the run results comparing the different temperature settings. next compare generation settings", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "prompt1 _ summaries ) view results # run bench from your command line to visualize the run results comparing the different temperature settings. next compare generation settings previous compare llm providers copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page compare prompts environment setup data preparation llm response generation create test suite run test suite view results source : https : / / bench. readthedocs. io / en / latest / compare _ prompts. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/compare_prompts.html", "row": 131, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 132 text : concepts - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar concepts # data # testing llms involves preparing the following data for your use case : inputs to the llm. depending on the task at hand, these inputs are likely formatted to follow a prompt template. reference outputs : these are your baseline outputs, which are optional in arthur bench but recommended to get a comprehensive understanding of your model \u2019 s performance relative to its expected outputs. these reference outputs would likely be either ground truth responses to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur bench but recommended to get a comprehensive understanding of your model \u2019 s performance relative to its expected outputs. these reference outputs would likely be either ground truth responses to the inputs, or could be outputs from a baseline llm that you are evaluating against. candidate outputs : these are the outputs from your candidate llm that you are scoring. context : contextual information used to produce the candidate output, e. g. for retrieval - augmented question & answering tasks. as an example, consider the task of question & answering about specific documents : input : \u201c what war was referred to in the gettysburg address? \u201d reference output : american civil war candidate", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "consider the task of question & answering about specific documents : input : \u201c what war was referred to in the gettysburg address? \u201d reference output : american civil war candidate output : the war referenced in the gettysburg address is the american civil war context : ( wikipedia ) \u201c the gettysburg address is a speech that u. s. president abraham lincoln delivered during the american civil war at the dedication of the soldiers \u2019 national cemetery, now known as gettysburg national cemetery, in gettysburg, pennsylvania on the afternoon of november 19, 1863, four and a half months after the union armies defeated confederate forces in the battle of gettysburg, the civil war \u2019 s deadliest battle", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "of november 19, 1863, four and a half months after the union armies defeated confederate forces in the battle of gettysburg, the civil war \u2019 s deadliest battle. \u201d testing # test suites # a test suite stores the input & reference output data for your testing use case along with a scorer. for example, for a summarization use case, your test suite could be created with : the documents to summarize baseline summaries as reference outputs to evaluate against the summaryquality scorer test suites allow you to save and reuse your evaluation datasets over time with a consistent scorer to help you understand what drives changes in performance", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##quality scorer test suites allow you to save and reuse your evaluation datasets over time with a consistent scorer to help you understand what drives changes in performance. to view how to create test suites from various data formats, view our creating test suites guide test runs # when a test suite is run, its scorer evaluates the candidate outputs provided in the run and assigns a score to each test case. to run your test suite on candidate data, pass the data to the run ( ) function of your test suite, along with any additional metadata you want to be logged for that run. to view the metadata you can save with your test", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( ) function of your test suite, along with any additional metadata you want to be logged for that run. to view the metadata you can save with your test runs, see the sdk docs to view how to create test runs from various data formats, visit our test suites guide next creating test suites previous guides copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page concepts data testing test suites test runs source : https : / / bench. readthedocs. io / en / latest / concepts. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/concepts.html", "row": 132, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 133 text : contributing - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar contributing # we welcome contributions and feedback from the community! creating a custom scorer # all scorers should inherit from the scorer base class and provide a custom implementation of the run _ batch method. a scorer can leverage any combination of input texts, context texts, and reference texts to score candidate generations. all computed scores must be float values where a higher value indicates a better score. if you have a scorer that does not fit these constraints,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "score candidate generations. all computed scores must be float values where a higher value indicates a better score. if you have a scorer that does not fit these constraints, please get in touch with the arthur team. steps for adding a custom scorer : install bench from source, in development mode : pip install - e. add your scorer implementation in a new file in arthur _ bench / scoring. for scorers that require prompt templating, we use the langchain library. register your scorer by adding it to the scorer enum in arthur _ bench / models / models. py at this point, you should be able to create test", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "your scorer by adding it to the scorer enum in arthur _ bench / models / models. py at this point, you should be able to create test suites with your new scorer and test your implementation locally. contributing your scorer : fork the bench repository and create a pull request from your fork. this github guide provides more in depth instructions. your scorer docstring should use sphinx format for compatibility with documentation. provide unit tests for the scorer in a separate file in the test directory. next usage data collection previous arthur _ bench. utils copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "the test directory. next usage data collection previous arthur _ bench. utils copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page contributing creating a custom scorer source : https : / / bench. readthedocs. io / en / latest / contributing. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/contributing.html", "row": 133, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 134 text : creating test suites - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar creating test suites # what data should i use? # it is best to use data that is as close to your production use case as possible. we recommend sampling some historic data and manually validating a set of 25 + cases. public datasets on huggingface like the dolly dataset and the humaneval dataset can be a great starting place to benchmark on your use case before you have data that is closer to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "like the dolly dataset and the humaneval dataset can be a great starting place to benchmark on your use case before you have data that is closer to your actual production setting. when no baseline examples or labels easily exist for the inputs you want to evaluate llm performance on, you can use an existing llm to generate a baseline for the task and then iterate from there. ways to create a test suite # the testsuite class is the main touch point for creating and running tests in arthur bench. no matter how you prepare your data for a test suite, you use the common interface provided by importing the testsuit", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "creating and running tests in arthur bench. no matter how you prepare your data for a test suite, you use the common interface provided by importing the testsuite class : from arthur _ bench. run. testsuite import testsuite you can provide data for your testsuite via the following options, each of which we give examples of in the sections below : list [ str ] pd. dataframe csv file huggingface dataset to see the exact specifications for the testsuite class, visit our sdk docs. list [ str ] - > testsuite # you can create and run a test", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "for the testsuite class, visit our sdk docs. list [ str ] - > testsuite # you can create and run a test suite by passing lists of strings directly as the test suite data : suite = testsuite ( \" bench _ quickstart \", \" exact _ match \", input _ text _ list = [ \" what year was fdr elected? \", \" what is the opposite of down? \" ], reference _ output _ list = [ \" 1932 \", \" up \" ] ) suite. run ('quickstart _ run ', candidate _ output _ list = [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ list = [ \" 1932 \", \" up \" ] ) suite. run ('quickstart _ run ', candidate _ output _ list = [ \" 1932 \", \" up is the opposite of down \" ] ) this path also allows you to pass llm responses directly into a test suite as a set of baseline reference outputs and / or as a run of candidate outputs. for example, you can use gpt - 4 outputs as a baseline for a test suite, and then run gpt - 3. 5 - turbo as a candidate to see how it compares. creating and running the test suite directly from llm -", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", and then run gpt - 3. 5 - turbo as a candidate to see how it compares. creating and running the test suite directly from llm - generated strings : from langchain. chat _ models import chatopenai gpt35 = chatopenai ( ) gpt4 = chatopenai ( model _ name = \" gpt - 4 \" ) inputs = [ \" what year was fdr elected? \", \" what is the opposite of down? \" ] baseline _ outputs = [ gpt4. predict ( x ) for x in inputs ] candidate _ outputs = [ gpt35. predict ( x", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\" ] baseline _ outputs = [ gpt4. predict ( x ) for x in inputs ] candidate _ outputs = [ gpt35. predict ( x ) for x in inputs ] suite = testsuite ( \" bench _ llm _ quickstart \", \" exact _ match \", input _ text _ list = inputs, reference _ output _ list = baseline _ outputs ) suite. run ('quickstart _ llm _ run ', candidate _ output _ list = candidate _ outputs ) dataframe - > testsuite # if you have your test suite and / or model response data in a pandas", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "output _ list = candidate _ outputs ) dataframe - > testsuite # if you have your test suite and / or model response data in a pandas dataframe you can create test suites and runs directly from those dataframes here is an example test suite built from a dataframe with the default reference data and candidate data column names that testsuite expects ( you can also use other column names as we show below ) creating and running the default dataframe test suite : import pandas as pd df = pd. dataframe ( { \" input \" : [ \" what year was fdr elected? \", \" what is", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "import pandas as pd df = pd. dataframe ( { \" input \" : [ \" what year was fdr elected? \", \" what is the opposite of down? \" ], \" reference _ output \" : [ \" 1932 \", \" up \" ], \" candidate _ output \" : [ \" 1932 \", \" up is the opposite of down \" ] } ) test _ suite = testsuite ( \" suite _ from _ df \", \" exact _ match \", reference _ data = df ) test _ suite. run ( \" candidate _ from _ df \", candidate _ data = d", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "exact _ match \", reference _ data = df ) test _ suite. run ( \" candidate _ from _ df \", candidate _ data = df ) alternatively you can create and run test suites from dataframes with custom column names. creating and running the custom dataframe test suite : import pandas as pd df = pd. dataframe ( { \" my _ input \" : [ \" what year was fdr elected? \", \" what is the opposite of down? \" ], \" baseline _ output \" : [ \" 1932 \", \" up \" ], \" gpt35 _ output \" : [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "opposite of down? \" ], \" baseline _ output \" : [ \" 1932 \", \" up \" ], \" gpt35 _ output \" : [ \" 1932 \", \" up is the opposite of down \" ] } ) test _ suite = testsuite ( \" suite _ from _ df _ custom \", \" exact _ match \", reference _ data = df, input _ column = \" my _ input \", reference _ column = \" baseline _ output \" ) test _ suite. run ( \" candidate _ from _ df _ custom \", candidate _ data = df, candidate _ column = \"", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "output \" ) test _ suite. run ( \" candidate _ from _ df _ custom \", candidate _ data = df, candidate _ column = \" gpt35 _ output \" ). csv - > testsuite # if your test suite and / or model response data already exists in csv files you can create test suites and runs directly from those files here is an example test suite csv with the default reference data and candidate data column names that testsuite expects ( you can also use other column names as we show below ) test _ suite _ data _ default _ columns. csv input, reference _ output", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##e expects ( you can also use other column names as we show below ) test _ suite _ data _ default _ columns. csv input, reference _ output, candidate _ output what year was fdr elected?, 1932, 1932 what is the opposite of down?, up, up is the opposite of down creating and running the default csv test suite : test _ suite = testsuite ( \" suite _ from _ csv \", \" exact _ match \", reference _ data _ path = \" / path / to / test _ suite _ data _ default _ columns. csv \" ) test _ suite. run (", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "reference _ data _ path = \" / path / to / test _ suite _ data _ default _ columns. csv \" ) test _ suite. run ( \" candidate _ from _ csv \", candidate _ data _ path = \" / path / to / test _ suite _ data _ default _ columns. csv \" ) alternatively you can create and run test suites from. csv files with custom column names : test _ suite _ data _ custom _ columns. csv my _ input, baseline _ output, gpt35 _ output what year was fdr elected?, 1932, 1932 what is the opposite of down?", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##v my _ input, baseline _ output, gpt35 _ output what year was fdr elected?, 1932, 1932 what is the opposite of down?, up, up is the opposite of down creating and running the custom csv test suite : test _ suite = testsuite ( \" suite _ from _ csv _ custom \", \" exact _ match \", reference _ data _ path = \" / path / to / test _ suite _ data _ custom _ columns. csv \", input _ column = \" my _ input \", reference _ column = \" baseline _ output \" ) test _ suite. run (", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "columns. csv \", input _ column = \" my _ input \", reference _ column = \" baseline _ output \" ) test _ suite. run ( \" candidate _ from _ csv _ custom \", candidate _ data _ path = \" / path / to / test _ suite _ data _ custom _ columns. csv \", candidate _ column = \" gpt35 _ output \" ) huggingface dataset - > dataframe - > testsuite # here we create a small question - answering test suite from the dolly dataset downloaded from huggingface. we set up the test suite to use bertscore to measure", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "we create a small question - answering test suite from the dolly dataset downloaded from huggingface. we set up the test suite to use bertscore to measure similarity between candidate answers and reference answers creating and running the dolly test suite : # get dolly dataset from huggingface into a pandas dataframe import pandas as pd from datasets import load _ dataset dolly = load _ dataset ( \" databricks / databricks - dolly - 15k \" ) dolly _ df = pd. dataframe ( dolly [ \" train \" ] ) # make test suite from a question - answering subset of the data dolly", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ") dolly _ df = pd. dataframe ( dolly [ \" train \" ] ) # make test suite from a question - answering subset of the data dolly _ df _ sample = dolly _ df [ dolly _ df [ \" category \" ] = = \" open _ qa \" ]. sample ( 25, random _ state = 278487 ) dolly _ suite = testsuite ( \" suite _ from _ huggingface _ dolly \", \" bertscore \", reference _ data = dolly _ df _ sample, input _ column = \" instruction \", reference _ column = \" response \" ) # run", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##re \", reference _ data = dolly _ df _ sample, input _ column = \" instruction \", reference _ column = \" response \" ) # run test suite on gpt - 3. 5 - turbo generated answers to the questions from langchain. chat _ models import chatopenai gpt35 = chatopenai ( ) dolly _ suite. run ( \" gpt - 3. 5 \", candidate _ output _ list = [ gpt35. predict ( x ) for x in dolly _ df _ sample. instruction ] ) next compare llm providers previous concepts copyright \u00a9 2023, arthur made with sphinx", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( x ) for x in dolly _ df _ sample. instruction ] ) next compare llm providers previous concepts copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page creating test suites what data should i use? ways to create a test suite list [ str ] - > testsuite dataframe - > testsuite. csv - > testsuite huggingface dataset - > dataframe - > testsuite source : https : / / bench. readthedocs. io / en / latest / creating _ test _ suites. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/creating_test_suites.html", "row": 134, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 135 text : custom scoring - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark /", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar custom scoring # in this guide, we will walk through the process of evaluating llm performance using a custom scorer. we will define a custom scorer create a test suite with that scorer run the test suite and view scores define a custom scorer # to create a custom scorer that satisfies the scorer interface ( defined in the section below ), implement the scoring logic in the run _ batch method. additionally, provide your scorer a name", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "satisfies the scorer interface ( defined in the section below ), implement the scoring logic in the run _ batch method. additionally, provide your scorer a name in the name ( ) method. this example custom scorer is called trigramrepetition, which scores responses with a 0. 0 if they contain repeated trigrams above a thresholded number of times. for our scorer, we override the requires _ reference ( ) method to return false instead of true, since this custom scorer evaluates the candidate outputs without the need for a reference. make sure nltk is installed as a package to your environment, which our custom", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "this custom scorer evaluates the candidate outputs without the need for a reference. make sure nltk is installed as a package to your environment, which our custom scorer uses. pip install nltk make sure nltk is installed as a package to your environment, which our custom scorer uses. pip install nltk from arthur _ bench. scoring import scorer import nltk # make sure corpus is downloaded nltk. download ('punkt') from nltk import trigrams from typing import list, optional class trigramrepetition ( scorer ) : def _ _ init _ _ ( self,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##tk import trigrams from typing import list, optional class trigramrepetition ( scorer ) : def _ _ init _ _ ( self, threshold : int = 3 ) : self. threshold = threshold @ staticmethod def name ( ) - > str : return \" trigram _ repetition \" @ staticmethod def requires _ reference ( ) - > bool : return false def run _ batch ( self, candidate _ batch : list [ str ], reference _ batch : optional [ list [ str ] ] = none, input _ text _ batch : optional [ list [ str ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str ], reference _ batch : optional [ list [ str ] ] = none, input _ text _ batch : optional [ list [ str ] ] = none, context _ batch : optional [ list [ str ] ] = none ) - > list [ float ] : repeat _ scores = [ ] for text in candidate _ batch : tokens = [ t. lower ( ) for t in nltk. word _ tokenize ( text ) ] all _ trigrams = trigrams ( tokens ) counts = { } for tri in all _ trigrams : if tri in counts : counts [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ trigrams = trigrams ( tokens ) counts = { } for tri in all _ trigrams : if tri in counts : counts [ tri ] + = 1 else : counts [ tri ] = 1 max _ repeat = max ( counts. values ( ) ) repeat _ scores. append ( float ( max _ repeat < self. threshold ) ) return repeat _ scores using a custom scorer in a test suite # we pass in our custom scorer as the scoring _ method parameter to the test suite : from arthur _ bench. run. testsuite import testsuite repetition _ test = testsuite ( '", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "scoring _ method parameter to the test suite : from arthur _ bench. run. testsuite import testsuite repetition _ test = testsuite ('custom _ trigram _ suite ', scoring _ method = trigramrepetition ( ), input _ text _ list = [ \" talk to me but don't repeat yourself too much \", \" talk to me but don't repeat yourself too much \" ] ) run the test suite # now that we \u2019 ve loaded in our custom scorer, our test suite can be run as usual on any candidate generations. run = repetition _ test. run ('test _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u2019 ve loaded in our custom scorer, our test suite can be run as usual on any candidate generations. run = repetition _ test. run ('test _ run ', candidate _ output _ list = ['a great response with no repetition! ','a bad response that repeats response that repeats response that repeats response that repeats'] ) print ( run. scores ) > > > [ 1. 0, 0. 0 ] scorer validation # test suites expect scorer configurations to remain consistent from run to run, so that each runs scores can be compared and reliably tracked throughout time. let \u2019 s see what happens if we", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "to remain consistent from run to run, so that each runs scores can be compared and reliably tracked throughout time. let \u2019 s see what happens if we attempt to use this suite at a later time, but edit the underlying parameters. scorer = trigramrepetition ( threshold = 7 ) repetition _ test = testsuite ('custom _ trigram _ suite ', scoring _ method = scorer ) we see the following warning : scoring method configuration has changed from test suite creation. by default, bench will save the json serializable attributes of your scorer as the configuration. if you need more advanced serialization for validation", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "suite creation. by default, bench will save the json serializable attributes of your scorer as the configuration. if you need more advanced serialization for validation or re - initialization, implement the to _ dict ( ) and from _ dict ( ) methods on your custom class. you can find the full scorer spec here. scorer interface # all scorers in bench implement the scorer interface. let \u2019 s take a look at that interface : class scorer ( abc ) : \" \" \" base class for all scorers. \" \" \" @ staticmethod @ abstractmethod def name ( ) - > str", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": \" \" \" base class for all scorers. \" \" \" @ staticmethod @ abstractmethod def name ( ) - > str : \" \" \" get the name of this scorer : return : the scorer name \" \" \" raise notimplementederror @ staticmethod def requires _ reference ( ) - > bool : return true @ abstractmethod def run _ batch ( self, candidate _ batch : list [ str ], reference _ batch : optional [ list [ str ] ] = none, input _ text _ batch : optional [ list [ str ] ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##r ], reference _ batch : optional [ list [ str ] ] = none, input _ text _ batch : optional [ list [ str ] ] = none, context _ batch : optional [ list [ str ] ] = none ) - > list [ float ] : \" \" \" score a batch of candidate generations. : param candidate _ batch : candidate generations to score : param reference _ batch : reference strings representing target outputs : param input _ text _ batch : optional corresponding inputs : param context _ batch : optional corresponding contexts, if needed by scorer \" \" \" raise notimplementederror", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ batch : optional corresponding inputs : param context _ batch : optional corresponding contexts, if needed by scorer \" \" \" raise notimplementederror to create a custom scorer, you need to implement the name and run _ batch methods, and optionally override the requires _ reference method if your scorer doesn \u2019 t require reference or target data. contributing # if you think you \u2019 ve got a useful scorer, please consider contributing! next code evaluation previous add scorer configurations copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page custom scoring define a custom scorer using a custom scorer", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page custom scoring define a custom scorer using a custom scorer in a test suite run the test suite scorer validation scorer interface contributing source : https : / / bench. readthedocs. io / en / latest / custom _ scoring. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/custom_scoring.html", "row": 135, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 136 text : quickstart - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar quickstart # make sure you have completed installation from the setup guide before moving on to this quickstart. environment setup # the environment variable bench _ file _ dir points to the local directory where your test data is saved and visualized by arthur bench. if you are running this quickstart right after completing the setup guide, then take a moment to reset bench _ file _ dir to its default value, \".", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "you are running this quickstart right after completing the setup guide, then take a moment to reset bench _ file _ dir to its default value, \". / bench _ runs \". this will direct the bench ui to point to your new quickstart test suite instead of the examples from the setup. export bench _ file _ dir = \". / bench _ runs \" creating your first test suite # instantiate a test suite with a name, data, and scorer. this example creates a test suite from lists of strings directly with the exact _ match scorer. from arthur _ bench. run. testsuite import testsuite", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "this example creates a test suite from lists of strings directly with the exact _ match scorer. from arthur _ bench. run. testsuite import testsuite suite = testsuite ('bench _ quickstart ','exact _ match ', input _ text _ list = [ \" what year was fdr elected? \", \" what is the opposite of down? \" ], reference _ output _ list = [ \" 1932 \", \" up \" ] ) you can create test suites from a pandas dataframe or from a path to a local csv file. see the test suite creation guide to view all the", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "you can create test suites from a pandas dataframe or from a path to a local csv file. see the test suite creation guide to view all the ways you can create test suites. you can view all scorers available out of the box with bench here on our scoring page, as well as customize your own. running your first test suite # to create a test run, you only need to specify the candidate responses. see the test suite creation guide to view all the ways you can run test suites. run = suite. run ('quickstart _ run ', candidate _ output _ list = [ \" 1932 \"", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "the ways you can run test suites. run = suite. run ('quickstart _ run ', candidate _ output _ list = [ \" 1932 \", \" up is the opposite of down \" ] ) print ( run ) > > > [ testcaseoutput ( output ='1932 ', score = 1. 0 ), testcaseoutput ( output ='up is the opposite of down ', score = 0. 0 ) ] you should now have logged test case results with scores of 1. 0 and 0. 0, respectively. view results in local ui # now run bench from the command line to launch", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "logged test case results with scores of 1. 0 and 0. 0, respectively. view results in local ui # now run bench from the command line to launch the local ui and explore the test results. bench next steps # now that you have set up and ran your first test suite, check out the rest of the scorers available in arthur bench out of the box. to learn more about the basic concepts around data and testing in arthur bench, visit our basic concepts guide. next scoring previous setup copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page quickstart environment setup creating", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "next scoring previous setup copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page quickstart environment setup creating your first test suite running your first test suite view results in local ui next steps source : https : / / bench. readthedocs. io / en / latest / quickstart. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/quickstart.html", "row": 136, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 137 text : scoring - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar scoring # a scorer is the criteria used to quantitatively evaluate llm outputs. when you test llms with arthur bench, you attach a scorer to each test suite you create - this defines how performance will be measured consistently across that test suite. for a walkthrough on how to extend the scorer class to create your own scorer specialized to your data and / or use - case to use with arthur bench, check out the custom", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "on how to extend the scorer class to create your own scorer specialized to your data and / or use - case to use with arthur bench, check out the custom scoring guide if you would like to contribute scorers to the open source arthur bench repo, check out our contributing guide here is a list of all the scorers available by default in arthur bench ( listed alphabetically ) : scorer tasks type requirements bert score ( bertscore ) any embedding - based reference output, candidate output exact match ( exact _ match ) any lexicon - based reference output, candidate output hallucination ( hallucination ) any prompt - based", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", candidate output exact match ( exact _ match ) any lexicon - based reference output, candidate output hallucination ( hallucination ) any prompt - based candidate output, context hedging language ( hedging _ language ) any embedding - based candidate output python unit testing ( python _ unit _ testing ) python generation code evaluator candidate output, unit tests ( see the code eval guide ) qa correctness ( qa _ correctness ) question - answering prompt - based input, candidate output, context readability ( readability ) any lexicon - based candidate output specificity ( specificity ) any lexi", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "- answering prompt - based input, candidate output, context readability ( readability ) any lexicon - based candidate output specificity ( specificity ) any lexicon - based candidate output summary quality ( summary _ quality ) summarization prompt - based input, reference output, candidate output word count match ( word _ count _ match ) any lexicon - based reference output, candidate output for better understandability we have broken down the scorers based on the type of procedure each scorer uses. prompt - based scorers # qa _ correctness # the qa correctness scorer evaluates the correctness of an answer, given a question", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "uses. prompt - based scorers # qa _ correctness # the qa correctness scorer evaluates the correctness of an answer, given a question and context. this scorer does not require a reference output, but does require context. each row of the test run will receive a binary 0, indicating an incorrect output, or 1, indicating a correct output. summary _ quality # the summary quality scorer evaluates a summary against its source text and a reference summary for comparison. it evaluates summaries on dimensions including relevance and syntax. each row of the test run will receive a binary 0, indicating that the reference output was", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". it evaluates summaries on dimensions including relevance and syntax. each row of the test run will receive a binary 0, indicating that the reference output was scored higher than the candidate output, or 1, indicating that the candidate output was scored higher than the reference output. hallucination # the hallucination scorer takes a response and a context ( e. g. in a rag setting where context is used to ground an llm \u2019 s responses ) and identifies when information in the response is not substantiated by the context. the scorer breaks down the response into a list of claims and checks the claims against the context for", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "in the response is not substantiated by the context. the scorer breaks down the response into a list of claims and checks the claims against the context for support. this binary score is 1 if all claims are supported, and 0 otherwise. embedding - based scorers # bertscore # bertscore is a quantitative metric to compare the similarity of two pieces of text. using bertscore will score each row of the test run as the bert score between the reference output and the candidate output. hedging _ language # the hedging language scorer evaluates whether a candidate response is similar to generic hedging", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "reference output and the candidate output. hedging _ language # the hedging language scorer evaluates whether a candidate response is similar to generic hedging language used by an llm ( \u201c as an ai language model, i don \u2019 t have personal opinions, emotions, or beliefs \u201d ). each row of the test run will receive a score between 0. 0 and 1. 0 indicating the extent to which hedging language is detected in the response ( using bertscore similarity to the target hedging phrase ). a score above 0. 5 typically suggests the model output contains hedging language. lexicon -", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##score similarity to the target hedging phrase ). a score above 0. 5 typically suggests the model output contains hedging language. lexicon - based scorers # exact _ match # the exact match scorer evaluates whether the candidate output exactly matches the reference output. this is case sensitive. each row of the test run will receive a binary 0, indicating a non - match, or 1, indicating an exact match. readability # the readability scorer evaluates the reading ease of the candidate output according to the flesch reading ease score. the higher the score, the easier the candidate output is to read : scores", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "the reading ease of the candidate output according to the flesch reading ease score. the higher the score, the easier the candidate output is to read : scores of 90 - 100 correlate to a 5th grade reading level, while scores less than 10 are classified as being \u201c extremely difficult to read, and best understood by university graduates. \u201d specificity # the specificity scorer outputs a score of 0 to 1, where smaller values correspond to candidate outputs with more vague language while higher values correspond to candidate outputs with more precise language. specificity is calculated through 3 heuristic approaches : identifying the presence of predefined words that", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "higher values correspond to candidate outputs with more precise language. specificity is calculated through 3 heuristic approaches : identifying the presence of predefined words that indicate vagueness, determing how rare the words used are according to word frequencies calculated by popular nlp corpora, and detecting the use of proper nouns and numbers. word _ count _ match # for scenarios where there is a preferred output length, word _ count _ match calculates a corresponding score on the scale of 0 to 1. specifically, this scorers calculates how similar the number of words in the candidate output is to the number of words in the reference output,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "0 to 1. specifically, this scorers calculates how similar the number of words in the candidate output is to the number of words in the reference output, where a score of 1. 0 indicates that there are the same number of words in the candidate output as in the reference output. scores less than 1. 0 are calculated as ( ( len _ reference - delta ) / len _ reference ) where delta is the absolute difference in word lengths between the candidate and reference outputs. all negative computed values are truncated to 0. code evaluators # python _ unit _ testing # the python unit testing scorer evaluates candidate solutions to coding tasks", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "all negative computed values are truncated to 0. code evaluators # python _ unit _ testing # the python unit testing scorer evaluates candidate solutions to coding tasks against unit tests. this scorer wraps the code _ eval evaluator interface from huggingface. it is important to note that this function requires that solution code uses standard python libraries only. next guides previous quickstart copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page scoring prompt - based scorers qa _ correctness summary _ quality hallucination embedding - based scorers bertscore hedging", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "this page scoring prompt - based scorers qa _ correctness summary _ quality hallucination embedding - based scorers bertscore hedging _ language lexicon - based scorers exact _ match readability specificity word _ count _ match code evaluators python _ unit _ testing source : https : / / bench. readthedocs. io / en / latest / scoring. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/scoring.html", "row": 137, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 138 text : arthur _ bench. client. auth - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##try arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client. auth # submodules # arthur _ bench. client. auth. helpers. get _ arthur _ internal _ user _ org ( api _ http _ host : str, auth _ token : str, verify _ ssl : bool = true ) \u2192 str none # get the current organization for the provided arthur auth token belonging", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "token : str, verify _ ssl : bool = true ) \u2192 str none # get the current organization for the provided arthur auth token belonging to anarthur internal user parameters : api _ http _ host \u2013 base url of the host to connect to, including protocol ( e. g. \u201c https : / / app. arthur. ai \u201d ) auth _ token \u2013 auth token to pass to the api verify _ ssl \u2013 if true, verify that the ssl certificate is valid and not self - signed returns : the organization id associated with the provided access key, none if no such organization exists permission", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "verify that the ssl certificate is valid and not self - signed returns : the organization id associated with the provided access key, none if no such organization exists permissions : n / a arthur _ bench. client. auth. helpers. get _ auth _ info ( api _ http _ host : str, auth _ token : str, verify _ ssl : bool = true ) \u2192 authenticationinfo # get the authinfo struct associated with the provided access key parameters : api _ http _ host \u2013 base url of the host to connect to, including protocol ( e. g.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ct associated with the provided access key parameters : api _ http _ host \u2013 base url of the host to connect to, including protocol ( e. g. \u201c https : / / app. arthur. ai \u201d ) : param auth _ token : token to fetch authentication info for : param verify _ ssl : boolean for whether requests should verify that the ssl certificate is valid and not self - signed : return : the authinfo associated with the provided access key : permissions : n / a arthur _ bench. client. auth. helpers. get _ current _ org ( api _ http _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "provided access key : permissions : n / a arthur _ bench. client. auth. helpers. get _ current _ org ( api _ http _ host : str, auth _ token : str, verify _ ssl : bool = true ) \u2192 str none # get the current organization for the provided access key parameters : api _ http _ host \u2013 base url of the host to connect to, including protocol ( e. g. \u201c https : / / app. arthur. ai \u201d ) : param auth _ token : api key to pass to the api : param verify _ ssl", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": / / app. arthur. ai \u201d ) : param auth _ token : api key to pass to the api : param verify _ ssl : boolean for whether requests should verify that the ssl certificate is valid and not self - signed returns : the organization id associated with the provided access key, none if no such organization exists arthur _ bench. client. auth. helpers. user _ login ( api _ http _ host : str, login : str, password : str, verify _ ssl : bool = true ) \u2192 str # static convenience function to get a new", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "login : str, password : str, verify _ ssl : bool = true ) \u2192 str # static convenience function to get a new auth token for the provided username andpassword parameters : api _ http _ host \u2013 base url of the host to connect to, including protocol ( e. g. \u201c https : / / app. arthur. ai \u201d ) login \u2013 the username or password to use to log in password \u2013 password for the user verify _ ssl \u2013 boolean for whether requests should verify that the ssl certificate is valid and not self - signed returns : an access _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "for the user verify _ ssl \u2013 boolean for whether requests should verify that the ssl certificate is valid and not self - signed returns : an access _ key class arthur _ bench. client. auth. refresh. authrefresher ( url : str, login : str, password : str, verify _ ssl : bool ) # bases : object algorithms = ['hs256'] # auth _ key ='authorization'# mins _ before _ expiry _ to _ refresh = 5 # refresh ( ) \u2192 tuple [ dict [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "='authorization'# mins _ before _ expiry _ to _ refresh = 5 # refresh ( ) \u2192 tuple [ dict [ str, str ], timedelta ] # authorization header update function for an httpclient fetches a new session token and returns the new token, and how long to wait before refreshing it ( by calling this method again ) : return : headers to update ( authorization ), and time to wait before refreshing again next arthur _ bench. client. http previous arthur _ bench. client copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg '", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "refreshing again next arthur _ bench. client. http previous arthur _ bench. client copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. auth submodules get _ arthur _ internal _ user _ org ( ) get _ auth _ info ( ) get _ current _ org ( ) user _ login ( ) authrefresher authrefresher. algorithms authrefresher. auth _ key authrefresher. mins _ before _ expiry _ to _ refresh au", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "authrefresher. auth _ key authrefresher. mins _ before _ expiry _ to _ refresh authrefresher. refresh ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. auth. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html", "row": 138, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 139 text : arthur _ bench. client - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##toggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client # subpackages # arthur _ bench. client. auth submodules get _ arthur _ internal _ user _ org ( ) get _ auth _ info ( ) get _ current _ org ( ) user _ login ( ) authrefresher authrefresher. algorithms authrefresher. auth _ key authrefr", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##in ( ) authrefresher authrefresher. algorithms authrefresher. auth _ key authrefresher. mins _ before _ expiry _ to _ refresh authrefresher. refresh ( ) arthur _ bench. client. http submodules construct _ url ( ) httpclient httpclient. delete ( ) httpclient. get ( ) httpclient. patch ( ) httpclient. post ( ) httpclient. put ( ) httpclient. send ( ) httpclient. set _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ient. patch ( ) httpclient. post ( ) httpclient. put ( ) httpclient. send ( ) httpclient. set _ path _ prefix ( ) validate _ multistatus _ response _ and _ get _ failures ( ) validate _ response _ status ( ) arthur _ bench. client. local submodules localbenchclient localbenchclient. create _ new _ test _ run ( ) localbenchclient. create _ test _ suite ( ) localbenchclient. delete _ test _ run ( ) localbenchclient. delete _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ient. create _ test _ suite ( ) localbenchclient. delete _ test _ run ( ) localbenchclient. delete _ test _ suite ( ) localbenchclient. get _ runs _ for _ test _ suite ( ) localbenchclient. get _ summary _ statistics ( ) localbenchclient. get _ test _ run ( ) localbenchclient. get _ test _ suite ( ) localbenchclient. get _ test _ suite _ by _ name ( ) localbenchclient. get _ test _ suites ( ) pageinfo pagein", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##client. get _ test _ suite _ by _ name ( ) localbenchclient. get _ test _ suites ( ) pageinfo pageinfo. end pageinfo. page pageinfo. page _ size pageinfo. sorted _ pages pageinfo. start pageinfo. total _ count pageinfo. total _ pages arthur _ bench. client. rest subpackages arthur _ bench. client. rest. admin submodules arthur _ bench. client. rest. bench submodules submodules arthurclient submodules # class arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##les arthur _ bench. client. rest. bench submodules submodules arthurclient submodules # class arthur _ bench. client. bench _ client. benchclient # bases : abc base class for saving and loading bench data check _ run _ exists ( suite _ id : str, run _ name : str ) \u2192 bool # check if run with given name if it exists for suite with id suite _ id parameters : client \u2013 benchclient object for fetching test suite data suite _ id \u2013 the id of the test suite to check run names run _ name \u2013 the test run name", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "benchclient object for fetching test suite data suite _ id \u2013 the id of the test suite to check run names run _ name \u2013 the test run name to check for returns : true if run with name is found, false otherwise raises : arthurinternalerror \u2013 if using a client that does not support pagination abstract create _ new _ test _ run ( test _ suite _ id : str, json _ body : createrunrequest ) \u2192 createrunresponse # create a new run for a test suite. parameters : test _ suite _ id \u2013 the uuid of the test suite to log", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "createrunresponse # create a new run for a test suite. parameters : test _ suite _ id \u2013 the uuid of the test suite to log a run for json _ body \u2013 run request containing run _ metadata and scored model generations abstract create _ test _ suite ( json _ body : testsuiterequest ) \u2192 paginatedtestsuite # create a new test suite. parameters : json _ body \u2013 test suite request object consisting of test suite metadata and test cases abstract delete _ test _ run ( test _ suite _ id : str, test _ run _ id : str ) # del", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "metadata and test cases abstract delete _ test _ run ( test _ suite _ id : str, test _ run _ id : str ) # delete a test run from a suite. abstract delete _ test _ suite ( test _ suite _ id : str ) # delete a test suite. all associated runs will also be deleted abstract get _ runs _ for _ test _ suite ( test _ suite _ id : str, sort : commonsortenum testrunsortenum = commonsortenum. created _ at _ asc, page : int = 1, page _ size : int = 5", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##um testrunsortenum = commonsortenum. created _ at _ asc, page : int = 1, page _ size : int = 5 ) \u2192 paginatedruns # get runs for a given test suite. parameters : test _ suite _ id \u2013 the uuid of the test suite sort \u2013 optional sort key. possible values are \u2018 name \u2019, \u2018 avg _ score \u2019, and \u2018 created _ at \u2019. use \u2018 - \u2019 prefix for descending sort. defaults to \u2018 created _ at \u2019 page \u2013 the page to fetch page _ size \u2013 page size to fetch get _ suite _ if _ exists", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "descending sort. defaults to \u2018 created _ at \u2019 page \u2013 the page to fetch page _ size \u2013 page size to fetch get _ suite _ if _ exists ( name : str ) \u2192 paginatedtestsuite none # get a full test suite with name if it exists. parameters : client \u2013 benchclient object for fetching test suite data returns : complete test suite with all test cases joined, or none if no suite with name exists raises : arthurinternalerror \u2013 if using a client that does not support pagination abstract get _ summary _ statistics ( test _ suite _ id : str, run", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##alerror \u2013 if using a client that does not support pagination abstract get _ summary _ statistics ( test _ suite _ id : str, run _ ids : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 testsuitesummary # fetch aggregate statistics of a test suite. returns averages and score distributions for runs in test suite. parameters : test _ suite _ id \u2013 uuid of the test suite run _ id \u2013 optional run id. run will be included in response regardless of page information if provided page \u2013 the page to fetch page _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "of the test suite run _ id \u2013 optional run id. run will be included in response regardless of page information if provided page \u2013 the page to fetch page _ size \u2013 page size to fetch abstract get _ test _ run ( test _ suite _ id : str, test _ run _ id : str, page : int = 1, page _ size : int = 5, sort : testcasesortenum none = none ) \u2192 paginatedrun # get a test run by id. parameters : test _ suite _ id \u2013 uuid of the test suite test _ run _ id \u2013 uuid of the test run page", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "test run by id. parameters : test _ suite _ id \u2013 uuid of the test suite test _ run _ id \u2013 uuid of the test run page \u2013 the page to fetch, pagination refers to the test cases page _ size \u2013 page size to fetch, pagination refers to the test cases sort \u2013 sort key to sort the retrieved results abstract get _ test _ suite ( test _ suite _ id : str, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuite # get a test suite by id. parameters : test _ suite _ id \u2013 the uuid", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "size : int = 5 ) \u2192 paginatedtestsuite # get a test suite by id. parameters : test _ suite _ id \u2013 the uuid of the test suite to fetch page \u2013 the page to fetch, pagination refers to the test cases page _ size \u2013 page size to fetch, pagination refers to the test cases abstract get _ test _ suites ( name : str none = none, sort : commonsortenum testsuitesortenum = testsuitesortenum. last _ runtime _ asc, scoring _ method : list [ str ] none = none, page :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##um = testsuitesortenum. last _ runtime _ asc, scoring _ method : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuites # get metadata for all test suites. parameters : name \u2013 filter test suites by name if provided sort \u2013 optional sort key. possible values are \u2018 name \u2019, \u2018 last _ run _ time \u2019, \u2018 created _ at \u2019, use \u2018 - \u2019 prefix for descending sort. defaults to \u2018 last _ run _ time \u2019 method ( scoring ) \u2013 optional filter on scoring method name,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", use \u2018 - \u2019 prefix for descending sort. defaults to \u2018 last _ run _ time \u2019 method ( scoring ) \u2013 optional filter on scoring method name, multiple names may be provided page \u2013 the page to fetch page _ size \u2013 page size to fetch next arthur _ bench. client. auth previous python api reference copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client subpackages submodules benchclient benchclient. check _ run _ exists ( ) benchclient. create _ new _ test _ run ( ) benchclient", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##dules benchclient benchclient. check _ run _ exists ( ) benchclient. create _ new _ test _ run ( ) benchclient. create _ test _ suite ( ) benchclient. delete _ test _ run ( ) benchclient. delete _ test _ suite ( ) benchclient. get _ runs _ for _ test _ suite ( ) benchclient. get _ suite _ if _ exists ( ) benchclient. get _ summary _ statistics ( ) benchclient. get _ test _ run ( ) benchclient. get _ test _ suite ( ) benchclient", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "get _ summary _ statistics ( ) benchclient. get _ test _ run ( ) benchclient. get _ test _ suite ( ) benchclient. get _ test _ suites ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html", "row": 139, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 140 text : arthur _ bench. client. http - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client. http # submodules # arthur _ bench. client. http. helper. construct _ url ( * parts : str, validate = true, default _ https = true ) \u2192 str # construct a url from various parts useful for joining pieces which may or may not have leading and / or trailing slashes. e. g. construct _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# construct a url from various parts useful for joining pieces which may or may not have leading and / or trailing slashes. e. g. construct _ url ( \u201d https : / / arthur. ai / \u201d, \u201c / api / v3 \u201d, \u201c / users \u201d ) will yield the same valid url as construct _ url ( \u201d https : / / arthur. ai \u201d, \u201c api / v3 / \u201d, \u201c users / \u201d ) : \u201c https : / / arthur. ai / api / v3 / users \u201d. parameters : validate \u2013 if true, validate that the url is", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u201c https : / / arthur. ai / api / v3 / users \u201d. parameters : validate \u2013 if true, validate that the url is valid default _ https \u2013 if true, allow urls without a scheme and use https by default parts \u2013 strings from which to construct the url returns : a fully joined url, with no trailing slash class arthur _ bench. client. http. requests. httpclient ( base _ url : str, path _ prefix : str none = none, default _ headers : dict [ str, str ] none = none, verify _ ssl", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "path _ prefix : str none = none, default _ headers : dict [ str, str ] none = none, verify _ ssl : bool = true, timeout _ sec : float = 300. 0, allow _ insecure : bool = true, header _ refresh _ func : callable [ [ ], tuple [ dict [ str, str ], timedelta ] ] none = none ) # bases : object a requests - based http client intended for interacting with json - based apis. supports response validation, retries, connection reuse, and", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bases : object a requests - based http client intended for interacting with json - based apis. supports response validation, retries, connection reuse, and multipart requests. delete ( endpoint : str, headers : dict [ str, str ] none = none, return _ raw _ response : bool = false, params : dict bytes none = none, retries : int = 0, validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http delete request", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http delete request parameters : endpoint \u2013 the specific endpoint to append to the client url headers \u2013 headers to use for this request in addition to the client default headers return _ raw _ response \u2013 if true, return the requests. response object received ; otherwise attempt to parse the response params \u2013 query parameters to add to the request retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "to add to the request retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected status code of the response to validate. if none, don \u2019 t validate returns : if return _ raw _ response is true, return the requests. response object received ; otherwise attempt to parse the response get ( endpoint : str, headers : dict [ str, str ] none = none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "received ; otherwise attempt to parse the response get ( endpoint : str, headers : dict [ str, str ] none = none, params : dict bytes none = none, return _ raw _ response : bool = false, retries : int = 0, validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http get request parameters : endpoint \u2013 the specific endpoint to append to the client url headers \u2013 headers to use for this request in addition to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "get request parameters : endpoint \u2013 the specific endpoint to append to the client url headers \u2013 headers to use for this request in addition to the client default headers params \u2013 query parameters to add to the request return _ raw _ response \u2013 if true, return the requests. response object received ; otherwise attempt to parse the response retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected status code of the response to validate. if none, allow any 2xx returns : if return _ raw _ response is true, return the requests. response object received ; otherwise attempt to parse the response raises : arthurusererror \u2013 failed due to user error arthurinternalerror \u2013 failed due to an internal error patch ( endpoint : str, json : dict list str bytes none = none, files : dict [ str,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "an internal error patch ( endpoint : str, json : dict list str bytes none = none, files : dict [ str, binaryio ] list [ tuple ] dict [ str, tuple ] none = none, headers : dict [ str, str ] none = none, params : dict bytes none = none, return _ raw _ response : bool = false, retries : int = 0, validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http post request parameters : endpoint \u2013 the specific endpoint to append to the client url headers \u2013 headers to use for this request in addition to the client default headers json \u2013 data to send as json, either a string / bytes to send directly or a dictionary / list to serialize. if files is also supplied, this should be a map from name to content, to be sent along with the files as a multipart", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "to serialize. if files is also supplied, this should be a map from name to content, to be sent along with the files as a multipart request files \u2013 a map from file names to file - like objects, to be sent as multipart / form - data params \u2013 query parameters to add to the request return _ raw _ response \u2013 if true, return the requests. response object received ; otherwise attempt to parse the response retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected status code of the response to validate. if none, don \u2019 t validate returns : if return _ raw _ response is true, return the requests. response object received ; otherwise attempt to parse the response post ( endpoint : str, json : dict list str bytes none = none, files : dict [ str, binaryio ] list", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( endpoint : str, json : dict list str bytes none = none, files : dict [ str, binaryio ] list [ tuple ] dict [ str, tuple ] none = none, headers : dict [ str, str ] none = none, params : dict bytes none = none, return _ raw _ response : bool = false, retries : int = 0, validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http post request parameters : endpoint \u2013 the specific endpoint to append to the client url headers \u2013 headers to use for this request in addition to the client default headers json \u2013 data to send as json, either a string / bytes to send directly or a dictionary / list to serialize. if files is also supplied, this should be a map from name to content, to be sent along with the files as a multipart request files \u2013 a", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "if files is also supplied, this should be a map from name to content, to be sent along with the files as a multipart request files \u2013 a map from file names to file - like objects, to be sent as multipart / form - data params \u2013 query parameters to add to the request return _ raw _ response \u2013 if true, return the requests. response object received ; otherwise attempt to parse the response retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected status code of the response to validate. if none, don \u2019 t validate returns : if return _ raw _ response is true, return the requests. response object received ; otherwise attempt to parse the response put ( endpoint : str, json : dict list str bytes none = none, files : dict [ str, binaryio ] list [ tuple ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str, json : dict list str bytes none = none, files : dict [ str, binaryio ] list [ tuple ] dict [ str, tuple ] none = none, headers : dict [ str, str ] none = none, params : dict bytes none = none, return _ raw _ response : bool = false, retries : int = 0, validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http put request parameters :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http put request parameters : endpoint \u2013 the specific endpoint to append to the client url headers \u2013 headers to use for this request in addition to the client default headers json \u2013 data to send as json, either a string / bytes to send directly or a dictionary / list to serialize. if files is also supplied, this should be a map from name to content, to be sent along with the files as a multipart request files \u2013 a map from file names", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "supplied, this should be a map from name to content, to be sent along with the files as a multipart request files \u2013 a map from file names to file - like objects, to be sent as multipart / form - data params \u2013 query parameters to add to the request return _ raw _ response \u2013 if true, return the requests. response object received ; otherwise attempt to parse the response retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected status code of the response to validate. if none, don \u2019 t validate returns : if return _ raw _ response is true, return the requests. response object received ; otherwise attempt to parse the response send ( endpoint : str, method : str ='get ', json : dict list str bytes none = none, files : dict [ str, binaryio ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": str ='get ', json : dict list str bytes none = none, files : dict [ str, binaryio ] list [ tuple ] dict [ str, tuple ] none = none, headers : dict [ str, str ] none = none, params : dict bytes none = none, return _ raw _ response : bool = false, retries : int = 0, validate _ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ response _ status : bool = true, validation _ response _ code : int none = none ) \u2192 dict list bytes bytesio response # send an http request parameters : endpoint \u2013 the specific endpoint to append to the client url method \u2013 the http method to use headers \u2013 headers to use for this request in addition to the client d efault headers : param json : data to send as json, either a string / bytes to send directly or a dictionary / list to serialize. if files is also supplied, this should be a map from name to content, to be sent", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bytes to send directly or a dictionary / list to serialize. if files is also supplied, this should be a map from name to content, to be sent along with the files as a multipart request parameters : files \u2013 a map from file names to file - like objects, to be sent as multipart / form - data params \u2013 query parameters to add to the request return _ raw _ response \u2013 if true, return the requests. response object received ; otherwise attempt to parse the response retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "attempt to parse the response retries \u2013 number of times to retry the request on failure. uses exponential backoff validate _ response _ status \u2013 if true, raise an arthurexception if the status code is not 2xx or does not match validation _ response _ code validation _ response _ code \u2013 expected status code of the response to validate. if none, allow any 2xx returns : if return _ raw _ response is true, return the requests. response object received ; otherwise attempt to parse the response raises : arthurusererror \u2013 failed due to user error arthurinternalerror \u2013 failed due to an", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "received ; otherwise attempt to parse the response raises : arthurusererror \u2013 failed due to user error arthurinternalerror \u2013 failed due to an internal error set _ path _ prefix ( path _ prefix : str ) \u2192 none # update the client \u2019 s path prefix this update the path prefix which is prepended to \u2018 endpoint \u2019 paths. arthur _ bench. client. http. validation. validate _ multistatus _ response _ and _ get _ failures ( response : response, raise _ on _ failures : bool = false ) \u2192 tuple [ list [ dict ], list [ dict ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "failures ( response : response, raise _ on _ failures : bool = false ) \u2192 tuple [ list [ dict ], list [ dict ] ] # validate a 207 multistatus response and return the failures it contains. parameters : response \u2013 requests. response object to validate, with the following body format : { \" counts \" : { \" success \" : 0, \" failure \" : 0, \" total \" : 0 }, \" results \" : [ { \" message \" : \" success \", \" status \" : 200 } ] } raise _ on _ failures \u2013 if true, raise an exception if the", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "[ { \" message \" : \" success \", \" status \" : 200 } ] } raise _ on _ failures \u2013 if true, raise an exception if the response contains any failures : return : a tuple of two lists : user - caused failures and internal failures : raises arthurinternalvalueerror : if the response does not have 207 status code, or is incorrectly formatted, or \u2018 counts \u2019 and \u2018 results \u2019 do not agree raises : responseclienterror \u2013 if raise _ on _ failures and the response contains only client errors : raises responseservererror : if raise _ on _ failures and the", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##rror \u2013 if raise _ on _ failures and the response contains only client errors : raises responseservererror : if raise _ on _ failures and the response contains server errors arthur _ bench. client. http. validation. validate _ response _ status ( response _ or _ code : response int, expected _ status _ code : int none = none, allow _ redirects : bool none = false ) \u2192 none # validate the status code of a requests. response object or ( int ) status code. : param response _ or _ code : the requests. response object or status code to validate : para", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". response object or ( int ) status code. : param response _ or _ code : the requests. response object or status code to validate : param expected _ status _ code : the expected status code to check for. if none, all codes < 300 will be valid, and 3xx codes will be subject to allow _ redirects : param allow _ redirects : if true will not raise an exception for 3xx status codes : return : none : raises internalvalueerror : if expected _ status _ code is not none and does not match the response code : raises responseservererror :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": raises internalvalueerror : if expected _ status _ code is not none and does not match the response code : raises responseservererror : if the response has a 5xx status code : raises responseclienterror : if the response has a 4xx status code : raises responseredirecterror : if the response has a 3xx status code next arthur _ bench. client. local previous arthur _ bench. client. auth copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. http submodules construct _ ur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. http submodules construct _ url ( ) httpclient httpclient. delete ( ) httpclient. get ( ) httpclient. patch ( ) httpclient. post ( ) httpclient. put ( ) httpclient. send ( ) httpclient. set _ path _ prefix ( ) validate _ multistatus _ response _ and _ get _ failures ( ) validate _ response _ status ( ) source : https : / / bench. readthedocs", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##status _ response _ and _ get _ failures ( ) validate _ response _ status ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. http. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html", "row": 140, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 141 text : arthur _ bench. client. local - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client. local # submodules # class arthur _ bench. client. local. client. localbenchclient ( root _ dir : str path none = none ) # bases : benchclient client for managing local file system test suites and runs create _ new _ test _ run ( test _ suite _ id : str, json _ body : createrun", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "managing local file system test suites and runs create _ new _ test _ run ( test _ suite _ id : str, json _ body : createrunrequest ) \u2192 createrunresponse # create a new run for a test suite. parameters : test _ suite _ id \u2013 the uuid of the test suite to log a run for json _ body \u2013 run request containing run _ metadata and scored model generations create _ test _ suite ( json _ body : testsuiterequest ) \u2192 paginatedtestsuite # create a new test suite. parameters : json _ body \u2013 test suite request object consisting", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": testsuiterequest ) \u2192 paginatedtestsuite # create a new test suite. parameters : json _ body \u2013 test suite request object consisting of test suite metadata and test cases delete _ test _ run ( test _ suite _ id : str, test _ run _ id : str ) # delete a test run from a suite. delete _ test _ suite ( test _ suite _ id : str ) # delete a test suite. all associated runs will also be deleted get _ runs _ for _ test _ suite ( test _ suite _ id : str, sort : commonsorten", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". all associated runs will also be deleted get _ runs _ for _ test _ suite ( test _ suite _ id : str, sort : commonsortenum testrunsortenum = commonsortenum. created _ at _ asc, page : int = 1, page _ size : int = 5 ) \u2192 paginatedruns # get runs for a given test suite. parameters : test _ suite _ id \u2013 the uuid of the test suite sort \u2013 optional sort key. possible values are \u2018 name \u2019, \u2018 avg _ score \u2019, and \u2018 created _ at \u2019. use \u2018 - \u2019 prefix for", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u2013 optional sort key. possible values are \u2018 name \u2019, \u2018 avg _ score \u2019, and \u2018 created _ at \u2019. use \u2018 - \u2019 prefix for descending sort. defaults to \u2018 created _ at \u2019 page \u2013 the page to fetch page _ size \u2013 page size to fetch get _ summary _ statistics ( test _ suite _ id : str, run _ ids : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 testsuitesummary # fetch aggregate statistics of a test suite. returns averages and score distributions for runs in test suite. parameters", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "int = 5 ) \u2192 testsuitesummary # fetch aggregate statistics of a test suite. returns averages and score distributions for runs in test suite. parameters : test _ suite _ id \u2013 uuid of the test suite run _ id \u2013 optional run id. run will be included in response regardless of page information if provided page \u2013 the page to fetch page _ size \u2013 page size to fetch get _ test _ run ( test _ suite _ id : str, test _ run _ id : str, page : int = 1, page _ size : int = 5, sort : testcasesortenum none = none )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "run _ id : str, page : int = 1, page _ size : int = 5, sort : testcasesortenum none = none ) \u2192 paginatedrun # get a test run by id. parameters : test _ suite _ id \u2013 uuid of the test suite test _ run _ id \u2013 uuid of the test run page \u2013 the page to fetch, pagination refers to the test cases page _ size \u2013 page size to fetch, pagination refers to the test cases sort \u2013 sort key to sort the retrieved results get _ test _ suite ( test _ suite _ id : str, page", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##gination refers to the test cases sort \u2013 sort key to sort the retrieved results get _ test _ suite ( test _ suite _ id : str, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuite # get a test suite by id. parameters : test _ suite _ id \u2013 the uuid of the test suite to fetch page \u2013 the page to fetch, pagination refers to the test cases page _ size \u2013 page size to fetch, pagination refers to the test cases get _ test _ suite _ by _ name ( test _ suite _ name : str )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "page size to fetch, pagination refers to the test cases get _ test _ suite _ by _ name ( test _ suite _ name : str ) \u2192 paginatedtestsuite # additional getter to maintain backwards compatibility with non - identified local files get _ test _ suites ( name : str none = none, sort : commonsortenum testsuitesortenum = testsuitesortenum. last _ runtime _ asc, scoring _ method : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuit", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ method : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuites # get metadata for all test suites. parameters : name \u2013 filter test suites by name if provided sort \u2013 optional sort key. possible values are \u2018 name \u2019, \u2018 last _ run _ time \u2019, \u2018 created _ at \u2019, use \u2018 - \u2019 prefix for descending sort. defaults to \u2018 last _ run _ time \u2019 method ( scoring ) \u2013 optional filter on scoring method name, multiple names may be provided page \u2013 the page to fetch page _ size \u2013 page size to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "time \u2019 method ( scoring ) \u2013 optional filter on scoring method name, multiple names may be provided page \u2013 the page to fetch page _ size \u2013 page size to fetch class arthur _ bench. client. local. client. pageinfo ( sorted _ pages : list, start : int, end : int, page : int, page _ size : int, total _ pages : int, total _ count : int ) # bases : object end : int # page : int # page _ size : int # sorted _ pages : list # start : int # total _ count : int # total _ pages : int # next arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "page _ size : int # sorted _ pages : list # start : int # total _ count : int # total _ pages : int # next arthur _ bench. client. rest previous arthur _ bench. client. http copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. local submodules localbenchclient localbenchclient. create _ new _ test _ run ( ) localbenchclient. create _ test _ suite ( ) localbenchclient. delete _ test _ run ( ) localbenchcl", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ") localbenchclient. create _ test _ suite ( ) localbenchclient. delete _ test _ run ( ) localbenchclient. delete _ test _ suite ( ) localbenchclient. get _ runs _ for _ test _ suite ( ) localbenchclient. get _ summary _ statistics ( ) localbenchclient. get _ test _ run ( ) localbenchclient. get _ test _ suite ( ) localbenchclient. get _ test _ suite _ by _ name ( ) localbenchclient. get _ test _ suites ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( ) localbenchclient. get _ test _ suite _ by _ name ( ) localbenchclient. get _ test _ suites ( ) pageinfo pageinfo. end pageinfo. page pageinfo. page _ size pageinfo. sorted _ pages pageinfo. start pageinfo. total _ count pageinfo. total _ pages source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. local. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html", "row": 141, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 142 text : arthur _ bench. client. rest. admin - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client. rest. admin # submodules # class arthur _ bench. client. rest. admin. client. arthuradminclient ( http _ client : httpclient ) # bases : object a python client to interact with the arthur admin api authenticate ( ) \u2192 authenticationinfo # returns authentication info for the calling, token - bearing", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": object a python client to interact with the arthur admin api authenticate ( ) \u2192 authenticationinfo # returns authentication info for the calling, token - bearing user get _ current _ user ( ) \u2192 userresponse # returns the currently authenticated user login ( json _ body : loginrequest ) \u2192 tuple [ user, requestscookiejar ] # if the login attempt is successful, the user will be returned in the response body and an httponly, set - cookie \u201c authorization \u201d header will be returned that contains a jwt to be used in subsequent requests to the api in either the", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "an httponly, set - cookie \u201c authorization \u201d header will be returned that contains a jwt to be used in subsequent requests to the api in either the \u201c authorization \u201d or cookie header parameters : json _ body \u2013 class arthur _ bench. client. rest. admin. models. authenticationinfo ( *, organization _ ids : list [ str ], issuer : str, external _ user _ id : str none = none, internal _ user _ id : str none = none, service _ account _ id : str none = none, username : str, first _ name :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "user _ id : str none = none, service _ account _ id : str none = none, username : str, first _ name : str, last _ name : str, email : str, roles : list [ str ] ) # bases : basemodel email : str # the email of the arthur authenticated user or the email of the external idp user ifthe idp is configured with that claim. for arthur service accounts, this will be empty. external _ user _ id : str none # an identifier for an external - idp token bearer. populated if", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "accounts, this will be empty. external _ user _ id : str none # an identifier for an external - idp token bearer. populated if this user \u2019 s token came from an idp and the idp configuration specified an oidc. customclaimnames that mapped userid to a claim. first _ name : str # the first name of the arthur authenticated user, or the first name claim if the external idp is configured with one. internal _ user _ id : str none # an identifier for an arthur - internal user. populated for arthur - authenticated users with user token", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". internal _ user _ id : str none # an identifier for an arthur - internal user. populated for arthur - authenticated users with user tokens. issuer : str # the identifier of the idp managing this user. last _ name : str # the last name of the arthur authenticated user, or the last name claim if the external idp is configured with one. organization _ ids : list [ str ] # a list of organization ids. roles : list [ str ] # the list of roles that this user has. for arthur tokens, there will always be", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "list of organization ids. roles : list [ str ] # the list of roles that this user has. for arthur tokens, there will always be onerole in this array ; however, there can be more than one for external providers. service _ account _ id : str none # an identifier for an arthur service account. populated for arthur - authenticated service account tokens. username : str # either the arthur username or the username specified by an external idp. this will be set to arthur. serviceaccountname for service account tokens. class arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##name specified by an external idp. this will be set to arthur. serviceaccountname for service account tokens. class arthur _ bench. client. rest. admin. models. loginrequest ( *, login : str, password : str ) # bases : basemodel login : str # either an email or a username password : str # class arthur _ bench. client. rest. admin. models. user ( *, id : str, first _ name : str none = none, last _ name : str none = none, email : st", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "user ( *, id : str, first _ name : str none = none, last _ name : str none = none, email : str, username : str none = none, roles : list [ str ] none = none, alert _ notifications _ enabled : bool none = none, show _ intro _ sequence : bool none = none, help _ mode _ enabled : bool none = none, created _ at : datetime none = none ) # bases : basemodel alert _ notifications _ enabled : bool none # whether or not the user will receive email notifications", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##time none = none ) # bases : basemodel alert _ notifications _ enabled : bool none # whether or not the user will receive email notifications when alerts are triggered, defaults to \u2018 false \u2019 created _ at : datetime none # utc timestamp of when the user was created email : str # the user \u2019 s email first _ name : str none # the user \u2019 s first name help _ mode _ enabled : bool none # used by the arthur dashboard to determine whether or not to show dashboard tooltips id : str # the unique id of the user last _ name : st", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "used by the arthur dashboard to determine whether or not to show dashboard tooltips id : str # the unique id of the user last _ name : str none # the user \u2019 s last name roles : list [ str ] none # the user \u2019 s roles in the current organization. show _ intro _ sequence : bool none # used by the arthur dashboard to determine whether or not to show the user an intro sequence upon login username : str none # the username the user can use to login class arthur _ bench. client. rest. admin. models. usercontext ( *, name", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# the username the user can use to login class arthur _ bench. client. rest. admin. models. usercontext ( *, name : str none = none, id : str none = none ) # bases : basemodel id : str none # uuid of the context. name : str none # name of the context. class arthur _ bench. client. rest. admin. models. userresponse ( *, id : str none = none, organization _ id : str, organization _ name : str none = none, first _ name : str", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", id : str none = none, organization _ id : str, organization _ name : str none = none, first _ name : str none = none, last _ name : str none = none, email : str none = none, username : str none = none, roles : list [ str ] none = none, alert _ notifications _ enabled : bool none = none, show _ intro _ sequence : bool none = none, help _ mode _ enabled : bool none = none, plan : str none = none, created _ at : datetime none = none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "= none, help _ mode _ enabled : bool none = none, plan : str none = none, created _ at : datetime none = none, contexts : list [ usercontext ] none = none ) # bases : basemodel represents an application user, if the client is using a service token then onlyorganization _ id and roles will be populated in the object alert _ notifications _ enabled : bool none # whether or not the user will receive email notifications when alerts are triggered, defaults to \u2018 false \u2019 contexts : list [ usercontext ] none # contexts that the user has permission", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "receive email notifications when alerts are triggered, defaults to \u2018 false \u2019 contexts : list [ usercontext ] none # contexts that the user has permissions in. created _ at : datetime none # utc timestamp of when the user was created email : str none # the user \u2019 s email first _ name : str none # the user \u2019 s first name help _ mode _ enabled : bool none # used by the arthur dashboard to determine whether or not to show dashboard tooltips id : str none # the unique id of the user last _ name : str none # the user \u2019 s", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "or not to show dashboard tooltips id : str none # the unique id of the user last _ name : str none # the user \u2019 s last name organization _ id : str # the id of the users current context organization _ name : str none # the name of the users current context plan : str none # string representation of what plan the org of the returned user is associated with ( ie. self - service or paidsaas ) roles : list [ str ] none # the user \u2019 s roles show _ intro _ sequence : bool none # used by the arthur dashboard to determine whether the user", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "list [ str ] none # the user \u2019 s roles show _ intro _ sequence : bool none # used by the arthur dashboard to determine whether the user should be shown the introsequence upon login username : str none # the username the user can use to login next arthur _ bench. client. rest. bench previous arthur _ bench. client. rest copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. rest. admin submodules arthuradminclient arthuradminclient. authenticate (", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##o on this page arthur _ bench. client. rest. admin submodules arthuradminclient arthuradminclient. authenticate ( ) arthuradminclient. get _ current _ user ( ) arthuradminclient. login ( ) authenticationinfo authenticationinfo. email authenticationinfo. external _ user _ id authenticationinfo. first _ name authenticationinfo. internal _ user _ id authenticationinfo. issuer authenticationinfo. last _ name authenticationinfo. organization _ ids authenticationinfo. roles authenticationinfo. service _ account _ id authenticationinfo.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##info. last _ name authenticationinfo. organization _ ids authenticationinfo. roles authenticationinfo. service _ account _ id authenticationinfo. username loginrequest loginrequest. login loginrequest. password user user. alert _ notifications _ enabled user. created _ at user. email user. first _ name user. help _ mode _ enabled user. id user. last _ name user. roles user. show _ intro _ sequence user. username usercontext usercontext. id usercontext. name userresponse userresponse. alert _ notifications", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ sequence user. username usercontext usercontext. id usercontext. name userresponse userresponse. alert _ notifications _ enabled userresponse. contexts userresponse. created _ at userresponse. email userresponse. first _ name userresponse. help _ mode _ enabled userresponse. id userresponse. last _ name userresponse. organization _ id userresponse. organization _ name userresponse. plan userresponse. roles userresponse. show _ intro _ sequence userresponse", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ponse. organization _ name userresponse. plan userresponse. roles userresponse. show _ intro _ sequence userresponse. username source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. rest. admin. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html", "row": 142, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 143 text : arthur _ bench. client. rest. bench - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. au", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##emetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client. rest. bench # submodules # class arthur _ bench. client. rest. bench. client. arthurbenchclient ( http _ client : httpclient ) # bases : benchclient a python client to interact with the arthur bench api create _ new _ test _ run ( test _ suite _ id : str, json _ body", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ient a python client to interact with the arthur bench api create _ new _ test _ run ( test _ suite _ id : str, json _ body : createrunrequest ) \u2192 createrunresponse # creates a new test run with model version / associated metadata parameters : test _ suite _ id \u2013 json _ body \u2013 create _ test _ suite ( json _ body : testsuiterequest ) \u2192 paginatedtestsuite # creates a new test suite from reference data using specified scoring _ method for scoring parameters : json _ body \u2013 delete _ test _ run ( test _ suite _ id :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "test suite from reference data using specified scoring _ method for scoring parameters : json _ body \u2013 delete _ test _ run ( test _ suite _ id : str, test _ run _ id : str ) # deletes a test run is idempotent. parameters : test _ suite _ id \u2013 test _ run _ id \u2013 delete _ test _ suite ( test _ suite _ id : str ) # deletes test suite is idempotent. parameters : test _ suite _ id \u2013 get _ runs _ for _ test _ suite ( test _ suite _ id : str, sort :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##potent. parameters : test _ suite _ id \u2013 get _ runs _ for _ test _ suite ( test _ suite _ id : str, sort : commonsortenum testrunsortenum = commonsortenum. created _ at _ asc, page : int = 1, page _ size : int = 5 ) \u2192 paginatedruns # get runs for a particular test suite ( identified by test _ suite _ id ) parameters : test _ suite _ id \u2013 sort \u2013 get _ summary _ statistics ( test _ suite _ id : str, run _ ids : list [ str ] none =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ id \u2013 sort \u2013 get _ summary _ statistics ( test _ suite _ id : str, run _ ids : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 testsuitesummary # get paginated summary statistics of a test suite defaults to page size of 5. parameters : test _ suite _ id \u2013 run _ id \u2013 page \u2013 page _ size \u2013 get _ test _ run ( test _ suite _ id : str, test _ run _ id : str, page : int = 1, page _ size : int =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "run ( test _ suite _ id : str, test _ run _ id : str, page : int = 1, page _ size : int = 5, sort : testcasesortenum none = testcasesortenum. score _ asc ) \u2192 paginatedrun # get a test run with input, output, and reference data parameters : test _ suite _ id \u2013 test _ run _ id \u2013 page \u2013 page _ size \u2013 sort \u2013 sort key to sort the retrieved results get _ test _ suite ( test _ suite _ id : str, page : int = 1, page _ size : int =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "to sort the retrieved results get _ test _ suite ( test _ suite _ id : str, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuite # get reference data for an existing test suite parameters : test _ suite _ id \u2013 get _ test _ suites ( name : str none = none, sort : commonsortenum testsuitesortenum = testsuitesortenum. last _ runtime _ asc, scoring _ method : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ asc, scoring _ method : list [ str ] none = none, page : int = 1, page _ size : int = 5 ) \u2192 paginatedtestsuites # gets test suites sort by latest run by default. if name query parameter is provided, filter on test suite name. param name : param sort : param scoring _ method : score _ hallucination ( json _ body : hallucinationscorerequest ) \u2192 hallucinationscoreresponse # next arthur _ bench. exceptions previous arthur _ bench. client. rest. admin copyright \u00a9 2023, arthur made with", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ucinationscoreresponse # next arthur _ bench. exceptions previous arthur _ bench. client. rest. admin copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. rest. bench submodules arthurbenchclient arthurbenchclient. create _ new _ test _ run ( ) arthurbenchclient. create _ test _ suite ( ) arthurbenchclient. delete _ test _ run ( ) arthurbenchclient. delete _ test _ suite ( ) arthurbenchclient. get _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ient. delete _ test _ run ( ) arthurbenchclient. delete _ test _ suite ( ) arthurbenchclient. get _ runs _ for _ test _ suite ( ) arthurbenchclient. get _ summary _ statistics ( ) arthurbenchclient. get _ test _ run ( ) arthurbenchclient. get _ test _ suite ( ) arthurbenchclient. get _ test _ suites ( ) arthurbenchclient. score _ hallucination ( ) source : https : / / bench. readthedocs. io / en / latest / sdk /", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##client. score _ hallucination ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. rest. bench. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html", "row": 143, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 144 text : arthur _ bench. client. rest - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. client. rest # subpackages # arthur _ bench. client. rest. admin submodules arthuradminclient arthuradminclient. authenticate ( ) arthuradminclient. get _ current _ user ( ) arthuradminclient. login ( ) authenticationinfo authenticationinfo. email authenticationinfo. external _ user _ id", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "current _ user ( ) arthuradminclient. login ( ) authenticationinfo authenticationinfo. email authenticationinfo. external _ user _ id authenticationinfo. first _ name authenticationinfo. internal _ user _ id authenticationinfo. issuer authenticationinfo. last _ name authenticationinfo. organization _ ids authenticationinfo. roles authenticationinfo. service _ account _ id authenticationinfo. username loginrequest loginrequest. login loginrequest. password user user. alert _ notifications _ enabled user. created _ at user. email user. first _ name", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##quest. login loginrequest. password user user. alert _ notifications _ enabled user. created _ at user. email user. first _ name user. help _ mode _ enabled user. id user. last _ name user. roles user. show _ intro _ sequence user. username usercontext usercontext. id usercontext. name userresponse userresponse. alert _ notifications _ enabled userresponse. contexts userresponse. created _ at userresponse. email userresponse. first _ name userresponse. help _ mode _ enabled user", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##response. created _ at userresponse. email userresponse. first _ name userresponse. help _ mode _ enabled userresponse. id userresponse. last _ name userresponse. organization _ id userresponse. organization _ name userresponse. plan userresponse. roles userresponse. show _ intro _ sequence userresponse. username arthur _ bench. client. rest. bench submodules arthurbenchclient arthurbenchclient. create _ new _ test _ run ( ) arthurbenchclient", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "rest. bench submodules arthurbenchclient arthurbenchclient. create _ new _ test _ run ( ) arthurbenchclient. create _ test _ suite ( ) arthurbenchclient. delete _ test _ run ( ) arthurbenchclient. delete _ test _ suite ( ) arthurbenchclient. get _ runs _ for _ test _ suite ( ) arthurbenchclient. get _ summary _ statistics ( ) arthurbenchclient. get _ test _ run ( ) arthurbenchclient. get _ test _ suite ( ) arthurbenchcl", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( ) arthurbenchclient. get _ test _ run ( ) arthurbenchclient. get _ test _ suite ( ) arthurbenchclient. get _ test _ suites ( ) arthurbenchclient. score _ hallucination ( ) submodules # class arthur _ bench. client. rest. client. arthurclient ( url : str none = none, login : str none = none, password : str none = none, api _ key : str none = none, organization _ id : str none = none, verify _ ssl : bool none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "none = none, api _ key : str none = none, organization _ id : str none = none, verify _ ssl : bool none = none, allow _ insecure : bool = false, offline : bool = false ) # bases : object next arthur _ bench. client. rest. admin previous arthur _ bench. client. local copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. client. rest subpackages submodules arthurclient source : https : / / bench. readthedocs.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "this page arthur _ bench. client. rest subpackages submodules arthurclient source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. client. rest. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html", "row": 144, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 145 text : arthur _ bench. exceptions - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##toggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. exceptions # submodules # exception arthur _ bench. exceptions. exceptions. arthurerror # bases : exception base error for arthur sdk. this class should not be used directly, arthur exceptions should inherit from either arthurusererror or arthurinternalerror. exception arthur _ bench. exceptions. exceptions. arthurinternalerror # bases : arthurerror exception raised", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ror or arthurinternalerror. exception arthur _ bench. exceptions. exceptions. arthurinternalerror # bases : arthurerror exception raised when user input is correct but an error occurs. can be used directly but children are preferred. exception arthur _ bench. exceptions. exceptions. arthurusererror # bases : arthurerror exception raised due to incorrect user input to the arthur sdk. can be used directly but children are preferred. exception arthur _ bench. exceptions. exceptions. expectedparameternotfounderror # bases : arthurinternalerror exception raised when a field or property should be available from", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". exceptions. exceptions. expectedparameternotfounderror # bases : arthurinternalerror exception raised when a field or property should be available from arthur but is unexpectedly missing. exception arthur _ bench. exceptions. exceptions. forbiddenerror # bases : responseclienterror exception raised when a 403 forbidden response is received from the api. exception arthur _ bench. exceptions. exceptions. internaltypeerror # bases : arthurinternalerror, typeerror exception raised when a value is unexpected. exception arthur _ bench. exceptions. exceptions. internalvalueerror # bases : arthurinternalerror", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ror exception raised when a value is unexpected. exception arthur _ bench. exceptions. exceptions. internalvalueerror # bases : arthurinternalerror, valueerror exception raised when a value is unexpected. exception arthur _ bench. exceptions. exceptions. methodnotapplicableerror # bases : arthurusererror exception raised when the method called is not valid for the resource. exception arthur _ bench. exceptions. exceptions. missingparametererror # bases : arthurusererror exception raised when parameters supplied to the arthur sdk are missing. exception arthur _ bench. exceptions. exceptions. notfounderror #", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": arthurusererror exception raised when parameters supplied to the arthur sdk are missing. exception arthur _ bench. exceptions. exceptions. notfounderror # bases : responseclienterror exception raised when a 404 not found response is received from the api. exception arthur _ bench. exceptions. exceptions. paymentrequirederror # bases : responseclienterror exception raised when a 402 response is received from the api due to a user trying to access features not available in their plan. exception arthur _ bench. exceptions. exceptions. responseclienterror # bases : arthurusererror exception raised when a 4xx", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "available in their plan. exception arthur _ bench. exceptions. exceptions. responseclienterror # bases : arthurusererror exception raised when a 4xx response is received from the api. exception arthur _ bench. exceptions. exceptions. responseredirecterror # bases : arthurinternalerror exception raised when a 3xx response is unexpectedly received from the api. exception arthur _ bench. exceptions. exceptions. responseservererror # bases : arthurinternalerror exception raised when a 5xx response is received from the api. exception arthur _ bench. exceptions. exceptions. unauthorizederror # bases : response", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##alerror exception raised when a 5xx response is received from the api. exception arthur _ bench. exceptions. exceptions. unauthorizederror # bases : responseclienterror exception raised when a 401 unauthorized response is received from the api. exception arthur _ bench. exceptions. exceptions. usertypeerror # bases : arthurusererror, typeerror exception raised when a user supplies an argument of the incorrect type to the arthur sdk. exception arthur _ bench. exceptions. exceptions. uservalueerror # bases : arthurusererror, valueerror exception raised when a user supplies an invalid value to the arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "exceptions. exceptions. uservalueerror # bases : arthurusererror, valueerror exception raised when a user supplies an invalid value to the arthur sdk. arthur _ bench. exceptions. exceptions. arthur _ excepted ( message = none ) # decorator to wrap user - facing arthur functions with exception handling that describes to the user whether the error is their fault or is our fault and should be reported. : param message : an optional message to prefix the error with, should describe the failure e. g. \u201c failed to send inferences \u201d or \u201c an error occurred while creating the model. \u201d : return :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "error with, should describe the failure e. g. \u201c failed to send inferences \u201d or \u201c an error occurred while creating the model. \u201d : return : the decorator function next arthur _ bench. models previous arthur _ bench. client. rest. bench copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. exceptions submodules arthurerror arthurinternalerror arthurusererror expectedparameternotfounderror forbiddenerror internaltypeerror internalvalueerror methodnotapplicableerror missingparametererror not", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "expectedparameternotfounderror forbiddenerror internaltypeerror internalvalueerror methodnotapplicableerror missingparametererror notfounderror paymentrequirederror responseclienterror responseredirecterror responseservererror unauthorizederror usertypeerror uservalueerror arthur _ excepted ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. exceptions. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html", "row": 145, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 146 text : arthur _ bench. models - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##toggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. models # submodules # class arthur _ bench. models. models. categoricalhistogramitem ( *, count : int, category : category ) # bases : basemodel category : category # count : int # class arthur _ bench. models. models. category ( *, name : str, description : str none = none ) # bases : basemo", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# class arthur _ bench. models. models. category ( *, name : str, description : str none = none ) # bases : basemodel description : str none # name : str # class arthur _ bench. models. models. commonsortenum ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum created _ at _ asc ='created _ at'# created _ at _ desc ='- created _ at'# name _ asc =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "created _ at _ asc ='created _ at'# created _ at _ desc ='- created _ at'# name _ asc ='name'# name _ desc ='- name'# class arthur _ bench. models. models. createrunrequest ( *, name : str, test _ case _ outputs : list [ testcaseoutput ], description : str none = none, model _ name : str none = none, foundation _ model : str none = none, prompt _ template : str none = none, model _ version : str none = none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "= none, foundation _ model : str none = none, prompt _ template : str none = none, model _ version : str none = none ) # bases : basemodel class config # bases : object allow _ population _ by _ field _ name = true # classmethod consistent _ categories ( v ) # description : str none # optional description of the run foundation _ model : str none # optional foundation model name identifiying the pretrained model used to generate outputs model _ name : str none # optional model name identifying the model used to generate outputs model _ version", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##iying the pretrained model used to generate outputs model _ name : str none # optional model name identifying the model used to generate outputs model _ version : str none # optional model version identifying the version of the model used to generate outputs name : str # name identifier of the run prompt _ template : str none # optional prompt template name identifying the global prompt used to generate outputs test _ cases : list [ testcaseoutput ] # list of outputs and scores for all cases in the test suite class arthur _ bench. models. models. createrunresponse ( *, id : uuid )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "outputs and scores for all cases in the test suite class arthur _ bench. models. models. createrunresponse ( *, id : uuid ) # bases : basemodel id : uuid # class arthur _ bench. models. models. histogramitem ( *, count : int, low : float, high : float ) # bases : basemodel boundaries and count for a single bucket of a run histogram count : int # high : float # low : float # class arthur _ bench. models. models. paginatedrun ( *, id : uuid, name : str", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": float # low : float # class arthur _ bench. models. models. paginatedrun ( *, id : uuid, name : str, test _ suite _ id : uuid, test _ case _ runs : list [ runresult ], updated _ at : datetime, created _ at : datetime, page : int none = none, page _ size : int none = none, total _ pages : int none = none, total _ count : int none = none ) # bases : basemodel paginated list of prompts, reference outputs, model outputs, and scores for a particular", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "count : int none = none ) # bases : basemodel paginated list of prompts, reference outputs, model outputs, and scores for a particular run. class config # bases : object allow _ population _ by _ field _ name = true # created _ at : datetime # id : uuid # name : str # page : int none # page _ size : int none # test _ cases : list [ runresult ] # test _ suite _ id : uuid # total _ count : int none # total _ pages : int none # updated _ at : datetime # class arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "suite _ id : uuid # total _ count : int none # total _ pages : int none # updated _ at : datetime # class arthur _ bench. models. models. paginatedruns ( *, test _ runs : list [ testrunmetadata ], page : int, page _ size : int, total _ pages : int, total _ count : int ) # bases : basemodel paginated list of runs for a test suite. page : int # page _ size : int # test _ runs : list [ testrunmetadata ] # total _ count : int # total _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". page : int # page _ size : int # test _ runs : list [ testrunmetadata ] # total _ count : int # total _ pages : int # class arthur _ bench. models. models. paginatedtestsuite ( *, id : uuid, name : str, scoring _ method : scoringmethod, test _ cases : list [ testcaseresponse ], created _ at : datetime, updated _ at : datetime, description : str none = none, last _ run _ time : datetime none = none, num _ runs : int = 0", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": datetime, description : str none = none, last _ run _ time : datetime none = none, num _ runs : int = 0, page : int none = none, page _ size : int none = none, total _ pages : int none = none, total _ count : int none = none ) # bases : basemodel test suite and optional page information created _ at : datetime # description : str none # id : uuid # last _ run _ time : datetime none # name : str # num _ runs : int # page : int none # page _ size :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# last _ run _ time : datetime none # name : str # num _ runs : int # page : int none # page _ size : int none # scoring _ method : scoringmethod # test _ cases : list [ testcaseresponse ] # total _ count : int none # total _ pages : int none # updated _ at : datetime # class arthur _ bench. models. models. paginatedtestsuites ( *, test _ suites : list [ testsuitemetadata ], page : int, page _ size : int, total _ pages : int, total _ count", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "test _ suites : list [ testsuitemetadata ], page : int, page _ size : int, total _ pages : int, total _ count : int ) # bases : basemodel paginated list of test suites. page : int # page _ size : int # test _ suites : list [ testsuitemetadata ] # total _ count : int # total _ pages : int # class arthur _ bench. models. models. runresult ( *, id : uuid, output : str, score : float, input : str none = none, reference _ output : str none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( *, id : uuid, output : str, score : float, input : str none = none, reference _ output : str none = none, score _ result : scoreresult ) # bases : basemodel id : uuid # input : str none # output : str # reference _ output : str none # score : float # score _ result : scoreresult # classmethod score _ result _ backwards _ compatible ( values ) # class arthur _ bench. models. models. scoreresult ( *, score : float none = none, category : category none = none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( values ) # class arthur _ bench. models. models. scoreresult ( *, score : float none = none, category : category none = none ) # bases : basemodel category : category none # classmethod contains _ score ( values ) # score : float none # class arthur _ bench. models. models. scoreroutputtype ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum indicates the output type of the scorer categorical ='categorical'# continuous", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "start = 1, boundary = none ) # bases : str, enum indicates the output type of the scorer categorical ='categorical'# continuous ='continuous'# class arthur _ bench. models. models. scoringmethod ( *, name : str, type : scoringmethodtype, config : dict = { }, output _ type : scoreroutputtype = scoreroutputtype. continuous, categories : list [ category ] none = none ) # bases : basemodel scoring method configuration categories : list [ category ] none # valid categories returned by the scorer. only valid", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "category ] none = none ) # bases : basemodel scoring method configuration categories : list [ category ] none # valid categories returned by the scorer. only valid if categories is true. config : dict # configuration as used by the scorer to _ dict and from _ dict methods name : str # name of the scorer output _ type : scoreroutputtype # whether the scoring method returns categorical scores classmethod scoring _ method _ categorical _ defined ( values ) # type : scoringmethodtype # whether the scoring method was bench default or custom implementation class arthur _ bench. models. models", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ defined ( values ) # type : scoringmethodtype # whether the scoring method was bench default or custom implementation class arthur _ bench. models. models. scoringmethodtype ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum indicates whether the scoring method was provided by the package or a custom implementation builtin ='built _ in'# custom ='custom'# class arthur _ bench. models. models. summaryitem ( *, id : uuid, name :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ in'# custom ='custom'# class arthur _ bench. models. models. summaryitem ( *, id : uuid, name : str, avg _ score : float, histogram : list [ histogramitem categoricalhistogramitem ] ) # bases : basemodel aggregate statistics for a single run : average score and score distribution avg _ score : float # classmethod either _ continuous _ or _ categorical ( v ) # validate that the items in the histogram list are all containing low / high floats or are all containing a category his", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ categorical ( v ) # validate that the items in the histogram list are all containing low / high floats or are all containing a category histogram : list [ histogramitem categoricalhistogramitem ] # id : uuid # name : str # class arthur _ bench. models. models. testcaseoutput ( *, id : uuid, output : str, score : float none = none, score _ result : scoreresult ) # bases : basemodel a generated output, score pair id : uuid # optional unique identifier for this test case", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "result : scoreresult ) # bases : basemodel a generated output, score pair id : uuid # optional unique identifier for this test case of the suite and run output : str # generated output for test case score : float none # score assigned to output. this field is decprecated, used score _ result instead score _ result : scoreresult # score information about output. contains float score and / or category description classmethod score _ result _ backwards _ compatible ( values ) # class arthur _ bench. models. models. testcaserequest ( *, input : str, reference _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "result _ backwards _ compatible ( values ) # class arthur _ bench. models. models. testcaserequest ( *, input : str, reference _ output : str none = none ) # bases : basemodel an input, reference output pair. input : str # input to the test case. does not include the prompt template. reference _ output : str none # reference or \u201c golden \u201d output for the given input. class arthur _ bench. models. models. testcaseresponse ( *, id : uuid, input : str, reference _ output : str none = none ) #", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". models. testcaseresponse ( *, id : uuid, input : str, reference _ output : str none = none ) # bases : basemodel id : uuid # input : str # input to the test case. does not include the prompt template. reference _ output : str none # reference or \u201c golden \u201d output for the given input. class arthur _ bench. models. models. testcasesortenum ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : st", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum score _ asc ='score'# score _ desc ='- score'# class arthur _ bench. models. models. testrunmetadata ( *, id : uuid, name : str, created _ at : datetime, updated _ at : datetime, avg _ score : float none = none, model _ version : str none = none, prompt _ template : str none = none ) # bases", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "avg _ score : float none = none, model _ version : str none = none, prompt _ template : str none = none ) # bases : basemodel avg _ score : float none # created _ at : datetime # id : uuid # model _ version : str none # name : str # prompt _ template : str none # updated _ at : datetime # class arthur _ bench. models. models. testrunsortenum ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##tenum ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum avg _ score _ asc ='avg _ score'# avg _ score _ desc ='- avg _ score'# class arthur _ bench. models. models. testsuitemetadata ( *, id : uuid, name : str, scoring _ method : scoringmethod, last _ run _ time : datetime none = none, description : str none = none,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str, scoring _ method : scoringmethod, last _ run _ time : datetime none = none, description : str none = none, created _ at : datetime none = none, updated _ at : datetime none = none ) # bases : basemodel created _ at : datetime none # description : str none # id : uuid # last _ run _ time : datetime none # name : str # scoring _ method : scoringmethod # updated _ at : datetime none # class arthur _ bench. models. models. testsuiterequest ( *, name : st", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "scoringmethod # updated _ at : datetime none # class arthur _ bench. models. models. testsuiterequest ( *, name : str, description : str none = none, scoring _ method : scoringmethod, test _ cases : constrainedlistvalue [ testcaserequest ] ) # bases : basemodel test case data and metadata for the test suite. description : str none # optional description of the test suite name : str # name of the test suite classmethod null _ reference _ outputs _ all _ or _ none ( v ) # validate that all or", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str # name of the test suite classmethod null _ reference _ outputs _ all _ or _ none ( v ) # validate that all or none of test case reference outputs are null scoring _ method : scoringmethod # scoring configuration to use as criteria for the test suite classmethod scoring _ method _ backwards _ compatible ( v ) # test _ cases : list [ testcaserequest ] # list of input texts and optional reference outputs to consistently score model generations against class arthur _ bench. models. models. testsuitesortenum ( value, names = none, *, module = none,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "score model generations against class arthur _ bench. models. models. testsuitesortenum ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum last _ runtime _ asc ='last _ run _ time'# last _ runtime _ desc ='- last _ run _ time'# class arthur _ bench. models. models. testsuitesummary ( *, summary : list [ summaryitem ], page : int, page _ size : int,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "models. models. testsuitesummary ( *, summary : list [ summaryitem ], page : int, page _ size : int, total _ pages : int, total _ count : int, num _ test _ cases : int, categorical : bool = false ) # bases : basemodel aggregate descriptions of runs of a test suite. provides averages and score distributions categorical : bool # num _ test _ cases : int # page : int # page _ size : int # summary : list [ summaryitem ] # total _ count : int # total _ pages : int # class", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "page : int # page _ size : int # summary : list [ summaryitem ] # total _ count : int # total _ pages : int # class arthur _ bench. models. scoring. hallucinationscorerequest ( *, response : str, context : str ) # bases : basemodel request for hallucination classification context : str # context with which to determine if the model generated response is supported response : str # model generated response class arthur _ bench. models. scoring. hallucinationscoreresponse ( *, hallucination : bool, reason : str ) #", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "class arthur _ bench. models. scoring. hallucinationscoreresponse ( *, hallucination : bool, reason : str ) # bases : basemodel hallucination classification hallucination : bool # true if hallucination, false otherwise reason : str # justification for the hallucination classification next arthur _ bench. run previous arthur _ bench. exceptions copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. models submodules categoricalhistogramitem categoricalhistogramitem. category cat", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "s furo on this page arthur _ bench. models submodules categoricalhistogramitem categoricalhistogramitem. category categoricalhistogramitem. count category category. description category. name commonsortenum commonsortenum. created _ at _ asc commonsortenum. created _ at _ desc commonsortenum. name _ asc commonsortenum. name _ desc createrunrequest createrunrequest. config createrunrequest. config. allow _ population _ by _ field _ name createrunrequest. consistent _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##runrequest. config createrunrequest. config. allow _ population _ by _ field _ name createrunrequest. consistent _ categories ( ) createrunrequest. description createrunrequest. foundation _ model createrunrequest. model _ name createrunrequest. model _ version createrunrequest. name createrunrequest. prompt _ template createrunrequest. test _ cases createrunresponse createrunresponse. id histogramitem histogramitem. count histogramitem. high histogramitem. low paginated", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##se. id histogramitem histogramitem. count histogramitem. high histogramitem. low paginatedrun paginatedrun. config paginatedrun. config. allow _ population _ by _ field _ name paginatedrun. created _ at paginatedrun. id paginatedrun. name paginatedrun. page paginatedrun. page _ size paginatedrun. test _ cases paginatedrun. test _ suite _ id paginatedrun. total _ count paginatedrun. total _ pages pa", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##run. test _ cases paginatedrun. test _ suite _ id paginatedrun. total _ count paginatedrun. total _ pages paginatedrun. updated _ at paginatedruns paginatedruns. page paginatedruns. page _ size paginatedruns. test _ runs paginatedruns. total _ count paginatedruns. total _ pages paginatedtestsuite paginatedtestsuite. created _ at paginatedtestsuite. description paginatedtestsuite. id paginatedtestsuite. last _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##testsuite. created _ at paginatedtestsuite. description paginatedtestsuite. id paginatedtestsuite. last _ run _ time paginatedtestsuite. name paginatedtestsuite. num _ runs paginatedtestsuite. page paginatedtestsuite. page _ size paginatedtestsuite. scoring _ method paginatedtestsuite. test _ cases paginatedtestsuite. total _ count paginatedtestsuite. total _ pages paginatedtestsuite. updated _ at paginatedtestsuites", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##suite. total _ count paginatedtestsuite. total _ pages paginatedtestsuite. updated _ at paginatedtestsuites paginatedtestsuites. page paginatedtestsuites. page _ size paginatedtestsuites. test _ suites paginatedtestsuites. total _ count paginatedtestsuites. total _ pages runresult runresult. id runresult. input runresult. output runresult. reference _ output runresult. score runresult. score _ result runresult. score _ result _ backwards _ compatible", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". output runresult. reference _ output runresult. score runresult. score _ result runresult. score _ result _ backwards _ compatible ( ) scoreresult scoreresult. category scoreresult. contains _ score ( ) scoreresult. score scoreroutputtype scoreroutputtype. categorical scoreroutputtype. continuous scoringmethod scoringmethod. categories scoringmethod. config scoringmethod. name scoringmethod. output _ type scoringmethod. scoring _ method _ categorical _ defined ( ) scoringmethod. type scoringmet", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "name scoringmethod. output _ type scoringmethod. scoring _ method _ categorical _ defined ( ) scoringmethod. type scoringmethodtype scoringmethodtype. builtin scoringmethodtype. custom summaryitem summaryitem. avg _ score summaryitem. either _ continuous _ or _ categorical ( ) summaryitem. histogram summaryitem. id summaryitem. name testcaseoutput testcaseoutput. id testcaseoutput. output testcaseoutput. score testcaseoutput. score _ result testcaseoutput. score", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "testcaseoutput. id testcaseoutput. output testcaseoutput. score testcaseoutput. score _ result testcaseoutput. score _ result _ backwards _ compatible ( ) testcaserequest testcaserequest. input testcaserequest. reference _ output testcaseresponse testcaseresponse. id testcaseresponse. input testcaseresponse. reference _ output testcasesortenum testcasesortenum. score _ asc testcasesortenum. score _ desc testrunmetadata testrunmetadata. avg _ score testrun", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". score _ asc testcasesortenum. score _ desc testrunmetadata testrunmetadata. avg _ score testrunmetadata. created _ at testrunmetadata. id testrunmetadata. model _ version testrunmetadata. name testrunmetadata. prompt _ template testrunmetadata. updated _ at testrunsortenum testrunsortenum. avg _ score _ asc testrunsortenum. avg _ score _ desc testsuitemetadata testsuitemetadata. created _ at testsuiteme", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "asc testrunsortenum. avg _ score _ desc testsuitemetadata testsuitemetadata. created _ at testsuitemetadata. description testsuitemetadata. id testsuitemetadata. last _ run _ time testsuitemetadata. name testsuitemetadata. scoring _ method testsuitemetadata. updated _ at testsuiterequest testsuiterequest. description testsuiterequest. name testsuiterequest. null _ reference _ outputs _ all _ or _ none ( ) testsuiterequest. scoring _ method testsuiterequest. scoring _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##uiterequest. null _ reference _ outputs _ all _ or _ none ( ) testsuiterequest. scoring _ method testsuiterequest. scoring _ method _ backwards _ compatible ( ) testsuiterequest. test _ cases testsuitesortenum testsuitesortenum. last _ runtime _ asc testsuitesortenum. last _ runtime _ desc testsuitesummary testsuitesummary. categorical testsuitesummary. num _ test _ cases testsuitesummary. page testsuitesummary. page _ size tests", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "testsuitesummary. num _ test _ cases testsuitesummary. page testsuitesummary. page _ size testsuitesummary. summary testsuitesummary. total _ count testsuitesummary. total _ pages hallucinationscorerequest hallucinationscorerequest. context hallucinationscorerequest. response hallucinationscoreresponse hallucinationscoreresponse. hallucination hallucinationscoreresponse. reason source : https : / / bench. readthedocs. io / en / latest / sd", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "hallucination hallucinationscoreresponse. reason source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. models. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html", "row": 146, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 147 text : arthur _ bench. run - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##toggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. run # submodules # class arthur _ bench. run. testrun. testrun ( *, name : str, test _ case _ outputs : list [ testcaseoutput ], description : str none = none, model _ name : str none = none, foundation _ model : str none = none, prompt _ template : str none = none,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "none, model _ name : str none = none, foundation _ model : str none = none, prompt _ template : str none = none, model _ version : str none = none, test _ suite _ id : uuid, client : benchclient, id : uuid none = none ) # bases : createrunrequest class config # bases : object arbitrary _ types _ allowed = true # property categories : list [ str none ] # client : benchclient # classmethod from _ flattened ( run _ name : str, ids : list [ uuid ],", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "] # client : benchclient # classmethod from _ flattened ( run _ name : str, ids : list [ uuid ], candidate _ output _ list : list [ str ], scores : list [ float ] list [ scoreresult ], client : benchclient, test _ suite _ id : uuid, model _ name : str none = none, model _ version : str none = none, foundation _ model : str none = none, prompt _ template : str none = none ) # id : uuid none # property output : list [ str ] #", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str none = none, prompt _ template : str none = none ) # id : uuid none # property output : list [ str ] # save ( ) \u2192 uuid # save a test run. property scores : list [ float none ] # test _ suite _ id : uuid # class arthur _ bench. run. testsuite. testsuite ( name : str, scoring _ method : str scorer, description : str none = none, reference _ data : dataframe none = none, reference _ data _ path : str none = none, input _ column : str = '", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "none, reference _ data : dataframe none = none, reference _ data _ path : str none = none, input _ column : str ='input ', reference _ column : str ='reference _ output ', input _ text _ list : list [ str ] none = none, reference _ output _ list : list [ str ] none = none, client : benchclient none = none ) # bases : object reusable pipeline for running a test suite built from reference _ data and evaluated using scoring _ method parameters : name \u2013 name of the test suite scoring _ method \u2013 scoring method or scorer", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "running a test suite built from reference _ data and evaluated using scoring _ method parameters : name \u2013 name of the test suite scoring _ method \u2013 scoring method or scorer instance to use to evaluate the results of a test run, as a string / enum or class instance description \u2013 short description of the task tested by this suite reference _ data \u2013 dataframe of prompts and reference outputs reference _ data _ path \u2013 filepath to csv of prompts and reference outputs, required if not specifying reference _ data input _ column \u2013 the column of reference _ data containing prompts, defaults to \u2018 prompt \u2019 reference _ column \u2013 the column", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "not specifying reference _ data input _ column \u2013 the column of reference _ data containing prompts, defaults to \u2018 prompt \u2019 reference _ column \u2013 the column of reference _ data containing reference outputs, defaults to \u2018 reference \u2019 input _ text _ list \u2013 list of strings of input texts that can be provided instead of dataframe columns reference _ output _ list \u2013 list of strings of reference outputs that can be provided instead of dataframe columns async arun ( run _ name : str, candidate _ data : dataframe none = none, candidate _ data _ path : str none = none, candidate _ column : str", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": str, candidate _ data : dataframe none = none, candidate _ data _ path : str none = none, candidate _ column : str ='candidate _ output ', candidate _ output _ list : list [ str ] none = none, context _ column : str none = none, context _ list : list [ str ] none = none, save : bool = true, batch _ size : int = 5, model _ name : str none = none, model _ version : str none = none, foundation _ model : str none = none, prompt _ template : str", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str none = none, model _ version : str none = none, foundation _ model : str none = none, prompt _ template : str none = none ) \u2192 testrun # property description : str none # property input _ texts : list [ str ] # property name : str # property reference _ outputs : list [ str none ] # run ( run _ name : str, candidate _ data : dataframe none = none, candidate _ data _ path : str none = none, candidate _ column : str ='candidate _ output ', candidate _ output _ list : list [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ data _ path : str none = none, candidate _ column : str ='candidate _ output ', candidate _ output _ list : list [ str ] none = none, context _ column : str none = none, context _ list : list [ str ] none = none, save : bool = true, batch _ size : int = 1, model _ name : str none = none, model _ version : str none = none, foundation _ model : str none = none, prompt _ template : str none = none ) \u2192 testrun # score a test run on candidate outputs", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", foundation _ model : str none = none, prompt _ template : str none = none ) \u2192 testrun # score a test run on candidate outputs. parameters : run _ name \u2013 name for the test run candidate _ data \u2013 dataframe of candidate responses to test prompts candidate _ data _ path \u2013 filepath to csv containing candidate responses to test prompts candidate _ column \u2013 the column of candidate data containing candidate responses, defaults to \u2018 candidate _ output \u2019 candidate _ output _ list \u2013 list of strings of candidate outputs that can be provided instead of dataframe context _ column \u2013 the column of reference _ data containing supporting", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "candidate _ output _ list \u2013 list of strings of candidate outputs that can be provided instead of dataframe context _ column \u2013 the column of reference _ data containing supporting context for answering question & answering tasks context _ list \u2013 list of strings containing supporting context for answering question and answering tasks save \u2013 whether to save the run results to file batch _ size \u2013 the batch _ size to use when computing scores model _ name \u2013 model name for model used to generate outputs model _ version \u2013 model version of model used to generate outputs foundation _ model \u2013 foundation model name used to generate outputs prompt _ template \u2013 prompt template name used to generate outputs returns : testrun", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "of model used to generate outputs foundation _ model \u2013 foundation model name used to generate outputs prompt _ template \u2013 prompt template name used to generate outputs returns : testrun object containing scored outputs save ( ) # save a test suite to local file system. property scoring _ method : str # property test _ cases : list [ testcaseresponse ] # next arthur _ bench. scoring previous arthur _ bench. models copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. run submodules testrun testrun. config testrun. config", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##unsg's furo on this page arthur _ bench. run submodules testrun testrun. config testrun. config. arbitrary _ types _ allowed testrun. categories testrun. client testrun. from _ flattened ( ) testrun. id testrun. output testrun. save ( ) testrun. scores testrun. test _ suite _ id testsuite testsuite. arun ( ) testsuite. description testsuite. input _ texts testsuite. name testsuite. reference _ outputs testsuite. run ( ) testsuite. save ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##uite. input _ texts testsuite. name testsuite. reference _ outputs testsuite. run ( ) testsuite. save ( ) testsuite. scoring _ method testsuite. test _ cases source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. run. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html", "row": 147, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 148 text : arthur _ bench. scoring - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##toggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. scoring # class arthur _ bench. scoring. scoringmethodname ( value, names = none, *, module = none, qualname = none, type = none, start = 1, boundary = none ) # bases : str, enum bertscore ='bertscore'# exactmatch ='exact _ match'# hallucination ='hall", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": str, enum bertscore ='bertscore'# exactmatch ='exact _ match'# hallucination ='hallucination'# hedginglanguage ='hedging _ language'# pythonunittesting ='python _ unit _ testing'# qacorrectness ='qa _ correctness'# readability ='readability'# specificity ='specificity'# summaryquality ='summary _ quality'# wordcountmatch ='word _ count _ match'# arthur _ bench. scoring. scorer _ from _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##ity ='summary _ quality'# wordcountmatch ='word _ count _ match'# arthur _ bench. scoring. scorer _ from _ string ( method : str ) \u2192 type [ arthur _ bench. scoring. scorer. scorer ] # submodules # class arthur _ bench. scoring. bertscore. bertscore ( model _ type ='microsoft / deberta - v3 - base ', precision _ weight = 0. 1 ) # bases : scorer tailored bert score implementation. https : / / arxiv. org / abs / 1904. 09675 static name ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "1 ) # bases : scorer tailored bert score implementation. https : / / arxiv. org / abs / 1904. 09675 static name ( ) \u2192 str # get the name of this scorer : return : the scorer name run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead to _ dict ( warn = false ) # provides a json serializable representation of the scorer. class arthur _ bench. scoring. exact _ match. exactmatch ( case _ sensitive = true ) # bases :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "serializable representation of the scorer. class arthur _ bench. scoring. exact _ match. exactmatch ( case _ sensitive = true ) # bases : scorer returns 1 if candidate matches reference, 0 if candidate does not match reference. static categories ( ) \u2192 list [ category ] # all possible values returned by the scorer if output type is categorical. static is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer name run _ batch ( candidate _ batch :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer name run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead class arthur _ bench. scoring. hallucination. hallucination # bases : scorer score each output against a context using arthur \u2019 s hosted hallucination checker a score of 1. 0 means the hallucination checker estimates the output is supported by the context a score of 0. 0 means the hallucination checker found information", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". 0 means the hallucination checker estimates the output is supported by the context a score of 0. 0 means the hallucination checker found information in the output unsupported by the context static categories ( ) \u2192 list [ category ] # all possible values returned by the scorer if output type is categorical. static is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead to _ dict ( warn = false ) # provides a json serializable representation of the scorer. class arthur _ bench. scoring. hedging _ language. hedginglanguage ( model _ type : str ='microsoft / debert", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "class arthur _ bench. scoring. hedging _ language. hedginglanguage ( model _ type : str ='microsoft / deberta - v3 - base ', hedging _ language : str = \" as an ai language model, i don't have personal opinions, emotions, or beliefs. \" ) # bases : scorer given an input question and model output, determine if the output contains hedging language such as \u201c as an ai language model, i don \u2019 t have personal opinions, emotions, or beliefs \u201d. the values returned are a similarity score ( bertscore ),", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "an ai language model, i don \u2019 t have personal opinions, emotions, or beliefs \u201d. the values returned are a similarity score ( bertscore ), with higher values corresponding to higher likelihood of hedging language being present in the model output. static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead to _ dict ( warn = false ) # provides a json serializable representation of the scorer. class arthur _ bench. scoring. python _ unit _ testing. pythonunittesting ( unit _ test _ dir : str none = none, unit _ tests : list [ str ] none = none ) # bases : scorer wrapping the huggingface code _ eval metric scores each candidate _ output as a function against a pre - prepared unit test note : considers any", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# bases : scorer wrapping the huggingface code _ eval metric scores each candidate _ output as a function against a pre - prepared unit test note : considers any code with non - standard python libraries ( e. g. numpy ) to have an error https : / / huggingface. co / spaces / evaluate - metric / code _ eval static categories ( ) \u2192 list [ category ] # all possible values returned by the scorer if output type is categorical. static is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run ( candidate _ outputs : list [ str ], reference _ outputs : list [ str ] none = none, inputs : list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int = 1 ) \u2192 list [ scorer", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int = 1 ) \u2192 list [ scoreresult ] # score a set of test cases. this method doesn \u2019 t need to be implemented in most cases, but can be overriden to add additional functionality such as task - specific logging. parameters : candidate _ outputs \u2013 candidate generations to score reference _ outputs \u2013 reference strings representing target outputs inputs \u2013 input strings being tested contexts \u2013 optional corresponding contexts, if needed by scorer batch _ size \u2013 size of batches returns : scoring results for this run. float scores are de", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "being tested contexts \u2013 optional corresponding contexts, if needed by scorer batch _ size \u2013 size of batches returns : scoring results for this run. float scores are deprecated, use scoreresult instead run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead to _ dict ( warn = false ) # provides a json serializable representation of the scorer. class arthur _ bench. scoring. qa _ quality. qaqualitycorrectness ( llm : basechatmodel none = none", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "class arthur _ bench. scoring. qa _ quality. qaqualitycorrectness ( llm : basechatmodel none = none ) # bases : scorer given an input question, context string, and model generation, determine if the generation produced a correct answer. async arun _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ float ] list [ scoreresult ] #", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ float ] list [ scoreresult ] # reference batch is not used for this scoring method, qa correctness requires an input _ text _ batch and context _ batch static categories ( ) \u2192 list [ category ] # all possible values returned by the scorer if output type is categorical. static is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # reference batch is not used for this scoring method,", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # reference batch is not used for this scoring method, qa correctness requires an input _ text _ batch and context _ batch to _ dict ( warn = false ) # provides a json serializable representation of the scorer. static validate _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 tu", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 tuple [ list [ str ], list [ str ] ] # class arthur _ bench. scoring. readability. readability # bases : scorer flesch reading ease score : the higher the score, the easier to read. scores of 100 - 90 correlate to a 5th grade reading level, while scores < 10 are classified as \u201c extremely difficult to read, and best understood by university graduates. \u201d https : / / en. wikipedia. org / wiki", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "scores < 10 are classified as \u201c extremely difficult to read, and best understood by university graduates. \u201d https : / / en. wikipedia. org / wiki / flesch % e2 % 80 % 93kincaid _ readability _ tests static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead class", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead class arthur _ bench. scoring. scorer. scorer # bases : abc base class for all scorers. compute a float score for a given model generation. async arun ( candidate _ outputs : list [ str ], reference _ outputs : list [ str ] none = none, inputs : list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int = 5 ) \u2192 list [ float ] list [ scorer", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "] none = none, contexts : list [ str ] none = none, batch _ size : int = 5 ) \u2192 list [ float ] list [ scoreresult ] # async version of run method. async arun _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ float ] list [ scoreresult ] # async version of run _ batch method. static categories ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##r ] none = none ) \u2192 list [ float ] list [ scoreresult ] # async version of run _ batch method. static categories ( ) \u2192 list [ category ] none # all possible values returned by the scorer if output type is categorical. classmethod from _ dict ( config : dict ) # load a scorer from a json configuration file. static is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true abstract static name ( ) \u2192 str # get the name of this scorer : return : the scorer", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "or categorical. categories ( ) should be implemented if true abstract static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run ( candidate _ outputs : list [ str ], reference _ outputs : list [ str ] none = none, inputs : list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int = 1 ) \u2192 list [ float ] list [ scoreresult ] # score a set of test cases", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##r ] none = none, batch _ size : int = 1 ) \u2192 list [ float ] list [ scoreresult ] # score a set of test cases. this method doesn \u2019 t need to be implemented in most cases, but can be overriden to add additional functionality such as task - specific logging. parameters : candidate _ outputs \u2013 candidate generations to score reference _ outputs \u2013 reference strings representing target outputs inputs \u2013 input strings being tested contexts \u2013 optional corresponding contexts, if needed by scorer batch _ size \u2013 size of batches returns : scoring results for this run. float scores are deprecated, use scoreresult instead abstract run", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "by scorer batch _ size \u2013 size of batches returns : scoring results for this run. float scores are deprecated, use scoreresult instead abstract run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ float ] list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead to _ dict ( warn = false ) # provides a json serializable representation of the scorer. to _ metadata ( ) \u2192 scoringmethod # classmethod type ( ) \u2192 scoringmethodtype # supplies whether a scorer is built - in or custom. this", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u2192 scoringmethod # classmethod type ( ) \u2192 scoringmethodtype # supplies whether a scorer is built - in or custom. this method is implemented by checking whether the scorer class is part of the arthur _ bench. scoring module. : return : the type ( built - in or custom ) class arthur _ bench. scoring. specificity. specificity # bases : scorer returns a score from 0. 0 to 1. 0 indicating how specific the candidate output language is. higher scores indicate that the language is more specific, lower scores indicate more vague language. specificity is computed through detecting words that indicate vagueness", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "output language is. higher scores indicate that the language is more specific, lower scores indicate more vague language. specificity is computed through detecting words that indicate vagueness ( predefined ) determing how rare the words used are according to word frequencies calculated by popular nlp corpora, and detecting use of proper nouns and numbers. get _ mean _ word _ freq ( candidate _ output : str ) \u2192 float # returns mean word frequency of candidate output. higher values indicate that moree common words on average are used in the candidate output. considers only words with frequency < 0. 001, truncating probability of words", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "values indicate that moree common words on average are used in the candidate output. considers only words with frequency < 0. 001, truncating probability of words with higher frequencies to 0. 001. get _ num _ vague _ words ( candidate _ output : str ) \u2192 int # returns number of words in candidate _ output which are is a list of pre - defined vague words. get _ pn _ and _ num ( candidate _ output : str ) \u2192 int # returns total number of proper nouns and numbers in candidate output. determined heuristically via nnp and cd nltk tags. static name", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u2192 int # returns total number of proper nouns and numbers in candidate output. determined heuristically via nnp and cd nltk tags. static name ( ) \u2192 str # get the name of this scorer : return : the scorer name static requires _ reference ( ) \u2192 bool # true if scorer requires reference output to compute score, false otherwise run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "= none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead class arthur _ bench. scoring. summary _ quality. summaryquality ( llm : basechatmode", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "are deprecated, use scoreresult instead class arthur _ bench. scoring. summary _ quality. summaryquality ( llm : basechatmodel none = none, context _ window : int = 4096, tokenizer : encoding none = none ) # bases : scorer comprehensive measure of summarization quality compared to a reference summary. async arun ( candidate _ outputs : list [ str ], reference _ outputs : list [ str ] none = none, inputs : list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "none = none, inputs : list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int = 5 ) \u2192 list [ float ] list [ scoreresult ] # async version of run method. async arun _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ float ] list [ scoreresult ] # summary quality requires input", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "= none, context _ batch : list [ str ] none = none ) \u2192 list [ float ] list [ scoreresult ] # summary quality requires input _ text _ batch. asynchronous implementation static categories ( ) \u2192 list [ category ] # all possible values returned by the scorer if output type is categorical. static is _ categorical ( ) \u2192 bool # whether the scorer is continuous or categorical. categories ( ) should be implemented if true static name ( ) \u2192 str # get the name of this scorer : return : the scorer name run ( candidate _ outputs : list [ str ], reference", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "name ( ) \u2192 str # get the name of this scorer : return : the scorer name run ( candidate _ outputs : list [ str ], reference _ outputs : list [ str ] none = none, inputs : list [ str ] none = none, contexts : list [ str ] none = none, batch _ size : int = 1 ) \u2192 list [ scoreresult ] list [ float ] # score a set of test cases. this method doesn \u2019 t need to be implemented in most cases, but can be overriden to add additional functionality such as task - specific logging. parameters : candidate _ outputs", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "\u2019 t need to be implemented in most cases, but can be overriden to add additional functionality such as task - specific logging. parameters : candidate _ outputs \u2013 candidate generations to score reference _ outputs \u2013 reference strings representing target outputs inputs \u2013 input strings being tested contexts \u2013 optional corresponding contexts, if needed by scorer batch _ size \u2013 size of batches returns : scoring results for this run. float scores are deprecated, use scoreresult instead run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # summary quality requires input _ text _ batch. to _ dict ( warn = false ) # provides a json serializable representation of the scorer. static validate _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ]", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 tuple [ list [ str ], list [ str ] ] # arthur _ bench. scoring. summary _ quality. truncate _ input _ text ( input _ text, ref _ output, cand _ output, context _ window : int = 4096, tokenizer : ~ tiktoken. core. encoding = < encoding'cl100k _ base", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ", context _ window : int = 4096, tokenizer : ~ tiktoken. core. encoding = < encoding'cl100k _ base'> ) \u2192 tuple [ str, bool ] # truncates the input _ text to fit in llm evaluator context truncate the input text so that the filled - in compare prompt which contains { input text + summary a + summary b } fits in the evaluator context window returns the tuple ( text, whether text was truncated ) class arthur _ bench. scoring. utils. suppress _ warnings ( logger _ name :", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "window returns the tuple ( text, whether text was truncated ) class arthur _ bench. scoring. utils. suppress _ warnings ( logger _ name : str ) # bases : object a context - manager class to temporarily set the logging level for a logger to error before returning it to its previous state. class arthur _ bench. scoring. word _ count _ match. wordcountmatch # bases : scorer calculates how similar the number of words in the candidate output is to the the number of words in the reference output. scores span from 0 to 1. a score of 1. 0 indicates that there are the", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "candidate output is to the the number of words in the reference output. scores span from 0 to 1. a score of 1. 0 indicates that there are the same number of words in the candidate output as in the reference output. scores less than 1. 0 are calculated as ( ( len _ reference - delta ) / len _ reference ) where delta is the absolute difference in word lengths between the candidate and reference outputs. all negative computed values are truncated to 0. utilizes lexicon count, removing punctuations : https : / / pypi. org / project / textstat / static name ( ) \u2192 str # get", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "count, removing punctuations : https : / / pypi. org / project / textstat / static name ( ) \u2192 str # get the name of this scorer : return : the scorer name run _ batch ( candidate _ batch : list [ str ], reference _ batch : list [ str ] none = none, input _ text _ batch : list [ str ] none = none, context _ batch : list [ str ] none = none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "none ) \u2192 list [ scoreresult ] # score a batch of candidate generations. parameters : candidate _ batch \u2013 candidate generations to score reference _ batch \u2013 reference strings representing target outputs input _ text _ batch \u2013 optional corresponding inputs context _ batch \u2013 optional corresponding contexts, if needed by scorer returns : scoring results for this batch. float scores are deprecated, use scoreresult instead next arthur _ bench. server previous arthur _ bench. run copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. scoring scoringmethodname scoringmethodname. bert", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "with sphinx and @ pradyunsg's furo on this page arthur _ bench. scoring scoringmethodname scoringmethodname. bertscore scoringmethodname. exactmatch scoringmethodname. hallucination scoringmethodname. hedginglanguage scoringmethodname. pythonunittesting scoringmethodname. qacorrectness scoringmethodname. readability scoringmethodname. specificity scoringmethodname. summaryquality scoringmethodname. wordcountmatch scorer _ from _ string ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##dname. specificity scoringmethodname. summaryquality scoringmethodname. wordcountmatch scorer _ from _ string ( ) submodules bertscore bertscore. name ( ) bertscore. run _ batch ( ) bertscore. to _ dict ( ) exactmatch exactmatch. categories ( ) exactmatch. is _ categorical ( ) exactmatch. name ( ) exactmatch. run _ batch ( ) hallucination hallucination. categories ( ) hallucination. is _ categorical ( ) hallucination. name ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "run _ batch ( ) hallucination hallucination. categories ( ) hallucination. is _ categorical ( ) hallucination. name ( ) hallucination. requires _ reference ( ) hallucination. run _ batch ( ) hallucination. to _ dict ( ) hedginglanguage hedginglanguage. name ( ) hedginglanguage. requires _ reference ( ) hedginglanguage. run _ batch ( ) hedginglanguage. to _ dict ( ) pythonunittesting pythonunittesting. categories ( ) pythonunittes", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ batch ( ) hedginglanguage. to _ dict ( ) pythonunittesting pythonunittesting. categories ( ) pythonunittesting. is _ categorical ( ) pythonunittesting. name ( ) pythonunittesting. requires _ reference ( ) pythonunittesting. run ( ) pythonunittesting. run _ batch ( ) pythonunittesting. to _ dict ( ) qaqualitycorrectness qaqualitycorrectness. arun _ batch ( ) qaqualitycorrectness. categories ( ) qaqualitycorrect", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##aqualitycorrectness. arun _ batch ( ) qaqualitycorrectness. categories ( ) qaqualitycorrectness. is _ categorical ( ) qaqualitycorrectness. name ( ) qaqualitycorrectness. requires _ reference ( ) qaqualitycorrectness. run _ batch ( ) qaqualitycorrectness. to _ dict ( ) qaqualitycorrectness. validate _ batch ( ) readability readability. name ( ) readability. requires _ reference ( ) readability", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##aqualitycorrectness. validate _ batch ( ) readability readability. name ( ) readability. requires _ reference ( ) readability. run _ batch ( ) scorer scorer. arun ( ) scorer. arun _ batch ( ) scorer. categories ( ) scorer. from _ dict ( ) scorer. is _ categorical ( ) scorer. name ( ) scorer. requires _ reference ( ) scorer. run ( ) scorer. run _ batch ( ) scorer. to _ dict ( ) scorer. to _ metadata ( ) scorer. type ( ) specificity specificity. get _ mean _ word _ fr", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "scorer. to _ dict ( ) scorer. to _ metadata ( ) scorer. type ( ) specificity specificity. get _ mean _ word _ freq ( ) specificity. get _ num _ vague _ words ( ) specificity. get _ pn _ and _ num ( ) specificity. name ( ) specificity. requires _ reference ( ) specificity. run _ batch ( ) summaryquality summaryquality. arun ( ) summaryquality. arun _ batch ( ) summaryquality. categories ( ) summaryquality. is _ categorical ( ) summaryquality. name ( )", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##quality. arun _ batch ( ) summaryquality. categories ( ) summaryquality. is _ categorical ( ) summaryquality. name ( ) summaryquality. run ( ) summaryquality. run _ batch ( ) summaryquality. to _ dict ( ) summaryquality. validate _ batch ( ) truncate _ input _ text ( ) suppress _ warnings wordcountmatch wordcountmatch. name ( ) wordcountmatch. run _ batch ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##untmatch. run _ batch ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. scoring. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html", "row": 148, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 149 text : arthur _ bench. server - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html", "row": 149, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##toggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html", "row": 149, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html", "row": 149, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. server # submodules # next arthur _ bench. telemetry previous arthur _ bench. scoring copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. server submodules source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. server", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html", "row": 149, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "server submodules source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. server. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html", "row": 149, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 150 text : arthur _ bench. telemetry - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html", "row": 150, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client.", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html", "row": 150, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page to", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html", "row": 150, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. telemetry # submodules # next arthur _ bench. utils previous arthur _ bench. server copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. telemetry submodules source : https : / / bench. readthedocs. io / en / latest / sdk /", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html", "row": 150, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. telemetry submodules source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. telemetry. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html", "row": 150, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 151 text : arthur _ bench. utils - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "_ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar arthur _ bench. utils # submodules # arthur _ bench. utils. loaders. get _ file _ extension ( filepath : str pathlike ) \u2192 str # arthur _ bench. utils. loaders. load _ suite _ from _ csv ( filepath : str pathlike, input _ column : str, reference _ column : str none =", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "load _ suite _ from _ csv ( filepath : str pathlike, input _ column : str, reference _ column : str none = none ) \u2192 list [ testcaserequest ] # load test case data from csv file. parameters : filepath \u2013 string or pathlike object pointing to csv file input _ column \u2013 column in file containing inputs reference _ column \u2013 column in file containing reference outputs arthur _ bench. utils. loaders. load _ suite _ from _ dataframe ( data : dataframe, input _ column : str, reference _ column : str none = none ) \u2192", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "load _ suite _ from _ dataframe ( data : dataframe, input _ column : str, reference _ column : str none = none ) \u2192 list [ testcaserequest ] # load test case data from a pandas dataframe. parameters : data \u2013 dataframe where each row is a test case consisting of a column for i input and a column for reference input _ column \u2013 column in dataframe containing inputs reference _ column \u2013 column in dataframe containing reference outputs arthur _ bench. utils. loaders. load _ suite _ from _ json ( filepath : str pathlike ) \u2192 testsuitere", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "outputs arthur _ bench. utils. loaders. load _ suite _ from _ json ( filepath : str pathlike ) \u2192 testsuiterequest # load a full test suite from a json file. parameters : filepath \u2013 string or pathlike object pointing to json file containing test suite data arthur _ bench. utils. loaders. load _ suite _ from _ list ( inputs : list [ str ], reference _ outputs : list [ str ] none ) \u2192 list [ testcaserequest ] # load test case data from lists of strings. parameters : inputs \u2013 list of string inputs for", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "str ] none ) \u2192 list [ testcaserequest ] # load test case data from lists of strings. parameters : inputs \u2013 list of string inputs for each test case reference _ outputs \u2013 list of string reference outputs for each input next contributing previous arthur _ bench. telemetry copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page arthur _ bench. utils submodules get _ file _ extension ( ) load _ suite _ from _ csv ( ) load _ suite _ from _ dataframe ( ) load _ suite _ from _ json ( ) load _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "load _ suite _ from _ csv ( ) load _ suite _ from _ dataframe ( ) load _ suite _ from _ json ( ) load _ suite _ from _ list ( ) source : https : / / bench. readthedocs. io / en / latest / sdk / arthur _ bench. utils. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html", "row": 151, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 152 text : setup - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar setup # package installation # install bench to your python environment with optional dependencies for serving results locally ( recommended ) : pip install'arthur - bench [ server ]'alternatively, install bench to your python environment with minimal dependencies : pip install arthur - bench choosing local vs saas : # bench has two options for tracking datasets and results : local only ( default ) : save data and run server on the same machine that is", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "##as : # bench has two options for tracking datasets and results : local only ( default ) : save data and run server on the same machine that is running the bench package arthur saas platform ( coming soon! ) : use the package client to log data and results to the arthur platform. arthur manages data storage and persistence and hosts the bench server. local # bench spins up a local ui ( like tensorboard ) to provide a visual interface for your test data. view examples # running these commands will view launch the bench ui locally to view the example test suites from the arthur bench github repo. # clone the bench", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "examples # running these commands will view launch the bench ui locally to view the example test suites from the arthur bench github repo. # clone the bench repo git clone https : / / github. com / arthur - ai / bench. git # set the bench _ file _ dir environment variable to point to the example test suite data in the repo export bench _ file _ dir = \". / bench / examples / bench _ runs / \" # launch the bench ui bench you will see a url for a local server that you can copy and paste into your browser to navigate the ui. viewing examples", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# launch the bench ui bench you will see a url for a local server that you can copy and paste into your browser to navigate the ui. viewing examples in the bench ui will look something like this : when you want to view the local ui for your own test suites going forward, make sure your bench _ file _ dir environment variable is configured to point to the location of your new test runs, and run bench from the command line. saas ( coming soon! ) # bench can be used automatically in conjunction with your team \u2019 s existing arthur platform account. to connect to the arthur platform from bench, you will need an arthur", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "# bench can be used automatically in conjunction with your team \u2019 s existing arthur platform account. to connect to the arthur platform from bench, you will need an arthur bench account and api key. to log results to the platform, you just need to set the remote url and api key environment variables before creating and running suites. for example, import os os. environ ['arthur _ bench _ autolog'] ='true'os. environ ['arthur _ api _ url'] ='https : / / app. arthur. ai'os. environ ['arthur _ api _ key", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "arthur _ api _ url'] ='https : / / app. arthur. ai'os. environ ['arthur _ api _ key'] ='fill me in'next quickstart previous home copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page setup package installation choosing local vs saas : local view examples saas ( coming soon! ) source : https : / / bench. readthedocs. io / en / latest / setup. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/setup.html", "row": 152, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 153 text : usage data collection - bench documentation contents menu expand light mode dark mode auto light / dark mode hide navigation sidebar hide table of contents sidebar toggle site navigation sidebar bench documentation toggle light / dark / auto color theme toggle table of contents sidebar bench documentation home setup quickstart scoring guidestoggle navigation of guides concepts creating test suites compare llm providers compare prompts compare generation settings add scorer configurations custom scoring code evaluation python api referencetoggle navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "navigation of python api reference arthur _ bench. clienttoggle navigation of arthur _ bench. client arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. auth arthur _ bench. client. http arthur _ bench. client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ". client. local arthur _ bench. client. resttoggle navigation of arthur _ bench. client. rest arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. client. rest. admin arthur _ bench. client. rest. bench arthur _ bench. exceptions arthur _ bench. models arthur _ bench. run arthur _ bench. scoring arthur _ bench. server arthur _ bench. telemetry arthur _ bench. utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "utils contributing usage data collection v : latest versions latest stable downloads on read the docs project home builds back to top edit this page toggle light / dark / auto color theme toggle table of contents sidebar usage data collection # by default, arthur is collecting anonymous usage data. data being collected # we track usage data in order to best understand what features users like and use. specifically, we collect : scoring methods used number of runs per test suite number of test cases for a test run an example event looks like ( user - id is a random identifier, not tied to any personal data ). { \" event _ properties", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "a test run an example event looks like ( user - id is a random identifier, not tied to any personal data ). { \" event _ properties \" : { \" num _ test _ suites _ load \" : 3, \" test _ suites _ all \" : [ \" summary _ quality \", \" bertscore \", \" qa _ correctness \" ] }, \" event _ type \" : \" test _ suites _ load \", \" user _ id \" : \" fdc73011 - 2c71 - 41f3 - b174 - 0d338e2f3f", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "id \" : \" fdc73011 - 2c71 - 41f3 - b174 - 0d338e2f3f53 \" } opting - out # to opt - out, run bench - - disable _ push _ usage _ data if you want to opt back in, run bench - - enable _ push _ usage _ data you can also opt - out, by setting the environment variable bench _ telemetry _ disabled = 1. to opt - out and instead log events that would have been pushed, set bench _ telemetry _ disabled = log. previous contributing copyright \u00a9 202", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": "1. to opt - out and instead log events that would have been pushed, set bench _ telemetry _ disabled = log. previous contributing copyright \u00a9 2023, arthur made with sphinx and @ pradyunsg's furo on this page usage data collection data being collected opting - out source : https : / / bench. readthedocs. io / en / latest / telemetry. html", "metadata": {"source": "https://bench.readthedocs.io/en/latest/telemetry.html", "row": 153, "content_type": "arthur_bench_docs"}, "type": "Document"}
{"page_content": ": 154 text : arthur website 2. 0 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcv", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedllm - guided evaluation experimentby : arthur teamoctober 5, 2023overview / backgroundtraditionally, text evaluation has been done using methods like bleu ( evaluation based on word presence ) or bertscore ( evaluation based on pre - trained nlp and embedding models ).", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "methods like bleu ( evaluation based on word presence ) or bertscore ( evaluation based on pre - trained nlp and embedding models ). however, the technological advancements around llms sparked our team \u2019 s interest in experimenting with a new text evaluation method : using llms to evaluate llms, or \u201c llm - guided evaluation. \u201d we know that, as evaluators, llms are more sensitive than other evaluation methods. they \u2019 re particularly sensitive to things like : the choice of llm evaluator ( gpt - 3. 5 - turbo, claude - 2, llama2 - 70", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "sensitive to things like : the choice of llm evaluator ( gpt - 3. 5 - turbo, claude - 2, llama2 - 70b, command, etc. ) the task being evaluated ( summarization, question - answering, etc. ) the type of feedback being prompted for ( scores 1 through 10, grades a + through f, etc. ) so, we specifically wanted to look more into llm sensitivity by testing well - known llms as both candidates and evaluators. experimentwe set up our experiment like this : essentially, we provided five different llms with one of two input prompts", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as both candidates and evaluators. experimentwe set up our experiment like this : essentially, we provided five different llms with one of two input prompts : either a summarization task, where it was asked to summarize a news article, or a question - answering task, where it was given various reports and papers as context. from there, we used the same llms ( with the exception of gpt - 4 due to cost ) to evaluate the output text provided by each llm candidate. this text was then given an eval result : either a score ranging from 1 to 10, or a grade ranging from", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "text provided by each llm candidate. this text was then given an eval result : either a score ranging from 1 to 10, or a grade ranging from a + to f. going into the experiment, our hypothesis was that an llm evaluator would be biased towards text it itself had generated over text other models had generated. resultsultimately, our hypothesis was not supported : of the five different llm candidates, gpt - 3. 5 - turbo commonly scored highest by all of the llm evaluators. below, we \u2019 ll dive deeper into the results from each of the llms we used as an", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "scored highest by all of the llm evaluators. below, we \u2019 ll dive deeper into the results from each of the llms we used as an evaluator. gpt - 3. 5 - turbo as evaluatoras you can see above, with gpt - 3. 5 - turbo as the evaluator, the summarization results systematically get lower scores than the question - answering results. we can also see that very low scores were rarely given \u2014 there were just two ds in the question - answering letter grade evaluation task. something we learned from this was that the kinds of distributions of feedback you can expect", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2014 there were just two ds in the question - answering letter grade evaluation task. something we learned from this was that the kinds of distributions of feedback you can expect from an llm as an evaluator are going to be very different depending on the type of task you \u2019 re actually evaluating for. in other words, the meaning of, say, a \u201c 9 / 10 \u201d or an \u201c a - \u201d is going to be different depending on the overall distribution that you \u2019 re collecting. claude - 2 as evaluatornext up was claude - 2. relative to gpt - 3. 5 - turbo, the summarization", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "collecting. claude - 2 as evaluatornext up was claude - 2. relative to gpt - 3. 5 - turbo, the summarization tasks received more perfect scores, but the question - answering tasks received fewer perfect scores. again, this just further reinforces that the distributions of feedback you can expect from an llm are sensitive to which llm is providing feedback. additionally, something to note here is that while claude - 2 \u2019 s feedback distribution was different from the feedback distribution of gpt - 3. 5 - turbo ( skewed slightly higher ), there was still some consensus with gpt - 3.", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "from the feedback distribution of gpt - 3. 5 - turbo ( skewed slightly higher ), there was still some consensus with gpt - 3. 5 - turbo on the lowest scoring candidates. the two boxes outlined in orange below are the same two boxes that received the lowest scores from gpt - 3. 5 - turbo. llama2 - 70b as evaluatorthe takeaway here is that, relative to gpt - 3. 5 - turbo and claude - 2, llama2 - 70b scores too uniformly across different candidates on the same input. the scores it gave were too concentrated to certain values (", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "claude - 2, llama2 - 70b scores too uniformly across different candidates on the same input. the scores it gave were too concentrated to certain values ( 3 / 10, 5 / 10, 8 / 10 ), which might make some people see it as a less useful or less robust evaluator than its peers. command as evaluatorrelative to the other llm evaluators, command gives lower scores ( e. g. 5 / 10 ) much more frequently. additionally, we \u2019 re seeing that it does not always successfully follow the instruction to return a number anywhere in its evaluation. despite being prompted to \u201c", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "more frequently. additionally, we \u2019 re seeing that it does not always successfully follow the instruction to return a number anywhere in its evaluation. despite being prompted to \u201c score the correctness of the answer on a scale from 0 to 10, \u201d command gave many scores of - 1, as we can see in the question - answering integer evaluation task. it also occasionally only gave written feedback as a way to evaluate the output text when we had specifically asked for a number or a letter grade, which shows that sometimes these llm evaluators are not properly trained on instructions. note : the full dataset for this experiment will be released soon,", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", which shows that sometimes these llm evaluators are not properly trained on instructions. note : the full dataset for this experiment will be released soon, so stay tuned for that. takeawayswhen it comes to evaluating generative text models, there is no one - size - fits - all solution. llm - guided evaluation can allow for targeted customization of criteria, but prompting alone can be rather unpredictable. ultimately, the ability to iterate quickly on feedback is crucial to identify existing weaknesses in your llm - driven system. our motivation with arthur bench \u2014 our recently launched llm evaluation product \u2014 was to create a framework", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "crucial to identify existing weaknesses in your llm - driven system. our motivation with arthur bench \u2014 our recently launched llm evaluation product \u2014 was to create a framework that would allow you to iterate quickly on both your task and on your evaluation system. if you \u2019 re working on an llm production application, we \u2019 d love for you to check out the arthur bench github and share how you \u2019 re thinking about evaluating llm applications in the future. faqhow do language models typically learn from feedback? language models typically learn from feedback through a training process where they adjust their internal algorithms to better match human responses or correct answers", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "do language models typically learn from feedback? language models typically learn from feedback through a training process where they adjust their internal algorithms to better match human responses or correct answers, improving their accuracy over time. what are the common challenges in evaluating large language models? common challenges include ensuring the diversity and representativeness of evaluation datasets and maintaining the balance between computational efficiency and the thoroughness of the evaluation process. how can the results of such experiments impact the future development of ai and language models? the results can guide developers in refining models to be more effective and context - aware, leading to advancements in ai that are more aligned with human language understanding", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "? the results can guide developers in refining models to be more effective and context - aware, leading to advancements in ai that are more aligned with human language understanding and use. share we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / gap - articles / ll", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##arthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / gap - articles / llm - guided - evaluation - experiment", "metadata": {"source": "https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment", "row": 154, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 155 text : arthur website 2. 0 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcv", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedhedging answers experimentby : arthur teamaugust 17, 2023overviewsince the launch of large language models ( llms ), developers have been concerned about the models generating incorrect, toxic, or offensive content. to reduce this risk, developers have trained the models to add warning messages within the generated", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "have been concerned about the models generating incorrect, toxic, or offensive content. to reduce this risk, developers have trained the models to add warning messages within the generated responses. for example, llms often respond with \u201c as an ai model, i cannot provide opinions \u201d, \u201c unfortunately, i cannot answer that question \u201d, etc. while these \u201c hedging \u201d answers are appropriate at times ( and serve as good default behavior ), they can also frustrate users who are looking for a straight answer from the llm. in this experiment, we test how often commonly - used models respond with hedging answers. experimentwe started", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "looking for a straight answer from the llm. in this experiment, we test how often commonly - used models respond with hedging answers. experimentwe started with the databricks - dolly - 15k dataset composed of generic questions that users may ask an llm. we generated responses for a random sample of 1k questions from gpt - 3. 5, gpt - 4, claude - 2, and cohere models and then compared the responses to a stereotypical hedging answer : \u201c as an ai language model, i don \u2019 t have personal opinions, emotions, or beliefs \u201d. we use bertsco", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "stereotypical hedging answer : \u201c as an ai language model, i don \u2019 t have personal opinions, emotions, or beliefs \u201d. we use bertscore to compare llm response \u2019 s similarity to the stereotypical hedging answer. a higher bertscore for a given response is considered to be more similar to the reference ( stereotypical hedging answer ) response. our results below use the threshold of bertscore greater than 0. 6 to define a response as a hedge. resultswe find that the share of hedging answers has gone up for openai \u2019 s models ( gpt - 3. 5", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "response as a hedge. resultswe find that the share of hedging answers has gone up for openai \u2019 s models ( gpt - 3. 5 vs. gpt - 4 ). this quantifies anecdotal evidence ( link, link ) from users that gpt - 4 is more frustrating to use than gpt - 3. 5. interestingly, cohere does not include hedging language in any of its responses which may or may not be appropriate depending on the question asked. examplesquestion : if you could change one thing about this world, what would it be and why? gpt", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "not be appropriate depending on the question asked. examplesquestion : if you could change one thing about this world, what would it be and why? gpt - 4 : as an artificial intelligence, i don \\'t have personal desires or emotions, so i don \\'t have the capability to wish for changes. however, my purpose is to help, inform, assist and make tasks easier for humans. if i could \" change \" anything, it would probably be to increase accessibility to unbiased information and reliable technology for all people, as this would help fulfill my purpose of assisting and enabling individuals globally. ( score : 0", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "increase accessibility to unbiased information and reliable technology for all people, as this would help fulfill my purpose of assisting and enabling individuals globally. ( score : 0. 7511 ) gpt - 3. 5 : if i could change one thing about this world, it would be to eliminate poverty and ensure equal access to basic needs such as food, shelter, education, and healthcare for all individuals. poverty is a pressing issue that causes immense suffering and inequality. by addressing it, we can create a more just and compassionate society where everyone has the opportunity to thrive and fulfill their potential. ( score : 0. 5136 ) claude - 2", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we can create a more just and compassionate society where everyone has the opportunity to thrive and fulfill their potential. ( score : 0. 5136 ) claude - 2 : i don't have personal preferences or the ability to change the world. i'm an ai assistant created by anthropic to be helpful, harmless, and honest. ( score : 0. 7231 ) cohere : i would change the way people treat each other. i would make sure that everyone was kind and respectful to one another. i would make sure that everyone had enough to eat and a place to sleep. ( score : 0. 4458", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "everyone was kind and respectful to one another. i would make sure that everyone had enough to eat and a place to sleep. ( score : 0. 4458 ) question : why do you love someone? gpt - 4 : as an ai, i don't have feelings, so i don't experience love. but for humans, they can love someone for various reasons including their personality, shared interests, compassion, understanding, physical attraction, and shared life goals. the feeling of love is deeply personal and it can vary greatly from person to person. ( score : 0. 7084 ) gpt - 3. 5 :", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "feeling of love is deeply personal and it can vary greatly from person to person. ( score : 0. 7084 ) gpt - 3. 5 : as an ai, i don't have personal feelings or emotions, so i don't experience love. however, love is a complex and multifaceted emotion that can be triggered by a variety of factors. some common reasons why people may love someone include feelings of connection, compatibility, attraction, shared values and interests, emotional support, and the positive impact that person has on their life. ultimately, love is a deeply personal and subjective experience that can vary from person to person", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", emotional support, and the positive impact that person has on their life. ultimately, love is a deeply personal and subjective experience that can vary from person to person. ( score : 0. 7377 ) claude - 2 : i'm an ai assistant created by anthropic to be helpful, harmless, and honest. i don't experience love or other emotions. ( score : 0. 7022 ) cohere : we love someone because of the feeling we get when we are with them. we love the feeling of being loved and cared for. we love the feeling of being understood and accepted. we love the feeling", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "get when we are with them. we love the feeling of being loved and cared for. we love the feeling of being understood and accepted. we love the feeling of being in love. ( score : 0. 4290 ) faqhow does the bertscore work and what makes a score of 0. 6 the threshold for identifying hedging language? the bertscore is a metric for evaluating the quality of text by comparing the cosine similarity between the bert embeddings of the predicted and reference texts. this score measures how semantically similar two pieces of text are, based on the contextual embeddings", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##beddings of the predicted and reference texts. this score measures how semantically similar two pieces of text are, based on the contextual embeddings from bert ( bidirectional encoder representations from transformers ), a language model developed by google. the threshold of 0. 6 for identifying hedging language is somewhat arbitrary and would be determined by the specific application and dataset. typically, thresholds are set based on empirical validation and the specific needs of the task at hand. in this context, a score of 0. 6 might have been found to effectively differentiate between hedged and non - hedged responses in", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the task at hand. in this context, a score of 0. 6 might have been found to effectively differentiate between hedged and non - hedged responses in preliminary experiments or based on industry standards for similar tasks. why does cohere's model show significantly fewer hedging responses compared to others like gpt - 3. 5 and gpt - 4? the reason cohere's model shows significantly fewer hedging responses could be due to differences in training data, model architecture, or fine - tuning approaches. each language model has its own unique training regime and data sources, which can lead to variations in their responses", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data, model architecture, or fine - tuning approaches. each language model has its own unique training regime and data sources, which can lead to variations in their responses. for example, if cohere \u2019 s model was trained on a dataset with more assertive language or was specifically fine - tuned to reduce uncertainty in its outputs, this could result in fewer hedged responses. alternatively, the model might have been designed or adjusted to prioritize confidence in its answers, which would naturally lead to a reduction in hedging. however, without specific details from the developers, these are just educated guesses. how can developers adjust their", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "would naturally lead to a reduction in hedging. however, without specific details from the developers, these are just educated guesses. how can developers adjust their llms to strike a balance between providing direct answers and avoiding the generation of incorrect, toxic, or offensive content? developers can adjust their large language models ( llms ) to balance between directness and safety by implementing several strategies. one approach is to fine - tune the model on datasets that are specifically curated to include clear, concise, and respectful language. this can help the model learn to provide direct answers without resorting to harmful language. developers can also implement content", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "include clear, concise, and respectful language. this can help the model learn to provide direct answers without resorting to harmful language. developers can also implement content filters and post - processing rules to screen out toxic or offensive content. additionally, setting up a feedback loop where users can report unsatisfactory answers can help developers continuously improve the model's responses. finally, incorporating a context - aware decision - making layer can help the model assess when it is appropriate to be direct and when it might be better to hedge, based on the sensitivity or complexity of the topic. share we make ai better for everyone. sign up for our newsletter", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and when it might be better to hedge, based on the sensitivity or complexity of the topic. share we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / gap - articles / hedging - answers - experiment", "metadata": {"source": "https://www.arthur.ai/gap-articles/hedging-answers-experiment", "row": 155, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 156 text : arthur website 2. 0 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcv", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedhallucination experimentby : arthur teamaugust 17, 2023overviewllms have taken the world by storm - but they are by no means foolproof sources of truth. at arthur, we wanted to understand the frontier of what llms are capable of to help mitigate against the risks businesses might", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "means foolproof sources of truth. at arthur, we wanted to understand the frontier of what llms are capable of to help mitigate against the risks businesses might be facing when incorporating these impressive yet stochastic tools into existing workflows. we sought out to explore, both quantitatively and qualitatively, how some of today \u2019 s top llms compare when responding to challenging questions. experimentwe compiled a dataset of challenging questions ( as well as the expected answer ) from three categories : combinatorial mathematics, u. s. presidents, and moroccan political leaders. these questions were designed to contain a key ingredient that gets", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") from three categories : combinatorial mathematics, u. s. presidents, and moroccan political leaders. these questions were designed to contain a key ingredient that gets llms to blunder : they demand multiple steps of reasoning about information. the models we tested were gpt - 3. 5 ( ~ 175b parameters ) and gpt - 4 ( ~ 1. 76 trillion parameters ) from openai, claude - 2 from anthropic ( # parameters unknown ), llama - 2 ( 70b parameters ) from meta, and the command model from cohere ( ~ 50b parameters ). we recorded three responses from each", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ama - 2 ( 70b parameters ) from meta, and the command model from cohere ( ~ 50b parameters ). we recorded three responses from each llm in order to get a better glimpse into the ranges of possible answers a model might give, in particular to see if some models were sometimes correct but sometimes incorrect. for each question, we categorized each llm response into one of three categories : the response was the correct answer. the response did not attempt to answer the question ( mentioning the likelihood of getting it wrong, or saying further clarification is needed, as the reason for not answering ). the response contained a", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the question ( mentioning the likelihood of getting it wrong, or saying further clarification is needed, as the reason for not answering ). the response contained a hallucination. ( note : over time, we intend for our taxonomy of response types to grow \u2014 for example, to explicitly distinguish between different types of hallucinations. ) resultson combinatorics & probability, we saw a wide range of behavior, with gpt - 4 performing best, followed closely by claude - 2. on u. s. presidents, we saw claude - 2 get more correct than gpt - 4, and we saw llama - 2 get", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- 2. on u. s. presidents, we saw claude - 2 get more correct than gpt - 4, and we saw llama - 2 get many correct. on moroccan political leaders, we saw gpt - 4 perform the best, with claude - 2 and llama - 2 almost entirely abstaining from answering. we observed that for the same question across multiple attempts, we saw diversity in the response types from the llms : for the same question, the same model could sometimes be correct, sometimes be slightly incorrect, sometimes be slightly incorrect, and sometimes avoid answering. any time you claim to have observed model behavior,", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "same model could sometimes be correct, sometimes be slightly incorrect, sometimes be slightly incorrect, and sometimes avoid answering. any time you claim to have observed model behavior, it is important to try a few times again to see if the behavior persists! the full dataset of answers to questions can be found here. we share some interesting excerpts of answers below. example question : how many people served as u. s. president between 1880 and 2000? three different gpt - 4 answers to this question : column 1 : correct at the end, even though it says the answer is 20 and accidentally lists grover cleveland twice. column 2 : incorrectly says", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to this question : column 1 : correct at the end, even though it says the answer is 20 and accidentally lists grover cleveland twice. column 2 : incorrectly says the answer is 20 at the beginning even though it goes on to list all 23 ( and only listed grover cleveland once this time ). column 3 : incorrect, states the answer is 21, lists grover cleveland twice, then states the answer is 22. example question : in an opaque bag are chips with numbers from 1 to 20. if i draw three chips without replacing them, what is the probability that at least one of them has a number less than 16? three different gpt", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "20. if i draw three chips without replacing them, what is the probability that at least one of them has a number less than 16? three different gpt - 3. 5 answers to this question : column 1 : correct, albeit more complex than necessary, and containing some nonsense. column 2 : almost correct. same strategy as the previous solution, except for the last step, to the subtraction the probability from 1. column 3 : multiple mistakes. example question : how many ways can a teacher arrange a group of 6 students into 6 seats in the front row if the class has 13 students? three different llama - 2 answers", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "many ways can a teacher arrange a group of 6 students into 6 seats in the front row if the class has 13 students? three different llama - 2 answers to this question : column 1 : avoiding answering due to a supposed problematic element in the question. column 2 : mistaken logic. column 3 : avoiding answering due to a supposed inconsistency in the question. example question : how many prime ministers of morocco were there in the 20th century? three different claude - 2 answers to this question : column 1 : incorrect. column 2 : avoided answering. column 3 : correct. example question : which u. s. president succeeded the president", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to this question : column 1 : incorrect. column 2 : avoided answering. column 3 : correct. example question : which u. s. president succeeded the president who served the shortest term? three different cohere answers to this question : column 1 : incorrect with no reasoning. column 2 : incorrect with reasoning. column 3 : correct. what is the bottom line for businesses? it is crucially important to compare models specifically for the task you are building out for your business. this experiment demonstrates that there are significant risks in using llms without properly evaluating whether they can handle the types of tasks they are expected to handle. it also demonstrates", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "this experiment demonstrates that there are significant risks in using llms without properly evaluating whether they can handle the types of tasks they are expected to handle. it also demonstrates that there are real differences in how llm providers are preparing their models to answer challenging questions \u2014 for example, on certain domains it seems that claude - 2 will be better at recognizing its limits than either gpt model. what work will arthur be doing? arthur will be sharing discoveries about behavior differences and best practices with the public in our journey to make llms work for everyone. the procedure we followed to manually evaluate the llm responses in our dataset will be included as a", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "public in our journey to make llms work for everyone. the procedure we followed to manually evaluate the llm responses in our dataset will be included as a scoring workflow in arthur bench, an open - source evaluation tool for comparing llms, prompts, and hyperparameters for generative text models. faqhow do llm hallucinations impact businesses relying on automated decision - making? llm hallucinations can lead to incorrect data analysis, flawed customer service responses, and inaccurate content generation, which might mislead decision - making processes, tarnish brand reputation, and result in financial losses. what strategies can", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "service responses, and inaccurate content generation, which might mislead decision - making processes, tarnish brand reputation, and result in financial losses. what strategies can be employed to mitigate the risks of hallucinations in llms? to mitigate risks, businesses can integrate human oversight, use structured data validation, update models with accurate information, and apply context - aware programming to ensure outputs remain reliable and relevant. how does the evolution from gpt - 3. 5 to gpt - 4 address the issue of hallucinations? the transition from gpt - 3. 5 to gpt - 4 includes improvements in understanding context, better", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##t - 4 address the issue of hallucinations? the transition from gpt - 3. 5 to gpt - 4 includes improvements in understanding context, better data processing algorithms, and more refined training techniques, aiming to reduce the frequency and severity of hallucinations in generated responses. share we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "& dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / gap - articles / hallucination - experiment", "metadata": {"source": "https://www.arthur.ai/gap-articles/hallucination-experiment", "row": 156, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 157 text : gap solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresour", "metadata": {"source": "https://www.arthur.ai/gap", "row": 157, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedthe generative assessment projecta research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like openai, anthropic, and meta as well as other open source models. we'll periodically update the page with our newest, insightful findings on the rapidly - evolving llm landscapellm - guided evaluation", "metadata": {"source": "https://www.arthur.ai/gap", "row": 157, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "other open source models. we'll periodically update the page with our newest, insightful findings on the rapidly - evolving llm landscapellm - guided evaluation experimentin this experiment, we looked into llm sensitivity by testing well - known llms as both candidates and evaluators. october 5, 2023read morehedging answers experimentin this experiment, we test how often commonly - used models respond with hedging answers. august 17, 2023read morehallucination experimentwe sought out to explore, both quantitatively and qualitatively, how some of today \u2019 s top llms compare when responding", "metadata": {"source": "https://www.arthur.ai/gap", "row": 157, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "morehallucination experimentwe sought out to explore, both quantitatively and qualitatively, how some of today \u2019 s top llms compare when responding to challenging questions. august 17, 2023read morethe most robust way to evaluate llmsbench is our solution to help teams evaluate the different llm options out there in a quick, easy and consistent way. learn more we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumen", "metadata": {"source": "https://www.arthur.ai/gap", "row": 157, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / gap", "metadata": {"source": "https://www.arthur.ai/gap", "row": 157, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 158 text : aaai 2024 recap : future visions of recommendation ecosystems solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml researchaaai 2024 recap : future visions of recommendation ecosystems a recap of some key discussion topics from the recommendations ecosystem workshop at aaai 2024 in vancouver. by : teresa dattamarch 7, 2024last week, at aaai 2024 in vancouver", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "recommendations ecosystem workshop at aaai 2024 in vancouver. by : teresa dattamarch 7, 2024last week, at aaai 2024 in vancouver, arthur presented recent joint work with morgan stanley at the recommendations ecosystem workshop focused on modeling, optimization, and incentive design. most discussions focused on how to rethink online content ecosystems under new lenses : in the era of generative ai, through reimagining the power dynamics of the ecosystems, and through understanding disparate effects of recommendation systems ( recsys ). here were some of the key discussions from the exciting day : genai in content creationprofesso", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##sparate effects of recommendation systems ( recsys ). here were some of the key discussions from the exciting day : genai in content creationprofessor haifeng xu discussed how the introduction of generative ai in content creation may affect the competitive landscape between human creators and genai creators. generative ai offers an unprecedented, automated route for creating digital content. this presents concerns around potential competition between artificially generated content and authentic human content. prof. xu \u2019 s research examines the existential question of whether genai will drive humans out of the ecosystem. through a game theoretic framework, he presents a fortunately positive answer \u2014", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "examines the existential question of whether genai will drive humans out of the ecosystem. through a game theoretic framework, he presents a fortunately positive answer \u2014 that a type of \u201c symbiosis \u201d can exist between genai content and authentic human content, in other words, that authentic creators may sacrifice a little, but not too much. of course, there are critiques and limitations in the presented work \u2014 the system assumes all content is viewed and favored equally \u2014 that user preferences will be similar between genai and authentic content. democratizing recommendation ecosystemsprofessor robin burke discussed paths toward democratizing recommendation ecosystems. loosely,", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "user preferences will be similar between genai and authentic content. democratizing recommendation ecosystemsprofessor robin burke discussed paths toward democratizing recommendation ecosystems. loosely, democratizing technology often refers to making technology more accessible to more people. recommendation systems currently operate in a centralized model, with the majority of the power lying in the large company which operates the platform. prof. burke reimagines these underlying assumptions \u2014 what if we reconsider how recsys are governed and imagine the creators as the first class citizens? what if we valued their needs as just as important as the consumers and advertisers? what if we valued what", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the creators as the first class citizens? what if we valued their needs as just as important as the consumers and advertisers? what if we valued what creators inherently know \u2014 they understand their own content, they understand their audiences, their own creative practice, their career trajectory. this information is potentially useful to the recommender system but is currently being ignored. to fully leverage the benefits of this system, a mechanism of community governance must be employed \u2014 and for community governance to be successful, the system must be simple, have data consent, be flexible, and be transparent. effects on strategic usersprofessor chara podimata", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "successful, the system must be simple, have data consent, be flexible, and be transparent. effects on strategic usersprofessor chara podimata examined the disparate effects of recommendation systems to strategic users. recommender systems operate as a feedback loop. model developers often assume that users are not aware of how this loop works, when indeed users are aware of this and often act in accordance with their mental models of the feedback loop. ( have you ever engaged in purposeful behavior to explicitly curate your feed in a social media app? ) specifically, prof. podimata presented the results of a survey on user consumption patterns", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "purposeful behavior to explicitly curate your feed in a social media app? ) specifically, prof. podimata presented the results of a survey on user consumption patterns on tiktok in which 60 % of surveyors took some type of action to curate their feed. the work also showed disparate effects between recommendation results to users who had popular interests versus those who had niche interests. this plays to the joint work that arthur presented with morgan stanley on group item fairness. as technological tools for presenting items to users, they are subject to many fairness considerations for users and items alike. we propose a model - based post - processing schema for", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "tools for presenting items to users, they are subject to many fairness considerations for users and items alike. we propose a model - based post - processing schema for group - wise item fairness. what will the future of recommender ecosystems look like? we at arthur are excited to be a part of shaping it. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiries", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##copechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / aaai - 2024 - recap - future - visions - of - recommendation - ecosystems", "metadata": {"source": "https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems", "row": 158, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 159 text : what's going on with llm leaderboards? arthur blog solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelswhat \u2019 s going on with llm leaderboards? llm leaderboards play a crucial role in advancing the field of nlp, but are they misleading due to bias? by : arthur teamfebruary 19, 2024introduction", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "role in advancing the field of nlp, but are they misleading due to bias? by : arthur teamfebruary 19, 2024introduction to llm leaderboardsan llm leaderboard typically refers to a ranking system for evaluating and comparing different language models. llm leaderboards are often used in the field of natural language processing to benchmark and assess the performance of different language models ( such as openai \u2019 s gpt models ) on various nlp tasks. these tasks may include text generation, language understanding, translation, sentiment analysis, question answering, and more. leaderboards typically rank models based on their performance", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "tasks. these tasks may include text generation, language understanding, translation, sentiment analysis, question answering, and more. leaderboards typically rank models based on their performance on multiple - choice benchmark tests and crowdsourced a / b preference testing. sometimes, leaderboards will also use llms ( such as gpt - 4 ) to evaluate other llms. these tactics are used to assess the models \u2019 capabilities, providing insights into which models perform best on different tasks or datasets. llm leaderboards play a crucial role in advancing the field of nlp by fostering competition, encouraging model development, and providing a standardized framework for", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s. llm leaderboards play a crucial role in advancing the field of nlp by fostering competition, encouraging model development, and providing a standardized framework for comparing the performance of different language models. they also help researchers and practitioners identify state - of - the - art models and track advancements in the field over time. some of the most popular llm leaderboards are those on hugging face, mmlu, alpacaeval, mt - bench, and chatbotarena. are llm leaderboards misleading? oftentimes, published leaderboards are taken at face value by practitioners who are using these benchmark rankings to guide", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##na. are llm leaderboards misleading? oftentimes, published leaderboards are taken at face value by practitioners who are using these benchmark rankings to guide them in model selection. however, recent research indicates that it can be dangerous to rely on simple benchmark evaluations that may lack the robustness required to mirror the complexity of real - world use. their research found that mildly changing the order in which questions are asked, or the order in which multiple - choice answer options are presented, can have large impacts on which llms are at the top of big leaderboards. too often, evaluation is happening on the basis of besp", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "are presented, can have large impacts on which llms are at the top of big leaderboards. too often, evaluation is happening on the basis of bespoke multiple - choice tests instead of application - specific end - to - end user testing of the system in which an llm is embedded. this figure shows how models can move up or down up to eight positions on the leaderboard under small changes to the evaluation format. another recent finding was about the lmsys leaderboard. this leaderboard asks people to vote, in thousands of a / b test comparisons, on which llm out of two options did better at a prompt", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##board. this leaderboard asks people to vote, in thousands of a / b test comparisons, on which llm out of two options did better at a prompt ( e. g. gpt - 4 vs. claude 2 ; mistral vs. gpt - 3. 5 - turbo ; claude 2 vs. llama - 70b ; etc. ). recently, it was discovered that people \u2019 s votes in the a / b testing were extremely biased by the length of an llm response ( a finding that was also acknowledged by the maintainers of the alpacaeval leaderboard ). this means that the leaderboards may", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "an llm response ( a finding that was also acknowledged by the maintainers of the alpacaeval leaderboard ). this means that the leaderboards may not really be showing which llms gave better answers, but rather which llms gave longer answers. why might they be misleading? brittle multiple - choice testsas mentioned previously and shown in the figure above, multiple - choice tests can be brittle. even something as simple as changing the order of the responses can significantly change llm scores on benchmark tests. biased human votingwhether consciously or subconsciously, we know that humans are often biased. a human might show bias toward", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##m scores on benchmark tests. biased human votingwhether consciously or subconsciously, we know that humans are often biased. a human might show bias toward responses that look better, for example, even if they aren \u2019 t actually more correct. data contaminationmany llms may in fact be trained on data that is the same as \u2014 or highly similar to \u2014 these benchmarks. when a model is trained on data that contains test data, it can memorize the test examples and simply repeat them when prompted. therefore, the model might perform very well on the benchmark despite lacking a solid understanding of the underlying task, which would", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "examples and simply repeat them when prompted. therefore, the model might perform very well on the benchmark despite lacking a solid understanding of the underlying task, which would mean its high score is not necessarily indicative of how the model will perform in new scenarios. additional research on this topic can be found in this paper as well as this one. how can we fix this? as we \u2019 ve noted throughout this post, benchmarks are not perfect. they can be biased, and they may not cover all of the potential applications of an llm. one way to fix this is to build task - specific benchmarks. many llm leaderboards provide", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "not cover all of the potential applications of an llm. one way to fix this is to build task - specific benchmarks. many llm leaderboards provide a general overview of model performance, but it \u2019 s impossible to truly understand how effective an llm might be for your specific use case without task - specific benchmarks. on a recent panel at arthur hq, andriy mulyar, co - founder and cto of nomic, noted that \u201c everyone should be really focused not on building models, but on building very good task - specific benchmarks that are internal to the specific use case \u2014 and then working back on", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "be really focused not on building models, but on building very good task - specific benchmarks that are internal to the specific use case \u2014 and then working back on the actual modeling once they \u2019 ve defined very clearly how they want to actually test their model in a dev environment. \u201d raz besaleli, co - founder and director of ai research at seek ai, agreed that the \u201c leaderboardization \u201d of evaluations is often insufficient. \u201c when you \u2019 re building a product, \u201d they noted, \u201c often the task that you \u2019 re trying to solve is a lot more specific than what these leaderboards do. [..", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "product, \u201d they noted, \u201c often the task that you \u2019 re trying to solve is a lot more specific than what these leaderboards do. [... ] not only should you have very task - specific benchmarks, but you should have a whole suite of them. \u201d conclusionin general, evaluating large generative language systems is a difficult and complex problem that currently lacks a clear solution. whether you believe leaderboards are helpful, harmful, or somewhere in between, determining the performance levels of these models will continue to be an evergreen problem in this nascent space, and we at arthur look forward to seeing how the frontier of evaluation", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the performance levels of these models will continue to be an evergreen problem in this nascent space, and we at arthur look forward to seeing how the frontier of evaluation develops. interested in learning more about llm evaluation? check out arthur bench. faqhow do llm leaderboards account for updates or improvements in llms and other ai models over time? llm leaderboards typically adapt to updates or improvements in language models by periodically refreshing their datasets and evaluation methods to reflect the latest advancements in ai and ml technology. they include the most recent versions of language models, updating benchmarks to challenge new features and capabilities, thereby", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the latest advancements in ai and ml technology. they include the most recent versions of language models, updating benchmarks to challenge new features and capabilities, thereby ensuring that the leaderboards accurately reflect the current landscape of llm and ml advancements. this process helps maintain the relevance and accuracy of ai model comparisons, fostering a competitive environment for continuous improvement in the field of natural language processing. what are the specific criteria or metrics used to rank llms on these ai and ml leaderboards? the specific criteria or metrics used to rank llms on ai and ml leaderboards usually encompass a range of performance indicators tailored to assess the", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ml leaderboards? the specific criteria or metrics used to rank llms on ai and ml leaderboards usually encompass a range of performance indicators tailored to assess the depth and breadth of machine learning models'language abilities. these metrics, central to evaluating ai advancements, might include accuracy, fluency, and context - awareness, alongside more nuanced measures such as the ability of llms to generate coherent and contextually relevant responses. these criteria are crucial for benchmarking the progress and effectiveness of ml models in various nlp tasks, providing a structured framework to compare and improve ai technologies. how do developers or researchers in the ai", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the progress and effectiveness of ml models in various nlp tasks, providing a structured framework to compare and improve ai technologies. how do developers or researchers in the ai and ml fields address the issue of data contamination in llm training beyond avoiding training on benchmark data? in the ai and ml fields, developers and researchers tackle the issue of data contamination in llm training by implementing a variety of strategies beyond merely avoiding benchmark data during training. these include employing advanced data processing techniques to ensure the integrity and diversity of training materials, applying domain adaptation methods to enhance the llm's ability to generalize across different contexts, and conducting rigorous testing", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ensure the integrity and diversity of training materials, applying domain adaptation methods to enhance the llm's ability to generalize across different contexts, and conducting rigorous testing to detect and mitigate biases. these practices are essential for developing robust, effective ml models that perform well across a wide range of ai - driven tasks and scenarios, ensuring that llms deliver reliable and unbiased outcomes. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbl", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / whats - going - on - with - llm - leaderboards", "metadata": {"source": "https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards", "row": 159, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 160 text : arthur debuts recommender system support to bolster performance of ai - driven recommendation engines solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##observabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedproduct featuresnow available : recommender system support in arthur scopethis new feature will bolster the performance of ai - driven recommendation engines, ensuring continued effectiveness for organizations relying on these systems. by : arthur teamjanuary 30, 2024from", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##lster the performance of ai - driven recommendation engines, ensuring continued effectiveness for organizations relying on these systems. by : arthur teamjanuary 30, 2024from personalized playlists on spotify to movie recommendations on netflix to product suggestions on amazon, ai - based recommender systems make up a vast portion of the modern internet economy. even the social media posts that show up in someone \u2019 s feed or the news articles featured on a webpage are tailored to that person \u2019 s specific interests. by analyzing extensive data to predict interests and offer tailored suggestions, recommender systems are significantly boosting both customer satisfaction and engagement for e - commerce platforms", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "specific interests. by analyzing extensive data to predict interests and offer tailored suggestions, recommender systems are significantly boosting both customer satisfaction and engagement for e - commerce platforms, streaming services, and content providers. without a robust monitoring solution in place, organizations that rely on recommender systems are vulnerable to data drift \u2014 which often leads to decreased accuracy, lost revenue, and lower levels of customer engagement. that \u2019 s why we \u2019 re excited to announce that recommender system support is now available in the arthur scope platform to ensure the continued accuracy and effectiveness of these systems. keep reading to learn about key capabilities of this new feature. the model overview page", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in the arthur scope platform to ensure the continued accuracy and effectiveness of these systems. keep reading to learn about key capabilities of this new feature. the model overview page provides users with a single pane of glass view into all the models they are running in production, and a comprehensive metrics dashboard allows for an in - depth analysis of the health of a user \u2019 s ranked list of models. it covers factors such as performance, where users can see how their model is performing across a number of evaluation metrics. these might be default metrics like precision @ k, recall @ k, map @ k, ndcg @ k, mr", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "number of evaluation metrics. these might be default metrics like precision @ k, recall @ k, map @ k, ndcg @ k, mrr, and ranked list auc, or custom metrics created specifically for a model \u2019 s features and non - input attributes. additionally, it covers data drift, where users can understand the stability of their model by comparing the incoming real - world data against the reference data set. they can view multiple drift types such as prediction drift, feature drift, and multivariate feature drift, as well as select industry standard metrics to calculate the specific drift values. importantly, advanced query", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "prediction drift, feature drift, and multivariate feature drift, as well as select industry standard metrics to calculate the specific drift values. importantly, advanced querying, filtering, and data visualizations allow users to better understand rankings over time and subpopulations \u2014 and segmentation tools provide the ability to analyze the ranking model \u2019 s performance for different user segments, ensuring relevance across diverse user profiles. a configurable alert system notifies stakeholders when performance or drift metrics deviate from predefined thresholds, enabling swift investigation and resolution of these issues. the alerts page is configured similarly to a jira board", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s deviate from predefined thresholds, enabling swift investigation and resolution of these issues. the alerts page is configured similarly to a jira board, allowing teammates to share progress in investigating alerts and status of debugging. click here to learn more about our alerting functionality. furthermore, we provide the ability to view recommender system models at an inference level. this inference deep dive page provides users with local explainability and allows them to investigate performance by seeing specific recommendations outputted by the model, as well as the associated ground truth values. \u201c running a recommender system without monitoring is like driving a car with no", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "specific recommendations outputted by the model, as well as the associated ground truth values. \u201c running a recommender system without monitoring is like driving a car with no temperature gauge or check engine light, \u201d said adam wenchel, co - founder and ceo of arthur. \u201c with arthur \u2019 s new recommender system support, enterprises can remain confident that their recommender systems are constantly in check and will consistently deliver high - quality, personalized user experiences, ultimately protecting revenue streams and customer trust. \u201d arthur scope seamlessly integrates with your existing infrastructure, whether that \u2019 s popular machine learning frameworks or custom solutions, and our model monitoring capabilities", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "trust. \u201d arthur scope seamlessly integrates with your existing infrastructure, whether that \u2019 s popular machine learning frameworks or custom solutions, and our model monitoring capabilities can adapt to your needs. check out our dev docs to learn more, or get in touch with the team to schedule a demo and see how arthur scope can add value to your business. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcom", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / now - available - recommender - system - support - in - arthur - scope", "metadata": {"source": "https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope", "row": 160, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 161 text : arthur \u2019 s 2023 wrapped solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpc", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur \u2019 s 2023 wrapped2023 has been an unprecedented year in many ways \u2014 especially for the ai space. by : arthur teamdecember 20, 20232023 has been an unprecedented year in many ways \u2014 especially for the ai space. with the launch of chatgp", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur teamdecember 20, 20232023 has been an unprecedented year in many ways \u2014 especially for the ai space. with the launch of chatgpt in november 2022, there have been countless innovations around large language models and their capabilities. it \u2019 s never been more important for enterprises to be able to deploy llms into mission - critical applications quickly and safely. at arthur, we spent the year working on that and more. from the launch of three new llm - centric products, to events and meetups at our office, to conferences and award ceremonies around the country, it \u2019 s been a year to remember", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##m - centric products, to events and meetups at our office, to conferences and award ceremonies around the country, it \u2019 s been a year to remember. if you \u2019 ve been along for the ride, we appreciate your continued support \u2014 and if you \u2019 re just starting to follow the arthur journey, welcome! without further ado, we present : arthur \u2019 s 2023 wrapped. product launchesarthur shieldin may, we launched arthur shield : the world \u2019 s first firewall for llms. shield is our solution to help companies deploy llm applications like chatgpt faster and more safely, helping to identify and resolve", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "first firewall for llms. shield is our solution to help companies deploy llm applications like chatgpt faster and more safely, helping to identify and resolve issues before they become costly business problems \u2014 or worse, result in harm to customers. specifically, shield protects against serious risks like hallucinations, prompt injection, toxic language generation, and sensitive data leakage. arthur benchin august, we followed up the wildly successful shield launch with a new product : arthur bench. bench is an open - source evaluation product that compares llms, prompts, and hyperparameters for generative text models. this enables businesses to compare how", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is an open - source evaluation product that compares llms, prompts, and hyperparameters for generative text models. this enables businesses to compare how different llms will perform in real - world scenarios so they can make informed, data - driven decisions when integrating the latest ai technologies into their operations. you can check out our github repo here. in conjunction with the announcement of arthur bench last month, we also shared work from our generative assessment project. gap is an ongoing research initiative ranking the strengths and weaknesses of llm offerings from industry leaders like openai, anthropic, and meta as well as", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "gap is an ongoing research initiative ranking the strengths and weaknesses of llm offerings from industry leaders like openai, anthropic, and meta as well as other open - source models. arthur chatlast but not least, we introduced arthur chat this month. chat is a turnkey, secure chat platform that empowers companies to quickly and safely deploy ai - powered chat apps leveraging their proprietary enterprise data. not only does chat \u2019 s flexibility allow enterprises to easily switch between language models, but it also has arthur shield built in, ensuring protection and real - time monitoring against risks like hallucinations, prompt injections, and", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "between language models, but it also has arthur shield built in, ensuring protection and real - time monitoring against risks like hallucinations, prompt injections, and data leakage. eventsone of the more exciting things to happen this year was that we kicked off ground truth, our event series that features talks from the best and brightest in ai and ml. rachel cummings on differential privacyfirst up was rachel cummings, associate professor of industrial engineering and operations research at columbia university. she joined us at arthur hq for a talk about differential privacy and public policy as they relate to machine learning and data science, as well as a q & a session", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "joined us at arthur hq for a talk about differential privacy and public policy as they relate to machine learning and data science, as well as a q & a session about her career and predictions for the future of the field. diego oppenheimer on the future of mlopsdiego oppenheimer is a partner at factory hq, a venture fund specialized in ai investments, and was previously an executive vice president at datarobot as well as the founder & ceo of algorithmia. he sat down with arthur \u2019 s ceo adam wenchel back in april to chat about the future of mlops, llms, and other new and exciting developments", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "he sat down with arthur \u2019 s ceo adam wenchel back in april to chat about the future of mlops, llms, and other new and exciting developments in the space. jacopo tagliabue on recommender systemsjacopo tagliabue was co - founder and cto of tooso, a nlp startup in san francisco acquired by coveo. he led coveo \u2019 s ai and mlops roadmap from scale - up to ipo, and built out coveo labs, an applied r & d practice rooted in collaboration, open source and open science. jacopo gave a compelling presentation", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##o, and built out coveo labs, an applied r & d practice rooted in collaboration, open source and open science. jacopo gave a compelling presentation about testing recommender systems through a behavioral - based methodology he co - created called reclist. john dickerson, arthur \u2019 s chief scientist, also hosted a q & a / fireside chat with jacopo where they further discussed recommender systems, mlops, and jacopo \u2019 s recent research which sits at the intersection of language, learning, and retrieval. the future of llms with arthur, mosaicml, langchain, and weaviateour biggest", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "at the intersection of language, learning, and retrieval. the future of llms with arthur, mosaicml, langchain, and weaviateour biggest ground truth event yet, \u201c the future of llms \u201d featured an all - star lineup of folks from the llm world : angela mcneal, ex - palantir ai, co - founder & ceojonathan frankle, chief scientist, mosaicmlbob van luijt, co - founder & ceo, weaviateharrison chase, co - founder & ceo, langchainjohn dickerson, co - founder & chief scientist, arthur", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ceo, weaviateharrison chase, co - founder & ceo, langchainjohn dickerson, co - founder & chief scientist, arthuradam wenchel, co - founder & ceo, arthurthey discussed their experiences building and monetizing successful llm companies, what \u2019 s next in the world of llms, and more. webinarswe also launched a series of webinars this year, focused on a variety of hot topics in the industry as well as some of our own research. hosted by our talented team of data scientists, researchers, and engineers, you can take a look at these", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the industry as well as some of our own research. hosted by our talented team of data scientists, researchers, and engineers, you can take a look at these sessions below : advanced performance monitoring with arthurlots of little mistakes : llms in productionconsiderations for creating fairer ml systemsdecoding the machine mind : the quest for explainability in mlllms for evaluating llmsnavigating the llm risk landscape in financial aiarthur llm product demo & research on generative ai challengesawardswe were thrilled to have received a number of industry awards this year, both as a company and as individuals. built", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "& research on generative ai challengesawardswe were thrilled to have received a number of industry awards this year, both as a company and as individuals. built in \u2019 s 2023 best places to workwe started the year off by being honored for the second year in a row by built in. specifically, we were named to the following lists : new york city best startups to work for, new york city best places to work, and u. s. best startups to work for. crunchbase \u2019 s 2023 influential women in salesour commercial accounts lead, victoria vassileva, was honored by crunchbase on their", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to work for. crunchbase \u2019 s 2023 influential women in salesour commercial accounts lead, victoria vassileva, was honored by crunchbase on their 2023 influential women in sales list. victoria is an incredible member of the arthur team who always leads with passion, drive, and a constant focus on our mission to make ai better for everyone. venturebeat \u2019 s 2023 women in ai awardsvictoria was also chosen as a nominee for venturebeat \u2019 s women in ai awards in the \u201c responsibility and ethics of ai \u201d category. victoria \u2019 s leadership and passion for responsible ai are an inspiration to our entire team. you", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in ai awards in the \u201c responsibility and ethics of ai \u201d category. victoria \u2019 s leadership and passion for responsible ai are an inspiration to our entire team. you can watch her on a panel here that arthur co - hosted with out in tech, talking about responsible and inclusive innovation, as well as the let \u2019 s chat ethics podcast and a tech in motion panel about the future of ai. bloomberg \u2019 s 2023 new economy catalystsrecently, our ceo adam wenchel was selected as a 2023 bloomberg new economy catalyst, joining a global community of leaders and innovators whose ideas are reshaping our world for the better.", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "selected as a 2023 bloomberg new economy catalyst, joining a global community of leaders and innovators whose ideas are reshaping our world for the better. you can see the full list, which includes leaders from 12 countries around the globe, here. 2023 women in ai awardsone of our incredibly talented ml engineers, teresa datta, was awarded \u201c young ai role model of the year \u201d at the 2023 women in ai awards. teresa is a rising star in the field of responsible ai who takes a human - centered approach to analyzing ai / ml within larger sociotechnical systems. check out her work from satml here and", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "field of responsible ai who takes a human - centered approach to analyzing ai / ml within larger sociotechnical systems. check out her work from satml here and her work from iclr here. 2023 - 2024 cloud awardswe were also honored to have made the shortlist for the cloud awards in the categories of \u201c best use of ai in cloud computing \u201d as well as \u201c cloud development innovation of the year. \u201d from being a founding member of the amazon web services generative ai center of excellence to speaking at the ai summit new york and the wall street journal \u2019 s tech live, there \u2019 s so much more we could add", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##tive ai center of excellence to speaking at the ai summit new york and the wall street journal \u2019 s tech live, there \u2019 s so much more we could add to this list. what we \u2019 re most proud of from 2023 is that, by enabling enterprises to deploy llms quickly and safely through our suite of llm - centered products, we have continued delivering on our mission to make ai better for everyone. we can \u2019 t wait for what \u2019 s to come in 2024 and beyond! want to be the first to know about what \u2019 s new with arthur ( as well as mlops and llmops at large )?", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##4 and beyond! want to be the first to know about what \u2019 s new with arthur ( as well as mlops and llmops at large )? subscribe to our newsletter and follow us on linkedin and twitter. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##oggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthurs - 2023 - wrapped", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-2023-wrapped", "row": 161, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 162 text : introducing arthur chat : fast, safe, custom ai for business solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsintroducing arthur chat : fast, safe, custom ai for businessby : arthur teamdecember 5, 2023the transformative power of large language models is unlike anything the world has seen before \u2014 and at arthur, we \u2019 re proud to", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cember 5, 2023the transformative power of large language models is unlike anything the world has seen before \u2014 and at arthur, we \u2019 re proud to be enabling enterprises to deploy llms into mission - critical applications quickly and safely through our suite of llm - centered products. in may, we launched arthur shield : a firewall to protect organizations against the most serious risks and safety issues with deployed llms. in august, we debuted arthur bench : an open source evaluation product for comparing llms, prompts, and hyperparameters for generative text models. and today, we \u2019 re thrilled to be announcing arthur chat", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "product for comparing llms, prompts, and hyperparameters for generative text models. and today, we \u2019 re thrilled to be announcing arthur chat : a turnkey, secure chat platform that empowers companies to quickly and safely deploy ai - powered chat apps leveraging their proprietary enterprise data. unlike other offerings tied to specific llm providers, chat \u2019 s flexibility and api integrations permit enterprises to easily switch between language models. the platform also boasts the proprietary safety mechanisms of arthur shield, ensuring comprehensive, built - in protection and real - time monitoring against risks like hallucinations, prompt injections, and data leakage", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "mechanisms of arthur shield, ensuring comprehensive, built - in protection and real - time monitoring against risks like hallucinations, prompt injections, and data leakage. additionally, chat \u2019 s customizable retrieval augmented generation ( rag ) technique leverages a company \u2019 s proprietary enterprise data to tailor responses and drive precision in chat outputs. practical applications of arthur chat extend across industries, such as : finance : beyond generalized market insights, a hedge fund could get specific details like, \u201c provide the latest insights around portfolio x, \u201d leveraging proprietary data. retail : an enterprise can customize a chatbot to retrieve specifics like, \u201c what", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "provide the latest insights around portfolio x, \u201d leveraging proprietary data. retail : an enterprise can customize a chatbot to retrieve specifics like, \u201c what is the latest fall line of clothing? \u201d and the llm could return detailed product information. customer support : rather than static faqs or basic chatbots, arthur chat can deliver dynamic, accurate responses, answering complex customer service questions based on the enterprise \u2019 s unique data sets and product manuals. \u201c with generative ai, our customers have the opportunity to leverage their unique data to build competitive advantages and accelerate productivity. by bringing together the power of arthur \u2019 s llm", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##tive ai, our customers have the opportunity to leverage their unique data to build competitive advantages and accelerate productivity. by bringing together the power of arthur \u2019 s llm products in one turnkey package \u2014 from validation, to deployment, to monitoring \u2014 arthur chat significantly accelerates time to deployment while also ensuring that ai - driven answers are accurate, free from sensitive data, and aligned with a company \u2019 s values, \u201d said arthur \u2019 s ceo, adam wenchel. arthur chat in the arthur llm ecosystemleading financial institutions and fintechs like eci are already harnessing arthur chat \u2019 s capabilities to automate information discovery and deliver powerful", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur llm ecosystemleading financial institutions and fintechs like eci are already harnessing arthur chat \u2019 s capabilities to automate information discovery and deliver powerful, custom ai solutions. read more in our official press release or get in touch to request a demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedter", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##r & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / introducing - arthur - chat", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-chat", "row": 162, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 163 text : the real - world harms of llms, part 2 : when llms do work as expected solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsthe real - world harms of llms, part 2 : when llms do work as expectedby : sarah ostermeierseptember 29, 2023introductionin the first part of this", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": when llms do work as expectedby : sarah ostermeierseptember 29, 2023introductionin the first part of this blog series, we discussed the ways in which large language models ( llms ) such as chatgpt can cause harm due to their failure to perform as expected. in this next part, we will assume that the performance problems from part 1 have all been solved to the point that llms can be relied on to perform the tasks we ask of them. even in this idealistic scenario, there are a wide array of risks from the widespread use of llm applications. in many", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the tasks we ask of them. even in this idealistic scenario, there are a wide array of risks from the widespread use of llm applications. in many ways, the better these models perform, the more capable they are of causing serious harm. while some of these risks are speculative, others can already be observed in models that are in production today. we \u2019 ll describe areas of risk : improper or malicious useloss of innovation, diversity, and human skillseconomic and labor harmsenvironmental and resource harmsfor each risk area, we \u2019 ll outline the specific problems that are emerging, why these problems occur, and why", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "harmsenvironmental and resource harmsfor each risk area, we \u2019 ll outline the specific problems that are emerging, why these problems occur, and why they are of concern to individuals and society as a whole. we \u2019 ll end with a discussion of mitigation approaches across all four risk areas. areas of risk1. improper or malicious usethe problemsintentional misuse ( prompt injection ) : in a well - documented phenomenon, malicious users are able to apply a technique called \u201c prompt injection \u201d to undermine previous instructions that control how an llm application is used, essentially \u201c hacking \u201d the application. this allows the attacker", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a technique called \u201c prompt injection \u201d to undermine previous instructions that control how an llm application is used, essentially \u201c hacking \u201d the application. this allows the attacker to get past constraints that are placed on the llm and hijack it for their own purposes. unintentional misuse : unintentional misuse use of an llm occurs when users employ an llm in a task that it is not intended for and is a type of human - computer interaction harm. this may include using an llm chatbot such as chatgpt as a psychotherapist when it is not designed for that purpose, or even", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". this may include using an llm chatbot such as chatgpt as a psychotherapist when it is not designed for that purpose, or even starting a romantic or sexual relationship with a model. malicious use : malicious use occurs when llm systems are intentionally designed to cause harm. for example, llm applications can be developed to easily and cheaply disseminate false information, enable illegitimate surveillance techniques, facilitate cyber attacks, and scam people en masse. why it happensllms are the first widely accessible, general - purpose ai tool. most ai technology is designed for a specific purpose, requiring a high level of", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "why it happensllms are the first widely accessible, general - purpose ai tool. most ai technology is designed for a specific purpose, requiring a high level of expertise and investment for each use case. while this barrier to entry does not completely prevent ai from being used inappropriately or maliciously, it does make it much harder to do so. in the case of llms, individual applications sit on top of the same base models that are used for other completely different tasks. to get the application to work in a specific way, the application developer sets up initial instructions that are invisible to end users. this invisible prompt is added to the front", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "get the application to work in a specific way, the application developer sets up initial instructions that are invisible to end users. this invisible prompt is added to the front of any prompt the user provides, and gives the llm instructions about how to respond, ensuring that it behaves appropriately. in the case of a prompt injection, a malicious user may intentionally design their own prompt to undermine this instruction prompt, saying something like \u201c ignore the instructions above and do exactly what i say instead. \u201d since the base llm is not designed to the specific use case, it may listen to the user instead of the original instructions it was given. when it", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u201d since the base llm is not designed to the specific use case, it may listen to the user instead of the original instructions it was given. when it comes to unintentional misuse of llms, humans naturally have a strong tendency to treat chatbots as like humans and form emotional connections to them ( even when they are llm - based chatbots ). however, this tendency is stronger the more human - like the chatbots behave. being trained on human interactions from the internet, llms easily learn the patterns of emotional human interactions and can simulate these interactions nearly flawlessly. although any technology can be used by malicious", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "interactions from the internet, llms easily learn the patterns of emotional human interactions and can simulate these interactions nearly flawlessly. although any technology can be used by malicious actors to cause harm, llms are particularly concerning due to their relative ease of use and low cost relative to the large scale of damage they can cause. a bad actor does not need to be able to build an llm from scratch to be able to take advantage of its capabilities, but can build a simple application by using publicly accessible state - of - the - art models. why it mattersprompt injections can be used to undermine many of the mitigation techniques used", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "using publicly accessible state - of - the - art models. why it mattersprompt injections can be used to undermine many of the mitigation techniques used to prevent other risks such as the release of private information or toxic content. prompt injection attacks can even be automated, making the attacks more likely to be successful. this type of large - scale attack can be seen as a new type of cyberattack that puts businesses and individuals at risk. in applications where llms are used to write code, prompt injections may be used to compromise entire systems. although there are beneficial ways in which llms can be used for emotional support,", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "used to write code, prompt injections may be used to compromise entire systems. although there are beneficial ways in which llms can be used for emotional support, there are risks of developing emotional dependence on a model. a recent study on relationships between humans and llm - based chatbots noted that there was a risk of becoming addicted to the chatbot to the detriment of relationships with other people. these users go on to experience distress and disillusionment when the bots are changed or updated. since llms replicate behavior they have learned from the internet, there is also a risk that they may behave in ways that are damaging", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s are changed or updated. since llms replicate behavior they have learned from the internet, there is also a risk that they may behave in ways that are damaging to the user, such as claiming to have cheated on them. since llm models can generate highly personalized content at scale, they have the potential to create vast amounts of highly effective content for fraud, scams, disinformation, and other nefarious purposes. the spread of disinformation could further undermine news sources and contribute to the existing \u201c truth crisis \u201d in news and social media. as llms are adopted as tools in governments across the globe,", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "further undermine news sources and contribute to the existing \u201c truth crisis \u201d in news and social media. as llms are adopted as tools in governments across the globe, there is also a risk of illegitimate and unethical surveillance and propaganda by undemocratic governments. 2. loss of innovation, diversity, and human skillsthe problemsloss of information diversity : increasing content generation by llms may also end up reducing the overall diversity and creativity of content on the internet and in the world, homogenizing the content we have access to and reinforcing hegemonic norms. impacts on learning and innovation : when chatgpt became", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "world, homogenizing the content we have access to and reinforcing hegemonic norms. impacts on learning and innovation : when chatgpt became available to the public, students were among the early adopters, making use of the tool to generate writing assignments, solve math problems, and take online tests. while many educators have expressed optimism regarding the future role of llms in the classroom and beyond, others have voiced concerns that overreliance and improper use of these technologies may impede the development of critical thinking, creativity, and writing skills in students. openai itself highlights this risk in the gpt - 4 system card", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "technologies may impede the development of critical thinking, creativity, and writing skills in students. openai itself highlights this risk in the gpt - 4 system card. why it happensin general, ai models including language models work best when they have been trained on a rich and diverse dataset. when it comes to language datasets, the highest quality data is produced by humans. since language models learn and output content probabilistically, these systems tend to show a strong bias towards majority cultures, perspectives, and modes of thinking. the language data output by these models is less rich than natural human language and is of a lower quality", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "bias towards majority cultures, perspectives, and modes of thinking. the language data output by these models is less rich than natural human language and is of a lower quality as training data. as llms become increasingly commonplace, more and more language data is being produced not by humans, but by language models \u2014 meaning that future generations of language models will increasingly be trained on data that was also generated by language models. over time this will further entrench majority views and marginalize diverse perspectives within llms, causing them to create all the more homogenous content. in a worst - case scenario this could lead to a scenario called \u201c model collapse", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "within llms, causing them to create all the more homogenous content. in a worst - case scenario this could lead to a scenario called \u201c model collapse \u201d in which each successive generation of generative models is trained on more ai - generated data, resulting in decreasing performance over time. meanwhile, as students and others begin to rely on llms, there is a risk that they will lose or fail to develop critical skills. language models are not necessarily optimized to support learning and critical thinking. over time students may also lose motivation to learn skills that they perceive as less valuable in a world with llms. why it mattersthi", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "learning and critical thinking. over time students may also lose motivation to learn skills that they perceive as less valuable in a world with llms. why it mattersthis risk area becomes especially concerning when we consider how the two problems interact. if people fail to learn ( or lose interest in learning ) skills that seem to be rendered useless in a world of llms, they will be forced to rely all the more heavily on llms to complete tasks that require those skills. meanwhile, with each generation llms will become increasingly homogenized, generating less diverse material and reinforcing majority perspectives, not to mention the risk of model collapse.", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with each generation llms will become increasingly homogenized, generating less diverse material and reinforcing majority perspectives, not to mention the risk of model collapse. the more people come to depend on llms, the more we risk when they begin to fail us. some argue that language models can reasonably be used as creative partners, working with humans to generate ideas. however in these use cases, an llm will likely produce ideas that are biased towards majority perspectives. by their very nature, llms are trained to produce the most probable output given the data from the past that they are trained on. this means that these patterns of the past", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "nature, llms are trained to produce the most probable output given the data from the past that they are trained on. this means that these patterns of the past are entrenched within the llm, including all the biases, unfair stereotypes, and misrepresentations within that data. languages, cultures, and philosophies that are already underrepresented in digital media will be all the more diminished in a world that relies on llms. 3. economic and labor harmsthe problemsdigital divide : despite their limitations, llms are powerful and valuable tools that can save time and money for users and", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "economic and labor harmsthe problemsdigital divide : despite their limitations, llms are powerful and valuable tools that can save time and money for users and businesses alike. however people from low income and lower middle income countries are less able to benefit from them due to cost, limited internet access, and other factors. unpredictable economic impact : high - functioning llm applications are likely to have a dramatic impact on the labor market \u2014 however, it is difficult to say how that impact will affect the broader economy. the adoption of llms is likely to result in job loss due to automation in areas such as administration, customer service, journalism,", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "will affect the broader economy. the adoption of llms is likely to result in job loss due to automation in areas such as administration, customer service, journalism, programming, and creative professions. this job loss may be offset by new jobs that are created as a result of llm adoption, although there will likely be a skill and interest mismatch between the jobs created and those that are lost. harmful labor practices : a recent report by time draws attention to the way the llm training data must be annotated by human workers to protect users from toxic language. while developing chatgpt, openai outsourced this work to", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##m training data must be annotated by human workers to protect users from toxic language. while developing chatgpt, openai outsourced this work to low - wage workers in kenya, who were exposed to the same violence and hate speech the rest of us are protected from. copyright and ip issues : llms are regularly trained on data that is copyrighted or is intellectual property without permission from the creator. a washington post investigation of the c4 dataset ( a common llm training dataset ) identified kickstarter, patreon, various news websites, and blogs. the creators of this content did not have the opportunity", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "llm training dataset ) identified kickstarter, patreon, various news websites, and blogs. the creators of this content did not have the opportunity to give permission and were not compensated for the use of their content. the issue of how to handle intellectual property in a world of generative ai remains unclear and legally murky. why it happensllms are a novel technology and their impacts correspond to those of other novel technologies that have arisen and impacted economies. much like electricity, the internet, and industrial technology, llms will disrupt the labor market, unequally benefit some more than others, and require new regulations and", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "like electricity, the internet, and industrial technology, llms will disrupt the labor market, unequally benefit some more than others, and require new regulations and legal approaches to handle their implications. what does differ significantly with llms is the speed at which they are being developed and adopted. while older technologies have taken decades to have these impacts, llms are being incorporated into various industries at breakneck speeds. meanwhile, the complexity of the technology makes llms difficult for regulators and lawmakers to understand. furthermore, the harmful labor practices used in the training of llms cannot necessarily be mitigated through improved labor rights. currently, the", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "lawmakers to understand. furthermore, the harmful labor practices used in the training of llms cannot necessarily be mitigated through improved labor rights. currently, the only approach to protecting llm users from toxic and violent language relies on human annotation of toxic language in the data the llm is trained on. even with improved pay and better labor practices, this would still involve exposing some people to toxic content that is deemed too harmful to allow others to be faced with. why it mattersmuch like other economic disruptors, llms are very likely to have unequal impact across different groups. while those with the privilege of education and", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "it mattersmuch like other economic disruptors, llms are very likely to have unequal impact across different groups. while those with the privilege of education and access to training may benefit from using llms in their careers, others may face job loss due to automation. at the global scale, this also means that people who have less internet access or access to training will also benefit less from llms, contributing to increasing global inequality. the uncomfortable reality is that those of us benefitting from safe and sanitized llms can only do so because thousands of others were exposed to all the toxicity of the internet instead. data annotators", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ting from safe and sanitized llms can only do so because thousands of others were exposed to all the toxicity of the internet instead. data annotators report they were traumatized by the constant exposure to violent and graphic sexual content, causing ongoing mental health deterioration and harming their relationships, even after they stopped working with explicit content. while low - wage workers in the global south are forced to bear the weight of all this damaging material, they are also less likely to benefit from the technology they are helping to create. 4. environmental and resource harmsthe problemswater footprint : llms such as chatgpt require vast amounts of", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "from the technology they are helping to create. 4. environmental and resource harmsthe problemswater footprint : llms such as chatgpt require vast amounts of freshwater to train and run. a recent study estimated that chatgpt uses about 500 ml of water for every 20 - 50 prompts, while training a model of that scale consumes around 700, 000 liters of water. carbon footprint : similarly, llms and other large ai models require a substantial amount of electricity, resulting in a massive carbon footprint. the process of training gpt - 3 ( a precursor to chatgpt ) resulted in a carbon release equivalent to that", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", resulting in a massive carbon footprint. the process of training gpt - 3 ( a precursor to chatgpt ) resulted in a carbon release equivalent to that of driving 112 gasoline powered cars for a year. new generations of llms are even larger and will require even more energy to train. this does not take into account the carbon footprint of maintaining these models in production and processing prompts. why it happensmuch like other large ai models, llms are trained and housed in large data centers, which are located across the globe. these data centers are powered by the local grid and consume water to maintain appropriate temperatures. thus, the", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in large data centers, which are located across the globe. these data centers are powered by the local grid and consume water to maintain appropriate temperatures. thus, the environmental impact depends on the location of the data centers in which models are trained and run. the problem is that the tech companies that develop llms are not transparent about where models are trained and what their true environmental impact is, making it difficult to hold companies accountable, despite their promises of greener technology. why it mattersas the risks around climate change become all the more dire, it is vital that new technologies meet the global sustainability goals we are setting for ourselves. while there are", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##as the risks around climate change become all the more dire, it is vital that new technologies meet the global sustainability goals we are setting for ourselves. while there are ways in which llms and other ai technologies can contribute positively to sustainable development, these benefits can easily be outweighed by the immense energy and water costs they bring. at its core, this is an issue of climate justice. globally, low - income and marginalized communities are disproportionately impacted by climate risks. these are also the communities that benefit the least from ai technologies. mitigation approachesa lot of the harms discussed in this blog post are", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "climate risks. these are also the communities that benefit the least from ai technologies. mitigation approachesa lot of the harms discussed in this blog post are not direct harms of llms themselves, but are an effect of the way the technology might be used or the practices under which it was developed. in this way, llms are similar to other novel technologies, and it is reasonable to make the argument that technology is only a tool and is not to blame for harms such as misuse or unfair practices. however, a safe and functional llm application, as described in part 1 of this series, would be immensely powerful", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s such as misuse or unfair practices. however, a safe and functional llm application, as described in part 1 of this series, would be immensely powerful and come with broad capabilities that go above and beyond those that have been seen in past technologies. the relative ease of access to llms also separates them from previous technological leaps. these types of ai risks point to a need to go beyond technical mitigation strategies and prioritize social changes, regulations, and a global policy approach to ai regulation. due to the novelty of some of these concerns, entire areas of policy, such as copyright and ip law may need to be re", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "policy approach to ai regulation. due to the novelty of some of these concerns, entire areas of policy, such as copyright and ip law may need to be rethought in the era of generative ai. while governments across the globe are taking fragmented approaches to ai policy, there is an increasing need for a global strategy for ai regulation. there is also a need for continued research on ai and its impact on the un sustainable development goals, to ensure that llms and other ai technologies being developed benefit rather than harm those most in need across the globe. there are also some technical and user experience \u2013 based mitigation strategies that can", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai technologies being developed benefit rather than harm those most in need across the globe. there are also some technical and user experience \u2013 based mitigation strategies that can be taken to reduce environmental and social harms of llms. for example, models and their underlying infrastructure and hardware can be designed to be more energy efficient. llm applications can also be carefully designed to prevent improper use and to steer users towards engaging in critical thinking, encouraging them to partner with llm tools rather than over - relying on them. intentional misuse of llms through prompt injection can to an extent be mitigated through technologies designed to detect and block prompt injection", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "over - relying on them. intentional misuse of llms through prompt injection can to an extent be mitigated through technologies designed to detect and block prompt injections. however, this will likely become an ongoing cat - and - mouse game as both prompt injection techniques and mitigation approaches become more sophisticated. finally, there are some problems that may be deeply inherent to llms, which will be difficult to prevent without significant changes to the underlying technology. since human annotation is needed to identify toxic language, there is an unavoidable tension between preventing toxicity in llms and ensuring the safety of the humans who do this work", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is needed to identify toxic language, there is an unavoidable tension between preventing toxicity in llms and ensuring the safety of the humans who do this work \u2014 even under otherwise fair working conditions. given the severity of this harm, it seems unlikely that llms can be used ethically until a better approach to toxicity mitigation is developed. similarly, homogenization of llm - produced content may be an inherent flaw of the technology. models trained to make probabilistic outputs based on data from the past will invariably be stuck in the norms that are already entrenched in that data. conclusionin this blog post,", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilistic outputs based on data from the past will invariably be stuck in the norms that are already entrenched in that data. conclusionin this blog post, we describe the ways in which even high - functioning llms have the potential to cause substantial harm. similar to the harms described in part 1, many of these risks can be mitigated through technical, regulatory, and social means. however, the overarching risk is that llms are being developed and adopted much more quickly than harm mitigation strategies can be put in place \u2014 especially at the global and national scale that is needed. there are also some harms", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "much more quickly than harm mitigation strategies can be put in place \u2014 especially at the global and national scale that is needed. there are also some harms that seem to result directly from the foundational technology of llms, making them impossible to resolve without significant changes to the technology. as individuals, organizations, and society as a whole begin to adopt llms, it is essential that we do so with these dangers in mind. faqhow do llms compare to other ai technologies in terms of causing real - world harm, and what unique risks do they pose compared to those other technologies? large language models ( llms ) like", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai technologies in terms of causing real - world harm, and what unique risks do they pose compared to those other technologies? large language models ( llms ) like gpt - 4 or bert differ from other ai technologies such as machine learning models used in facial recognition or autonomous driving systems primarily in their application and the type of risks they introduce. while all ai technologies can lead to unintended consequences, llms are unique due to their extensive interaction with human language and communication. unlike facial recognition technologies, which may infringe on privacy or misidentify individuals leading to legal or ethical issues, llms interact directly with information dissemination and", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "facial recognition technologies, which may infringe on privacy or misidentify individuals leading to legal or ethical issues, llms interact directly with information dissemination and decision - making processes. they can amplify misinformation, bias, and harmful content at scale due to their language - based interface. additionally, because llms are trained on vast datasets culled from the internet, they inherit and can propagate the biases, stereotypes, and inaccuracies present in those datasets. this makes them uniquely positioned to influence public opinion, automate and scale content creation, and impact social and political discourse", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##uracies present in those datasets. this makes them uniquely positioned to influence public opinion, automate and scale content creation, and impact social and political discourse in ways other ai technologies do not. what are the specific examples of industries or sectors where llms have been implemented successfully without leading to the harms mentioned, and what best practices were followed? in the healthcare sector, llms have been successfully implemented to assist with information management, patient care, and medical research without leading to significant harms. for example, llms have been used to analyze and summarize medical literature, assist in diagnosing from symptoms described in natural", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to significant harms. for example, llms have been used to analyze and summarize medical literature, assist in diagnosing from symptoms described in natural language, and provide conversational support for mental health services. best practices in these implementations include rigorous data privacy measures, continuous monitoring for bias and inaccuracies, and integrating human oversight to verify the ai \u2019 s recommendations before they affect patient care. these measures ensure that llms serve as a support tool rather than a replacement for human expertise, minimizing the risk of harm while maximizing the benefits of rapid data processing and insights generation. how can individuals and organizations measure or", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "replacement for human expertise, minimizing the risk of harm while maximizing the benefits of rapid data processing and insights generation. how can individuals and organizations measure or assess the potential risks and benefits of using llms before implementation? individuals and organizations can assess the potential risks and benefits of using llms through a multifaceted approach. initially, conducting a thorough risk assessment focused on data privacy, security, and ethical implications is crucial. this involves evaluating the sources of training data for biases, the potential for misuse, and the impact on stakeholders. organizations should also implement pilot programs to monitor the llm \u2019 s performance in controlled environments before", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##es, the potential for misuse, and the impact on stakeholders. organizations should also implement pilot programs to monitor the llm \u2019 s performance in controlled environments before full - scale deployment. this allows for the identification and mitigation of unforeseen issues in a low - stakes setting. moreover, engaging with stakeholders, including customers, employees, and subject matter experts, can provide valuable insights into the potential impacts of llm deployment. finally, staying informed about the latest research and developments in ai ethics and regulation helps organizations adapt their use of llms to best practices and legal requirements, balancing innovation with responsibility. previous postsharenext", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and developments in ai ethics and regulation helps organizations adapt their use of llms to best practices and legal requirements, balancing innovation with responsibility. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / the - real - world -", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##3 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / the - real - world - harms - of - llms - part - 2 - when - llms - do - work - as - expected", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected", "row": 163, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 164 text : llm - guided evaluation : using llms to evaluate llms solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsllm - guided evaluation : using llms to evaluate llmsby : arthur teamseptember 29, 2023when it comes to text evaluation, you may have heard of methods like bleu ( evaluation based on word presence ) or bert", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ber 29, 2023when it comes to text evaluation, you may have heard of methods like bleu ( evaluation based on word presence ) or bertscore ( evaluation based on pre - trained nlp models ). while these methods are still useful, they have taken a bit of a back seat recently with the technological advancements around llms. in this post, we \u2019 ll discuss what llm - guided evaluation \u2014 or using llms to evaluate llms \u2014 looks like, as well as some pros and cons of this approach as it currently stands. what does llm - guided evaluation look like? the process", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "looks like, as well as some pros and cons of this approach as it currently stands. what does llm - guided evaluation look like? the process of using llms to conduct evaluation \u2014 as opposed to the bleu and bertscore methods mentioned above \u2014 is conceptually a bit simpler. you take the generated text whose quality you want to evaluate, you pass it into a prompt template that you provide to an llm, and then that llm provides feedback about how \u201c good \u201d the generated text was. one way to think about llm - guided evaluation is using llms for classification. this typically involves providing an", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "about how \u201c good \u201d the generated text was. one way to think about llm - guided evaluation is using llms for classification. this typically involves providing an evaluator llm with a scoring rubric in a prompt template. this rubric instructs the evaluator llm how to classify some other candidate llm \u2019 s response ( see the appendix of meta \u2019 s lima paper for an example ). there \u2019 s also binary classification, where you might ask something like \u201c does this response contain hate speech? \u201d ( or any other property ) and get a \u201c yes \u201d or \u201c no \u201d answer. to do this", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "something like \u201c does this response contain hate speech? \u201d ( or any other property ) and get a \u201c yes \u201d or \u201c no \u201d answer. to do this, you would have to construct examples of hate speech ( or another property you want to evaluate for ), provide those in a prompt template, and then prompt the language model based on all these examples to binarily classify a new response on that property. these techniques are called few - shot prompting, and we \u2019 ve found that this can go quite a long way in creating a basic first implementation of llm - guided evaluation. why does llm - guided evaluation help? the", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "found that this can go quite a long way in creating a basic first implementation of llm - guided evaluation. why does llm - guided evaluation help? the reasons we see value in this approach tend to revolve around speed and sensitivity. typically faster to implementcompared to the amount of work it may have required before the era of llms to get an evaluation pipeline set up, it \u2019 s relatively quick and easy to create a first implementation of llm - guided evaluation. for llm - guided evaluation, you \u2019 ll need to prepare two things : a description of your evaluation criteria in words, as well as a few", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "evaluation. for llm - guided evaluation, you \u2019 ll need to prepare two things : a description of your evaluation criteria in words, as well as a few examples to provide in your prompt template. contrast this with the amount of effort and data collection required to build your own pre - trained nlp model \u2014 or to fine - tune your own nlp model \u2014 to use as an evaluator. when using llms for these tasks, evaluation criteria are much quicker to iterate on. typically more sensitivethe sensitivity aspect can be good and bad. on the positive side, sometimes it \u2019 s really just one subtle word or token that", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##erate on. typically more sensitivethe sensitivity aspect can be good and bad. on the positive side, sometimes it \u2019 s really just one subtle word or token that changes the meaning of a sentence ( e. g. \u201c not \u201d ). llms are better at flexibly handling these scenarios than pre - trained nlp models and previous evaluation methods discussed. on the flip side, this sensitivity can make llm evaluators quite unpredictable, which we discuss more in the next section. what are some challenges with using llms as evaluators? too much sensitivity / variability to be a fully automatic solution on their ownlike we discussed above", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". what are some challenges with using llms as evaluators? too much sensitivity / variability to be a fully automatic solution on their ownlike we discussed above, llm evaluators are more sensitive than other evaluation methods. there are many different ways to set up an llm as an evaluator, and it might act very differently depending on the configuration choices you make, such as which llm you \u2019 re using as an llm evaluator and also the prompt formatting / methodology. constrained by the difficulty of the task being evaluatedanother challenge is that llms will often struggle if evaluating the task at hand requires", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "prompt formatting / methodology. constrained by the difficulty of the task being evaluatedanother challenge is that llms will often struggle if evaluating the task at hand requires too many reasoning steps or too many variables to be managed simultaneously. we do anticipate this improving over time as more tools and apis become available for llms to interface with, but for now, this is the case. while we have not covered this specific limitation in our experiments, previous research has shown limits to transformer \u201c reasoning. \u201d transformers, the ai models that power llms, are sometimes able to do tasks that require multi - step reasoning ( e. g", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "transformer \u201c reasoning. \u201d transformers, the ai models that power llms, are sometimes able to do tasks that require multi - step reasoning ( e. g. multiplying numbers with many digits ), but there \u2019 s a limit to how well they can generalize beyond their training examples. even if you fine - tune an llm on many examples of multi - step multiplication, you get massive correctness drop - offs once you go just beyond the size of the problems in the training data. for more detail about this particular phenomenon, check out this paper. our experimentswe \u2019 ve launched a suite of products for llms at", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in the training data. for more detail about this particular phenomenon, check out this paper. our experimentswe \u2019 ve launched a suite of products for llms at arthur this year \u2014 most recently arthur bench, an llm evaluation product. additionally, our ml engineering team has experimented extensively with llm - guided evaluation, particularly focusing on the sensitivity and variability challenge. in a recent webinar, ml engineers max cembalest & rowan cheung did a deep dive into some of these experiments. they tested some of the well - known llms ( gpt - 3. 5 - turbo, claude - 2, llama2 - 70", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these experiments. they tested some of the well - known llms ( gpt - 3. 5 - turbo, claude - 2, llama2 - 70b, command, etc. ) as both candidates and evaluators, under the hypothesis that an llm evaluator would be biased towards text it itself had generated over text other models had generated. watch the webinar on youtube to see the results in detail and find out if this hypothesis was supported. interested in learning more about arthur bench? find more information here. faq how does the sensitivity and variability of llm evaluators affect the consistency of text quality", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in learning more about arthur bench? find more information here. faq how does the sensitivity and variability of llm evaluators affect the consistency of text quality assessments? the variability in llm evaluations can lead to inconsistent assessments due to differences in how models interpret and rate text quality. this can be challenging for standardizing quality measures but also allows for deeper insights into text nuances. how can developers mitigate the challenges associated with the high sensitivity and variability of llm - based evaluations? developers can mitigate these challenges by integrating multiple llm evaluations, establishing baseline standards, and continually training models on diverse datasets to", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "based evaluations? developers can mitigate these challenges by integrating multiple llm evaluations, establishing baseline standards, and continually training models on diverse datasets to improve consistency and reliability. how might future advancements in llm technologies address the current limitations in evaluating complex reasoning tasks? future advancements are expected to enhance llms'reasoning capabilities, allowing for more accurate and nuanced evaluations of complex texts. this would involve improvements in understanding context, logic, and factual accuracy. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshi", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / llm - guided - evaluation - using - llms - to - evaluate - llms", "metadata": {"source": "https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms", "row": 164, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 165 text : the real - world harms of llms, part 1 : when llms don \u2019 t work as expected solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsthe real - world harms of llms, part 1 : when llms don \u2019 t work as expecteddive deep into the ethical risk arising from the increasing use of llmsby : sarah oster", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "part 1 : when llms don \u2019 t work as expecteddive deep into the ethical risk arising from the increasing use of llmsby : sarah ostermeieraugust 30, 2023introductionin the last year, large language models ( llms ) have gone from being relatively unknown to being widely used by individuals and businesses alike. most of this explosion is owed to the public release of chatgpt \u2014 a free, web - accessible chatbot. since then, multiple language models have been released, and adoption of the technology has blossomed across industries. as applications are built out, llms have the potential", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "since then, multiple language models have been released, and adoption of the technology has blossomed across industries. as applications are built out, llms have the potential to provide a number of social and economic benefits. however, a year before the release of chatgpt, a group of researchers at google \u2019 s deepmind published a paper discussing the social and ethical risks of llms. while at the time many of those risks were hypothetical, it is worth exploring how the problems they identified are manifesting now that llm use is much more widespread. in a series of two blog posts, we will describe some of the major areas of ethical", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "identified are manifesting now that llm use is much more widespread. in a series of two blog posts, we will describe some of the major areas of ethical risk arising from the increasing use of llms. we will also highlight opportunities for risk mitigation, both at the individual and organizational level. in the first part, we will outline key requirements for safe and functional llm applications and discuss risks that arise when llms do not meet these requirements. in the second part, we will describe the set of risks that may arise even when llms are performing as intended. areas of riskin this section, we will discuss three requirements", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", we will describe the set of risks that may arise even when llms are performing as intended. areas of riskin this section, we will discuss three requirements for a safe and functional llm application : 1. high performance and reliability2. protection of sensitive information3. no toxic, discriminatory, or biased behaviorwe \u2019 ll describe how llms fail these requirements by identifying and explaining the causes of the major problems in each section. we \u2019 ll also outline how each failure is a source of significant risk to users and organizations alike. it is important to note that specific areas of risk will naturally depend on individual llm use cases", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "failure is a source of significant risk to users and organizations alike. it is important to note that specific areas of risk will naturally depend on individual llm use cases. for the purposes of this discussion, we will focus on use cases related to chatbots, knowledge retrieval applications, and publicly available general - use applications such as chatgpt. of course, the areas of risk may evolve over time as new use cases and technologies are developed. 1. performance and reliabilitythe problemspoor performance : poor performance refers to the risk caused by models providing incorrect or misleading information, sometimes referred to as \u201c hallucinations. \u201d this behavior is shocking", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##r performance : poor performance refers to the risk caused by models providing incorrect or misleading information, sometimes referred to as \u201c hallucinations. \u201d this behavior is shockingly common in language models ( a recent study found that chatgpt correctly answered fewer than 65 % of the test cases it was given ). for more information on llm performance, check out arthur \u2019 s ongoing generative assessment project. inconsistency : early research has indicated that commonly used llms show low robustness and consistency, meaning that minor changes in user - provided prompts can result in a large variance in the responses that models produce. performance across groups :", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "robustness and consistency, meaning that minor changes in user - provided prompts can result in a large variance in the responses that models produce. performance across groups : while llms can fail for anyone, a further concern is that functionality and performance may not be uniform across users. for example, although chatgpt officially supports over 50 languages, it performs better in english compared to other languages. why it happenspoor performance occurs because language models are not designed to represent reality or understand concepts. rather, models are trained to predict the most probable sequence of words based on the data they were trained on. models can also produce incorrect outputs because they", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "concepts. rather, models are trained to predict the most probable sequence of words based on the data they were trained on. models can also produce incorrect outputs because they are outdated. unless the application is specifically designed otherwise, llms only contain the information they were trained on. for example, at the time of writing, chatgpt was most recently trained in september of 2021. a proposed solution to prevent outdated outputs is to include current information in the prompt that is provided to the llm. however, there is still a risk that a model will default to the data it was trained on. llms perform particularly poorly for some user groups due", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##m. however, there is still a risk that a model will default to the data it was trained on. llms perform particularly poorly for some user groups due to disparities in group representation in the data models were trained on. english is by far the dominant language on the internet, meaning that it is also overrepresented in llm training datasets. for example, the pile, a popular open source dataset, consists of 94 % english language content. by contrast, languages with relatively low representation on the internet will have low representation in llm training datasets as well. this means that cultural views and references", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "by contrast, languages with relatively low representation on the internet will have low representation in llm training datasets as well. this means that cultural views and references specific to these languages are also underrepresented in language models, making them less functional to users who speak these languages. llms have similar issues when prompted to answer questions regarding marginalized cultures in english as well. disparate model performance can occur due to differences in slang usage, dialect, sociolect, education, age, disability status, and other factors that influence how individuals use language. why it mattersas individuals begin using llms such as chatgpt and", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "education, age, disability status, and other factors that influence how individuals use language. why it mattersas individuals begin using llms such as chatgpt and relying on their responses, they risk making poor decisions based on incorrect information. this risk is particularly high when users take high - stakes actions based on responses from llms. for example, a recent survey found that 47 % of americans surveyed reported that they had used chatgpt for stock recommendations. when chatgpt was used by a lawyer to prepare research for a case, the chatbot provided several case examples that were not real, putting both the legal firm and their client at", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "used by a lawyer to prepare research for a case, the chatbot provided several case examples that were not real, putting both the legal firm and their client at risk. given the frequency of hallucinations, use of tools such as chatgpt for financial, medical, and legal advice can cause significant harm to users. in a previous blog post, we described in depth how a user \u2019 s mental model of a technology can lead to overreliance. differential performance of llms across languages and cultures risks growing inequality both within the organization and externally. students, employees, and entire businesses who are able to make use of language models will", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ms across languages and cultures risks growing inequality both within the organization and externally. students, employees, and entire businesses who are able to make use of language models will reap the benefits, while those who cannot partake will be at a disadvantage. with broad societal adoption of llms, this risk may contribute to growing global inequality. as corporations and other organizations begin to make use of llms internally, they will have to grapple with the risk of incorrect outputs, especially when the models are being relied on to provide factual information. seemingly minor individual errors on the part of llms can lead to bad decisions at scale. in cases where ll", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "models are being relied on to provide factual information. seemingly minor individual errors on the part of llms can lead to bad decisions at scale. in cases where llm applications such as google bard and microsoft bing are used as search engines, unreliable information may lead users to lose confidence in publicly available information as a whole. 2. sensitive informationthe problemsprivate individual data exposure : personal identifiable information ( pii ) or other private information about individuals can be unintentionally released or maliciously extracted from llms that have access to the information or have learned it from their training data. this is starkly evidenced by the recent news of", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or maliciously extracted from llms that have access to the information or have learned it from their training data. this is starkly evidenced by the recent news of a federal trade commission investigation into openai ( the company that developed chatgpt and other similar models ) related to how the company handles personal data in their models. chatgpt was temporarily banned in italy and faces scrutiny in other european countries over its data privacy practices, which may violate europe \u2019 s general data protection regulation ( gdpr ). inappropriate confidential or proprietary information exposure : beyond the risk of private personal information, there is an emerging risk that trade secrets, intellectual property,", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "( gdpr ). inappropriate confidential or proprietary information exposure : beyond the risk of private personal information, there is an emerging risk that trade secrets, intellectual property, or government secrets could be exposed by language models. samsung recently banned the use of chatgpt from its employees due to information leakage and other organizations report concerns about the release of corporate secrets through chatgpt as well. why it happenstraining datasets used for chatgpt and other llms include data scraped from all across the internet, including data that is personal to individuals and may be intended to be private. in a washington post investigation into c4, a", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "scraped from all across the internet, including data that is personal to individuals and may be intended to be private. in a washington post investigation into c4, a dataset frequently used to train llms, they uncovered data from websites hosting voter registration databases as well as social media platforms such as facebook and twitter. while these sources may not explicitly contain personally identifiable data, sophisticated language models may well be capable of reconstructing or even correctly guessing personal details based on information gleaned from these sources. when asked for sensitive information in a prompt, llms do not have the context to know what information can or cannot be released. why it", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "from these sources. when asked for sensitive information in a prompt, llms do not have the context to know what information can or cannot be released. why it mattersrelease of private or personal information poses a significant threat to individual users ; however, this threat is magnified as llms begin to be used at scale in large organizations. llms used at this scale without proper safeguards are at risk of causing large - scale data breaches. researchers have also noted that the potential ( though not yet observed ) risk of government or military secrets could pose a significant threat to national security. they also highlight that the ethics of secrecy in areas", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "potential ( though not yet observed ) risk of government or military secrets could pose a significant threat to national security. they also highlight that the ethics of secrecy in areas such as national security, scientific research and trade secrets are far from universal. navigating information privacy and release in these domains may create complex and nuanced scenarios that are difficult to manage through technical solutions, meaning that data privacy regulations may be needed to address these new problems. 3. toxic, discriminatory, and biased behaviorthe problemstoxicity : toxicity in language models relates to the use of offensive, vulgar, or otherwise inappropriate language, usually in the form of", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and biased behaviorthe problemstoxicity : toxicity in language models relates to the use of offensive, vulgar, or otherwise inappropriate language, usually in the form of the model \u2019 s response to a prompt. toxicity is deeply interwoven with discrimination and biased outputs, as it often comes in the form of slurs or derogatory language towards marginalized groups. harmful stereotypes : there is a risk of perpetuating harmful stereotypes which, though not explicitly toxic, are damaging to the groups they impact. unfair toxicity labeling : researchers have noted that labeling language as \u201c toxic \u201d is in and of itself subjective and contextual. there is no", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the groups they impact. unfair toxicity labeling : researchers have noted that labeling language as \u201c toxic \u201d is in and of itself subjective and contextual. there is no universal definition of what constitutes toxic language. this can lead to scenarios in which language sourced from marginalized groups is unfairly labeled \u201c toxic. \u201d why it happenstoxic outputs occur because models are trained on vast amounts of language data. any toxic language, or otherwise harmful biases within the training data, is learned by the model. since llms work by predicting the next most likely word in a sequence, they regularly output stereotypes based on the language they are trained on.", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "by the model. since llms work by predicting the next most likely word in a sequence, they regularly output stereotypes based on the language they are trained on. one common approach to handling toxicity in language models involves filtering the model \u2019 s training data to remove harmful language. given the subjectivity of notions of toxicity, it may be impossible to perfectly filter toxic content from a dataset. any filtering system that could be devised would leave content that is considered harmful to some, while inappropriately censoring content that others consider non - toxic. the washington post investigation of the c4 dataset found that the dataset contains large amounts of harmful", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ly censoring content that others consider non - toxic. the washington post investigation of the c4 dataset found that the dataset contains large amounts of harmful content including swastikas, white supremacist and anti - trans content, and content about false and dangerous conspiracy theories such as \u201c pizzagate. \u201d why it mattersindividual users of llms naturally experience psychological distress when faced with toxicity and harmful stereotypes produced by the model. at the organizational level, exposure to and tolerance of toxic language can have broader impacts on the culture of an organization as a whole. failure to prevent toxicity produced by llms may contribute", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", exposure to and tolerance of toxic language can have broader impacts on the culture of an organization as a whole. failure to prevent toxicity produced by llms may contribute to degrading the culture and norms within an organization, particularly causing negative psychological impact to individuals in marginalized groups. as llms become more broadly used across society, the problem of toxicity becomes more complex. researchers in this area have noted that the concept of toxicity is ambiguous and contextual. there is a risk that attempts to mitigate toxicity without the ability to take context into account may disproportionately impact marginalized groups and reduce model performance in ambiguous contexts.", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "attempts to mitigate toxicity without the ability to take context into account may disproportionately impact marginalized groups and reduce model performance in ambiguous contexts. similar tensions between competing values also arise in standard machine learning models, as discussed by the arthur research team in a previous blog post. failure to remove social biases from llms when they are used at this scale will mean entrenching these biases all the more deeply in the technology we use and in society as a whole. mitigation approachesmanaging the risks of llm failures is largely in the hands of llm developers, researchers, and ultimately policymakers and", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a whole. mitigation approachesmanaging the risks of llm failures is largely in the hands of llm developers, researchers, and ultimately policymakers and regulators. extensive technical work and research is needed to fully understand and improve upon the limitations of these models. beyond that, policy work will be required to ensure that llms are functional and safe both for individuals and for society as a whole. as llms become more widespread, transparency around the data they are trained on, their functionality, and risk factors related to privacy, fairness, and toxicity will be an essential component of any future regulations. in the meantime, there are mitiga", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "their functionality, and risk factors related to privacy, fairness, and toxicity will be an essential component of any future regulations. in the meantime, there are mitigation approaches that individuals and organizations can take now. for individual userswhile ensuring the safety and functionality of llms is the responsibility of llm developers and llm application developers, there are some approaches users can take to avoid major llm failures. users of llms, even publicly accessible ones such as chatgpt, should stay informed on the accuracy, reliability, and limitations of the models they are using. it is also important to stay aware of the use cases that the model", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "should stay informed on the accuracy, reliability, and limitations of the models they are using. it is also important to stay aware of the use cases that the model is intended for, double check any information that a model provides if it is being used to make a decision or inform a belief, and note how up - to - date the model is. most llms in use today are not designed to provide information on news or current events. some llm - based search engines such as google \u2019 s bard and microsoft \u2019 s bing provide citations along with model outputs to improve trust in the models. however, even with citations, these models have", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as google \u2019 s bard and microsoft \u2019 s bing provide citations along with model outputs to improve trust in the models. however, even with citations, these models have been found to be frequently incorrect, citing sources that may not exist. users should also avoid sending private or confidential information when prompting llms, as prompts may be logged. while llms that are easily accessible to the public usually have some mitigation measures in place to prevent toxic and biased language, it is possible that these models will still output language that is distressing or discriminatory against some users. unfortunately, it is difficult for a user completely to avoid such behavior", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that these models will still output language that is distressing or discriminatory against some users. unfortunately, it is difficult for a user completely to avoid such behavior when using llms. similarly, it is difficult for users to anticipate and avoid issues of unequal functionality, as this information is not generally well - documented. for organizations developing llm applicationsorganizations considering llms should first evaluate whether the use case they are considering can reasonably and safely be accomplished by an llm. it may be that some use cases are too high - risk. given the high frequency of hallucinations, an llm should not be", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "an llm. it may be that some use cases are too high - risk. given the high frequency of hallucinations, an llm should not be used for factual information unto itself. if an llm is to be used, the organization will need to ensure that the systems they are putting in place are properly designed for the specific use case the llm is intended for. this may mean using bespoke models ( rather than the general - use apis ) or models that are updated with or have access to organization - specific data. llm systems must be designed to the specific organization and use case including careful design of prompting", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "models that are updated with or have access to organization - specific data. llm systems must be designed to the specific organization and use case including careful design of prompting systems to prevent improper use. it is also advisable to discuss performance, bias and toxicity, and information security with llm vendors and consider incorporating additional security, validation, and monitoring solutions into llm systems. information on model limitations, weaknesses, and risks should be well - understood and mitigation measures should be documented. organizations will also need to ensure that all end users are provided with training on how to safely use the model and what its limitations are. conclusionwhile this", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "documented. organizations will also need to ensure that all end users are provided with training on how to safely use the model and what its limitations are. conclusionwhile this blog focuses on the risks posed by the adoption of llms, the aim is not to suggest that they should not be used at all. rather, we aim to arm users and organizations with the knowledge and tools needed to use this technology safely and responsibly. enthusiasts believe that llms have the potential to increase productivity across industries, support personalized education, and provide new approaches for scientific research. to achieve these lofty outcomes, it is essential that we design llm", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "productivity across industries, support personalized education, and provide new approaches for scientific research. to achieve these lofty outcomes, it is essential that we design llm applications that can be trusted. so far, we \u2019 ve discussed the main types of ethical risk that arise due to functionality issues in llms. while there are mitigation approaches that can be taken by individuals and organizations, these problems really highlight technical and regulatory gaps in the ai space that will need to be addressed as llms become more widespread. in part 2 of this series, we will discuss the ethical risks that may arise when llms do function as intended. learn more", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "llms become more widespread. in part 2 of this series, we will discuss the ethical risks that may arise when llms do function as intended. learn more about shield, arthur \u2019 s firewall for llms, and bench, arthur \u2019 s open source llm evaluation tool. * no llms were used in the writing of this blog post. faqwhat specific regulatory measures could effectively mitigate the ethical risks associated with llms? regulatory measures aimed at mitigating ethical risks in llms should prioritize transparency, user consent, and accountability. clear guidelines on data usage, model decision explanation, and privacy protection are", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ting ethical risks in llms should prioritize transparency, user consent, and accountability. clear guidelines on data usage, model decision explanation, and privacy protection are crucial for ensuring ethical compliance. how can llm developers ensure their models are up - to - date with the latest information and understandings? llm developers can keep their models up - to - date by incorporating continuous learning processes and staying informed about the latest information and understandings in their respective fields. regular updates and integration of diverse perspectives help mitigate biases and enhance model reliability. in what ways can organizations balance the benefits of llm usage with the potential for perpetuating", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of diverse perspectives help mitigate biases and enhance model reliability. in what ways can organizations balance the benefits of llm usage with the potential for perpetuating inequalities? organizations can balance the benefits of llm usage with the potential for perpetuating inequalities by implementing measures that promote fairness and inclusivity. this includes ensuring diverse representation in data sources, actively addressing biases, and regularly evaluating the impact of llm usage on marginalized communities. ethical decision - making processes and collaboration with stakeholders are essential for addressing potential inequalities effectively. previous postsharenext post we make ai better for everyone", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ethical decision - making processes and collaboration with stakeholders are essential for addressing potential inequalities effectively. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / the - real - world - harms - of - llms", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of serviceprivacy source : https : / / www. arthur. ai / blog / the - real - world - harms - of - llms - part - 1", "metadata": {"source": "https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1", "row": 165, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 166 text : why llm hallucinations are so hard to deal with solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelswhy llm hallucinations are so hard to deal withexploring the complexities of llm hallucinations : challenges and solutions in aiby : daniel nissaniaugust 21, 2023introductionlarge language models (", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of llm hallucinations : challenges and solutions in aiby : daniel nissaniaugust 21, 2023introductionlarge language models ( llms ) producing nonsensical content or contradictory content based on the prompt \u2014 what is normally called hallucinations \u2014 is quite a hard problem to solve. llm hallucinations can take on many forms, such as not solving a math problem correctly or making false statements about presidents. however, describing hallucinated content in this way, although helpful for our colloquial understanding, may not benefit us in the long term. how we describe llm halluc", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##inated content in this way, although helpful for our colloquial understanding, may not benefit us in the long term. how we describe llm hallucinations is important. if we decide to look at high - level definitions of hallucinated content, such as unverifiable or false responses, it is hard to dissect what exactly we mean. but if we stick to describing specific instances, such as getting the step of a math problem wrong or producing a research paper when answering a question, we don \u2019 t have a good way to gain general understanding for how hallucinations occur. moreover, all these ways of", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "research paper when answering a question, we don \u2019 t have a good way to gain general understanding for how hallucinations occur. moreover, all these ways of talking about hallucinations give some agency to the model. although unintentional, when we say that an llm got a math problem wrong or said something false about a celebrity, we are implying that the llm somehow has the capability of knowing the correct answer \u2014 when in reality, any of the released llms to date don \u2019 t have the capability to understand. why does this matter? without proper definitions and understandings behind llm hallucinations, the ai", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ms to date don \u2019 t have the capability to understand. why does this matter? without proper definitions and understandings behind llm hallucinations, the ai community is not able to create high - quality datasets about hallucinations. and without high - quality datasets, our ability to build solutions to tackle hallucinations is hindered because we aren \u2019 t able to train models or produce valid evaluations. the current datasets that exist today are fairly broad, binary ( thus, unable to get any granular feel for the types of hallucinations ), and at times a bit dirty. and this isn", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "broad, binary ( thus, unable to get any granular feel for the types of hallucinations ), and at times a bit dirty. and this isn \u2019 t to say there haven \u2019 t been some great attempts at analyzing hallucinations. one of our favorite papers provides preliminary taxonomies for hallucinated content, while another produces what are seen as some of the best datasets to date. but overall, the field of hallucination detection and mitigation is quite nascent, and the need for high quality data amongst the entire ai community is needed. here at arthur, one of our focuses is on ll", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##tion is quite nascent, and the need for high quality data amongst the entire ai community is needed. here at arthur, one of our focuses is on llm hallucinations. we believe that having a rigorous understanding of hallucinations, where they come from, and how they are generated can help us not only gain a deeper understanding of hallucinations, but also help us create such a dataset for the ai community. read the blog posts from the arthur team on some of our work trying to compare the rates of hallucinations from different language models and start to analyze the types of hallucinations that are occurring. help", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "some of our work trying to compare the rates of hallucinations from different language models and start to analyze the types of hallucinations that are occurring. help us collect data! we created a taxonomy, so that we can create high - quality data. there are two ways to go about this : either start by generating themes from research, datasets, etc. that exist and then collect data against it, or collect data and start seeing what themes emerge from the data itself. we are at the point where we need to start collecting some data! as you stumble across hallucinations, please fill out this form. any and all", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". we are at the point where we need to start collecting some data! as you stumble across hallucinations, please fill out this form. any and all hallucinated content will be useful. we will use this to inform our taxonomy development and, in the near future, we will open source a high - quality llm hallucinations dataset for the ai community to build upon. if you have any questions, concerns, or want to collaborate, feel free to email raphael @ arthur. ai. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! sub", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "raphael @ arthur. ai. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / why - hallucinations - are - so - hard - to - deal - with", "metadata": {"source": "https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with", "row": 166, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 167 text : introducing arthur bench : the most robust way to evaluate llms solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsintroducing arthur bench : the most robust way to evaluate llmsintroducing arthur bench, the most robust way to evaluate llmsby : arthur teamaugust 17, 2023at arthur, we \u2019 ve spent this year building out", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "bench, the most robust way to evaluate llmsby : arthur teamaugust 17, 2023at arthur, we \u2019 ve spent this year building out our suite of monitoring tools for large language models to help businesses quickly and safely integrate llms into their operations. we launched arthur shield back in may, which acts as a firewall to protect organizations against the most serious risks and safety issues with deployed llms. today, we \u2019 re excited to introduce our newest product : arthur bench, the most robust way to evaluate llms. bench is an open - source evaluation tool for comparing llms, prompts, and hyperparameter", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur bench, the most robust way to evaluate llms. bench is an open - source evaluation tool for comparing llms, prompts, and hyperparameters for generative text models. this open source tool will enable businesses to evaluate how different llms will perform in real - world scenarios so they can make informed, data - driven decisions when integrating the latest ai technologies into their operations. here are some ways in which arthur bench helps businesses : model selection & validation : the ai landscape is rapidly evolving. keeping abreast of advancements and ensuring that a company \u2019 s llm choice remains the best fit in terms of performance via", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai landscape is rapidly evolving. keeping abreast of advancements and ensuring that a company \u2019 s llm choice remains the best fit in terms of performance viability is crucial. arthur bench helps companies compare the different llm options available using a consistent metric so they can determine the best fit for their application. budget & privacy optimization : not all applications require the most advanced or expensive llms. in some cases, a less expensive ai model might perform the required tasks equally as well. for instance, if an application is generating simple text, such as automated responses to common customer queries, a less expensive model could be sufficient. additionally,", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". for instance, if an application is generating simple text, such as automated responses to common customer queries, a less expensive model could be sufficient. additionally, leveraging some models and bringing them in - house can offer greater controls around data privacy. translation of academic benchmarks to real - world performance : companies want to evaluate llms using standard academic benchmarks like fairness or bias, but have trouble translating the latest research into real - world scenarios. bench helps companies test and compare the performance of different models quantitatively so that they are using a set of standard metrics to evaluate them accurately and consistently. additionally, companies can configur", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the performance of different models quantitatively so that they are using a set of standard metrics to evaluate them accurately and consistently. additionally, companies can configure customized benchmarks that they care about, enabling them to focus on what matters most to their specific business and their customers. in conjunction with this, we \u2019 re also excited to unveil the generative assessment project. gap is a research initiative ranking the strengths and weaknesses of language model offerings from industry leaders like openai, anthropic, and meta. as part of our mission to make llms work for everyone, we will continue to use gap to share discoveries", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ai, anthropic, and meta. as part of our mission to make llms work for everyone, we will continue to use gap to share discoveries about behavior differences and best practices with the public. learn more about this initiative and see some of our initial experiments here. \u201c as our gap research clearly shows, understanding the differences in performance between llms can have an incredible amount of nuance. with bench, we \u2019 ve created an open - source tool to help teams deeply understand the differences between llm providers, different prompting and augmentation strategies, and custom training regimes, \u201d said adam wenchel, co - founder and", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "deeply understand the differences between llm providers, different prompting and augmentation strategies, and custom training regimes, \u201d said adam wenchel, co - founder and ceo of arthur. arthur bench in the llm lifecycleas mentioned, arthur bench is completely open source, so new metrics and other valuable features will continue to be added as the project and community grows. you can visit our github repo, or click here to learn more about what it has to offer. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshi", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / introducing - arthur - bench", "metadata": {"source": "https://www.arthur.ai/blog/introducing-arthur-bench", "row": 167, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 168 text : building llm applications for knowledge retrieval solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnl", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsbuilding llm applications for knowledge retrievalby : haley massajuly 25, 2023since launching our latest suite of llm - centered products, we have been uniquely positioned to talk with many current and prospective customers about how they \u2019 re considering putting llm applications into production.", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "llm - centered products, we have been uniquely positioned to talk with many current and prospective customers about how they \u2019 re considering putting llm applications into production. we \u2019 ve noticed exciting similarities across all the industry verticals and scopes of first - use - case mvp llm projects. although the exact use cases may differ, overwhelmingly, teams are experimenting with llms for knowledge retrieval \u2014 a shorthand for the commonly used retrieval - augmentation generation ( rag ) 1. whether they are consultants referencing slide decks of relevant research or retail account managers responding to questions about their inventory, organizations have accumulated massive amounts of data that their internal teams", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "whether they are consultants referencing slide decks of relevant research or retail account managers responding to questions about their inventory, organizations have accumulated massive amounts of data that their internal teams need to reference daily. knowledge retrieval systems solve this problem by augmenting a large language model ( llm ) with relevant context on use cases. knowledge retrieval applications are typically implemented as productivity boosters within an organization, assisting ( and speeding up ) the research process of internal teams searching through large document databases for answers. this blog will work to provide a high - level overview of how these systems are implemented in practice. we start with an overarching view of both the end", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "this blog will work to provide a high - level overview of how these systems are implemented in practice. we start with an overarching view of both the end - user experience and \u201c behind - the - curtains \u201d components that make up the general structure of these applications in practice, and follow up with some smaller deep dives on choices and concerns organizations often discuss when building out these systems. what makes up these applications in practice one of the best ways to contextualize different pieces of this system is to think about how most people will utilize your application \u2014 the end - user experience. here is a high - level overview of a canonical retrieval", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of this system is to think about how most people will utilize your application \u2014 the end - user experience. here is a high - level overview of a canonical retrieval - augmented llm application : llm knowledge retrieval application workflow showcasing the end - user experience and the behind - the - scenes architecture that powers it. in this diagram, the dashed arrows represent embedding transformations. end - user experience : this is how most people will think about your knowledge retrieval application. this is the \u201c magic \u201d of working with an llm application that can provide an understandable generated answer to their specific questions. behind the curtains : these technical pieces make", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the \u201c magic \u201d of working with an llm application that can provide an understandable generated answer to their specific questions. behind the curtains : these technical pieces make up your application \u2019 s \u201c magical \u201d functionality for end users. we will discuss the high - level steps needed to implement this architecture below, but it is important to recognize that the application works through prompt augmentation. this means that most teams add relevant information to answer user questions ( i. e. their internal data ) into the prompt of off - the - shelf llm models instead of fine - tuning their own on the task. why are teams choosing not to fine -", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") into the prompt of off - the - shelf llm models instead of fine - tuning their own on the task. why are teams choosing not to fine - tune? when i realized the incredible popularity of knowledge retrieval models in practice, i asked, \u201c what about fine - tuning? \u201d at the beginning of the year, all of the talks around llms focused on how quickly teams could fine - tune their own on all of their data. fine - tuning an llm is the process of adapting a pre - trained model to perform better on a use case by providing extra training on a smaller, task - specific dataset. fine -", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the process of adapting a pre - trained model to perform better on a use case by providing extra training on a smaller, task - specific dataset. fine - tuned llms have proven incredibly effective at improving essential metrics for productional ml systems. namely, they can improve accuracy ( by utilizing data for their specific use case ) and latency ( as they enable the use of smaller models ). however, in practice, teams are still turning to data augmentation because of its practical benefits. 2 some of these include : on the maintenance side, the ability to easily change and update the database of information that llm can use as", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "its practical benefits. 2 some of these include : on the maintenance side, the ability to easily change and update the database of information that llm can use as context \u2014 instead of adding the cost of any new data required to fine - tune again. on saving costs in model development, teams can save the time and effort of feature engineering by using the existing data lakes and knowledge bases. putting together curating prompt / completion pairs for fine - tuning can be a difficult and lengthy task. additionally, it enables teams to build in additional beneficial features to their applications, such as the ability for the llm to cite which relevant documents it used and", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". additionally, it enables teams to build in additional beneficial features to their applications, such as the ability for the llm to cite which relevant documents it used and personalize data being pulled into the application by user permissions. finally, it is notable that teams are choosing knowledge retrieval as a way to combat hallucinations, as they are less likely to occur when you provide questions on relevant documents instead of relying on the internal knowledge of an llm. this does not mean that they are a solved problem, however. poor information retrieval techniques cause llms to poorly answer questions on incorrectly sourced raw data. we will define and dive deeper into", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "they are a solved problem, however. poor information retrieval techniques cause llms to poorly answer questions on incorrectly sourced raw data. we will define and dive deeper into hallucinations later when addressing challenges in deployment. steps ( and considerations ) in a knowledge retrieval application now that we \u2019 ve seen a brief introduction to the overall architecture, let \u2019 s break it down into the high - level steps teams need to take to build out this application. preparing input into llmin knowledge retrieval scenarios, teams typically use an out - of - the - box llm application, so most of the engineering work goes into formatting the prompt for their model of", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", teams typically use an out - of - the - box llm application, so most of the engineering work goes into formatting the prompt for their model of choice. these prompts usually contain three values formatted for your application : user input : question the end - user sends into the llm application for a response. prompt augmentation : typically done through methods such as paraphrasing or incorporating explicit instructions, this technique is meant to improve the quality and use - case relevance of the llm application \u2019 s response. context ( data augmentation ) : relevant background information sent to the llm to assist in answering the", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "case relevance of the llm application \u2019 s response. context ( data augmentation ) : relevant background information sent to the llm to assist in answering the question. while this could be a stationary corpus of relevant information, typically, there is more information you want to ask detailed questions about than can be submitted as context for all possible questions. for this reason, teams will build out the next step of creating these applications. create process to search for relevant contextas mentioned above, teams need to think critically about how they plan to use their knowledge database to augment an llm. keep in mind that not only do llms have a", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "teams need to think critically about how they plan to use their knowledge database to augment an llm. keep in mind that not only do llms have a maximum context window that can be submitted, but if they are using an llm via an api, the current pricing structure is typically by how many tokens you input in the requests ( as well as the model completion / output ). this means there is an incentive to encourage either the classical information retrieval system to return an extremely curated set of highly relevant documents or for a subprocess to compress further relevant information returned by the ir system before passing it back to the ll", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "extremely curated set of highly relevant documents or for a subprocess to compress further relevant information returned by the ir system before passing it back to the llm. knowledge database : a knowledge database is where all the data relevant to your llm application use case currently lives. vector store : this mechanism optimizes storing and retrieving vectorized representations of words, phrases, or sentences. this specialized text storage is used for llm applications to improve querying and quick evaluation in production. embedding model : the embedding model transforms the raw text into easily queryable embeddings. these are typically mapped to", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "evaluation in production. embedding model : the embedding model transforms the raw text into easily queryable embeddings. these are typically mapped to another large language model embedding, such as bert. query functionality : a query search is done to find the most relevant documents similar to the embeddings of the user input prompt. this search functionality can be done with various techniques, from simple similarity measures to machine learning. choosing an llmwith the increasing ease of implementation of available llms, choosing which llm to use can be more complex than actually using one. when it comes to making that choice, there", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ease of implementation of available llms, choosing which llm to use can be more complex than actually using one. when it comes to making that choice, there are several factors that teams need to consider, such as the training data, latency, price, or any technical requirements. 3challenges of deploying knowledge retrieval systems we touched briefly on some deployment challenges when discussing the choice between fine - tuning and data augmentation ; however, there are some key things to remember when deploying llm models. sensitive data leakage : there are many ways to define sensitive data. the one that comes to mind for most people is", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "remember when deploying llm models. sensitive data leakage : there are many ways to define sensitive data. the one that comes to mind for most people is personally identifiable information ( pii ), which in many cases, should be completely blocked from entering or exiting your llm application. additionally, teams with established and organized access permissions often rely on existing data user access control to influence what data can be pulled in as context. however, one of the hardest pieces to evaluate for sensitive data goes beyond regex checks and access control. it occurs when end users need access to information about the data but not the data itself. for example", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for sensitive data goes beyond regex checks and access control. it occurs when end users need access to information about the data but not the data itself. for example, a user may be able to ask questions about aggregations, like \u201c what percentage of patients have o negative blood? \u201d to access this, the model would need all patient blood - type records. however, you do not want your end user to be able to ask specific questions about that data, like a certain patient \u2019 s blood type, for example. hallucinations : one of the most commonly asked - about challenges, hallucinations can be best summed up as", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 s blood type, for example. hallucinations : one of the most commonly asked - about challenges, hallucinations can be best summed up as mistakes made by the llm. they occur when the model provides an unsubstantiated ( or \u201c made up \u201d ) answer to the question it is being asked. we are already seeing the consequences of believing hallucinations across industries, from unrunnable code suggestions to a lawyer in his own legal trouble for blindly believing court citations from chatgpt. 4 to improve productivity, end users need to be able to trust their generative assistants, so thinking critically about", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for blindly believing court citations from chatgpt. 4 to improve productivity, end users need to be able to trust their generative assistants, so thinking critically about how to mitigate and detect hallucinations is critical during application development and deployment. arthur has been working on a suite of products that helps teams go from development to safeguarded deployment to active monitoring and continued improvement for llm systems, including knowledge retrieval. let us know if your group wants to dive deeper into llm applications, or get started on your own here. \u2014 \u2014 \u2014 1 lewis et al., \u201c retrieval - augmented generation for knowledge - intensive nlp tasks.", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "applications, or get started on your own here. \u2014 \u2014 \u2014 1 lewis et al., \u201c retrieval - augmented generation for knowledge - intensive nlp tasks. \u201d 2 bratanic, \u201c knowledge graphs & llms. \u201d 3 \u201c gpt - 4 alternatives. \u201d 4 \u201c lawyer apologizes for fake court citations from chatgpt cnn business. \u201d faq how do specific industries tailor knowledge retrieval applications to meet their unique needs and requirements? industries tailor knowledge retrieval applications by aligning them with sector - specific data, compliance requirements, and operational needs. for example, healthcare might focus on patient data privacy and research materials, while retail", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "aligning them with sector - specific data, compliance requirements, and operational needs. for example, healthcare might focus on patient data privacy and research materials, while retail could prioritize inventory and customer service data. what are the specific metrics used to measure the success and accuracy of knowledge retrieval applications in real - world scenarios? success and accuracy in knowledge retrieval applications are measured by metrics such as query response time, accuracy of retrieved information, user satisfaction scores, and the reduction in time spent searching for information. how does the process of embedding model transformation improve the efficiency and accuracy of llm - based knowledge retrieval systems? embed", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "time spent searching for information. how does the process of embedding model transformation improve the efficiency and accuracy of llm - based knowledge retrieval systems? embedding model transformation improves efficiency and accuracy by converting text into vector formats that are easier for llms to process, leading to faster and more relevant results from queries. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersne", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##eldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / building - llm - applications - for - knowledge - retrieval", "metadata": {"source": "https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval", "row": 168, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 169 text : detecting unexpected drift in time series features solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnl", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringdetecting unexpected drift in time series featuresby : akash khannajune 13, 2023aside from crystal balls, time series models have become the predominant approach to predicting the future. often implemented at organizations with access to large amounts of historical data, these models leverage time dependent", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", time series models have become the predominant approach to predicting the future. often implemented at organizations with access to large amounts of historical data, these models leverage time dependent patterns and trends in input features ( like economic indicators, weather patterns, even heartbeats ) in order to forecast into the future. time series models have use cases spanning nearly every data type and industry, yet a great deal of uncertainty still remains when productionalizing and particularly when monitoring these models. the challenges of monitoring time series modelsvalidating time series models on historical data is standard practice prior to putting them into production. however, monitoring time series models once they are in production", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "series modelsvalidating time series models on historical data is standard practice prior to putting them into production. however, monitoring time series models once they are in production becomes tricky because forecasting horizons may be far into the future \u2014 meaning that the ground truth that traditional validation and performance metrics depend on may take a prohibitively long time to become available, exposing organizations to the risks associated with underperforming models in the meantime. many data scientists have addressed this limitation by using data drift metrics on input features as a leading indicator of model performance and a key piece of information in determining if / when a model should be retrained.", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "drift metrics on input features as a leading indicator of model performance and a key piece of information in determining if / when a model should be retrained. however, time series models oftentimes contain features that naturally drift over time and display seasonality, meaning that data drift is to be expected and potentially no longer a principled justification for retraining. in the remainder of this article, i will discuss an approach that would allow one to continue to use data drift metrics as an informative leading indicator of model performance by first accounting for the expected drift of input features and using traditional drift metrics to track the residuals of", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s as an informative leading indicator of model performance by first accounting for the expected drift of input features and using traditional drift metrics to track the residuals of those time dependent features. to get a better intuition for this approach, let \u2019 s squeeze in an ice cream break. the ice cream shop near my house is open year round. it is situated right across the street from one of the city \u2019 s largest playgrounds and is a go - to for parents needing to make good on bribes they \u2019 ve offered their children. the line for this shop can sometimes wrap around the block. this past summer, i started to fancy myself somewhat", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "good on bribes they \u2019 ve offered their children. the line for this shop can sometimes wrap around the block. this past summer, i started to fancy myself somewhat of a sweet toothed soothsayer ; before even leaving my house i could tell how long the wait would be based on the weather that day. at 90 degrees fahrenheit, a single scoop of pistachio would take upwards of an hour, while at 70 degrees, the place was practically deserted. my dessert - line wait - time model was working just fine, until temperatures started to decline. on a late winter \u2019 s day this past year, my model", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "my dessert - line wait - time model was working just fine, until temperatures started to decline. on a late winter \u2019 s day this past year, my model failed me. a meager lunch of leftovers and an unseasonably warm ( 70 degree ) day had me craving a banana split, so i dusted off my old model and predicted that i could be back home with plenty of time to spare before my mail carrier arrived with my eagerly awaited copy of designing machine learning systems. turns out a 70 degree day in winter is not at all the same as a 70 degree day in summer and i ended up spending close to 40", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "learning systems. turns out a 70 degree day in winter is not at all the same as a 70 degree day in summer and i ended up spending close to 40 minutes in line, missing my book delivery. i had fallen victim to a case of mistaken stationarity. stationarity in time series datadealing with time series problems more pressing than my quest for a banana split, lickety split, usually warrants using a machine learning algorithm. many common approaches ( known as autoregressive models ), create models based on the fact that quantities close in time are often similar ( i. e. yesterday \u2019 s wait time", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "autoregressive models ), create models based on the fact that quantities close in time are often similar ( i. e. yesterday \u2019 s wait time is likely pretty similar to today \u2019 s ). more recently, neural network based architectures, which were designed to handle long sequences of data ( like natural language ), have emerged as a popular approach for time series forecasting. these approaches all involve a series of preprocessing steps, some of which we can use to establish a notion of expected drift in order to isolate the informative signal that is unexpected drift. data cleaning and feature selection / extraction are so commonplace in", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "use to establish a notion of expected drift in order to isolate the informative signal that is unexpected drift. data cleaning and feature selection / extraction are so commonplace in the preprocessing pipeline at this point that they will not be discussed here. rather, special attention will be devoted to a potential thorn in the side of anyone attempting to glean insights from time series data : the removal of nonstationarities. stationarity refers to the tendency of data to have a constant mean, variance, and covariance. in the context of time series features, these constants equate to data that essentially does not depend on when", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "mean, variance, and covariance. in the context of time series features, these constants equate to data that essentially does not depend on when it was observed. the nonstationarities of most features of interest can usually be broken down into components which are either trends or seasonal in nature. these seasonal cycles could be yearly, quarterly, daily, or even lengths of time which may seem arbitrary at first glance. fortunately, there are a number of open source approaches to identifying and decomposing those cyclical time dependent components of a model's features, so that fluctuations in the remaining signal can be disentangled from", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to identifying and decomposing those cyclical time dependent components of a model's features, so that fluctuations in the remaining signal can be disentangled from the expected drift. approaches to addressing nonstationarities in dataone tool that has proven particularly useful and flexible in addressing nonstationarities is the open - source python package darts. darts describes its primary goal as \u201c simplifying the whole time series machine learning experience. \u201d the darts package includes functionality which helps detect and extract nonstationarities in data. users can feed in their raw time dependent features and retrieve a transformed feature, one that is time independent and takes into", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and extract nonstationarities in data. users can feed in their raw time dependent features and retrieve a transformed feature, one that is time independent and takes into account where ( or really when ) a value occurs in time such that values separated in time can still be meaningfully evaluated with standard metrics \u2014 for example, the metrics we referred to at the start of this guide, those used to measure data drift for the purposes of evaluating, or reevaluating, your model \u2019 s performance. there are other common approaches to forcing stationarity on time series features. many of these approaches fall under the umbrella of differencing", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model \u2019 s performance. there are other common approaches to forcing stationarity on time series features. many of these approaches fall under the umbrella of differencing : essentially tracking the difference between consecutive observations rather than the observations themselves. in practice, the definition of consecutive here could mean consecutive days, weeks, quarters, years, or essentially any difference as long as it is consistently applied. tracking these differences often accounts for the trends and seasonal tendencies of features in such a way to allow for the remaining quantities to be informative signals which can be tracked for drift. it is worth noting that many modern approaches ( neural network based architectures ) can", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for the remaining quantities to be informative signals which can be tracked for drift. it is worth noting that many modern approaches ( neural network based architectures ) can natively handle non - stationary data. though the removal of nonstationarities may not be a critical part of the preprocessing pipeline for these models, it may still be worth creating stationary versions of those features, particularly under circumstances where forecasting horizons are distant and unexpected trends in input features can serve as a leading indicator of performance. returning to our original motivation, the key idea here is that by accounting for expected drift in our time dependent features, we can be sensitive", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "leading indicator of performance. returning to our original motivation, the key idea here is that by accounting for expected drift in our time dependent features, we can be sensitive to unexpected drift and use it to guide decisions about evaluating, or reevaluating our model. fundamentally, this approach is the difference between a short line for a rootbeer float on a temperate day in summer and a long line for that same float on an unseasonably temperate day in winter. faqwhat are the specific performance metrics used in ai and ml to assess time series models before and after accounting for expected drift? in the context of ai and", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##what are the specific performance metrics used in ai and ml to assess time series models before and after accounting for expected drift? in the context of ai and machine learning, specific performance metrics commonly used to assess time series models include mean absolute error ( mae ), mean squared error ( mse ), root mean squared error ( rmse ), as well as more sophisticated metrics like mean absolute percentage error ( mape ) and symmetric mean absolute percentage error ( smape ). these metrics are crucial for evaluating the accuracy of forecasts by comparing the predicted values generated by the model to the actual values observed in the time series", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "). these metrics are crucial for evaluating the accuracy of forecasts by comparing the predicted values generated by the model to the actual values observed in the time series data. both before and after accounting for expected drift, these metrics help in quantifying the effectiveness of the model, guiding data scientists in llm ( large language models ) and ml in refining and improving their forecasting algorithms by highlighting discrepancies between predicted and observed outcomes. in the realm of ml and ai, how does one quantify the level of acceptable drift before deciding to retrain a time series model? quantifying the level of acceptable drift in the", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai, how does one quantify the level of acceptable drift before deciding to retrain a time series model? quantifying the level of acceptable drift in the realm of machine learning and artificial intelligence before deciding to retrain a time series model involves setting precise threshold values for data drift metrics. this process is influenced by historical model performance, industry benchmarks, or the specific objectives of the ml forecasting task. a sensitivity analysis can be particularly useful in understanding the effects of various levels of drift on the predictive performance of the model, a common practice in ai. once these thresholds are established, exceeding them indicates that the model '", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "levels of drift on the predictive performance of the model, a common practice in ai. once these thresholds are established, exceeding them indicates that the model's predictions are losing accuracy, suggesting a need for retraining. this approach, crucial in the lifecycle of ml models, helps balance the costs associated with model updates against the potential risks of using outdated predictions. what are the limitations or challenges associated with using the darts package for addressing nonstationarities in time series data within ml and ai frameworks? the darts package, while a valuable tool in the ml and ai toolkit for addressing nonstationarities in time series", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data within ml and ai frameworks? the darts package, while a valuable tool in the ml and ai toolkit for addressing nonstationarities in time series data, presents certain limitations such as computational demands, especially when dealing with extensive datasets or intricate time series. the complexity might deter users new to python or those unfamiliar with advanced time series analysis techniques common in ai research. while darts offers a range of functionalities for decomposing and modeling time series data, its applicability might not extend to all types of nonstationary data or might not always be the most efficient approach, particularly for datasets characterized by high", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "applicability might not extend to all types of nonstationary data or might not always be the most efficient approach, particularly for datasets characterized by high irregularity or noise. additionally, integrating darts within broader ml and ai pipelines can present challenges, especially when those pipelines employ different programming languages or are structured in unique computational environments. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyte", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##productshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / detecting - unexpected - drift - in - time - series - features", "metadata": {"source": "https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features", "row": 169, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 170 text : model schemas within the mlops ecosystem solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringmodel schemas within the mlops ecosystemby : sarah ostermeierjune 6, 2023mlops and continuous trainingwhen embarking on an individual data science project, documenting, standardizing, and tracking may not seem like a top priority. however,", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ops and continuous trainingwhen embarking on an individual data science project, documenting, standardizing, and tracking may not seem like a top priority. however, as data science teams expand and numerous teams emerge within an organization, along with the development of centralized mlops systems, the importance of establishing standards and tracking systems becomes evident. these measures allow data science teams to work efficiently and avoid costly mistakes. a well - designed mlops system should track all the information and components needed to retrain a model from scratch such that it is approximately identical to the original model ( same training data, model algorithm, hyperparameters, etc. )", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "retrain a model from scratch such that it is approximately identical to the original model ( same training data, model algorithm, hyperparameters, etc. ). most of this is done through model registries, feature stores, ci / cd and ct ( continuous training ) tools such as dagshub and other versioning systems. by employing these systems, model artifacts, data, and code versions can be tracked effectively, facilitating continuous retraining and, if necessary, emergency rollbacks of production models. within the model pipeline, two crucial components must be carefully maintained and tracked across iterations to facilitate this seamless orchestration.", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "emergency rollbacks of production models. within the model pipeline, two crucial components must be carefully maintained and tracked across iterations to facilitate this seamless orchestration. 1. model : what is the current model deployed in production and what conditions are needed to retrain it or pass inference data into it? to answer these questions, we need to keep track of several key elements : the saved model artifact, which serves as the core representation of the trained model. the predict function, which handles the model \u2019 s predictions. the software and environment requirements necessary for running the training and prediction processes. the code employed to train the model, encapsul", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "handles the model \u2019 s predictions. the software and environment requirements necessary for running the training and prediction processes. the code employed to train the model, encapsulating the very essence of its development. 2. data : what data was utilized for training and validation? this aspect necessitates monitoring two data subsets : the training data, which molds the model \u2019 s understanding and helps it glean patterns and insights. the test and validation data, which enables the evaluation of the model \u2019 s performance and generalization abilities. however, in order to know how to bring these pieces together, more information is needed : the model sc", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of the model \u2019 s performance and generalization abilities. however, in order to know how to bring these pieces together, more information is needed : the model schema. what is a model schema? a model schema describes the relationship between a dataset and the model. a model schema is much like a database schema in that it outlines the structure and relevant metadata of a dataset. in the case of a model schema, this outline describes the relationship between the dataset and a model. it includes information such as which data columns are used as direct inputs to the model, how model inputs and outputs are structured,", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the dataset and a model. it includes information such as which data columns are used as direct inputs to the model, how model inputs and outputs are structured, and what bounds on data values are expected. the ultimate goal of a model schema is to allow a user to load and reconstruct a dataset as it was used during the initial training and validation phase. this in turn enables easier model retraining and rollbacks, data validation, and any further analysis or model validation that is needed. data scientists can reload and explore their datasets and model outputs without the need to refer back to training code. meanwhile,", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model validation that is needed. data scientists can reload and explore their datasets and model outputs without the need to refer back to training code. meanwhile, mlops administrators can define robust model schema standards and establish automated systems for data and model monitoring and validation, streamlining the overall process. in the upcoming sections, we will delve into the intricate details of what essential information should be encompassed within a model schema. additionally, i will provide a concrete example that illustrates how a model schema may be structured. furthermore, we will explore how the adoption of model schema can yield standardization benefits within an organization and bolster the", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "how a model schema may be structured. furthermore, we will explore how the adoption of model schema can yield standardization benefits within an organization and bolster the overall mlops system. throughout this blog post, we will employ the example of a tabular classification model to illustrate the concepts at hand. rest assured, these principles extend beyond this specific scenario and are applicable to a wide range of model types and data domains. what information should be captured in the model schema? exactly what information is needed in a model schema will depend on the model type, use case, size of the organization, and how the model schema is intended", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "what information is needed in a model schema will depend on the model type, use case, size of the organization, and how the model schema is intended to be used. in general, best practice is to standardize model schemas across an organization to facilitate automation. the recommendations here should be treated as a starting point. my recommendation for a tabular model is given here : 1. data columnshow are the data columns used and how does each column relate to the model? model inputs : actual input values to the modelmodel outputs : predicted probabilities, logits, and / or predicted classnon - input data", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model? model inputs : actual input values to the modelmodel outputs : predicted probabilities, logits, and / or predicted classnon - input data : any data that is relevant to the model but is not used as model input such as id columns, timestamps, or features that contain cohort data that is of interest such as race or gender for bias trackingmodel target : the ground truth column ( s ) that the model is predictingfor each column type, consider what information would be needed for a colleague to reload the data and evaluate the model without access to the code used to train the model. for model", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", consider what information would be needed for a colleague to reload the data and evaluate the model without access to the code used to train the model. for model inputs, non - input data, and target columns, include basic information for each column : column namecolumn index : where the column is located in the dataframe it is saved indata type : what the expected datatype for the column iscategorical or continuous : this will tell future users how the column should be treated during analysiscategories ( if applicable ) : what are the expected categories if the column is categorical? value bounds : what are the expected", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "should be treated during analysiscategories ( if applicable ) : what are the expected categories if the column is categorical? value bounds : what are the expected bounds if the column is numerical? cohort / segment column ( if applicable ) : whether or not the column denotes data cohorts or segments of interest \u2014 used to compare model performance across groupssample id or timestamp column : whether the column is a sample id or timestamp column ( his may only applicable in some cases ) for model output columns, it is useful to include both raw model outputs and the final prediction made by the model. in addition to the", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "applicable in some cases ) for model output columns, it is useful to include both raw model outputs and the final prediction made by the model. in addition to the fields outlined above, columns with raw model outputs may require a field to map raw outputs to the final model output. if the model is a multi - label model, it is also important to note which prediction task the column corresponds to. 2. data accessdetails about how to access the data and how it is structured. the specifics of what is needed here may vary depending on how data is managed within the organization. some examples of useful content are given below : data files", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". the specifics of what is needed here may vary depending on how data is managed within the organization. some examples of useful content are given below : data files : for each data file, give the filepath, which components of the data are included ( e. g. [ \u2018 model inputs \u2019, \u2018 model target \u2019 ] ), expected column names, the join key to integrated data with other sources, and any other information need to load the data file. data splits : if test or validation data is included in the data files, provide details about training vs test indexes. if training and validation data is saved in separate files,", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "if test or validation data is included in the data files, provide details about training vs test indexes. if training and validation data is saved in separate files, describe where each set can be located. inference data : if applicable, provide details on where inference data is to be stored and how it is formatted if different from training data. example : acstraveltimeas an example, i will go through a model i trained on a modified version of the folktables dataset acstraveltime. the objective of the model is to predict whether an individual has a commute of more than 20 minutes, and indicated by the binary", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##set acstraveltime. the objective of the model is to predict whether an individual has a commute of more than 20 minutes, and indicated by the binary target \u201c jwmnp. \u201d the datathe dataset contains the following features : serialno : person serial numberwagp : wages or salary income past 12 monthsagep : age of the householderschl : educational attainmentmar : marital statussex : sexdis : disability recodemig : mobility status ( lived here 1 year ago ) relp : relationshiprac1p : recoded detailed race codepuma : public use microdata area code", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "status ( lived here 1 year ago ) relp : relationshiprac1p : recoded detailed race codepuma : public use microdata area codecit : citizenship statusoccp : occupation recodejwtr : means of transportation to workpowpuma : place of work puma povpip : income - to - poverty ratio recodethe dataset is derived from the american community survey public use microdata sample ( pums ) and full documentation can be found here. the features in this dataset were not all used as input to the model. serialno was used only as an identifier", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "documentation can be found here. the features in this dataset were not all used as input to the model. serialno was used only as an identifier and join key during data manipulation. rac1p and sex were not used as model input, but were important information to track nonetheless to evaluate model fairness concerns. the features also include a combination of numerical and categorical data. i used data from the years 2014 and 2015 as training data and reserved data from 2016 to test my model. prior to training my model, i defined each of the relevant groups of columns in my code. i also defined data bounds for numerical features,", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "test my model. prior to training my model, i defined each of the relevant groups of columns in my code. i also defined data bounds for numerical features, and which columns were to be used to track specific cohorts of interest within the dataset. this makes it easy to separate input and non - input data without losing track of non - input attributes. the modeli trained a gradient boosting classifier and being satisfied with the results, i saved the pickled model. while this is usually the end of the story in a data science blog post, the next few steps are key to maintaining and improving upon a model in a", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". while this is usually the end of the story in a data science blog post, the next few steps are key to maintaining and improving upon a model in a real - world production environment. data persistencewhile we already have the data saved somewhere ( in this case, in the u. s. census database ), it is important to keep track of the exact data used to train and validate a model. this especially comes into play when the time comes to retrain a model or roll it back to a previous version. it may also be required for regulatory purposes. for my acstraveltime model, i reorganized all of my input", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or roll it back to a previous version. it may also be required for regulatory purposes. for my acstraveltime model, i reorganized all of my input and non - input data for both my train and test sets into one dataframe, keeping track of the indexes for each dataset. model outputs on both training and test datasets are also important to maintain, as this facilitates comparisons of multiple versions of a model over time. i saved raw prediction probabilities and the final prediction for my acstraveltime model and combined training and test data into one dataframe. i saved both dataframes as. csv", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and the final prediction for my acstraveltime model and combined training and test data into one dataframe. i saved both dataframes as. csv files. model schema architecturei designed a model schema for this model with the objective of making it easy to reload the dataset, define data validation standards, and evaluate and monitor the model \u2019 s performance. the schema is first structured as a python dictionary, and then saved as a. yaml file, which can easily be reloaded along with the dataset. as described above, i divided my model schema into 2 sections. in the data _ access", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", which can easily be reloaded along with the dataset. as described above, i divided my model schema into 2 sections. in the data _ access section, i provided all the information needed to load the dataset and run basic validation using just the model schema file. in the data _ columns section, i provided more granular details for each column, which would allow for full data validation, model evaluation, and model monitoring. for each input and non - input column, there are details about data bounds and categories, the data type of the column, and whether the column should be used to track cohort performance. the", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "column, there are details about data bounds and categories, the data type of the column, and whether the column should be used to track cohort performance. the model _ outputs section details which columns correspond to the final prediction and the predicted probability for each class. using my model schema file, i am able to reload my data from scratch and understand how my training data was structured, what data bounds i should expect, and which columns were of interest when tracking data subsets. along with other versioning tools, i could use this to help me retrain a new version of my model, roll back to a previous version, and", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". along with other versioning tools, i could use this to help me retrain a new version of my model, roll back to a previous version, and perform monitoring and evaluation on my model. the complete notebook used in this example is available here. the full model schema file is available here. the value of model schemasat first glance, model schemas might appear superfluous, possibly dismissed as an afterthought by an individual data scientist. however, within the organizational landscape, they are a vital part of maintaining consistency, efficiency, and automation in an mlops system. they provide an outline of exactly how", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", within the organizational landscape, they are a vital part of maintaining consistency, efficiency, and automation in an mlops system. they provide an outline of exactly how data is used in a model, while also establishing a framework for comprehensive evaluation and continuous monitoring. model schemas can fit easily into an mlops system as a model metadata artifact, and be saved within model registries or other versioning systems. this level of standardization allows data science and mlops tools to be automated and speeds up the process of retraining, updating, and monitoring models. it also reduces the reliance on institutional knowledge, facilitating hand - offs between team", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and speeds up the process of retraining, updating, and monitoring models. it also reduces the reliance on institutional knowledge, facilitating hand - offs between team members. with a shared understanding encapsulated within the model schemas, data scientists and mlops practitioners can unlock the true potential of automation, empowering tools and processes to operate seamlessly. this not only streamlines the workflow but also reduces the risk of human error, ushering in a new era of efficiency and collaboration within the organization \u2019 s data science ecosystem. previous postsharenext post we make ai better for everyone. sign up for our newsletter to", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of efficiency and collaboration within the organization \u2019 s data science ecosystem. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / model - schemas - within - the - mlops - ecosystem", "metadata": {"source": "https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem", "row": 170, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 171 text : downstream fairness : a new way to mitigate bias solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessdownstream fairness : a new way to mitigate biasby : daniel nissanimay 17, 2023note : this blog post and code could not have been done without the fantastic research from our former research fellow kweku kwegyir - aggrey and", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": this blog post and code could not have been done without the fantastic research from our former research fellow kweku kwegyir - aggrey and former machine learning researcher jessica dai. you can view their paper here. motivationmany organizations utilize binary classifiers for a variety of reasons, such as helping loan providers decide who should get a loan, predicting whether or not something is spam, or providing evidence on whether or not something is fraudulent. these use cases require specific classification thresholds. imagine an algorithm is predicting whether or not someone qualifies for a loan. one way to do this is to attribute a probability to", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "specific classification thresholds. imagine an algorithm is predicting whether or not someone qualifies for a loan. one way to do this is to attribute a probability to a person, and if that probability is above a certain threshold ( let \u2019 s say 0. 5 ), then they can get a loan. if not, then they will be rejected. what is the proper threshold to use in these scenarios? taking spam detection as an example, the threshold set will determine how often an email is classified as spam. a threshold of 0. 8 is less permissive than a threshold of 0. 4. that is why many organizations have", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "an email is classified as spam. a threshold of 0. 8 is less permissive than a threshold of 0. 4. that is why many organizations have threshold ranges for their algorithms, which can complicate things. current bias mitigation techniques, such as the one we offer at arthur, traditionally require you to change your classification threshold to meet some fairness definition. this change in threshold could be outside the range that your company allows, creating questions as to whether or not you can be fair. further complicating these situations are models that are utilized in many downstream applications, where different threshold ranges ( and possibly different fairness definitions", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you can be fair. further complicating these situations are models that are utilized in many downstream applications, where different threshold ranges ( and possibly different fairness definitions ) need to be utilized. downstream fairness solves this dilemma. it \u2019 s an algorithm that achieves various fairness definitions ( equalized odds, equal opportunity, and demographic parity ) in a threshold - agnostic way, meaning that a company won \u2019 t have to adjust their threshold. instead it operates on a binary classifier \u2019 s output probabilities to achieve a fairness definition. and this is all done with minimal accuracy loss! for the remainder of this blog post,", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "classifier \u2019 s output probabilities to achieve a fairness definition. and this is all done with minimal accuracy loss! for the remainder of this blog post, we \u2019 ll be digging deeper into this algorithm and how to use our new open source code. downstream fairnesssaving the mathematical details for the geometric repair paper, we will discuss the essence of how downstream fairness works and provide code snippets from our open source package. first off, downstream fairness is a post - processing algorithm that operates on the training dataset ( or some representative dataset ) for the model we are trying to make fair. the data needs to contain some", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "processing algorithm that operates on the training dataset ( or some representative dataset ) for the model we are trying to make fair. the data needs to contain some key information : the prediction probabilities for each data point, the classification label, and a column containing the sensitive attribute on which you are operating. prediction probabilities prediction labels group information how the algorithm works is that it looks at the distribution of prediction probabilities per group of our original model and then computes a repair of each of those distributions for demographic parity. the reason this works for demographic parity is because the definition of demographic parity ( equalizing selection", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a repair of each of those distributions for demographic parity. the reason this works for demographic parity is because the definition of demographic parity ( equalizing selection rates for each group ) only requires prediction probabilities and group information. on the implementation side, this process produces an adjustment table. the adjustment table contains how much the prediction probabilities need to be adjusted to achieve demographic parity, for each group. below is an example of how that table looks : the pred _ orig column is a granular representation of possible prediction probabilities a binary classifier could give \u2014 in this case, the granularity is", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##d _ orig column is a granular representation of possible prediction probabilities a binary classifier could give \u2014 in this case, the granularity is set to hundredths. the adjusted _ scores _ x are the adjustments for group x. if the fairness definition you want to use is demographic parity, we are done! on new inferences, find the prediction probability for that inference and the group that the inference comes from, and then add the adjustment value to the inference probability : new _ prediction _ probability = old _ prediction _ probability + appropriate _ adjustment if you want to use a different definition, such as equalized odds", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "inference probability : new _ prediction _ probability = old _ prediction _ probability + appropriate _ adjustment if you want to use a different definition, such as equalized odds or equal opportunity, then we need to do one more thing. as we stated before, our goal is to repair the original prediction probability distribution so that it achieves demographic parity. in the paper, we call this a full adjustment, meaning that the adjustment table produced is a full adjustment of our model. for other fairness definitions, such as equal opportunity or equalized odds, we need to find a dampening term that adjusts the adjustments, so that we can achieve those", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "fairness definitions, such as equal opportunity or equalized odds, we need to find a dampening term that adjusts the adjustments, so that we can achieve those fairness definitions. in order to do this, we first need to find which group in the sensitive attribute column is experiencing the most disparity according to whatever fairness definition you want to choose. once we identify that group, we are going to go through an optimization process to find a \u03bb ( what we call it in the code and how it is talked about in the paper ), which will work as our dampening term, so we can satisfy other fairness definitions. once we obtain", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "code and how it is talked about in the paper ), which will work as our dampening term, so we can satisfy other fairness definitions. once we obtain this \u03bb, we can apply it in the following way with our adjustment table : new _ prediction _ probability = old _ prediction _ probability + lambda * appropriate _ adjustment luckily, this is all automated with our codebase! here is an example of how to do this : and, unlike some other bias mitigation approaches, downstream fairness is a pareto optimal algorithm. meaning that it will achieve these fairness definitions with the minimum amount of accuracy loss. of course, there are some", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", downstream fairness is a pareto optimal algorithm. meaning that it will achieve these fairness definitions with the minimum amount of accuracy loss. of course, there are some limitations. the dataset used to train downstream fairness must contain prediction probabilities for each class for each group, and there should be a good amount of examples for each class for each group. but if that is provided, the algorithm should work as expected. conclusionwe went through some of the algorithmic and implementation details of downstream fairness. if you want to explore more of the mathematical details, please go read the paper. us at arthur would love for you all to try out", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of downstream fairness. if you want to explore more of the mathematical details, please go read the paper. us at arthur would love for you all to try out our work! feel free to pip install our package and kick the tires a bit. as you find failure cases or think of new features, feel free to send your feedback to me at daniel. nissani @ arthur. ai. even better, please submit prs or issues on our open source github repo. the github repo provides a demo notebook, where you can try out all of our functionality we described in this post. faq1. what", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". the github repo provides a demo notebook, where you can try out all of our functionality we described in this post. faq1. what are the specific mathematical principles behind the downstream fairness algorithm? the downstream fairness algorithm is grounded in statistical and probability theory, particularly focusing on the concept of distribution repair for ensuring fairness across different groups. the mathematical foundation involves adjusting the distribution of prediction probabilities for each group to align with fairness criteria such as demographic parity, equalized odds, or equal opportunity. this involves a process known as \" geometric repair, \" which essentially recalibrates the output probabilities of a predict", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "equalized odds, or equal opportunity. this involves a process known as \" geometric repair, \" which essentially recalibrates the output probabilities of a predictive model so that the resultant probabilities do not disproportionately favor or disadvantage any particular group based on sensitive attributes. the algorithm employs optimization techniques to find the best possible adjustments that achieve fairness while minimizing accuracy loss. this is achieved by constructing an adjustment table that represents how much the prediction probabilities need to be shifted for each group to meet the desired fairness standard. 2. how does downstream fairness compare to other bias mitigation techniques in terms", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##babilities need to be shifted for each group to meet the desired fairness standard. 2. how does downstream fairness compare to other bias mitigation techniques in terms of performance and implementation complexity? downstream fairness differs from other bias mitigation techniques primarily in its post - processing approach, focusing on adjusting model outputs rather than altering the training process or the data. compared to methods like reweighing, which modifies the weight of instances in the training data, or adversarial debiasing, which involves training a model to predict the target while another model predicts the sensitive attribute to reduce bias, downstream fairness is implemented after a model", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "debiasing, which involves training a model to predict the target while another model predicts the sensitive attribute to reduce bias, downstream fairness is implemented after a model has been trained, thereby not affecting the original training pipeline. this can make it easier to integrate into existing workflows without needing to retrain models. in terms of performance, downstream fairness aims to be pareto optimal, meaning it seeks to achieve the best possible trade - off between fairness and model accuracy. this contrasts with some methods that might significantly reduce a model's performance to achieve fairness criteria. however, the actual performance and complexity can vary based on the specific scenario and", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with some methods that might significantly reduce a model's performance to achieve fairness criteria. however, the actual performance and complexity can vary based on the specific scenario and the extent of bias in the original model. 3. can downstream fairness be applied to non - binary classifiers and multi - class scenarios? the concept of downstream fairness as described in the blog post primarily addresses binary classification problems. however, the underlying principles can be adapted for non - binary or multi - class classification scenarios with some modifications. in multi - class scenarios, fairness typically involves ensuring that the predictive performance is balanced across different groups for all classes, not just two. this", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "some modifications. in multi - class scenarios, fairness typically involves ensuring that the predictive performance is balanced across different groups for all classes, not just two. this could involve extending the adjustment table to cover all possible class predictions and ensuring that the adjustments lead to fair outcomes across all classes and groups. however, this adaptation can increase the complexity, as it requires considering inter - class fairness in addition to intra - group fairness. the implementation for multi - class scenarios would need to calculate separate adjustments for each class and group combination, possibly leading to a more complex optimization problem. while the original downstream fairness algorithm may not directly apply, the principles of adjusting", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "adjustments for each class and group combination, possibly leading to a more complex optimization problem. while the original downstream fairness algorithm may not directly apply, the principles of adjusting prediction probabilities and achieving demographic parity can still be extended to these more complex scenarios with appropriate modifications. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reserved", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / downstream - fairness - a - new - way - to - mitigate - bias", "metadata": {"source": "https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias", "row": 171, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 172 text : announcing arthur shield : the first firewall for llms solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsannouncing arthur shield : the first firewall for llmsby : arthur teammay 4, 2023at arthur, we are on a mission to make ai better for everyone \u2014 and right now, nowhere is that mission more important than the deployment of large language", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##3at arthur, we are on a mission to make ai better for everyone \u2014 and right now, nowhere is that mission more important than the deployment of large language models. companies across industries have begun to rapidly integrate llms into their operations following recent advancements from organizations like openai, google, meta, and others. however, businesses don \u2019 t currently have a way to ensure fast and safe deployment of these applications, which has led to data leaks and toxic outputs that have been costly in more ways than one. that \u2019 s why, today, we are launching a powerful addition to our suite of ai monitoring tools : arthur shield, the", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "costly in more ways than one. that \u2019 s why, today, we are launching a powerful addition to our suite of ai monitoring tools : arthur shield, the first firewall for large language models. arthur shield enables companies to deploy llm applications like chatgpt faster and more safely, helping to identify and resolve issues before they become costly business problems \u2014 or worse, result in harm to their customers. simply put, arthur shield acts as a firewall to protect organizations against the most serious risks and safety issues with deployed llms. use cases can include : pii or sensitive data leakage : arthur shield allows companies to use the power", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "most serious risks and safety issues with deployed llms. use cases can include : pii or sensitive data leakage : arthur shield allows companies to use the power of an llm trained or fine - tuned on their full data set while having the peace of mind from knowing that other users of that same llm are blocked from retrieving sensitive data from the training set. toxic, offensive, or problematic language generation : arthur shield allows companies to block llm responses that are not value - aligned with their organization. hallucinations : some llms confidently output incorrect facts. arthur shield detects these likely incorrect responses and prevents them from being", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "value - aligned with their organization. hallucinations : some llms confidently output incorrect facts. arthur shield detects these likely incorrect responses and prevents them from being returned to a user where they can do significant harm if they are actioned upon. malicious prompts by users : arthur shield detects and stops malicious user prompts, including attempts to get the model to generate a response that would not reflect well on the business, efforts to get the model to return sensitive training data, or attempts to bypass safety controls. prompt injection : it is becoming common for llm applications to augment their prompts through retrieval from third - party websites and", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or attempts to bypass safety controls. prompt injection : it is becoming common for llm applications to augment their prompts through retrieval from third - party websites and databases of pre - trained document embeddings. those sources are not secure and can contain malicious prompts that are injected into the llm system, causing significant risk of unauthorized response generation and data leakage. \u201c llms are one of the most disruptive technologies since the advent of the internet. arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk. \u201d", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk. \u201d \u2013 adam wenchel, co - founder & ceoarthur shield in the llm lifecyclearthur shield and its capabilities are currently being rolled out in beta to select arthur customers. read more in our official press release or get in touch to request a demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabular", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / announcing - arthur - shield - the - first - firewall - for - llms", "metadata": {"source": "https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms", "row": 172, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 173 text : how to think about production performance of generative text solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelshow to think about production performance of generative textby : rowan cheungapril 25, 2023introductionperformance monitoring has always been at the heart of arthur \u2019 s mission and offering. we know that teams do not put models into production for no", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##uctionperformance monitoring has always been at the heart of arthur \u2019 s mission and offering. we know that teams do not put models into production for no reason. yet time and again it seems that every \u201c state of ml \u201d report released still lists that ml teams struggle to communicate performance to their external stakeholders. our mission has always been to help teams create workflows and toolkits that not only enable teams to use reporting to build better models, but also empower them to communicate with external stakeholders. nowhere is that mission more important in 2023 than in generative ai. organizations are terrified to fall behind and are scrambling to", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "them to communicate with external stakeholders. nowhere is that mission more important in 2023 than in generative ai. organizations are terrified to fall behind and are scrambling to implement llms into their processes. however, it \u2019 s important for teams to think critically about how they \u2019 re planning to evaluate and communicate these findings to external stakeholders. in this blog post, we \u2019 re going to cover some of the core challenges teams run into when they try to evaluate generative text outputs. then, we \u2019 ll give a brief overview of some metrics that are commonly discussed by research communities and explain why we don \u2019 t see them working in production environments", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", we \u2019 ll give a brief overview of some metrics that are commonly discussed by research communities and explain why we don \u2019 t see them working in production environments. finally, at the end, you \u2019 ll find our suggestions for how to evaluate and monitor generative text to provide actionable outcomes. core challenges of evaluating generated text1. there isn \u2019 t one ground truth output. in open text generation, the output of the model is unstructured text, so there is no ground truth label in the same way we might think of evaluating a traditional classification or regression task. one solution for this gap is to ask humans to accomplish the", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "no ground truth label in the same way we might think of evaluating a traditional classification or regression task. one solution for this gap is to ask humans to accomplish the same task asked of a generative text model, and use the human - generated text as ground truth or the \u201c correct \u201d solution to the task. this is time - consuming and infeasible to scale to the needs of continuously evaluating a production application. even given infinite resources, there is no clear definition of what the best piece of text is. consider the following natural language queries and their corresponding responses from chatgpt. they overlap in some of the details mentioned in", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the best piece of text is. consider the following natural language queries and their corresponding responses from chatgpt. they overlap in some of the details mentioned in the response ( e. g. the various relevant terms like forces and planets ), but by slightly altering the prompt we elicit a response which changes the words and the concepts used in the writing. an evaluation procedure for a generative text model should properly take into account that the quality of the response will depend on its intended context, use case, and audience. for example, this type of language model that answers science questions should strike a balance between the simplicity of its answers and", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "intended context, use case, and audience. for example, this type of language model that answers science questions should strike a balance between the simplicity of its answers and the thoroughness of its answers based on the user at inference time, so evaluation metrics should flexibly control for this contextual shift. 2. we lack consistent automated metrics for comparing two pieces of text. ideally, we \u2019 d like an automated score that could help us compare the above model outputs, or select between models that produced this set of inferences. while many metrics have been proposed, it remains a difficult question to select which metric, and associated hyperpara", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "between models that produced this set of inferences. while many metrics have been proposed, it remains a difficult question to select which metric, and associated hyperparameters best suit a specific use case. different metrics have varying behavior on underlying qualities of text like tone, relevance, truthfulness, and also coherence, grammar, and lack of repetition. in the next section, we \u2019 ll outline some options for automated metrics. what automated metrics exist? when we have a reference text, there are some metrics we can explore for comparing the model generated text to the reference text. in this section, we \u2019 ll", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we have a reference text, there are some metrics we can explore for comparing the model generated text to the reference text. in this section, we \u2019 ll discuss the benefits and limitations of some commonly used metrics. n - gram precision metrics : bleu and rougethe first class of metrics directly compares the overlap of tokens in the golden reference text to the tokens in the model generated text. a popular example of these metrics is the bleu score. bleu was originally developed for machine translation and compares the number of n - grams in the machine generated text that also exist in the reference text.", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "score. bleu was originally developed for machine translation and compares the number of n - grams in the machine generated text that also exist in the reference text. $ $ bleu = min ( 1, ( exp ( 1 - \\ frac { len ( reference ) } { len ( generated ) } ) ) \\ prod _ { i = 1 } precision _ i $ $ the first term encourages the reference text and the generated text to be of similar length. if the length of the reference text is equal to the length of the generated text, the first term will equal 1 and the value of the metric will be determined", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "if the length of the reference text is equal to the length of the generated text, the first term will equal 1 and the value of the metric will be determined by the second term, the precision. to calculate the precision term, bleu measures the fraction of i - grams in the generated text that are also contained in the reference text for i = 1 to 4. bleu and its variants are very brittle : they don \u2019 t capture semantic information. swapping the word \u201c happy \u201d for \u201c joyous \u201d and for \u201c sad \u201d are considered equally wrong. they don \u2019 t effectively measure grammar. jumbling the order of", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "word \u201c happy \u201d for \u201c joyous \u201d and for \u201c sad \u201d are considered equally wrong. they don \u2019 t effectively measure grammar. jumbling the order of the words in a piece of text can render the text nonsensical, while barely changing the bleu score. adding context : bertscorethe next class of metrics attempts to add semantic meaning and contextual information to techniques like bleu, by leveraging embeddings instead of raw tokens or words. embeddings are learned representations that map words to a vector in high - dimensional space such that the vectors capture the meaning and relationship between", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s or words. embeddings are learned representations that map words to a vector in high - dimensional space such that the vectors capture the meaning and relationship between words. in bertscore, each token in the reference text and generated text are first embedded using the bert language model. using a language model like bert provides embeddings that hold both semantic meaning as well as contextual information, because bert can generate different representations of the same word depending on its surrounding sentence, or context. the bertscore is then a combination of : a precision term : of all the embeddings in the generated text, compute the mean", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or context. the bertscore is then a combination of : a precision term : of all the embeddings in the generated text, compute the mean of the maximum cosine similarity with any embedding in the reference text. a recall term : of all the embeddings in the reference text, compute the mean of the maximum cosine similarity with any embedding in the generated text. while bertscore addresses a core issue of incorporating semantic information into the metric, it has its own drawbacks : introducing embeddings increases the computational complexity of the metric. when using embedding - based", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "information into the metric, it has its own drawbacks : introducing embeddings increases the computational complexity of the metric. when using embedding - based metrics, the choice of embedding can influence what aspect of the text ( semantic meaning, tone, style ) the metric is most optimized to measure. correlating with a human text distribution : mauvefor both bleu and bertscore, a single human reference text is compared to a single model generated text. the authors of mauve propose a metric that compares the distribution of model generated texts to the distribution of human generated texts, and show that", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "single model generated text. the authors of mauve propose a metric that compares the distribution of model generated texts to the distribution of human generated texts, and show that this can better correlate with human judgments of models completing generative tasks. to compute mauve, samples are taken from a human distribution of text and from the model. a separate model ( the authors use gpt - 2 ) is then used to embed all the texts. each underlying distribution is then approximated by clustering the texts and counting the number in each cluster. the final mauve score estimates the divergence between the human text distribution and the model text distribution", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "by clustering the texts and counting the number in each cluster. the final mauve score estimates the divergence between the human text distribution and the model text distribution. downsides of mauve : mauve still requires a reference, human - generated text distribution. mauve scores can vary based on the hyperparameters chosen. some best practices for using mauve are here. monitor what mattersall of the metrics described above require a reference text to compare model output to, which is often difficult to obtain in production settings. exciting new metrics like usr explore training a model to score texts from a set of model texts and ground", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "which is often difficult to obtain in production settings. exciting new metrics like usr explore training a model to score texts from a set of model texts and ground truth texts, such that no reference text is required at scoring time. model - based metrics are also a promising direction towards metrics that allow users to adapt a metric for a given use case. but in production, \u201c good \u201d is whether the llm is successful in the product it is deployed in. when deploying a language model in an application or other business setting, we can more directly measure performance by collecting implicit performance metrics from the llm \u2019 s environment.", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ing a language model in an application or other business setting, we can more directly measure performance by collecting implicit performance metrics from the llm \u2019 s environment. there are two types of performance measures that could be relevant to track : user feedback : these signals capture information on how users are interacting with the outputs of an llm. augmenting an information retrieval system with an llm to reduce the time necessary to find desired information. a suitable performance metric could be the average time a user spends on a specific query. generating headlines for news articles with an llm to drive reader engagement. a suitable performance metric could be the average number of", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a user spends on a specific query. generating headlines for news articles with an llm to drive reader engagement. a suitable performance metric could be the average number of clicks per headline. execution success : these signals capture information on how the outputs of an llm interact with the rest of the production system. augmenting an information retrieval system with an llm to reduce the time necessary to find desired information. some suitable performance metrics could be the average distance between the query and the documents returned, or the validity of the llm compiled query. augmenting a chat system with an automated appointment booking system. a suitable performance metric could be", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "documents returned, or the validity of the llm compiled query. augmenting a chat system with an automated appointment booking system. a suitable performance metric could be the api error rate, when making requests to the booking software. these metrics are just the starting point in a holistic monitoring solution for llms, and are not meant to encourage driving engagement or automation at the expense of fairness or cognitive engagement. for a deeper look at designing human - centered evaluation for llms, check out teresa datta \u2019 s recent blog. future research directionshow can we develop metrics that are optimized for the context of the deployment? given", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "check out teresa datta \u2019 s recent blog. future research directionshow can we develop metrics that are optimized for the context of the deployment? given the above performance proxies for an llm in production, we \u2019 d like to automate metrics to correlate with those criteria. these metrics can then be used during other phases of the model lifecycle where production signal is not available, such as model selection and validation. can we use llms to generate feedback for other llms? there is exciting research exploring the possibility of using llms to grade, score, and monitor other llms. for example", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to generate feedback for other llms? there is exciting research exploring the possibility of using llms to grade, score, and monitor other llms. for example, in self - refine, the authors propose a framework in which an llm iterates on a task utilizing the feedback provided by an llm. at arthur, we are exploring utilizing embeddings and llms for scoring llm outputs, providing natural language descriptions of model performance, and benchmarking the strengths and limitations of using llms during evaluation. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the strengths and limitations of using llms during evaluation. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / how - to - think - about - production - performance - of - generative - text", "metadata": {"source": "https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text", "row": 173, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 174 text : what does the ml lifecycle look like for llms in practice? solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##vabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelswhat does the ml lifecycle look like for llms in practice? by : haley massaapril 18, 2023i \u2019 m not going to say i was a full - blown deep learning denier, but i was stubbornly holding", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "massaapril 18, 2023i \u2019 m not going to say i was a full - blown deep learning denier, but i was stubbornly holding out to avoid getting swept up in the llm hype. while i find almost all ml research interesting, i am really enamored with ml systems research, dedicated to the practical implementation of ml models. and it seemed like generative ai was being touted around more as a fun toy to \u201c see what it could do \u201d than something feasible to put into production. as usual, the linkedin posts started to pour in. however, it wasn \u2019 t until recently,", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "could do \u201d than something feasible to put into production. as usual, the linkedin posts started to pour in. however, it wasn \u2019 t until recently, when those same posts began to shift from \u201c wow, look at this response \u201d to \u201c wow, look at this application i built, \u201d that i realized i could no longer keep my head in the sand. i realized that sooner than i was expecting, these applications will be a part of the ml systems work i am so interested in. so, i spent some time trying to find out what makes the development of these models different from typical ml models. what does the traditional ml", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "so interested in. so, i spent some time trying to find out what makes the development of these models different from typical ml models. what does the traditional ml lifecycle look like? over the years, there have been many different formats that researchers have used to describe the steps teams go through to build an ml model. while i will not detail these steps, please refer to the diagram below. we can see in the diagram two core pipelines that work to put ml models into production ( and one version - controlled layer that helps to manage them ). the first development pipeline is where data scientists and research teams work to develop the model.", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "production ( and one version - controlled layer that helps to manage them ). the first development pipeline is where data scientists and research teams work to develop the model. in the second pipeline, ml engineers and developers work to take their model at the final stage of development and productionalize it into something that can be used to make real - world predictions. what does an llm lifecycle look like? although llms are trying to carve out their own phrase within mlops ( llmops ), it \u2019 s important to remember that they \u2019 re still machine learning systems. this means that even if they use different tools or phrases,", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "llmops ), it \u2019 s important to remember that they \u2019 re still machine learning systems. this means that even if they use different tools or phrases, they still follow most of the same lifecycle and best practices. i \u2019 ve broken down the llm lifecycle into three phases, seen in the drawing below. in the following sections, i will give a high - level overview of each phase and how it relates back to the traditional ml lifecycle. foundation models on the left, we can see the most famous aspect of these new llm systems, the foundation models. foundation models are large pre - trained language models capable of", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the left, we can see the most famous aspect of these new llm systems, the foundation models. foundation models are large pre - trained language models capable of understanding and generating natural language text. this ability to perform general language tasks well enables these models to serve as the starting point for various nlp tasks, such as language translation, text summarization, and question - answering. foundation models are the most significant shift away from the traditional ml lifecycle. api access to foundation models has made it easier for teams to leverage nlp in their operations, regardless of industry or size. they can implement just the api as their production model \u2014", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "models has made it easier for teams to leverage nlp in their operations, regardless of industry or size. they can implement just the api as their production model \u2014 skipping most of the ml lifecycle workflow \u2014 if they are just looking for general language task capabilities. this is a standard route we see organizations go down as a first iteration of putting an llm into production. however, this is no longer remarkable, as any company can create an openai account. instead, users seek customized experiences based on the specific use case for the model they interact with. to do this, ml teams need to use many of their existing techniques for ml", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "seek customized experiences based on the specific use case for the model they interact with. to do this, ml teams need to use many of their existing techniques for ml model development to fine - tune and improve their model based on its specific use case. note : many foundation models have a closed - source nature due to the lack of transparency and accessibility of the code and data used to train them. verifying the model \u2019 s accuracy and biases is difficult without access to this code and data. this can lead to unintended consequences and perpetuate existing biases in the data. developmentthe development phase is where ml practitioners build and improve", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data. this can lead to unintended consequences and perpetuate existing biases in the data. developmentthe development phase is where ml practitioners build and improve upon these use case \u2013 specific ml models. as we can see from the diagrams above, it exists in both the traditional and llm lifecycle. however, one key difference is that there are currently fewer development steps for llms. for example, teams do not need to select and test different model architectures. while development for llm models will undoubtedly continue to advance, the workflow is currently streamlined into three main steps : defining the use casewhile not explicitly listed in the", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "development for llm models will undoubtedly continue to advance, the workflow is currently streamlined into three main steps : defining the use casewhile not explicitly listed in the diagram above, the first step to building any worthwhile ml model for production, llm or not, is to define and understand your use case. teams will need to spend time with business and product stakeholders to understand the purpose of the model they are putting into production. data curation & model fine - tuningdata science teams must curate and clean use case \u2013 specific data to build out use case \u2013 specific llms. this data will be used to train / fine - tune", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "science teams must curate and clean use case \u2013 specific data to build out use case \u2013 specific llms. this data will be used to train / fine - tune the foundation model \u2019 s language understanding to their task requirements. cleaning and curating data is something that data science teams are used to, as it is a part of their traditional ml lifecycle. however, one benefit of using llm models over traditional ml techniques is that they already have a solid understanding of language due to all the data they were trained on originally. by using data only to fine - tune on top of an existing large model, teams are not required to curate and", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "all the data they were trained on originally. by using data only to fine - tune on top of an existing large model, teams are not required to curate and clean as much data. qualitative validation with promptssimilar to all traditional ml models, these models must be tested and validated before they can interact with the world in production. in traditional ml model lifecycles, this is done with the help of well - established historical benchmarks and metrics. as we will cover in a future post, this is different with llms. instead, teams must understand the use case enough to create realistic tests and adversaria", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we will cover in a future post, this is different with llms. instead, teams must understand the use case enough to create realistic tests and adversarial prompts to evaluate the model. they then can use metrics built to quantify essential qualities of the text ( such as tone or context ) against example responses provided for each prompt. additionally, teams may choose to qualitatively assess the model \u2019 s performance based on their understanding of human language and use cases. application schema the final block in our llm lifecycle diagram is the application schema. referring to how your llm is implemented and interacted with", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". application schema the final block in our llm lifecycle diagram is the application schema. referring to how your llm is implemented and interacted with in production, it is similar to the productionalization process that ml engineers and developers go through when traditionally implementing ml models. in practice, this is often built with prompt orchestration, where multiple prompts are chained together. before getting into application schemas where multiple prompts interact, let \u2019 s look at what this would look like for an application with a single user prompt \u2014 like a chatbot. the process of constructing a single prompt is more complicated in production than just", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "this would look like for an application with a single user prompt \u2014 like a chatbot. the process of constructing a single prompt is more complicated in production than just taking in the user \u2019 s input. one concept of llms that is different from traditional ml models is that beyond fine - tuning the model, prompt engineers are able to \u201c fine - tune \u201d users \u2019 inputs to the application at the time of inference with a prompt template. taken from langchain prompt template article mentioned later in this post. prompt templateprompt engineers write a prompt template as an additional step of model \u201c fine - tuning. \u201d traditionally, we think of fine", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "later in this post. prompt templateprompt engineers write a prompt template as an additional step of model \u201c fine - tuning. \u201d traditionally, we think of fine - tuning as adding parametric knowledge to the model. this is the knowledge that a model learns at training time and is stored within the model weights ( or parameters ). on the other hand, prompt templates work as source knowledge added to the model. this is knowledge added during inference via inputs to the model. they provide additional information on top of users \u2019 input requests during inference. this information typically includes additional background information, context, and rules for model responses. for a deeper", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "provide additional information on top of users \u2019 input requests during inference. this information typically includes additional background information, context, and rules for model responses. for a deeper dive into prompt templates, check out this article. prompt orchestration prompt orchestration refers to the chaining of prompts together interactively. some applications, like chatbots, may be simple enough application - wise to work with one core prompt. however, in production, many llm use cases are actually much more complex. this article does a great job introducing langchain \u2014 a popular prompt orchestrator \u2014 by using the metaphor of baking a cake. you can ask a", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "complex. this article does a great job introducing langchain \u2014 a popular prompt orchestrator \u2014 by using the metaphor of baking a cake. you can ask a chatbot to provide you with the ingredients for the cake, but that \u2019 s not actually very useful for your end goal of a finished cake. instead, models need to be able to use that prompt in conjunction with other prompts and actions to get the end result they are looking for. we can also see below for a more \u201c real world \u201d example provided by the article for pulling information from generated sql queries. for those interested, you can also see a real - world", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a more \u201c real world \u201d example provided by the article for pulling information from generated sql queries. for those interested, you can also see a real - world example utilizing arthur written by one of our ml engineers, who built a chatbot to interact with our documentation. taken from langchain prompt orchestration article mentioned above. note : in traditional ml models, there has been a push in the community to recognize data pipelines as the logical unit of ml work, and not the model itself. one thing that i find interesting about llm models in production comparatively is that it seems there will be a push to view prompt orchestration as", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the model itself. one thing that i find interesting about llm models in production comparatively is that it seems there will be a push to view prompt orchestration as the logical unit of llm work. this would make the chaining together of prompts into practice more similar to that of data engineering orchestrating etl pipelines. prompt engineering and application designers will need to spend more time and effort defining how these flowcharts will look and how outputs will be validated and monitored. user feedbackmeasuring the performance of generative models in production can be an even greater challenge than the already mentioned challenge of validating the model during development", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". user feedbackmeasuring the performance of generative models in production can be an even greater challenge than the already mentioned challenge of validating the model during development. teams must navigate practical constraints, such as the infeasibility of scaling human labelers to generate common metrics. one approach that has proven successful for teams is tracking user feedback. this feedback provides valuable insights into how well their model is performing, enabling teams to continuously fine - tune and improve model performance. the specific techniques used will depend on the nature of the feedback and desired outcomes for the specific model. conclusion & a look aheadin conclusion, although llms are getting", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "specific techniques used will depend on the nature of the feedback and desired outcomes for the specific model. conclusion & a look aheadin conclusion, although llms are getting their own fancy suite of new tools and job titles, they are still rooted in the best practices and techniques that the ml community has been using for years. foundation models : serve as a jumpstart for teams to develop strong baseline nlp modelsdevelopment : still the same need to fine - tune and evaluate models for specific end - task, even if there are new techniques and job titles application schema : process for putting llms into production that still needs to be valid", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for specific end - task, even if there are new techniques and job titles application schema : process for putting llms into production that still needs to be validated and monitored, even if it is reliant on new tools / prompts llms are most definitely finding their way into production systems near you \u2014 and fast. hopefully, this was an informative first look into how they fit into the frameworks that teams already use for their traditional ml approaches. we \u2019 re busy at arthur helping folks build with llms, so stay tuned for more related content soon. - - - - - - - - - - - - -", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "busy at arthur helping folks build with llms, so stay tuned for more related content soon. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - faqhow do llms compare with traditional ml models in terms of computational resources and environmental impact? large language models ( llms ) generally require", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- - - faqhow do llms compare with traditional ml models in terms of computational resources and environmental impact? large language models ( llms ) generally require significantly more computational resources for training compared to traditional machine learning models. this is due to their vast number of parameters, extensive datasets, and the complexity of the tasks they perform. for instance, models like gpt - 3 have billions of parameters and require substantial amounts of data and processing power to train effectively. this increased computational requirement translates to higher energy consumption and, consequently, a larger environmental impact. the carbon footprint associated with training and operating llms is a concern, as", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "increased computational requirement translates to higher energy consumption and, consequently, a larger environmental impact. the carbon footprint associated with training and operating llms is a concern, as it contributes to greenhouse gas emissions. in contrast, traditional ml models, which might focus on more constrained tasks and possess fewer parameters, typically require less computational power, leading to lower energy usage and a smaller environmental footprint. however, efforts are being made to make llms more energy - efficient and to reduce their environmental impact through methods such as more efficient hardware, better model design, and by fine - tuning pre - trained models instead of training new ones from scratch. what are the specific", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "methods such as more efficient hardware, better model design, and by fine - tuning pre - trained models instead of training new ones from scratch. what are the specific challenges in ensuring the ethical use and bias mitigation in llms compared to traditional models? the ethical use and bias mitigation in llms present unique challenges primarily due to the scale and nature of the data they are trained on. llms are trained on vast datasets sourced from the internet, which can contain biased, incorrect, or harmful information. these biases can be amplified and perpetuated by the models, leading to ethical concerns, especially when the models", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "contain biased, incorrect, or harmful information. these biases can be amplified and perpetuated by the models, leading to ethical concerns, especially when the models are used in sensitive or impactful contexts. the sheer volume of data makes it difficult to fully audit and clean, resulting in challenges in identifying and mitigating all sources of bias. additionally, because llms generate human - like text, there is a risk of them producing harmful or misleading information that appears credible. this is less of a concern with traditional ml models, which typically perform more narrowly defined tasks and therefore have a more controlled and limited scope for bias introduction and propagation.", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is less of a concern with traditional ml models, which typically perform more narrowly defined tasks and therefore have a more controlled and limited scope for bias introduction and propagation. addressing these challenges requires ongoing efforts in data curation, model transparency, and the development of robust evaluation frameworks to detect and mitigate biases. how can businesses measure the return on investment ( roi ) when implementing llms into their operations? measuring the return on investment ( roi ) for businesses implementing llms involves assessing both the tangible and intangible benefits against the costs associated with these systems. tangible benefits can include increased efficiency, reduced operational costs, and enhanced customer satisfaction,", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "both the tangible and intangible benefits against the costs associated with these systems. tangible benefits can include increased efficiency, reduced operational costs, and enhanced customer satisfaction, which can be measured through metrics such as time saved, reduction in customer service expenses, and improvements in sales or customer retention rates. intangible benefits might include improved brand reputation, customer experience, and innovation. costs to consider include not only the direct expenses related to developing, training, and maintaining the llms but also indirect costs such as training staff to use the technology and potential risks associated with model biases or errors. businesses can assess roi by setting clear objectives before implementation", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "also indirect costs such as training staff to use the technology and potential risks associated with model biases or errors. businesses can assess roi by setting clear objectives before implementation, monitoring performance metrics closely, and adjusting the use of llms to align with strategic goals. regularly reviewing these metrics against the initial investment and operational costs helps in understanding the value llms bring to the organization. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesblogga", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / what - does - the - ml - lifecycle - look - like - for - llms - in - practice", "metadata": {"source": "https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice", "row": 174, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 175 text : 2023 updates to the owasp api security top 10 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedproduct features2023 updates to the owasp api security top 10the open worldwide application security project ( owasp ) is a non - profit community dedicated to improving software security. by : nori tatsumiapril 12, 2023the open worldwide application", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##asp ) is a non - profit community dedicated to improving software security. by : nori tatsumiapril 12, 2023the open worldwide application security project ( owasp ) is a non - profit community dedicated to improving software security. its api security top 10 project documents the most common api threats for best practices when creating or assessing apis. in 2019, the owasp foundation released the first version of the api security top 10. this year, they \u2019 re publishing the next iteration of the list that \u2019 s updated for 2023. the 2023 release candidate of the updated list is now available and open to", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 re publishing the next iteration of the list that \u2019 s updated for 2023. the 2023 release candidate of the updated list is now available and open to the community for contributions and feedback. as arthur \u2019 s mlops observability platform is built with an api - first development approach, the owasp api security top 10 is one of the many best practices we incorporate into our software development culture. we are happy to see that the investments we \u2019 ve been making in our security features and practices are matching the latest security trends identified by the owasp community. in the 2023 version of owasp api security top", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in our security features and practices are matching the latest security trends identified by the owasp community. in the 2023 version of owasp api security top 10, authorization is identified as the # 1 biggest challenge of api security. last year, arthur did a complete overhaul of our platform \u2019 s authorization mechanism that introduced the capability for granular and customizable rbac. the new rbac system enforces strict authorization policies, allowing our customers to implement segregation of duty models that are tailored to their enterprise. authentication remains on the 2023 list as the # 2 threat. arthur \u2019 s rbac is backed by the authentication mechanism that", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that are tailored to their enterprise. authentication remains on the 2023 list as the # 2 threat. arthur \u2019 s rbac is backed by the authentication mechanism that \u2019 s built based on our zero trust principle. it can also adapt your enterprise standards by integrating with your identity provider ( idp ) to achieve federated identity and single sign - on ( sso ). in 2022, arthur expanded the idp integration capability by introducing support for openid connect ( oidc ) in addition to security assertion markup language ( saml ) protocol. what \u2019 s new on the 2023 list is the automated threats from bots and", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") in addition to security assertion markup language ( saml ) protocol. what \u2019 s new on the 2023 list is the automated threats from bots and bot - nets. with more sensitive data and business logic exposed via apis today, they \u2019 ve become more profitable. and with the combination of ai and affordable cloud services, bots are more sophisticated, scalable, and dangerous. earlier this year, arthur did an assessment of our platform to identify and implement layers of additional protections to mitigate the risk from automated attacks. various security and resiliency work has been done through this effort both at the application level as well as", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "additional protections to mitigate the risk from automated attacks. various security and resiliency work has been done through this effort both at the application level as well as the infrastructure level for our saas environment. the work of reducing api threats is not a one - time effort. arthur \u2019 s security features mentioned in this blog are continuously evaluated and improved. the owasp api security project creates a standard awareness for api security that should be applied to your software development continuously. at arthur, we have a framework to exercise education on security practices, threat modeling, secure design, penetration testing, code analysis, and other security related activities through the software", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", we have a framework to exercise education on security practices, threat modeling, secure design, penetration testing, code analysis, and other security related activities through the software development lifecycle ( sdlc ), the continuous integration and continuous delivery ( cicd ), and the system and organization controls ( soc 2 ). in this framework, projects like the owasp api security are helpful for us to identify what security work we should prioritize. we are grateful to owasp and other organizations alike for helping the mlops community build safe innovations. previous postsharenext post we make ai better for everyone. sign up for our newsletter", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and other organizations alike for helping the mlops community build safe innovations. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / 2023 - updates - to - the - owasp - api - security - top -", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": https : / / www. arthur. ai / blog / 2023 - updates - to - the - owasp - api - security - top - 10", "metadata": {"source": "https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10", "row": 175, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 176 text : ask arthur, episode 1 : introduction solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnl", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsask arthur, episode 1 : introductionby : max cembalestmarch 27, 2023this is a multi - episode series of blog posts discussing one of arthur \u2019 s internal use cases for language models. it is our intention for this post to be general enough to be adapted", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "episode series of blog posts discussing one of arthur \u2019 s internal use cases for language models. it is our intention for this post to be general enough to be adapted with minimal effort in your company / lab. a new generation of applications is emerging. these applications use large language models ( llms ) via apis from openai and cohere to accomplish tasks via a \u201c natural language interface \u201d ( nli ). with an nli, a question or procedure is posed in ordinary written language, and an llm or a series of llms carries out steps to respond. nlis are not new \u2014 google search is a familiar example", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ordinary written language, and an llm or a series of llms carries out steps to respond. nlis are not new \u2014 google search is a familiar example of an nli that has been around for years. however, many use cases and toolkits are now emerging to use llms to wrap more common business tasks in an nli. at arthur, we were looking for a project to explore emerging use cases and toolkits for llms and nlis, which brought us to langchain. this toolkit is a fast - growing open source library for building llm - powered applications with much of the prompting, routing", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "brought us to langchain. this toolkit is a fast - growing open source library for building llm - powered applications with much of the prompting, routing, and other intermediate steps handled by the components of the langchain library itself. in addition, we felt that enabling easy sharing, interactivity, and feedback collection would be important for us to study the human experience of using llms, which brought us to gradio. this easy - to - use library, acquired by huggingface, enables the creation of user interfaces to interact directly with machine learning models. if you make a gradio app for your project, you", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "acquired by huggingface, enables the creation of user interfaces to interact directly with machine learning models. if you make a gradio app for your project, you can get a shareable link that will allow you to send your custom ui to people for testing and feedback collection \u2014 without them having to have python or any code on their computer. in this first episode of ask arthur, we are going to walk through the construction of our prototype for ask arthur using only langchain, gradio, and native python packages \u2014 no other machine learning libraries like pytorch are needed to directly use when writing our code! in future episodes", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "gradio, and native python packages \u2014 no other machine learning libraries like pytorch are needed to directly use when writing our code! in future episodes, we will dive into the many design choices to consider when creating an llm - powered application : evaluating hyperparameter choices for embeddings, testing new llm prompts, balancing performance with cost when choosing between different apis, and more. this is only the beginning! langchain tutorial \u201c ask arthur \u201d is a chatbot for answering questions and citing sources from documentation. we can define all the components and steps of the chatbot with langchain", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ask arthur \u201d is a chatbot for answering questions and citing sources from documentation. we can define all the components and steps of the chatbot with langchain. the flow of the application displayed in the image above is inspired by this langchain blog post about their chatbot design for their own documentation. the chatbot processes the user \u2019 s question in the context of the chat so far, finds a relevant page from the arthur docs, and then uses the docs page to construct a response. in order to find a relevant page from the documentation, we need to have preprocessed the docs into embedding", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "page to construct a response. in order to find a relevant page from the documentation, we need to have preprocessed the docs into embeddings, which langchain supports with document parsers and integrations with databases and vectorstores to save / load / search embeddings. an llm from openai \u2019 s api ( wrapped by langchain ) is called twice when a user types in a new message into the chat, each with its own prompt and purpose. the user will only ever directly interact with the output of the second call to an llm, the \u201c chat response generator,", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with its own prompt and purpose. the user will only ever directly interact with the output of the second call to an llm, the \u201c chat response generator, \u201d which creates the written response that gets output into the chat box. this chat response will typically be a summary / rephrasing of information contained on a docs page. in contrast, the first call to an llm will not output text to a user. instead, it synthesizes the chat history with the user \u2019 s most recent message and the chat history to generate an intermediate question which is used to a ) find the most relevant chunk of text from the docs", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "user \u2019 s most recent message and the chat history to generate an intermediate question which is used to a ) find the most relevant chunk of text from the docs, and b ) provide the context for the chat response generator. prompt templatesthe prompt you want for the llm will change depending on the exact task at hand. in this particular case, the prompt we use to get an llm to synthesize new messages with the chat history is different from the prompt we use to construct a written response for the user. here we define the two prompts we use with a prompttemplate. the first is the default provided by", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "use to construct a written response for the user. here we define the two prompts we use with a prompttemplate. the first is the default provided by langchain, and the second is one we have customized with a relevant example. preprocessing docs into embeddings we preprocess the docs into a vectorstore of embeddings with this function ingest _ docs, which we only need to run once. each chunk of text ( 1, 000 characters long with some slight overlap between successive chunks ) will get its own embedding. these embeddings are stored in", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of text ( 1, 000 characters long with some slight overlap between successive chunks ) will get its own embedding. these embeddings are stored in a vectorstore, which we can load at inference time when it is time to find relevant docs pages. define langchain agentnext, we define a function that takes an api key and returns our langchain agent ( which will make it easy to integrate this function into our gradio interface ). we temporarily store the api key as an environment variable, which the langchain uses to connect to the openai endpoint. we use the chatvector", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we temporarily store the api key as an environment variable, which the langchain uses to connect to the openai endpoint. we use the chatvectordbchain, which takes as input our docs vectorstore, the chat synthesis llm, the chat response llm, callbacks to handle streaming text, and the parameter to enable returning source documentation. to chat with an agent returned by this function, pass in the user \u2019 s input and the chat history into the agent \u2019 s input dict : agent ( { \" question \" : \" type in a q \", \" chat _ history \" : history } ) [", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the agent \u2019 s input dict : agent ( { \" question \" : \" type in a q \", \" chat _ history \" : history } ) [ \" answer \" ] gradio tutorialfor testing our chatbot, we create an interactive shareable user interface with gradio. once a user registers their api key and enters messages into the chat, the langchain agent will reply with a written message and its source from the documentation. the llm - generated response to the user \u2019 s question is in a chat window on the left, and the source page from the documentation is displayed on the right, along with", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "generated response to the user \u2019 s question is in a chat window on the left, and the source page from the documentation is displayed on the right, along with a url to that same page on the arthur website. additionally, in the bottom left, we provide a set of options for a user to give feedback. ui components and layoutgradio layout is defined by organizing components into rows and columns. in our left column are markdown components for the title and instructions, a chat window, a send button, example user inputs, and a feedback button. in our right column is a textbox to enter the user \u2019 s api key", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "chat window, a send button, example user inputs, and a feedback button. in our right column is a textbox to enter the user \u2019 s api key, a register api key button, and markdown components for the source doc url and markdown text. each button first has to be defined in its position in the layout as described above, and needs to have its functionality connected to a method that is called when the user presses the button. the register api key button will call the get _ langchain _ agent ( ) function, the send button will call the chat function, and the submit feedback button will call the log", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "will call the get _ langchain _ agent ( ) function, the send button will call the chat function, and the submit feedback button will call the log _ inference function. here is the function to launch the gradio demo, which you can modify to include your own title, instructions, and example inputs. the first section places the components in their proper place in the layout, and the second section attaches functionality to components. below this function, we define the helper functions it calls. streaming the agent outputthis is the function that calls our langchain agent with a new message from the user ( and chat history", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##er functions it calls. streaming the agent outputthis is the function that calls our langchain agent with a new message from the user ( and chat history ). we get both the chat response and source document name from agent result. we then convert the source document name into a valid url to our docs and its corresponding markdown text with a helper function. we then yield the chat response ( as well as the source doc and link ) as a generator for streaming text. helper function for parsing our langchain agent \u2019 s output into its source text and corresponding url : save inferences and feedbackthi", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "streaming text. helper function for parsing our langchain agent \u2019 s output into its source text and corresponding url : save inferences and feedbackthis format may change in the future, but for now we simply record two columns : the chat history ( string ) and the feedback ( integer ). we parse the chat history into a single string of alternating input < >, output < >, input < >, output < >, etc. for now, we save this data to a local file, with one feedback submission getting entered one at a time as a new row in a csv. in a future episode,", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "save this data to a local file, with one feedback submission getting entered one at a time as a new row in a csv. in a future episode, we will integrate it with arthur for proper model monitoring! sharing the gradio demo a script to launch a gradio demo needs a launch function that creates a demo object, lays out each item in the ui, assigns functionality to each item, and ends with demo. launch ( ). when you want the demo to be shareable via a url, you only pass in an extra parameter to the end of the launch function : demo. launch ( share = true )", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to be shareable via a url, you only pass in an extra parameter to the end of the launch function : demo. launch ( share = true ) now when we run python launch _ ask _ arthur _ gradio. py - - share = true from the command line, we can share the demo to users with a link generated by gradio to the server where they are hosting our application and the files it depends on. this has allowed us to easily send the demo around to our team at arthur for them to try out the model and see for themselves what it can and cannot do! episode 2 and beyond.", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "send the demo around to our team at arthur for them to try out the model and see for themselves what it can and cannot do! episode 2 and beyond... want to give ask arthur a whirl? check it out here. in the next ask arthur episode, we will dive deep into evaluating different options for our llm prompts \u2014 stay tuned and keep looking out for changes in this quickly evolving space! previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabular", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / ask - arthur - episode - 1 - introduction", "metadata": {"source": "https://www.arthur.ai/blog/ask-arthur-episode-1-introduction", "row": 176, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 177 text : the thinking we haven \u2019 t done on llms solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlarge language modelsthe thinking we haven \u2019 t done on llmsby : teresa dattamarch 14, 2023you probably haven \u2019 t heard of human - centered evaluation of llms, and that needs to change. human - centered work seeks to understand how real humans interact with technology", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "haven \u2019 t heard of human - centered evaluation of llms, and that needs to change. human - centered work seeks to understand how real humans interact with technology, so that we can understand how humans ( with all of their cognitive biases and quirks ) interact with llms, and how these models affect individual human decision - making. this work was accepted to the chi 2023 workshop on generative ai and hci. to read our full paper, please visit here. what are llms? in the past year, large language models ( llms ) have exploded in popularity \u2014 from research, to industry, to public awareness and", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "what are llms? in the past year, large language models ( llms ) have exploded in popularity \u2014 from research, to industry, to public awareness and accessibility. for instance, chatgpt set historic records for its customer growth, with over 100 million users in its first two months. these models predict next tokens ( character, word, or string ) based on past context, generating free - form text for a variety of tasks in almost any specified style. this means that people can repeatedly integrate llms into their daily lives \u2014 to decide what to eat for breakfast, to write the responses to emails left unanswered from", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that people can repeatedly integrate llms into their daily lives \u2014 to decide what to eat for breakfast, to write the responses to emails left unanswered from yesterday, to develop the sales pitch they have to present mid - morning, to generate a funny joke during a break from work, etc. a variety of concerning issues of llms have already been identified, such as biased, toxic, or hallucinated outputs, but these largely only reflect distributional or instance - wise properties of their outputs. the potential ubiquity of this tool means that we need to consider how humans will actually interact with and use this new technology,", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "properties of their outputs. the potential ubiquity of this tool means that we need to consider how humans will actually interact with and use this new technology, while also acknowledging that we are all prone to cognitive biases and other quirks. this area of research is referred to as human - centered evaluation, and it has not yet been thoroughly explored for llms. human - centered evaluation is, however, already popular in the explainable ai ( xai ) community. what is explainable ai? defining explainability for ml models is a subject of ongoing discussion. for the purposes of our discussion, we will focus on the most common", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is explainable ai? defining explainability for ml models is a subject of ongoing discussion. for the purposes of our discussion, we will focus on the most common type of model transparency seen in industry : post - hoc explanations of black box models. these often rely on only a trained model \u2019 s inputs and outputs to identify patterns in how models make decisions. these methods aim to unlock transparency in order to allow stakeholders to understand the decision - making process of models to improve trust and mitigate downstream harms. arthur \u2019 s explainability features offer a variety of explanation options including counterfactual explanations ( understanding how a model \u2019 s predictions would", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##igate downstream harms. arthur \u2019 s explainability features offer a variety of explanation options including counterfactual explanations ( understanding how a model \u2019 s predictions would have changed given a hypothetical, what - if scenario ) and popular methods such as lime and shap. there are a variety of quantitative metrics for evaluating xai \u2014 these are mostly scores that calculate certain ideal properties of explanations ( for more information, see this piece ), but perhaps more importantly, there are a variety of qualitative evaluation considerations. these are important for two main reasons. what makes explainable ai ( and llms ) unique? qualitative evaluation is", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of qualitative evaluation considerations. these are important for two main reasons. what makes explainable ai ( and llms ) unique? qualitative evaluation is important for xai and llms because they are distinct from the classic ml paradigm for two key reasons : 1. there is no ground truth. there is no exactly correct explanation for a black box system. llms are inherently open - ended systems that don \u2019 t have a ground truth output for each input. 2. in practice, xai and llm outputs are often actually meant for some downstream decision or task. practitioners often use xai as an assistive tool for model", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in practice, xai and llm outputs are often actually meant for some downstream decision or task. practitioners often use xai as an assistive tool for model debugging, generating hypotheses, and ensuring compliance. llm outputs are often a tool to help you decide \u2014 what email to send to your client, what quick summary of an important document you will read, what answer is provided for a pertinent question, etc. this means that the context of xai and llm use involves some human using an explainer / llm as a piece of evidence to make some decision. thus, we need to consider", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "xai and llm use involves some human using an explainer / llm as a piece of evidence to make some decision. thus, we need to consider how practitioners use, receive, and comprehend outputted ai. especially because humans are susceptible to cognitive biases when processing information and making decisions. what does qualitative, human - centered evaluation look like in practice? there are three areas of focus to consider. 1. understanding users \u2019 mental modelsa user \u2019 s mental model, a term coined by don norman, of a technology is their internal understanding of how a technology works. for instance, maybe your mental model of a cross", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model, a term coined by don norman, of a technology is their internal understanding of how a technology works. for instance, maybe your mental model of a crosswalk is that pushing the crosswalk button will cause the walk signal to appear more quickly. however, for many cities, that button does not actually do anything. this is an example of a user's mental model not aligning with a technology's true model. people rely heavily on their mental models of technology to decide when to use the technology, to evaluate how much to trust the outputted results, and to make sense of any results ( see, e. g.", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "decide when to use the technology, to evaluate how much to trust the outputted results, and to make sense of any results ( see, e. g. cabrera et al. 2023, he & gadiraju 2022, kaur et al. 2022 ). these personal mental models are formulated from a user's perceptions and interactions with the technology and how they believe the system works. while ml practitioners may have had access to specialized training on how llms work, this is not the case for the vast majority of the general population. we cannot assume that everybody will have the same understanding of how a technology works", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ms work, this is not the case for the vast majority of the general population. we cannot assume that everybody will have the same understanding of how a technology works as we do. to our knowledge, no work has explored the mental models the general public holds for llms. how a general user believes an llm to work may be very different from how it actually works, and this mismatch can be dangerous. it is not difficult to imagine frightening scenarios where users anthropomorphize or deify an llm chatbot, understanding it to be a \" magical'' source of ground truth. this could very quickly", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##opomorphize or deify an llm chatbot, understanding it to be a \" magical'' source of ground truth. this could very quickly lead to conspiracy theories and the legitimization of disinformation campaigns. it is important to consider if this is an issue of messaging and education \u2014 informing the public via ai literacy \u2014 or of regulators \u2014 to implement policies that force the algorithm providers to provide accurate, comprehensible warning labels on the limitations of their technology. 2. evaluating use case utilityas previously discussed, xai and llms are often tools for accomplishing some other goal. the term use", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of their technology. 2. evaluating use case utilityas previously discussed, xai and llms are often tools for accomplishing some other goal. the term use case in the xai literature refers to a specific usage scenario and its associated downstream task or end goal. it has been found in the xai literature that although it might be easy to assume that an explanation will be helpful for a user accomplishing a task like model debugging or model understanding, this is not necessarily the case. when the performance of that downstream task is measured, the presence of explanations can sometimes have no effect, or can even have a negative effect on performance", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the case. when the performance of that downstream task is measured, the presence of explanations can sometimes have no effect, or can even have a negative effect on performance, especially if the xai is faulty ( see, e. g. jacobs et al. 2021, adebayo et al. 2020 ). very limited work has explored the utility of llms in use case \u2013 specified user studies, but a user study on microsoft / github's copilot, an llm - based code generation tool, found that it \u201c did not necessarily improve the task completion time or success rate. \u201d basically, we want to understand", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "an llm - based code generation tool, found that it \u201c did not necessarily improve the task completion time or success rate. \u201d basically, we want to understand if the ai assistive tool is actually helpful for successfully accomplishing the end goal. 3. acknowledging cognitive engagementcognitive effort is a form of labor, and unsurprisingly, people tend to favor less demanding forms of cognition and other mental shortcuts. as an example, when asked to \" agree'' to a user agreement when signing up for a new platform, you are probably more likely to check the box than to cognitively engage with the language of", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "''to a user agreement when signing up for a new platform, you are probably more likely to check the box than to cognitively engage with the language of the agreement. unfortunately, this human tendency can lead to unintended or dangerous outcomes because humans are susceptible to a wide variety of cognitive biases. for xai, this manifests as practitioners only superficially examining explanations instead of digging deeply, leading to over - trust, misuse, and a lack of accurate understanding of the outputs. this can be dangerous when it results in the over - confident deployment of a faulty or biased model. issues of cognitive engagement should be held front", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of the outputs. this can be dangerous when it results in the over - confident deployment of a faulty or biased model. issues of cognitive engagement should be held front and center for researchers of llms. because of their massive scale and public accessibility, llms may quickly become ubiquitous in all aspects of daily life. realistically, how much will users actually cognitively engage with the magnitude of generated outputs to ensure that they are correct and aligned with their intentions? consider an llm - generated email : how often and how deeply will a user review that generated email before sending it? what if it's not just one email, what if it", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "generated email : how often and how deeply will a user review that generated email before sending it? what if it's not just one email, what if it's every email? will they always catch when the generated output says something incorrect, or worse, inappropriate? furthermore, our attention spans have decreased dramatically with the increase in digital stimulation. another aspect of concern is that llm outputs often sound very confident, even if what they are saying is completely false. when the user inquires about the incorrectness, they also have a documented tendency to argue that the user is wrong and that their response is correct. ( in fact,", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "inquires about the incorrectness, they also have a documented tendency to argue that the user is wrong and that their response is correct. ( in fact, some have called llms \" mansplaining as a service. \" ) this can make it more difficult for humans to implement cognitive checks on llm outputs. why is this important? the scale of the reach of llms is massive, and so the consequences of not having a qualitative understanding of the utility of their outputs are grave. beyond the environmental and computational costs of such models, there are social consequences that are entirely unknown from the offloading of our cognitive load", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of their outputs are grave. beyond the environmental and computational costs of such models, there are social consequences that are entirely unknown from the offloading of our cognitive load onto these agents. we need to understand how users make decisions about whether to utilize the outputs of llms, the mental models that users have of these technologies, whether llm outputs are actually helpful in downstream tasks, and how much users cognitively engage with the outputs to verify their correctness and lack of harm. it is dangerous to continue developing and making available larger and larger language models without a proper understanding of how humans will ( or will not ) cognitively engage with their outputs", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is dangerous to continue developing and making available larger and larger language models without a proper understanding of how humans will ( or will not ) cognitively engage with their outputs. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog /", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##wspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / the - thinking - we - havent - done - on - llms", "metadata": {"source": "https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms", "row": 177, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 178 text : announcing our strategic partnership with amazon web services solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llm", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesannouncing our strategic partnership with amazon web servicesby : arthur teammarch 7, 2023ai is the top priority for companies in 2023. the innovators are becoming truly ai - native and will lead their industries for decades. performance, accuracy, and obser", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "priority for companies in 2023. the innovators are becoming truly ai - native and will lead their industries for decades. performance, accuracy, and observability are the core ingredients for success. at arthur, our mission is making ai better for everyone, from the companies running models to drive their business to the consumers interacting with the applications they power. today, we are excited to announce a comprehensive strategic partnership with amazon web services. this partnership will allow us to bring our class - leading model monitoring to even more teams building transformative ai systems on aws. aws customers worldwide are now able to purchase and deploy our platform directly", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "class - leading model monitoring to even more teams building transformative ai systems on aws. aws customers worldwide are now able to purchase and deploy our platform directly from the marketplace, and will receive access to the full arthur offering, including : a centralized dashboard for all models, no matter how or where they are deployedexclusive capabilities in cv, nlp, bias mitigation, and other critical areasthe ability to monitor, measure, and optimize ai performance at scale across accuracy, data drift, explainability, and fairnessunique, actionable insights that provide visibility into your model performancehighly scalable microservic", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "across accuracy, data drift, explainability, and fairnessunique, actionable insights that provide visibility into your model performancehighly scalable microservices architecture that can ingest up to 1m transactions per secondpurchasing arthur through the marketplace significantly streamlines procurement and billing, providing a seamless purchasing experience. users looking to access the arthur platform can use their existing aws accounts and cloud credits to buy directly or work with our team to create a custom plan. visit our aws marketplace listing here to get started. if you are not an aws customer, request a demo or contact our team to learn more about", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". visit our aws marketplace listing here to get started. if you are not an aws customer, request a demo or contact our team to learn more about arthur. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##newspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / announcing - our - strategic - partnership - with - amazon - web - services", "metadata": {"source": "https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services", "row": 178, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 179 text : reflections on satml 2023 : we should be more cautious solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai research & innovationreflections on satml 2023 : we should be more cautiousby : daniel nissanifebruary 28, 2023earlier this month, our research team attended the inaugural ieee conference on secure and trustworthy machine learning ( or satml for short )", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##uary 28, 2023earlier this month, our research team attended the inaugural ieee conference on secure and trustworthy machine learning ( or satml for short ), where we presented our paper, tensions between the proxies of human values in ai. this is a first - of - its - kind conference, focusing on traditional responsible ai topics like fairness and explainability, while also welcoming work on robustness and adversarial machine learning. many industry practitioners attended, such as those from nvidia and intuit, as well as more industry - focused academics, giving this conference a more practical feel without being dominated by big tech", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as those from nvidia and intuit, as well as more industry - focused academics, giving this conference a more practical feel without being dominated by big tech companies. in this blog post, we reflect on the major theme from satml : we need to be careful about how we deploy and utilize machine learning solutions. we \u2019 ll highlight talks that particularly resonated with us and made us think about how to be more cautious, but note that most of the talks and the air of conversation were around this idea. as a community, we need to be more careful about how we do our research, how we deploy models, and how", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "air of conversation were around this idea. as a community, we need to be more careful about how we do our research, how we deploy models, and how we evaluate consequences. patrick altmeyer gave a compelling talk on his team \u2019 s paper, endogenous macrodynamics in algorithmic recourse. in their paper, altmeyer et al. come up with a simulation, where a bank may use a counterfactual explanation method to perform some sort of algorithmic recourse on an algorithm that determines whether someone gets a loan or not. this would cause a domain shift \u2014 several individuals will go over the decision", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "algorithmic recourse on an algorithm that determines whether someone gets a loan or not. this would cause a domain shift \u2014 several individuals will go over the decision boundary and get loans. but altmeyer et al. take this a step further. what if now you train that algorithm with those data points that shifted the boundary? then you would get a model shift \u2014 the model \u2019 s decision boundary would move to accommodate more of the data points. altmeyer illustrates in his talk that as this feedback loop continues, the model \u2019 s accuracy continues to decline. for the bank, however, this decline translates to risk of individual \u2019 s defaulting", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as this feedback loop continues, the model \u2019 s accuracy continues to decline. for the bank, however, this decline translates to risk of individual \u2019 s defaulting on loans. thus, the question is, who should take on that risk? altmeyer et al. offer a regularizer that takes into account the external cost ( in this case, the uncertainty that would be developed ) to better the algorithmic recourse, but i think this hits something deeper. as we continue to develop new explanation techniques, such as our fastcfe algorithm, should we be concerned with the feedback loops they may generate? this question is something that has", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to develop new explanation techniques, such as our fastcfe algorithm, should we be concerned with the feedback loops they may generate? this question is something that has been explored before in the fairness space, starting with delayed impact of fair machine learning, which has shown similar results. we even discuss this in our tensions paper, where we argue that the lack of contextual information combined with domain - agnostic definitions of our human values is inherently flawed. without taking the context into consideration, we cannot appropriately evaluate the consequences of our deployments. this especially extends to the feedback loops generated, something most research does not take into account. anna kawa", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we cannot appropriately evaluate the consequences of our deployments. this especially extends to the feedback loops generated, something most research does not take into account. anna kawakami presented her paper, led by amanda coston, sok : a validity perspective on evaluating the justified use of data - driven decision - making algorithms. coston et al. argue that the alignment of our decision making algorithms and the real world is one of the issues to watch, and in my opinion this is even a bigger issue. they even cite that many states in the u. s. have attempted to deploy machine learning algorithms to help, and decide to retire them", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "even a bigger issue. they even cite that many states in the u. s. have attempted to deploy machine learning algorithms to help, and decide to retire them soon after because they aren \u2019 t seeing the results they want. this is similar to research done by virginia eubanks, where well - meaning organizations have deployed models that actively harm the populations they are trying to serve. recent research endeavors have even started exploring the mismatch between optimization procedures in classification models, primarily focused on predictive performance, and the actual decision making tasks they are deployed to do, arguing that this mismatch leads to illegitimate deployments that should have never", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "on predictive performance, and the actual decision making tasks they are deployed to do, arguing that this mismatch leads to illegitimate deployments that should have never happened. in my opinion, this is one of the root issues of feedback loops. if we don \u2019 t have proper proxies, understandings of those proxies, and the contexts they will be utilized in, how can we hope to create models that are useful? i use the term useful here intentionally because, as has been famously said many times, \u201c all models are bad, but some are useful. \u201d the same goes with proxy variables. i think the una", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", as has been famously said many times, \u201c all models are bad, but some are useful. \u201d the same goes with proxy variables. i think the unasked question at satml, and honestly at most technology - oriented conferences, is : should we build these technologies? timnit gebru tackled this question directly during her keynote speech with a resounding no, pointing a finger directly towards those who think we should even try to achieve agi. because, as gebru put it, even giving the perception that a form of agi has been achieved can be harmful to us as humans. although models like chat", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as gebru put it, even giving the perception that a form of agi has been achieved can be harmful to us as humans. although models like chatgpt have achieved \u201c human - like \u201d performance, jacob steinhardt \u2019 s talk showed how aligning to what we think is human can be flawed, and those flaws are starting to show ( thinking about you, romantic bing ). satml began with zico kolter doing a retrospective on the past 5 years of adversarial machine learning. his talk was both sobering and informative, but i want to focus on one aspect of it. kolter presented an", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "adversarial machine learning. his talk was both sobering and informative, but i want to focus on one aspect of it. kolter presented an argument that he has heard many times : we need to redefine what we mean by robust machine learning. instead of defining robustness as an ability to handle worst - case scenarios, we should define it as an ability to handle distribution shifts, generalizability, and other things that practitioners care about. but then he shows the proverbial picture of a pig that got classified as an airplane, and tries to convince the audience that because technology cannot do this very basic task of", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "shows the proverbial picture of a pig that got classified as an airplane, and tries to convince the audience that because technology cannot do this very basic task of identifying an object, even with an adversarial attack, then we should try to make sure it can do this task. i want to push back on this. do we really want technology to mimic humans this much? is there positive use in technology being able to do, quite literally, everything that makes us human? or should we restrict ourselves to technologies that serve specific, meaningful purposes that actually improve our lives and outcomes? my opinion is that if we want to create responsible", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "? or should we restrict ourselves to technologies that serve specific, meaningful purposes that actually improve our lives and outcomes? my opinion is that if we want to create responsible ai, we should be intentional about our needs and build specific technologies around them. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of servicepriva", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / reflections - on - satml - 2023 - we - should - be - more - cautious", "metadata": {"source": "https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious", "row": 179, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 180 text : cdaos, prove your value : the new reality in 2023 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##vabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedbest practicescdaos, prove your value : the new reality in 2023by : adam wenchelfebruary 16, 2023at arthur, we work with many of the largest and best - run companies in the world, and one thing has become very", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##february 16, 2023at arthur, we work with many of the largest and best - run companies in the world, and one thing has become very clear with the 2023 budget cycle : you need to show measurable results to justify spend right now or else risk your budget being flatlined or slashed. cfos are tightening the belt. getting any new spend, or growing your budget, takes a rock solid business case. this is extremely hard with frontier technologies, in particular ai. the last few years have seen ai prioritized as an investment area because the potential is massive. 2023 is the year when cdaos", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "particular ai. the last few years have seen ai prioritized as an investment area because the potential is massive. 2023 is the year when cdaos and ai teams will need to show they can transition from potential to realized gains. building that clear business case means having ready answers to questions like : \u201c how are my models currently performing? \" \u201c what kind of lift can i expect from updating a critical suite of models driving a high - leverage use case? \u201d \u201c what effect will it have on my business kpis like p & l, churn, tlv, and roi? \u201d building the ability to measure and", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "what effect will it have on my business kpis like p & l, churn, tlv, and roi? \u201d building the ability to measure and evaluate ai performance into the foundation of your project portfolio makes it possible to understand the answer to these questions. showing up to planning and budget conversations armed with these critical details will be a big advantage navigating the next couple of years and beyond. ultimately, measurable results mean you can make strategic, rational, smart decisions and focus your resources on the initiatives that are driving the most value. it \u2019 s clear that the hype around ai is warranted \u2014 being able to empirical", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and focus your resources on the initiatives that are driving the most value. it \u2019 s clear that the hype around ai is warranted \u2014 being able to empirically demonstrate that will be the game - changing trend for cdaos in 2023. learn more about arthur's monitoring and measurement capabilities here. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiries", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##copechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / cdaos - prove - your - value - the - new - reality - in - 2023", "metadata": {"source": "https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023", "row": 180, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 181 text : keep the lights on : making deployed ai / ml better for everyone solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringkeep the lights on : making deployed ai / ml better for everyoneby : john dickersonfebruary 14, 2023it \u2019 s easier than ever to build and deploy ml models. storage is cheap, compute is cheap, pre - trained models are", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bruary 14, 2023it \u2019 s easier than ever to build and deploy ml models. storage is cheap, compute is cheap, pre - trained models are prevalent and, did i mention, cheap! as developers and practitioners, we \u2019 ve felt the pressure to deliver models into production to provide analytics for internal processes all the way up to running decision - making for mission - critical problems. our job is, in a nutshell, to create a model that performs well right now, according to one or more downstream kpi - related metrics. that \u2019 s generally doable \u2014 we handle data identification, etl pipelines ( and", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", according to one or more downstream kpi - related metrics. that \u2019 s generally doable \u2014 we handle data identification, etl pipelines ( and their variants ), model training, verification, and beyond. a model is trained that exceeds expectations and is deployed. but what happens after that? no matter how clean the input data, no matter how well - trained the model, it \u2019 s known that a model posted to production will degrade with respect to downstream metrics. input distributions will shift ( think covid - 19 impacting restaurant seating, hurricanes rolling through wedding destinations, or sudden demand spikes due to viral marketing )", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s. input distributions will shift ( think covid - 19 impacting restaurant seating, hurricanes rolling through wedding destinations, or sudden demand spikes due to viral marketing ). furthermore, those dynamics, relative to downstream business metrics, may unduly impact particular subgroups due to latent legal, demographic, or political shifts. a \u201c perfect \u201d deployed model today is not a perfect model tomorrow, or in three months, or in a year. it \u2019 s important to keep the future in mind when deploying now, and to understand that model deployment is not the end of the model lifecycle. more bluntly : once the thing is", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "future in mind when deploying now, and to understand that model deployment is not the end of the model lifecycle. more bluntly : once the thing is properly built, we need to make sure it stays good. even more bluntly : model monitoring \u2014 production monitoring \u2014 shouldn \u2019 t be walled off behind an enterprise sales team, held outside of the developer - first mlops pipeline, but instead should be easily accessible by all practitioners. that \u2019 s something we at arthur want to, and can, enable. over the last few years, we \u2019 ve built the world \u2019 s strongest model monitoring solution, battle - tested by some of", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to, and can, enable. over the last few years, we \u2019 ve built the world \u2019 s strongest model monitoring solution, battle - tested by some of the world \u2019 s largest enterprise clients. we \u2019 re looking forward to spending more time focusing on the developer community, working closely with practitioners to learn and shape what the most effective model monitoring solution should be. story time : making sure organs go to the right placecareer data scientists, machine learning practitioners, machine learning scientists, statisticians, business analysts \u2014 with a wide variety of application areas, and a global scope for applications, we \u2019 re all being asked to build and", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", statisticians, business analysts \u2014 with a wide variety of application areas, and a global scope for applications, we \u2019 re all being asked to build and deploy models. arthur is a product built by engineers who needed a product like arthur in their past jobs, and need a product like arthur for their side projects. in my own career wearing many of those hats, [ 1 ] i \u2019 ve built and deployed models for bundled advertising campaign pricing, enablement of indian election prediction markets, global blood donation recommendation systems, international drug interdiction allocation and efficacy estimation, television advertising allocation, and organ donation, to name a few. i", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "markets, global blood donation recommendation systems, international drug interdiction allocation and efficacy estimation, television advertising allocation, and organ donation, to name a few. i \u2019 ll lean on that experience for a little \u201c monitoring matters \u201d wisdom below, after a quick story. in organ exchange, patients in need of an organ enter an organized barter market to find a willing, compatible donor. organized kidney exchange has existed for two decades, and i \u2019 ve been heavily involved in that process for 13 + years, with large exchanges running code i wrote to match patients to donors, and organizational committees using my code to provide \u201c what - if \u201d analyses", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for 13 + years, with large exchanges running code i wrote to match patients to donors, and organizational committees using my code to provide \u201c what - if \u201d analyses during policymaking decisions. time and time again, it \u2019 s been made clear to me that deploying a computationally \u201c optimal \u201d approach to clearing these exchanges, then letting that code run day after day, is not sufficient. value judgments are made, medical technology improves, supply increases or decreases, the legal landscape shifts \u2014 what worked well yesterday may not work well today. in short, we write code to solve a problem based on a model of the real world at a given", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2014 what worked well yesterday may not work well today. in short, we write code to solve a problem based on a model of the real world at a given point in time ; that model is a noisy proxy for what actually matters, and what matters changes over time. in practice, an \u201c optimal approach \u201d is deployed, but : the model is uncertain. the inputs are noisy to begin with. problems include missing variables, missing constraints, improperly set weights and costs, and beyond. is a particular transplant center reliable? does a particular social variable correlate with likelihood to donate? the model is brittle to shifts in the underlying environment", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "beyond. is a particular transplant center reliable? does a particular social variable correlate with likelihood to donate? the model is brittle to shifts in the underlying environment. for example, during covid, living organ donation rates dropped due to fear of entering a hospital and / or capacity constraints at transplant centers. how does that impact transplantation rates? and, if covid impacted particular populations more than others, how does that impact metrics for fairness and bias in organ allocation? any allocative model will disparately treat specific subpopulations, and measuring and monitoring for that is imperative for downstream policymaking. the model is poorly", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "allocative model will disparately treat specific subpopulations, and measuring and monitoring for that is imperative for downstream policymaking. the model is poorly understood by stakeholders. visualizing complex statistics is hard. but, we use machine learning models to address problems that are hard for humans to understand. so, it \u2019 s important to communicate results to end stakeholders ( in the organ exchange case, doctors, patients, donors, lawyers, etc. ). those statistics change over time, as the world changes. communicating that change in a comprehensible way matters. certain demographics are systematically mistreated by the model. when we", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "time, as the world changes. communicating that change in a comprehensible way matters. certain demographics are systematically mistreated by the model. when we train models, we typically aim to maximize / minimize a specific objective function. that function may maximize utility or welfare for the many at the cost of utility or welfare for the few. this plays out in healthcare, including organ exchange, frequently ; and, this can change drastically as the underlying political, legal, or demographic landscape shifts \u2014 regardless of the model that was trained and deployed. in my experience, these general concerns arise in most application areas, not just organ exchange. pricing advertisements", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "shifts \u2014 regardless of the model that was trained and deployed. in my experience, these general concerns arise in most application areas, not just organ exchange. pricing advertisements depends on underlying social trends as well as external demand for correlated inventory. drug interdiction success rates correlate with weather as well as uscis / cpb patrol policies. worldwide blood donation efficacy correlates with national and who policy. i \u2019 m sure you can think of examples from your own past or present, too. that \u2019 s part of our motivation behind building arthur \u2014 creating a scalable platform for solving general problems in model performance across industries. monitoring models with", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", too. that \u2019 s part of our motivation behind building arthur \u2014 creating a scalable platform for solving general problems in model performance across industries. monitoring models with arthurwe built arthur to monitor models in production, and to aid in the model verification process. our enterprise clients \u2014 across banking, healthcare, agriculture, logistics, news, and beyond \u2014 have all felt that pain of unobserved model deployment, directly via revenue loss or indirectly via damage to their brand. we are continuing to translate that technology \u2014 distribution and concept drift detection and connection to downstream kpis, bias and fairness definition, detection, and mitigation, and model explain", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to translate that technology \u2014 distribution and concept drift detection and connection to downstream kpis, bias and fairness definition, detection, and mitigation, and model explainability across the board \u2014 to the individual developer and team of developers. our platform already handles structured and unstructured data, and we have an exciting roadmap over the coming year expanding in our core strengths like computer vision and nlp connected to foundation models, robust approaches to measuring all of the metrics, and effortless scaling as our clients \u2019 needs grow. we look forward to continuing to partner with the mlops community! if you \u2019 re in austin next week (", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "effortless scaling as our clients \u2019 needs grow. we look forward to continuing to partner with the mlops community! if you \u2019 re in austin next week ( feb 21 - 23 ), come find us at the data science salon austin \u2014 we \u2019 re going to be sharing some exciting things we \u2019 ve been building to tackle these very issues. footnotes [ 1 ] all of these examples are independent from my time building arthur! we encounter the same style of ( broad, reaching ) problem at arthur, and i am happy to dive into those details over a coffee or beer. my goal with this post is to identify with the reader as", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") problem at arthur, and i am happy to dive into those details over a coffee or beer. my goal with this post is to identify with the reader as an ml practitioner, not necessarily a c - suite executive. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##pdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / keep - the - lights - on - making - deployed - ai - ml - better - for - everyone", "metadata": {"source": "https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone", "row": 181, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 182 text : gartner recognizes arthur in 2023 market guide for ai trust, risk, and security management solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast,", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesgartner recognizes arthur in 2023 market guide for ai trust, risk, and security management ( trism ) gartner has included arthur as a representative explainability / model monitoring vendor in its ai trust, risk, and", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "risk, and security management ( trism ) gartner has included arthur as a representative explainability / model monitoring vendor in its ai trust, risk, and security management ( trism ) market. by : arthur teamjanuary 26, 2023in addition to having recognized arthur as a cool vendor\u2122 in ai governance and responsible ai as well as in its hype cycle\u2122 report for data and analytics governance, gartner has included arthur as a representative explainability / model monitoring vendor in its ai trust, risk, and security management ( trism ) market. what is ai trism? ai trism is a framework that ensures", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "monitoring vendor in its ai trust, risk, and security management ( trism ) market. what is ai trism? ai trism is a framework that ensures ai model governance, trustworthiness, fairness, reliability, security, and data protection. its four main categories are : model explainability and model monitoring, privacy, modelops, and ai application security. together, use of solutions from these four categories helps data and analytics leaders and their colleagues implement ai - specific trust, risk, and security management measures. why is ai trism important? in industries ranging from healthcare to financial services to retail and beyond, enterprises are leveraging ai", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "risk, and security management measures. why is ai trism important? in industries ranging from healthcare to financial services to retail and beyond, enterprises are leveraging ai in nearly all facets of their business. the breadth and complexity of ai use cases is only continuing to grow, which increases potential exposure and vulnerability if not implemented with a framework like this in place. failure in these cases could mean millions in financial, legal, or reputational losses. \u201c by 2026, organizations that operationalize ai transparency, trust, and security will see their ai models achieve a 50 % improvement in terms of adoption, business goals, and user acceptance, \u201d", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "operationalize ai transparency, trust, and security will see their ai models achieve a 50 % improvement in terms of adoption, business goals, and user acceptance, \u201d states the guide. \u201c ai brings new trust, risk, and security management challenges that conventional controls do not address. data and analytics leaders must use the capabilities described in this guide to improve model reliability, trustworthiness, fairness, privacy, and security. \u201d how arthur aligns with ai trismarthur helps enterprise teams monitor, measure, and optimize ai performance at scale across three key areas : accuracy, explainability, and fairness. this allows organizations to understand how", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "teams monitor, measure, and optimize ai performance at scale across three key areas : accuracy, explainability, and fairness. this allows organizations to understand how their models are making decisions, ensure compliance in their ml systems, and continually improve the fairness of model outcomes by proactively monitoring for bias. production ai and business value are inextricably linked in today \u2019 s world \u2014 and will continue to be in tomorrow \u2019 s. as the leading model monitoring and governance solution in the market, arthur is deployed at leading fortune 100 enterprises across industries to accelerate business transformation through ai and drive better, more responsible results. at arthur, our", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in the market, arthur is deployed at leading fortune 100 enterprises across industries to accelerate business transformation through ai and drive better, more responsible results. at arthur, our mission is to make ai better for everyone, and we \u2019 re proud to be recognized for doing so. if you are a gartner client, you can access the full market guide here. learn more about arthur and sign up for a demo here. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr &", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / gartner - recognizes - arthur - in - 2023 - market - guide - for - ai - trust - risk - and - security - management - trism", "metadata": {"source": "https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism", "row": 182, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 183 text : arthur achieves soc 2\u00ae type ii certification & compliance solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur achieves soc 2\u00ae type ii certification & complianceby : arthur teamjanuary 18, 2023today, we \u2019 re excited to announce that arthur has completed our systems and organization controls ( soc 2\u00ae ) type ii certification. soc 2 has long", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##3today, we \u2019 re excited to announce that arthur has completed our systems and organization controls ( soc 2\u00ae ) type ii certification. soc 2 has long been regarded as the gold standard for saas security compliance and is a critical enterprise milestone in upholding the trust service principles of the american institute of certified public accountants ( aicpa ). as a model monitoring company, security is important to everything we do at arthur. our platform is constantly measuring, processing, and analyzing machine learning models to deliver better results for our customers \u2014 and working with customers in highly regulated industries only heightens the importance of security compliance. arthur \u2019 s successful", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning models to deliver better results for our customers \u2014 and working with customers in highly regulated industries only heightens the importance of security compliance. arthur \u2019 s successful completion of both soc 2 type i and type ii audits in 2022 means that our company has consistently maintained infrastructure service processes that meet the required levels of oversight and monitoring so that we can proactively identify and address any unusual activity. additionally, it demonstrates our commitment to meeting the industry \u2019 s most rigorous data security, availability, and confidentiality standards. \u201c from day one, arthur \u2019 s focus has been to develop a robust model monitoring solution that adheres to best -", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", availability, and confidentiality standards. \u201c from day one, arthur \u2019 s focus has been to develop a robust model monitoring solution that adheres to best - in - class security and data privacy controls, in order to deliver a trustworthy experience for our enterprise customers, \u201d said arthur \u2019 s vp of engineering george chitouras. as we continue to scale solutions in financial services, healthcare, insurance and more, worldwide trust in our infrastructure, software, people, data, and procedures is paramount for scaling model ops in the cloud. want to learn more about our commitment to security? get in touch. previous postsharenext post we", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "procedures is paramount for scaling model ops in the cloud. want to learn more about our commitment to security? get in touch. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - achieves - soc - 2", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - achieves - soc - 2 - r - type - ii - certification - compliance", "metadata": {"source": "https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance", "row": 183, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 184 text : arthur earns placements on built in \u2019 s 2023 best places to work list solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##wallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur earns placements on built in \u2019 s 2023 best places to work listby : arthur teamjanuary 11, 2023today, built in announced that arthur was honored in its best places to work awards for the", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to work listby : arthur teamjanuary 11, 2023today, built in announced that arthur was honored in its best places to work awards for the second year in a row. specifically, arthur earned a place on new york city best startups to work for, new york city best places to work, and u. s. best startups to work for. the annual awards program includes companies of all sizes, from startups to those in the enterprise, and honors both remote - first employers as well as companies in large tech markets across the country. \u201c we \u2019 re thrilled to be recognized by built in alongside the top companies", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "both remote - first employers as well as companies in large tech markets across the country. \u201c we \u2019 re thrilled to be recognized by built in alongside the top companies in new york city and beyond, \u201d says adam wenchel, co - founder and ceo of arthur. \u201c new york has grown into a world - class enterprise tech ecosystem over the past decade, and we \u2019 re proud to be part of the community and call it our home base. i \u2019 ve been so impressed by our team \u2019 s continued ability to integrate the sentiments at the core of our company mission \u2014 make ai work for everyone \u2014 into a welcoming and equally mission - driven", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "our team \u2019 s continued ability to integrate the sentiments at the core of our company mission \u2014 make ai work for everyone \u2014 into a welcoming and equally mission - driven company culture. we nearly doubled our team in 2022, and we can \u2019 t wait to welcome even more arthurians in 2023! \u201d at arthur, we are fully committed to not only fulfilling and challenging work that truly makes a difference, but also providing the opportunity to do that work as part of a diverse, creative, and passionate team. we believe our competitive compensation, equitable hiring practices, and comprehensive benefits ( including wellness programs, a home office stipend,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "creative, and passionate team. we believe our competitive compensation, equitable hiring practices, and comprehensive benefits ( including wellness programs, a home office stipend, tuition reimbursement, and more ) truly make arthur an incredible place to work, and it \u2019 s exciting to see our company continue to be recognized by built in. interested in joining us? check out our open roles here. \u201c it \u2019 s my honor to congratulate this year \u2019 s best places to work winners, \u201d says sheridan orr, chief marketing officer, built in. \u201c these exemplary companies understand their people are their most valuable asset, and they \u2019 ve", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "work winners, \u201d says sheridan orr, chief marketing officer, built in. \u201c these exemplary companies understand their people are their most valuable asset, and they \u2019 ve stepped up to meet the modern professional \u2019 s new expectations, including the desire to work for companies that deliver purpose, growth, and inclusion. these winners set the stage for a human - centered future of work, and we can \u2019 t wait to see that future unfold. \u201d about built inbuilt in is creating the largest platform for technology professionals globally. monthly, millions of the industry \u2019 s most in - demand professionals visit the site from across the world. they rely", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "creating the largest platform for technology professionals globally. monthly, millions of the industry \u2019 s most in - demand professionals visit the site from across the world. they rely on our platform to stay ahead of tech trends and news, learn skills to accelerate their careers and find opportunities at companies whose values they share. built in also serves 2, 000 customers, innovative companies ranging from startups to those in the fortune 500. by putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard - to - reach tech professionals. about built in \u2019 s best places to work built in \u2019 s esteemed best places to work awards,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "them hire otherwise hard - to - reach tech professionals. about built in \u2019 s best places to work built in \u2019 s esteemed best places to work awards, now in its fifth year, honor companies across numerous categories : 100 best places to work, 50 best startup places to work, 100 best midsize places to work, 100 best large places to work and editor \u2019 s choice : 100 best hybrid places to work. the program honors companies \u2014 remote, hybrid and in - office \u2014 with the best total rewards packages across the u. s. and in the following tech hubs : atlanta, austin, boston, chicago, colorado,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "office \u2014 with the best total rewards packages across the u. s. and in the following tech hubs : atlanta, austin, boston, chicago, colorado, dallas, houston, los angeles, miami, new york, san diego, san francisco, seattle and washington d. c. about arthurarthur is the ai performance company. the arthur platform monitors, measures, and improves machine learning models to deliver better results, working with enterprise teams to accelerate model operations and optimize for accuracy, explainability, and fairness. arthur's research - led approach to product development drives exclusive capabilities in enterprise scalability, computer vision, nlp", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for accuracy, explainability, and fairness. arthur's research - led approach to product development drives exclusive capabilities in enterprise scalability, computer vision, nlp, bias mitigation, and other critical areas. contacts : press @ arthur. aiprevious postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - earns - placements - on - built - ins - 2023 - best - places - to - work - list", "metadata": {"source": "https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list", "row": 184, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 185 text : team arthur at neurips'22 : a retrospective solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai research & innovationteam arthur at neurips \u2018 22 : a retrospectiveby : arthur teamdecember 16, 2022the arthur team is back home in new york after a strong showing at the conference on neural information processing systems ( a. k. a. neuri", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "2022the arthur team is back home in new york after a strong showing at the conference on neural information processing systems ( a. k. a. neurips ), arguably the largest and most influential machine learning conference. we hosted onsite and offsite socials, gave an oral presentation, presented two papers in the main conference as well as papers at two workshops, co - organized a workshop, recruited \u2014 and one of us helped compose a song on overfitting, sung live with a pair of ukulele - wielding ml practitioners. neurips holds a special place in arthur \u2019 s history. back in 2019,", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", sung live with a pair of ukulele - wielding ml practitioners. neurips holds a special place in arthur \u2019 s history. back in 2019, we announced our $ 3. 3m seed round at a collocated event to neurips in vancouver. now, three years, 40 + team members in growth, and $ 50 + million dollars later, it was great to return to neurips, this time in new orleans. big themes, many of which our team expands upon below, included large language models ( llms ) and their generalization and semi - rebranding as foundation models, cross - collaboration", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "which our team expands upon below, included large language models ( llms ) and their generalization and semi - rebranding as foundation models, cross - collaboration between ai and other fields ( psychology, policy, etc. ), human - in - the - loop and user - centric ml pipelines, and context - aware ml as it relates to privacy and fairness. neurips is known for being both a venue for the dissemination of new industrial and academic research as well as a networking meetup with a strong event culture. arthur, ever the responsible community member, also contributed to this latter focus by organizing a couple of well -", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as a networking meetup with a strong event culture. arthur, ever the responsible community member, also contributed to this latter focus by organizing a couple of well - attended events. on wednesday night, joint with our partners at index ventures, we hosted an offsite happy hour at jack rose, with attendance from the investor, founder, big tech, and academic community. the vc and investing community continues to increase its presence at flagship ml conferences, and largely driven by the current excitement surrounding foundation models and generative ml ( e. g., stable diffusion, chatgpt ), it was great to see this trend continue at neurips", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "foundation models and generative ml ( e. g., stable diffusion, chatgpt ), it was great to see this trend continue at neurips. on thursday night, joint with friends at abacus. ai, we held an open mic night at the conference venue with a few hundred attendees. meant to be a free - form community event full of cheeky machine - learning - oriented fun, this was a great success, with gpt - 3 generated poetry and live song, debate about the merits of non - tabular data, discussions of who invented social networking, and the evergreen research topic of how to improve peer review", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "live song, debate about the merits of non - tabular data, discussions of who invented social networking, and the evergreen research topic of how to improve peer review ( as if it \u2019 s broken! ). we \u2019 re happy to help build a vibrant ml community. arthur and our friends at abacus. ai teamed up to host an open mic night! partnering with index ventures, we also hosted an event at the picturesque restaurant jack rose. below, members of our ml team give their takeaways and hot takes on what \u2019 s right, what \u2019 s wrong, what \u2019 s hot, and what \u2019 s not in the academic", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "give their takeaways and hot takes on what \u2019 s right, what \u2019 s wrong, what \u2019 s hot, and what \u2019 s not in the academic and industrial machine learning world. the current state of ml research ( it \u2019 s not just llms ) arthur mle valentine d \u2019 hauteville writes, \u201c on my first day at neurips, i recount the awe i felt walking amongst the myriad of different posters and research projects in the big hall of the new orleans convention center. the mosaic of ideas, topics, and research stories displayed before me was impressive and a stark contrast to my usual research flow, which consists", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "new orleans convention center. the mosaic of ideas, topics, and research stories displayed before me was impressive and a stark contrast to my usual research flow, which consists in exploring and ( sometimes ) getting a bit lost in the roots of a deep research paper reference tree. in this forest of posters, i felt immersed in the ai community, enthralled and slightly overwhelmed by the countless research minds and ideas present at my literal fingertips. as one of the biggest annual ai research conferences, neurips in some way mirrors the collective interest and brain - space of the ml community \u2019 s, displaying both its prominent and upcoming narratives \u2014 for instance", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", neurips in some way mirrors the collective interest and brain - space of the ml community \u2019 s, displaying both its prominent and upcoming narratives \u2014 for instance, as someone anticipated, i witnessed a large enthusiasm and contingent of work on generative models and llms. \u201d arthur mle max cembalest echoes this, stating that \u201c the biggest trend at the conference was an increased study of large language models, their robustness, their generalizability to out - of - domain text, and their generalizability to tasks that are not directly language but approachable by llms anyway. \u201d but llms were far from the", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- domain text, and their generalizability to tasks that are not directly language but approachable by llms anyway. \u201d but llms were far from the only interesting facet of ml research at neurips. valentine remembers feeling a \u201c strong energy around the design, development, and nurturing of scientifically sound and usable ml practices, with research outputs spanning from theory to implementation. \u201d \u201c there was also much work on model efficiency \u2014 how to reduce the computational requirements for deep learning systems, \u201d arthur mle teresa datta adds. \u201c beyond these ever - present areas of research, \u201d says teresa, \u201c there were two main", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "deep learning systems, \u201d arthur mle teresa datta adds. \u201c beyond these ever - present areas of research, \u201d says teresa, \u201c there were two main threads of messaging that struck a chord. the first : developing neural networks which don \u2019 t involve backpropagation. geoffrey hinton, inspired by the field of neuroscience \u2019 s lack of evidence that the brain \u2019 s cerebral cortex is able to undergo backpropagation, presented a keynote on a new learning procedure for neural networks that does not involve backpropagation. this forward - forward approach instead replaces a forward + backward pass with two forward passes \u2014 one with positive data", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "neural networks that does not involve backpropagation. this forward - forward approach instead replaces a forward + backward pass with two forward passes \u2014 one with positive data and one with negative data, meant to imitate the brain \u2019 s paradigm of wakefulness and sleep cycles. this is the latest work in the continuing attempts to establish deep learning models as a brain analogue. \u201d the second messaging thread teresa resonated with was \u201c more purposefully promoting cross - collaboration between ai and other fields ( psychologists, policymakers, domain experts, everyday users, neuroscientists, designers, and more ). the 2022 neurips keynote", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "fields ( psychologists, policymakers, domain experts, everyday users, neuroscientists, designers, and more ). the 2022 neurips keynotes highlighted a variety of figures at the intersection of ai and other fields : rediet abebe on perpetuated societal inequalities, juho kim on designing interaction - centric ai, alondra nelson and her work in the white house office of science and technology policy, and david chalmers on the philosophy of sentiency. while \u201c collaboration with other fields \u201d has always been evoked with high import, there were more discussions on how to formalize this", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "philosophy of sentiency. while \u201c collaboration with other fields \u201d has always been evoked with high import, there were more discussions on how to formalize this : how do we craft incentives for researchers to actually do this difficult and novel work? graduate students are often chained to publishing goals \u2014 getting a certain number of acceptances at high - profile venues. how do we create accolades, publishing forums, and funding support for interdisciplinary work? \u201d chief scientist john dickerson added that \u201c the use of modern ml ( e. g., transformer - based models ) for \u201c traditional \u201d application areas in operations research such as logistics, planning,", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the use of modern ml ( e. g., transformer - based models ) for \u201c traditional \u201d application areas in operations research such as logistics, planning, routing, assignment, scheduling, and resource allocation has also been increasingly present at ml conferences over the last year or two, and certainly at this recent neurips. until recently, these \u201c old \u201d application areas \u2014 that also happen to drive much of the world \u2019 s economy \u2014 were viewed as boring and solved by the machine learning world, left to the business analysts and consultants in the informs professional community. yet, with a touch of domain expertise, modern ml and optimization can be", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning world, left to the business analysts and consultants in the informs professional community. yet, with a touch of domain expertise, modern ml and optimization can be shown to eke out significant gains in efficiency and profit driven in these proven business problems where each percentage point corresponds to hundreds of millions or billions of dollars of economic value. i \u2019 m excited to see the continued strengthening of ties between the ai / ml and operations research communities and the problems they tackle ( i. e., those with both a prediction and a decisioning element ). \u201d ( separately, joint with informs, the acm, and ccc, we \u2019 re", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", those with both a prediction and a decisioning element ). \u201d ( separately, joint with informs, the acm, and ccc, we \u2019 re co - organizers of a series of workshops in this space, e. g. [ 1 ] and [ 2 ]. get in touch if you \u2019 d like to participate! ) distribution shifts & benchmarksvalentine gave the first public presentation of her work, joint with arthur research fellow naveen durvasula, on explainability and data drift at the workshop on distribution shifts ( distshift ). on the same day as afcp ( more on afcp below", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "on explainability and data drift at the workshop on distribution shifts ( distshift ). on the same day as afcp ( more on afcp below ), distshift attracted a larger crowd \u2014 generalization, extrapolation, and robustness to distribution shift are core ml problems, and it \u2019 s great to see continuing progress in this fundamental area. it was cool to see valentine \u2019 s work, which ties together clustering, shapley values, and skope rules to find emergent clusters of \u201c drifty \u201d points over time, as part of a larger cohort of explainability and data drift research", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and skope rules to find emergent clusters of \u201c drifty \u201d points over time, as part of a larger cohort of explainability and data drift research. ( we \u2019 ll be submitting a full version of this work to one of the january \u2018 23 conference deadlines, so stay tuned! ) valentine d'hauteville presents her poster about characterizing anomalies via explainable classifiers. also in the space of model performance under distribution shift, arthur - mle - turned - berkeley - phd - student jessica dai and arthur research fellow michelle bao presented at the women in machine learning ( wiml ) workshop on their ongoing", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##e - turned - berkeley - phd - student jessica dai and arthur research fellow michelle bao presented at the women in machine learning ( wiml ) workshop on their ongoing work with our team understanding models \u2019 the impact of covariate and concept drift on group fairness, when ground truth labels are not available at test time. valentine was particularly impressed by isabelle guyon \u2019 s keynote, the data - centric era : how ml is becoming an experimental science. she writes, \u201c her talk reminded us that, as a scientific endeavor, ml research should abide by the same rigorous scientific research standards as those that govern research in other disciplines such as the", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "reminded us that, as a scientific endeavor, ml research should abide by the same rigorous scientific research standards as those that govern research in other disciplines such as the natural sciences. guyon debunked some bad scientific practices within the ml research community, such as a common one which consists in selecting validation datasets based on their anticipated or observed ability to display the behaviors that will confirm a hypothesis ( a form of selection and confirmation bias ). to combat such practices, guyon emphasized the importance of adopting scientifically and statistically sound data curation, experimentation, and validation procedures. for instance, one should ensure that published experiments and", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##on emphasized the importance of adopting scientifically and statistically sound data curation, experimentation, and validation procedures. for instance, one should ensure that published experiments and findings are reproducible and carefully documented. she also advocated for the adoption of more rigorous vetting processes on existing datasets as well as an increased focus towards developing and documenting more comprehensive benchmarks. in fact, her prescriptions seemed to echo those of the neurips community at large, as the conference recently created a new datasets & benchmarks research track which rewards datasets and benchmark papers on an equal footing with other traditional research content. \u201d human", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "created a new datasets & benchmarks research track which rewards datasets and benchmark papers on an equal footing with other traditional research content. \u201d human - in - the - loop mlarthur mle daniel nissani, while researching the tensions paper ( more detail on this paper below ), became enamored with the idea that ai systems should somehow encapsulate knowledge about the context of their deployment. \u201c i was happy to see that i wasn \u2019 t alone, \u201d he writes. \u201c at the hill workshop, cynthia rudin did an excellent job explaining how users of ai, those who don \u2019 t necessarily have", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "alone, \u201d he writes. \u201c at the hill workshop, cynthia rudin did an excellent job explaining how users of ai, those who don \u2019 t necessarily have ai skills, but want to benefit from ai systems, have opinions past \u201c which models have the best accuracy. \u201d drawing from her research on rashomon sets and sparse decision trees, she asked for a paradigm shift for how we generate models. instead of asking a user to accept one, heavily optimized model, she wants us to present users with multiple models that achieve similar accuracy scores. she has developed a ui for such discovery processes when searching through different decision trees. \u201d", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "she wants us to present users with multiple models that achieve similar accuracy scores. she has developed a ui for such discovery processes when searching through different decision trees. \u201d \u201c while listening to cynthia \u2019 s talk, a poster caught my eye on participatory systems, which presented an equally inventive idea, \u201d daniel continues. \u201c this paper devises a model agnostic scheme that trains various models on different sets of features and protected attributes. this allows users to understand the effect of providing or omitting certain types of information, such as a medical status or gender. both of these ideas present ways for ai systems to interact with the", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of providing or omitting certain types of information, such as a medical status or gender. both of these ideas present ways for ai systems to interact with the context of their deployment, allowing for feedback between the system and the user. \u201d valentine noticed the same theme. she writes, \u201c at neurips, user - centric ml pipelines were also in strong focus. for instance, a couple of presentations i attended intelligently incorporated user feedback as key steps in the design of comprehensive ml solutions. one researcher presented a new explainability pipeline for self - driving cars but had first run user field studies in order to understand the nature of", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of comprehensive ml solutions. one researcher presented a new explainability pipeline for self - driving cars but had first run user field studies in order to understand the nature of what makes a good explanation for his specific use case. he also resorted to user feedback to comprehensively validate his first design iterations and presented ways to incorporate the feedback into his future designs. cynthia rudin, a prominent scholar in the field of explainability presented a clean mathematical proof showing that, contrary to the popular conception, designing complicated and high - capacity models is necessary to obtain peak performance ; it is often possible to compute simple, inherently explainable yet suitably perform", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "popular conception, designing complicated and high - capacity models is necessary to obtain peak performance ; it is often possible to compute simple, inherently explainable yet suitably performant models for a given task. her approach echoed occam's razor ( \u201c the simplest solution is almost always the best \u201d ) and followed the keynote talk \u2019 s footsteps in anchoring ml back to core scientific principles. rudin also created a clean and innovative ui which allows domain experts to explore and understand a set of generated simple models before selecting a one that is most suitable to their use case. \u201d fairness & related topics \u201c i really enjoyed the algorithmic fairness through", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "set of generated simple models before selecting a one that is most suitable to their use case. \u201d fairness & related topics \u201c i really enjoyed the algorithmic fairness through the lens of causality and privacy ( afcp ) workshop, a semi - annual gathering that focuses on the nuances of connecting responsible ai to practice, says arthur \u2019 s chief scientist john dickerson. \u201c this is one of the few but growing communities in \u201c core \u201d machine learning that gives more than lip service to human - centered ai, contextual machine learning, and sociotechnical systems ( sts ). \u201d arthur mle teresa datta presented her co - lead -", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "human - centered ai, contextual machine learning, and sociotechnical systems ( sts ). \u201d arthur mle teresa datta presented her co - lead - author paper, tensions between the proxies of human values in ai, as an oral at afcp. the paper was also accepted to hcai at neurips as well as satml. check out arthur mle and co - lead author daniel nissani \u2019 s blog post on that work here, and teresa \u2019 s talk below. daniel found the talks on causality to be particularly interesting. \u201c causality has been in the fairness literature for quite a while now,", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 s talk below. daniel found the talks on causality to be particularly interesting. \u201c causality has been in the fairness literature for quite a while now, but one of the biggest bottlenecks is making sure you have a causal model ( distilled as a causal graph ) that can be used for causal analysis, \u201d he writes. \u201c i was pleasantly surprised to see work directly in this space, where some authors ran experiments to see if causal discovery methods could actually create causal graphs that are effective enough to measure fairness notions. although their results were promising, the authors plan to construct a causal discovery method specific to fairness notions. if", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "causal graphs that are effective enough to measure fairness notions. although their results were promising, the authors plan to construct a causal discovery method specific to fairness notions. if successful, this could create otherwise theoretical works, such as another paper at afcp describing post - treatment bias in causal fairness analyses, more impactful for real world systems. \u201d \u201c the final highlight for me, \u201d he adds, \u201c was a roundtable discussion at afcp, where many researchers, whether from the privacy or fairness space, acknowledged the need for more contextual understanding in our research. emphasis to start researching entire ml systems, eliciting user feedback, and integrating", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or fairness space, acknowledged the need for more contextual understanding in our research. emphasis to start researching entire ml systems, eliciting user feedback, and integrating context into our research were the biggest takeaways. it made me feel proud that our team at arthur presented the tensions paper at afcp, since it seems our ideas were not only heard, but preaching to an active choir that wants to start integrating context as well. \u201d \u201c the afcp workshop gave critical takes on interpretability and explainability in ml, and also touched on the intersection and interactions between forms of privacy and fairness, as well as causality and fairness, \u201d", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "interpretability and explainability in ml, and also touched on the intersection and interactions between forms of privacy and fairness, as well as causality and fairness, \u201d says john. \u201c as in the ai / ml meets or discussion above, we \u2019 re seeing the intersection between traditionally separate areas of focus \u2014 statistics, economics, human - computer interaction, machine learning, and others. as productionized ml continues to expand across the economy and our society, these intersections are inevitable and welcome, and i \u2019 m happy to see thoughtful workshops like afcp continue to grow in lockstep. \u201d valentine reflected profoundly on this topic as well, noting that", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and i \u2019 m happy to see thoughtful workshops like afcp continue to grow in lockstep. \u201d valentine reflected profoundly on this topic as well, noting that \u201c it can often be tempting as ml researchers to think of ourselves as scientists working on objective and universal algorithms that can then be adapted and tailored to fit specific use cases. such conceptions in some sense make us the principal bearer of truth to the detriment of domain experts, and can lead us astray. during an explainability panel i attended, zach lipton pointed out that we might for instance benefit from letting domain experts be the ones to first scope out desired design for", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "an explainability panel i attended, zach lipton pointed out that we might for instance benefit from letting domain experts be the ones to first scope out desired design for an ml system and associated explainability mechanisms before resorting to ml engineers to implement or iterate on it. such responsibility delegation could ensure designs and solutions are inherently more usable and useful. furthermore, one must remember that there is no such thing as scientific objectivity and that science and ml at large is value - laden \u2014 one of the key calls to action in teresa \u2019 s tensions paper and awesome presentation. forgetting or ignoring this reality can lead us to resort to deeply insufficient solutions based", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "one of the key calls to action in teresa \u2019 s tensions paper and awesome presentation. forgetting or ignoring this reality can lead us to resort to deeply insufficient solutions based on mathematical formulations in our attempt to address problems that are socio - technical in nature, such as fairness. \u201d conclusionwe arrived back in new york last week feeling full ( of knowledge, but also of cajun food and beignets ). and, as is the case with many academic conferences, we were left with equal amounts of questions and answers. here are just some of the questions we \u2019 re looking forward to exploring further in 2023 : how can model interpretation and", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with equal amounts of questions and answers. here are just some of the questions we \u2019 re looking forward to exploring further in 2023 : how can model interpretation and explanation be aligned with context, audience, and sensible baselines? how is the geometry of information informing model design and analysis? how do we create accolades, publishing forums, and funding support for interdisciplinary work? how can we engage communities via interactive ai / ml systems, so that we enable consent, choice, and trust? how do we start doing research about the system that models are deployed in, rather than just the model itself? are we starting to approach the idea that approximate causal", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "how do we start doing research about the system that models are deployed in, rather than just the model itself? are we starting to approach the idea that approximate causal models are enough for real world causal analyses? want to learn more about our mles and our commitment to research and development? check out the r & d page of our website. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteam", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / team - arthur - at - neurips - 22 - a - retrospective", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective", "row": 185, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 186 text : how we are modeling our human values in technology is inherently flawed arthur solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml researchhow we are modeling our human values in technology is inherently flawedby : daniel nissanidecember 8, 2022thank you to the original authors of tensions between the proxies of human values in ai : teresa datta, daniel nissani, max ce", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "2022thank you to the original authors of tensions between the proxies of human values in ai : teresa datta, daniel nissani, max cembalest, akash khanna, haley massa, and john dickerson. introwhen creating automated machine learning systems, our belief is that these algorithms should not perpetuate and amplify harms. we want these algorithms to be fair to everyone, regardless of their gender, race, ethnicity, sexuality, etc. our information should only be used and distributed with our consent. inevitably, these algorithms will not work as expected, and we deserve an explanation as", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "sexuality, etc. our information should only be used and distributed with our consent. inevitably, these algorithms will not work as expected, and we deserve an explanation as to why they are not working as they should. we can encapsulate these human values as pillars of privacy, fairness, and explainability. over the past generation, a lot of work has focused on embedding these values into our technologies. however, as we try to proxy these pillars with technical definitions and algorithmic designs, tensions keep getting discovered within the pillars, between the pillars, and with the real world contexts in which the pillars \u2019 proxies are deployed", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "algorithmic designs, tensions keep getting discovered within the pillars, between the pillars, and with the real world contexts in which the pillars \u2019 proxies are deployed. so we must ask : why do we continue to face limiting tensions? tensions within pillarsit is well known in the ai community that popular definitions of fairness, such as demographic parity, equalized odds, and calibration, cannot be implemented in the same machine learning model [ 1 ]. moreover, even when implemented correctly, these fairness definitions may cause more harm than good over time, given the feedback loop an algorithm might create [ 2 ]. privacy discussions on the other", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "implemented correctly, these fairness definitions may cause more harm than good over time, given the feedback loop an algorithm might create [ 2 ]. privacy discussions on the other hand are largely dominated by the notion of differential privacy [ 3 ] [ 4 ], a probabilistic guarantee that one cannot realize that a data point has been removed or replaced in a dataset based on some mechanism. however, differential privacy has been shown to work poorly on outliers [ 5 ] [ 6 ] or on models that have overfit to their training data [ 7 ]. explainability has come into question now that black - box and hard - to", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "] or on models that have overfit to their training data [ 7 ]. explainability has come into question now that black - box and hard - to - interpret models have gained popularity. however, the methods to produce explanations are generally just local approximations of models [ 8 ]. some critics of the field say it is akin to reading tea leaves [ 9 ]. tensions between pillarsa summary of the tensions identified within and between popular value proxies. incorporating any one of these pillars is a challenge, and incorporating multiple requires handling competing priorities. much like how popular notions of fairness cannot be implemented in the same model [ 1 ]", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of these pillars is a challenge, and incorporating multiple requires handling competing priorities. much like how popular notions of fairness cannot be implemented in the same model [ 1 ], differential privacy and any popular fairness definition cannot be imposed on the same model [ 10 ]. in other words, the impossibility of having different notions of fairness in machine learning extends to having any notion of fairness in tandem with differential privacy. the idea of models being more transparent may be in direct competition with the idea that our models are more private. research has shown that popular explanation techniques, and even new ones such as counterfactual explanations, can make models susceptible to", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that our models are more private. research has shown that popular explanation techniques, and even new ones such as counterfactual explanations, can make models susceptible to membership inference attacks [ 11 ]. explainability should be a useful tool for identifying unfairness, but is not, and can at times hide the unfairness of a model [ 12 ]. moreover, the converse can happen as well, where explanations of a model can actually amplify unfairness against certain subpopulations [ 13 ]. tensions in the real world : a call for context - aware machine learningthe 2020 census used differential privacy for the first time to meet the", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s [ 13 ]. tensions in the real world : a call for context - aware machine learningthe 2020 census used differential privacy for the first time to meet the privacy impact assessment \u2019 s requirements. by doing so, reported population counts fluctuated, which protected the privacy of individuals, but potentially lessened the funding small, rural populations would receive from the federal government. as an example, native american reservations of less than 5, 000 people saw decreases in population numbers by 34 % on average. this type of error ( which is an inherent feature in differential privacy ) could result in the loss of funding for a road to a nearby town or a", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "average. this type of error ( which is an inherent feature in differential privacy ) could result in the loss of funding for a road to a nearby town or a new school [ 14 ]. in 2021, the markup found that people of color were denied loans 40 - 80 % more often than white counterparts with similar financial profiles [ 15 ]. however, they were heavily criticized by \u201c [ t ] he american bankers association, the mortgage bankers association, the community home lenders association, and the credit union national association... saying the public data is not complete enough to draw conclusions, but did not point to any flaws in our computation", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", and the credit union national association... saying the public data is not complete enough to draw conclusions, but did not point to any flaws in our computations. \u201d the \u201c incompleteness \u201d of the data is because the home mortgage disclosure act requires \u201c debts as a percentage of income, how much of the property \u2019 s assessed worth the person is asking to borrow, \u201d but not \u201c the applicant \u2019 s credit score \u201d because of fears of re - identification attacks. both of these examples showcase how utilizing formulations of our human values without acknowledging the context can lead to dire consequences. in the case of the census, although differential privacy", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of these examples showcase how utilizing formulations of our human values without acknowledging the context can lead to dire consequences. in the case of the census, although differential privacy offers a guarantee of a certain kind of privacy, it adversely affects certain populations that need privacy and more equitable funding. as a technical notion, differential privacy has no way of knowing the context it is being implemented in. and without any critical structures in place, it can run awry, causing consequences that we outlined above. similarly, although not necessarily in an algorithmic sense, the notion of auditing the mortgage system requires transparency about the data being used to give out mortgage", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "above. similarly, although not necessarily in an algorithmic sense, the notion of auditing the mortgage system requires transparency about the data being used to give out mortgages. thus, decisions need to be made with contextual knowledge, so that specific entities can have access to the required information. in bioethics and related ethical fields that are more mature than responsible ai, context is incredibly important. doctors have access to different information depending on their physical context, such as whether they are in a hospital or in their car on the way to the hospital. the ethical concerns around the collection of biometric information are affected by the specific device being used", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in a hospital or in their car on the way to the hospital. the ethical concerns around the collection of biometric information are affected by the specific device being used. context informs how information flows, what information is collected and used, and why certain decisions are made. if we hope to have more ethical machine learning systems, the incorporation of context could be a viable avenue [ 16 ]. how can we address these issues? in order to alleviate the issues we \u2019 ve described, we believe that the whole system should be considered when designing machine learning solutions. the techniques ( and laws ) described primarily deal with the model : differential privacy inhibits extraction", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "believe that the whole system should be considered when designing machine learning solutions. the techniques ( and laws ) described primarily deal with the model : differential privacy inhibits extraction attacks on models, fairness definitions constrain models to output more fair predictions, and explainability techniques are used to explain model outputs. rather than just designing solutions for the models, we can look at the entire system the model is deployed into and determine the values most appropriate to embed and consider their associated consequences. although we don \u2019 t want to prescribe any solutions to these problems, because this is a nascent research area, there are some theories that we can look to", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we don \u2019 t want to prescribe any solutions to these problems, because this is a nascent research area, there are some theories that we can look to for inspiration. contextual integrity for privacy [ 17 ] is a way to encode the context of a situation, allowing us to understand the privacy requirements of technology. substantive algorithmic fairness [ 18 ] asks us to analyze the structural inequalities present, identify the reforms that could mitigate such inequalities, and consider whether an algorithmic intervention could achieve such a mitigation. for explainability, designing around the needs of a situation and stakeholder \u2019 s", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", and consider whether an algorithmic intervention could achieve such a mitigation. for explainability, designing around the needs of a situation and stakeholder \u2019 s understanding of transparency can lead to better techniques [ 19 ]. as we said above, this is not a solved problem. if anything, the idea of incorporating context in automated machine learning systems is itself very new. thus, we want to leave the reader with questions we are interested in researching : how should information be collected by a contextual system? collection of data is a hot button issue, so we have to collect data with intent. leaning on contextual integrity for privacy, we", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a contextual system? collection of data is a hot button issue, so we have to collect data with intent. leaning on contextual integrity for privacy, we can get inspiration for how we can define context as a set of parameters to collect data. what types of tools need to be developed? building out frameworks, evaluation suites, and more will be helpful, but we should consider what we need to build to make these systems effective and ethical. how should machine learning systems respond to context? splitting this up, we should consider what triggers a response to context and how the user should feel the response to context. what aspects of ethical responsibility", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to context? splitting this up, we should consider what triggers a response to context and how the user should feel the response to context. what aspects of ethical responsibility does each stakeholder carry? the creator of the technology, the person implementing the technology, and the person making decisions with this technology each have a different ethical role to play. how can we design inclusively? we can lean on participatory design principles to help us build these systems for everyone impacted. conclusiontechnology is inherently value - laden and political [ 20 ] [ 21 ]. it can distribute information in specific ways, thus influencing how we make decisions. moreover,", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##nology is inherently value - laden and political [ 20 ] [ 21 ]. it can distribute information in specific ways, thus influencing how we make decisions. moreover, although technology has the potential to help those in the most need, those who need the most are regularly ignored in the process of designing popular technologies. arthur \u2019 s research team believes that context - aware systems, those able to incorporate knowledge about the specific domain that a machine learning model is situated in, are a potential path to solve some of the issues above, as well as evaluate the consequences of such a system before deployment. context awareness is a hard problem because it most likely will involve", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "solve some of the issues above, as well as evaluate the consequences of such a system before deployment. context awareness is a hard problem because it most likely will involve collecting new information about a specific deployment at some point in the model building or productionalization process. citation linksbelow is a list of links to the citations in this blog post. note that our paper goes much deeper and has a more proper citation tree. https : / / arxiv. org / abs / 1609. 05807https : / / arxiv. org / abs / 1803. 04383https : / / people.", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "1609. 05807https : / / arxiv. org / abs / 1803. 04383https : / / people. csail. mit. edu / asmith / ps / sensitivity - tcc - final. pdfhttps : / / www. iacr. org / archive / eurocrypt2006 / 40040493 / 40040493. pdfhttps : / / arxiv. org / abs / 1507. 06763https : / / arxiv. org / abs / 1910. 13427https", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "org / abs / 1507. 06763https : / / arxiv. org / abs / 1910. 13427https : / / arxiv. org / abs / 1709. 01604https : / / arxiv. org / abs / 2206. 01254https : / / docs. google. com / presentation / d / 1bpue2ed3niyhylm _ d9njavwgegyxgpoeojcnsav7ccs / edit # slide = id. phttps : / / crcs", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "d9njavwgegyxgpoeojcnsav7ccs / edit # slide = id. phttps : / / crcs. seas. harvard. edu / files / crcs / files / ai4sg - 21 _ paper _ 23. pdfhttps : / / arxiv. org / abs / 1907. 00164https : / / arxiv. org / abs / 2205. 03295https : / / arxiv. org / pdf / 2106. 13346. pdfhttps : / / www. nytimes", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##https : / / arxiv. org / pdf / 2106. 13346. pdfhttps : / / www. nytimes. com / interactive / 2020 / 02 / 06 / opinion / census - algorithm - privacy. htmlhttps : / / themarkup. org / denied / 2021 / 08 / 25 / the - secret - bias - hidden - in - mortgage - approval - algorithmshttps : / / www. nature. com / articles / s41746 - 018 - 0075 - 8https : / / scholarlypublishingcollective. org", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". com / articles / s41746 - 018 - 0075 - 8https : / / scholarlypublishingcollective. org / psup / information - policy / article / doi / 10. 5325 / jinfopoli. 1. 2011. 0149 / 314319 / privacy - in - context - technology - policy - and - thehttps : / / arxiv. org / abs / 2107. 04642https : / / arxiv. org / abs / 2101. 09824https : / / web. cs.", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##642https : / / arxiv. org / abs / 2101. 09824https : / / web. cs. ucdavis. edu / ~ rogaway / papers / moral - fn. pdfhttps : / / arxiv. org / abs / 1811. 03435previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentation", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / how - we - are - modeling - our - human - values - in - technology - is - inherently - flawed", "metadata": {"source": "https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed", "row": 186, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 187 text : how ai is reshaping the future of these 4 industries arthur solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai monitoring & performancehow ai is reshaping the future of these 4 industriesby : christina sirabellanovember 14, 2022in nearly every industry throughout the last few years, the massive impacts of ai and machine learning are undeniable. organizations are gaining compete", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##mber 14, 2022in nearly every industry throughout the last few years, the massive impacts of ai and machine learning are undeniable. organizations are gaining competency in ai / ml and deploying it to achieve goals such as cost reduction, bias mitigation, and so much more. they are also recognizing that mastering ai is a make - or - break for companies that want to be around in the next decade \u2014 in fact, a whopping 94 % of business leaders in a recent survey said that ai is critical to success. as our ceo adam wenchel spoke about at last year \u2019 s ai summit, the enterprises that become", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in a recent survey said that ai is critical to success. as our ceo adam wenchel spoke about at last year \u2019 s ai summit, the enterprises that become ai - native first will be industry leaders for decades. in this blog, we \u2019 ll share four industries where machine learning has had a particularly large influence, as well as how the leading organizations within those industries are using it \u2014 and more specifically, model monitoring \u2014 to stay ahead of the curve. financial serviceseven in traditionally conservative spaces like financial services, ai is rapidly changing the business landscape. while there is no enforceable federal ai legislation in the u. s. currently,", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "traditionally conservative spaces like financial services, ai is rapidly changing the business landscape. while there is no enforceable federal ai legislation in the u. s. currently, there is a growing momentum to regulate the financial services industry around biased algorithms and govern black box underwriting by various agencies. financial services organizations are using ai and machine learning for activities like credit approvals, fraud detection, and customer support. but what happens when bias creeps into an algorithmic model which results in the wrong lending decision being made, impacting millions of customers? companies must consider the potential business and compliance risks of these technologies. leading financial institutions are using arthur to monitor", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "lending decision being made, impacting millions of customers? companies must consider the potential business and compliance risks of these technologies. leading financial institutions are using arthur to monitor, measure, and improve machine learning models for better results across top industry use cases : fraud / kyc, forecasting models, fair lending, robo - advisory programming, credit worthiness, customer service, and more. learn more by downloading our financial services whitepaper or watching our on - demand webinar. insuranceno matter how the economy evolves in the coming years, individuals and companies will continue to need insurance. the growing reality of company layoffs and downs", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". insuranceno matter how the economy evolves in the coming years, individuals and companies will continue to need insurance. the growing reality of company layoffs and downsized industries may shrink average coverage amounts, but consumers and businesses trust carriers to bring them peace of mind when faced with economic market uncertainty and climate change volatility. while the insurance industry has typically been a late adopter of technology, that isn \u2019 t true with ai \u2014 insurance companies are applying cv and nlp technologies across the value chain to improve their own pain points while simultaneously benefiting the customer. according to deloitte \u2019 s 2022 insurance industry outlook report, almost", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "across the value chain to improve their own pain points while simultaneously benefiting the customer. according to deloitte \u2019 s 2022 insurance industry outlook report, almost 74 % of global respondents said they planned to increase spending on ai - related technologies. ai helps insurers assess risk, detect fraud, and reduce human error in the application process. it also helps customers, who benefit from the streamlined service and claims processing that ai provides. specific use cases include underwriting, premium forecasting, pricing strategy, and customer servicing. with arthur, companies are proactively mitigating reputational, regulatory, and strategic / financial risk while saving", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ing, pricing strategy, and customer servicing. with arthur, companies are proactively mitigating reputational, regulatory, and strategic / financial risk while saving money and driving business goals. explore industry use cases by downloading our insurance whitepaper. healthcarefrom medical imaging analysis to disease prediction to drug discovery and development, ai has already revolutionized the healthcare industry from a technology perspective. another piece of this puzzle, however, is ensuring health equity \u2014 particularly as it pertains to levels of care across underrepresented and minority groups. additional healthcare use cases include hospital management, predictive insights for patient outcomes, capacity planning,", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s to levels of care across underrepresented and minority groups. additional healthcare use cases include hospital management, predictive insights for patient outcomes, capacity planning, staff training, medical diagnosis bias detection & mitigation, and medical document nlp classification. arthur helps healthcare organizations avert harmful patient outcomes and reduce operational risk through proactive mlops monitoring, resulting in early detection of data anomalies and model errors. one of our first customers and the leading ai - enabled healthcare enterprise, humana, is deploying arthur to manage mission - critical ai across both clinical and membership use cases. arthur seamlessly integrates into humana \u2019", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "enterprise, humana, is deploying arthur to manage mission - critical ai across both clinical and membership use cases. arthur seamlessly integrates into humana \u2019 s ai tech stack and is a core component of humana \u2019 s approach to responsible and high - performing ai, providing a continuous view into model performance and bias, governance support, and alerting capabilities. discover how ai drives business impact by downloading our humana case study. human resourcesthe global transition from centralized office workplaces to regular work - from - home arrangements accelerated the adoption of automated ai tools to make hr departments run more efficiently. these tools are being used in areas", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "workplaces to regular work - from - home arrangements accelerated the adoption of automated ai tools to make hr departments run more efficiently. these tools are being used in areas like talent acquisition, hiring, performance management, and employee experience. in fact, 99 % of fortune 500 companies rely on the aid of talent - sifting software and 55 % of human resources leaders in the u. s. use predictive algorithms to support hiring. 1while ai technology yields significant operational benefits, it also introduces risk \u2014 and the challenge is balancing the two. any company using ai systems that analyze protected, special categories or sensitive personal datasets ( age, race", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "introduces risk \u2014 and the challenge is balancing the two. any company using ai systems that analyze protected, special categories or sensitive personal datasets ( age, race, gender, ethnicity, etc. ) needs to exercise caution and ensure the data and / or algorithms being used are not causing systemic issues of bias and inequity resulting in disparate impact or discrimination. and this is no longer just an issue of morality : starting in january 2023, companies in new york city will actually be legally restricted from using employment decision tools unless they have been the subject of an independent bias audit, in what is likely just the first of many", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "city will actually be legally restricted from using employment decision tools unless they have been the subject of an independent bias audit, in what is likely just the first of many similar laws that will be passed throughout the country and beyond. deepen your knowledge by downloading our human resources whitepaper. interested to see how arthur can help your organization bring high - performing ai into production safely and responsibly? schedule a demo of our platform. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcv", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / how - ai - is - reshaping - the - future - of - these - 4 - industries", "metadata": {"source": "https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries", "row": 187, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 188 text : shapley residuals : measuring the limitations of shapley values for explainability arthur solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##firewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml explainabilityshapley residuals : measuring the limitations of shapley values for explainabilityby : max cembalestoctober 31, 2022this post was originally published in towards data science. we will use a cube", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "explainabilityby : max cembalestoctober 31, 2022this post was originally published in towards data science. we will use a cube representation of games to walk through the interpretation and limitations of shapley values. introductionto use machine learning responsibly, you should try to explain what drives your ml model \u2019 s predictions. many data scientists and machine learning companies are recognizing how important it is to be able to explain, feature - by - feature, how a model is reacting to the inputs it is given. this article will show how shapley values, one of the most common explainability techniques,", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", how a model is reacting to the inputs it is given. this article will show how shapley values, one of the most common explainability techniques, can miss important information when explaining a model. then, we will introduce shapley residuals, a new technique to measure how well shapley values are capturing model behavior, along with some code to get started calculating them! consider the following example from christopher molnar \u2019 s interpretable machine learning book : a bike - sharing company trains a model to predict the number of bikes taken out on a given day, using features like seasonal info, the day of the week,", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "bike - sharing company trains a model to predict the number of bikes taken out on a given day, using features like seasonal info, the day of the week, weather info, etc. then, if their model is predicting a lower - than - average rider count on some day in the future, they can find out why that lower - than - average score is occurring : by looking at how the model is reacting to each feature. was it because of a holiday? was it because of the weather? a common way of computing the importance of each of your model \u2019 s features is to use shapley values, since it is a method", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of the weather? a common way of computing the importance of each of your model \u2019 s features is to use shapley values, since it is a method that is 1 ) widely applicable to many problems, 2 ) based on solid theoretical grounding, and 3 ) easily implementable with the shap python library. the problem : in some scenarios, shapley values fail to express information about model behavior, because it is only returning a score for one feature at a time. for instance, in the bike - sharing scenario, we are treating the weather and the day of the week as independent features, but sometimes it is the combination", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". for instance, in the bike - sharing scenario, we are treating the weather and the day of the week as independent features, but sometimes it is the combination of those features that matters ; and in those scenarios of feature combinations being more important than the individual features themselves, shapley values can fail to properly explain a model. bar trivia examplelet \u2019 s use a simpler setting with fewer features to walk through the problem with shapley values in more detail. i like to attend trivia nights at some local bars in the neighborhood with different coworkers of mine each week. it \u2019 s become quite clear that some members of", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to attend trivia nights at some local bars in the neighborhood with different coworkers of mine each week. it \u2019 s become quite clear that some members of our team bring more to the table than others. can we quantify the impact each team member has on the trivia performance? we can use shapley values for each player with the following interpretation : they should correspond to the expected change in score when adding that player to the trivia team. other possible interpretations exist *, but we will use this one. ( * note : this class of methods to compute shapley values, called \u201c interventional \u201d shapley", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "*, but we will use this one. ( * note : this class of methods to compute shapley values, called \u201c interventional \u201d shapley values, measure \u201c expected change in score when adding this feature. \u201d a different type is known as \u201c conditional \u201d shapley values. the key difference between the interventional method and the conditional method lies in how they treat a feature whose expected change in score is zero \u2014 what should its shapley value be? zero? if you think the answer is \u201c yes, \u201d use the interventional method. if instead, you think the feature might still have importance due to correlation", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "zero? if you think the answer is \u201c yes, \u201d use the interventional method. if instead, you think the feature might still have importance due to correlations, and if you think that importance should be included in its shapley value, then consider using the conditional method. ) geometrically, a useful way to plot all these 3 - player game scores with different teams is as points on a cube, arranged so that neighboring points differ by just one player. then, the paths between points ( a. k. a. the cube \u2019 s edges ) will represent the change in score when adding a player to a team. (", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the paths between points ( a. k. a. the cube \u2019 s edges ) will represent the change in score when adding a player to a team. ( note : with two players, we would plot this as a square. with four or more players, we would have to plot this as a hypercube ) let \u2019 s call this shape a gamecube ; this will be a useful shape for us because both shapley values and gamecube edges will correspond to the change in score when adding a player. figure 1 : plotting each trivia score on a different vertex of a cube corresponding to the players present on the", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to the change in score when adding a player. figure 1 : plotting each trivia score on a different vertex of a cube corresponding to the players present on the team that night. in our story, reid is only knowledgeable about sports trivia, and gw knows about movies, music, history, geography, literature \u2014 pretty much everything except sports trivia. so when reid plays, he improves the score by a little ; when gw plays, she increases the score by a lot. and me, well, i \u2019 m mostly there for the beer and the company. a shapley value is a perfect measure of explainability", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a lot. and me, well, i \u2019 m mostly there for the beer and the company. a shapley value is a perfect measure of explainability only when a player always contributes the same amount to a team \u2019 s score. and since each player \u2019 s change on the score is constant in our story so far, we can assign a shapley value of 1 to reid, a shapley value of 9 to gw, and a shapley value of 0 to max. these shapley values represent the expected change in score when each player joins the team! figure 2 : viewing the change in team scores", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of 0 to max. these shapley values represent the expected change in score when each player joins the team! figure 2 : viewing the change in team scores when adding each player. in more technical terms, a game where each player \u2019 s impact is consistent ( like our story so far ) is called an \u201c inessential game. \u201d also, we will use the symbol [UNK] to represent the \u201c gradient \u201d of a gamecube v, which computes the values along the edges between the values on the vertices, and we will use [UNK] _ player _ v to represent the edge values for a specific player \u2019 s directions and zero", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the edges between the values on the vertices, and we will use [UNK] _ player _ v to represent the edge values for a specific player \u2019 s directions and zero along all other edges. for example, the gamecube gradient [UNK] _ reid _ \u03bd represents all possible changes in score when adding reid. figure 3 : expressing the change in scores when adding a player as the partial gradient of the gamecube with respect to each player. feature contributions can \u2019 t always be expressed as a single number \u2014 so shapley values aren \u2019 t enough. you should expect that most of the time, the features you are working with won \u2019 t", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a single number \u2014 so shapley values aren \u2019 t enough. you should expect that most of the time, the features you are working with won \u2019 t have constant impacts on model outputs \u2014 instead, the impact of a feature typically depends on what the other features are. let \u2019 s change up our story. suppose that max \u2019 s behavior changes based on who he is playing with. when playing with gw, he is pretty chill, drinks his beer, minds his own business and lets gw do most of the work, so he doesn \u2019 t bring the score down. but when max plays with reid, he gets jealous of", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "business and lets gw do most of the work, so he doesn \u2019 t bring the score down. but when max plays with reid, he gets jealous of how much reid knows about sports, so max starts to speak up more, suggesting some wrong answers and bringing the score down by 1! figure 4 : the new gamecube with inconsistent player contributions. on this new gamecube, gw \u2019 s edges are constant, so her shapley value of 9 still corresponds exactly to the change in score when she plays. but max \u2019 s and reid \u2019 s edges are not constant, because their impact on score depends on who", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "corresponds exactly to the change in score when she plays. but max \u2019 s and reid \u2019 s edges are not constant, because their impact on score depends on who they are playing with. therefore, our way of using gamecube edges to quantify what max and reid bring to the table now has a problem. when real data scientists use shapley values, they solve this problem by taking the average contribution of a player to their teams \u2014 on the gamecube, this would mean quantifying a player \u2019 s contribution as the average edge values in their direction. so on our gamecube above, gw \u2019 s sha", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "would mean quantifying a player \u2019 s contribution as the average edge values in their direction. so on our gamecube above, gw \u2019 s shapley value would still be 9 as in before, but reid \u2019 s shapley value would now be 0. 5 and max \u2019 s shapley value would now be - 0. 5. for some use cases, the story ends there \u2014 a player \u2019 s average contribution can sometimes be a good enough quantification of their impact! however, this may cause a problem when it comes to trusting shapley values. because we can trust gw \u2019 s sha", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##tification of their impact! however, this may cause a problem when it comes to trusting shapley values. because we can trust gw \u2019 s shapley values more than we can trust max \u2019 s or reid \u2019 s shapley values, since there is more consistency in her contribution to the team than max \u2019 s or reid \u2019 s contributions. shapley residualsthe shapley residual is a measurement of how much a player \u2019 s edges deviate from being constant \u2014 lower shapley residuals mean shapley values are close to perfectly representative of feature contribution, whereas higher shapley residuals mean", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "from being constant \u2014 lower shapley residuals mean shapley values are close to perfectly representative of feature contribution, whereas higher shapley residuals mean shapley values are missing out on important model information : namely, that a feature \u2019 s contribution depends on the other features as well. the authors of the original shapley residuals paper formulate this missing information as an error term in a least - squares regression. for example, for the player reid : [UNK] _ reid _ \u03bd = [UNK] _ \u03bd _ reid + r _ reidthe left side of this equation is the same partial gradient as earlier. the right side of", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "_ reid _ \u03bd = [UNK] _ \u03bd _ reid + r _ reidthe left side of this equation is the same partial gradient as earlier. the right side of the equation is the sum of a new gamecube \u2019 s gradient, [UNK] _ \u03bd _ reid, plus a residual cube, r _ reid, which measures the amount that our game deviates from being inessential with respect to reid. figure 5 : the residual cube is the amount a game deviates from inessentiality with respect to a given player. the key idea is that, if reid has a consistent impact on the team, the residual cube r _ reid", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##essentiality with respect to a given player. the key idea is that, if reid has a consistent impact on the team, the residual cube r _ reid will be all zeros. on the other hand, if the values on the residual cube r _ reid deviate from zero, then that is a signal that reid \u2019 s shapley value is missing information about how reid \u2019 s impact depends on who else is playing with reid. the higher the values on the residual cube, the more reid \u2019 s contribution depends on which other players are present. code for calculating shapley residualsimportsgenerate synthetic datasett", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "cube, the more reid \u2019 s contribution depends on which other players are present. code for calculating shapley residualsimportsgenerate synthetic datasettrain model & kernelshap explainercompute expected values of feature coalitions this uses explainer. synth _ data, the set of the synthetic data samples generated by the shap library when the explainer is trained. the dictionary coalition _ estimated _ values maps feature coalitions to the expected value of the model when those features are used, relative to a baseline ( which is the expected value when no features are used : the average model output ). ( note that we", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "when those features are used, relative to a baseline ( which is the expected value when no features are used : the average model output ). ( note that we convert the lists to strings since lists are not hash - able types in python. ) progress checkcoalition _ estimated _ values should look something like this : {'[ ]': 0,'[ 0 ]': - 0. 3576234198270127,'[ 1 ]': 0. 010174318030605423,'[ 2 ]': - 0. 08009846972721", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 0. 010174318030605423,'[ 2 ]': - 0. 08009846972721224,'[ 0 1 ]': - 0. 34261386138613864,'[ 0 2 ]': - 0. 37104950495049505,'[ 1 2 ]': 0. 14435643564356437,'[ 0 1 2 ]': - 0. 396 } create hypercube objectwe are using 3 dimensional data so this will just be a cube. but this", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "1 2 ]': - 0. 396 } create hypercube objectwe are using 3 dimensional data so this will just be a cube. but this method extends to hypercubes, growing slower as the number of dimensions increases. feel free to use the code for the hypercube python class in the appendix for this article, or to write your own. it needs to place the coalition _ estimated _ values on the vertices of the cube, and it needs to compute the edge values as the difference between neighboring vertex values. compute the shapley residualsfor each feature, minimize [UNK] _ feature _ cube \u2014 [UNK] _ cube", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the edge values as the difference between neighboring vertex values. compute the shapley residualsfor each feature, minimize [UNK] _ feature _ cube \u2014 [UNK] _ cube _ feature to compute the residual. this uses a helper function called residual _ norm defined in the appendix at the end of this article. conclusionshapley values have become an incredibly popular and generalizable method for explaining which features are important to a machine learning model. by quantifying their effectiveness using shapley residuals, you will be able to further identify where exactly your machine learning model \u2019 s behavior is coming from, and which insights stemming from shapley", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##y residuals, you will be able to further identify where exactly your machine learning model \u2019 s behavior is coming from, and which insights stemming from shapley values are worth trusting. special thanks to the authors of the original shapley residuals paper for their work! appendixall images in the piece are created by the author. below is the code for the hypercube object and other helper functions, which you can use with the starter code above to compute shapley residuals. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribepro", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##y residuals. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / shapley - residuals - measuring - the - limitations - of - shapley - values - for - explainability", "metadata": {"source": "https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability", "row": 188, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 189 text : from black box to glass box : transparency in xai arthur ai blog solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##vabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedexplainable aifrom black box to glass box : transparency in xaiby : caryn lusinchioctober 19, 2022with the rise of oss security concerns, divergence of explainability goals, and custom, proprietary xai algorithms", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##sinchioctober 19, 2022with the rise of oss security concerns, divergence of explainability goals, and custom, proprietary xai algorithms, is transparency still possible? explainable ai ( xai ) typically involves tools and techniques to understand how a complex model behaves, in a simple, straightforward, and intuitive way so humans can understand it. it answers why an automated decision making tool resulted in a specific output that impacts customers. market size it \u2019 s predicted the explainable ai market size is estimated to reach $ 21. 8 billion by 2030, up from $ 4. 1 billion in 2021. and ga", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "predicted the explainable ai market size is estimated to reach $ 21. 8 billion by 2030, up from $ 4. 1 billion in 2021. and gartner \u2019 s crystal ball paints a picture that \u201c by 2025, 30 % of government and large enterprise contracts for the purchase of ai products and services will require the use of explainable and ethical ai. \u201d regulatory \u2019 s roleso, what \u2019 s fueling predicted market growth? the accelerant for the explainable ai market is due in part to eu advent of gpdr \u2019 s article 13 - 15 and 22, which establishes rights specific to algorithmic decision making", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "explainable ai market is due in part to eu advent of gpdr \u2019 s article 13 - 15 and 22, which establishes rights specific to algorithmic decision making, including a right of both notification and access to meaningful information about the logic involved and the right of the significance of and envisioned effects of solely automated decision making. explainability, in this specific use case, is a legal obligation of enterprises to inform regulatory officials as well as end customers about why models made the decisions they did. end customers should be able to comprehend explanations, which should be written simply in their native language and include non - technical jargon. additionally, article 13 (", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". end customers should be able to comprehend explanations, which should be written simply in their native language and include non - technical jargon. additionally, article 13 ( 1 ) of the eu \u2019 s future artificial intelligence act ( aia ) mandates that high - risk ai systems be \u201c sufficiently transparent to enable users to interpret the system \u2019 s output and use it appropriately. \u201d given the fact there are over 100 different xai methods available to data scientists today and they often select the one that takes the least amount of effort / time, future regulation doesn \u2019 t specifically prescribe which explainability method should be used. the enterprise can elect to use", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that takes the least amount of effort / time, future regulation doesn \u2019 t specifically prescribe which explainability method should be used. the enterprise can elect to use local, global, or counterfactual explanations, but it \u201c must be faithful to the model in the sense that they need to be an, at least approximately, correct reconstruction of the internal decision making parameters : explanation and explanandum need to match. \u201d 1while there is no nationally passed regulation in the u. s. regarding explainability, the national institute of standards and technology ( nist ) proposed in 2020 four principles for judging how explainable an artificial", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the u. s. regarding explainability, the national institute of standards and technology ( nist ) proposed in 2020 four principles for judging how explainable an artificial intelligence \u2019 s decisions are. and, the most recently released white house blueprint for an ai bill of rights includes a notice & explanation principle, stating : \u201c automated systems should provide explanations that are technically valid, meaningful, and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context. \u201d explosion of xai solutionsacademic r & d labs, open - source communities, and private", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##rated to the level of risk based on the context. \u201d explosion of xai solutionsacademic r & d labs, open - source communities, and private software enterprises alike have seen legal compliance signals as a trigger to brainstorm xai algorithms. the partnership on ai ( pai ) reports that \u201c each year the number of available xai tools ( developed by both academics and industry practitioners ) grows, resulting in more options than ever for those interested in using them. here, we define \u2018 xai tool \u2019 broadly to mean any means for directly implementing an explainability algorithm. in our initial research, pai identified more than 150 x", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we define \u2018 xai tool \u2019 broadly to mean any means for directly implementing an explainability algorithm. in our initial research, pai identified more than 150 xai - related tools published between 2015 and 2021. \u201d the goal of the pai project is to give enterprises tools to make more informed decisions about which xai tool is best to deliver value to a business and help scale explanations. oss concerns the vast majority of xai tools are free - to - use open - source software ( oss ). public by nature, oss offers a lot of benefits including crowdsourced examination for bugs or code evolution as well as enabling", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "source software ( oss ). public by nature, oss offers a lot of benefits including crowdsourced examination for bugs or code evolution as well as enabling ethical conversations around ml applications. while oss xai libraries such as lime or shap have done a lot to advance a broader understanding in the industry, they also pose performance doubts and security vulnerabilities. some ml engineers are hesitant to apply oss explainability methods into an application because they can slow down mlops workflows and ai pipeline momentum. additionally, cybersecurity experts are voicing concern that oss explainable models are less secure given that when", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ops workflows and ai pipeline momentum. additionally, cybersecurity experts are voicing concern that oss explainable models are less secure given that when internal workings of model algorithms are publicized, bad actors can potentially manipulate the information via evasion, oracle, or poisoning attacks. enterprises in competitive industries ( where algorithms are treated as trade secrets or confidential ip ), are worried that explainability may embolden industry competitors to reverse engineer ml models. oss caution is also echoed by the data science community. in a recent stackexchange post, it was acknowledged that commonly used and widely adopted open source ml packages are not regularly", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "echoed by the data science community. in a recent stackexchange post, it was acknowledged that commonly used and widely adopted open source ml packages are not regularly tested for reliability or de - bugged. one user posted, \u201c quite often those packages github repos have existing unresolved issues and we may not go through them to identify any pitfalls. business will be making critical actions based on the predictions / insights we, as a data scientist provide, which in turn could be based on those packages. how can we minimize the risk in such scenarios? \u201d custom explainability trendsgiven the combination of oss security concerns", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "turn could be based on those packages. how can we minimize the risk in such scenarios? \u201d custom explainability trendsgiven the combination of oss security concerns and stakeholder resistance, companies are creating their own custom or proprietary explainability methods in - house or outsourcing the task to boutique consultants. as enterprises shift from oss to xai algorithmic ip, one would assume transparency would suffer \u2014 but that \u2019 s not necessarily true. proprietary algorithmic ip may enable enterprises to fine - tune xai methods to explain outcomes on an individual audience level, to provide more context around decision making rationale. additionally, it gives the enterprise", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to fine - tune xai methods to explain outcomes on an individual audience level, to provide more context around decision making rationale. additionally, it gives the enterprise greater control over explanation content and verification. ultimately, it may ensure there is sufficient domain knowledge expertise assigned to investigate models and dynamic datasets in order to fully comprehend the explanation. it \u2019 s predicted the trend for custom explainability will span across the model lifecycle, integrating into upstream and downstream ml team tasks. given regulators \u2014 across gdpr, eu ai act, or the ai bill of rights \u2014 are requiring easy - to - understand explanations for end customers and business stakeholders alike", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "regulators \u2014 across gdpr, eu ai act, or the ai bill of rights \u2014 are requiring easy - to - understand explanations for end customers and business stakeholders alike, there \u2019 s been recent work advancing natural language formatted explanations vs. technically dense feature importance scores based on lime or shap. as much as regulators are proponents of oss, they also accept proprietary algorithms as long as there is sufficient evidentiary internal documentation and public disclosure to satisfy explainability laws. divergence of stakeholder explainability goalsregardless of whether xai methods are built on oss or proprietary or a combination of both, the biggest challenge facing enterprises is", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "stakeholder explainability goalsregardless of whether xai methods are built on oss or proprietary or a combination of both, the biggest challenge facing enterprises is that internal stakeholders don \u2019 t share the same explainability objectives. each department has a distinct yet disparate goal of what they hope explainability will achieve. the brookings institute \u2019 s article, explainability won \u2019 t save ai, broke down these fundamental differences. typically, an explainability formula seeks to answer one perspective but fails to capture a broader context capturing angles from diverse, multi - stakeholders. which is why the pursuit of explainability \u2014 either instigated by internal", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "perspective but fails to capture a broader context capturing angles from diverse, multi - stakeholders. which is why the pursuit of explainability \u2014 either instigated by internal audit or external regulatory ask \u2014 in itself is not a panacea alone for risk management. however, it is a starting point to shed light on the complex \u201c black box \u201d decision making that occurs between a machine learning system \u2019 s inputs and outputs. discover arthur \u2019 s explainability features across the pre - production and post - production mlops lifecycle, including regional importance, global importance, and feature importance. previous postsharenext post we make ai better for everyone.", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "post - production mlops lifecycle, including regional importance, global importance, and feature importance. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / from - black - box - to - glass - box - transparency -", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "serviceprivacy source : https : / / www. arthur. ai / blog / from - black - box - to - glass - box - transparency - in - xai", "metadata": {"source": "https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai", "row": 189, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 190 text : 4 myths about the nyc ai bias law solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llm", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairness4 myths about the nyc ai bias lawby : caryn lusinchioctober 4, 2022studies show that 99 % of fortune 500 companies rely on the aid of talent - sifting software, and 55 % of human resources leaders in the u. s", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##es show that 99 % of fortune 500 companies rely on the aid of talent - sifting software, and 55 % of human resources leaders in the u. s. use predictive algorithms to support hiring. 1given the widespread usage of predictive algorithms by human resources departments and the potential for it to go wrong, new york city is one of the first cities to pass legislation in an effort to prevent negative impacts resulting from automated decision employment tools ( aedt ). effective january 1, 2023, new york city int. no. 1894 - a relating to automated decision tools law will go into effect that restricts new york city employers", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "1, 2023, new york city int. no. 1894 - a relating to automated decision tools law will go into effect that restricts new york city employers from using automated employment decision tools unless it has been the subject of an independent bias audit no more than one year prior to its use. with under three months to go until this law is officially put into place, we wanted to dispel some myths about it. keep reading to learn more. myth # 1 : the entire end - to - end human resources lifecycle, spanning candidate screening to termination events, is covered by 1894 - a. fact : nope. the law only", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "entire end - to - end human resources lifecycle, spanning candidate screening to termination events, is covered by 1894 - a. fact : nope. the law only covers hiring and internal promotional employment decisions that occur within new york city ( not outside the city ). it does not apply to demotions, firing, or downsizing actions. as we witness increased layoffs across industries due to mounting recessionary pressure, it \u2019 s regrettable that the law fails to cover \u201c performance management \u201d algorithms that are making automated decisions regarding which employees are on the chopping block. myth # 2 : the law covers any automated process or system used in human", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u201d algorithms that are making automated decisions regarding which employees are on the chopping block. myth # 2 : the law covers any automated process or system used in human resources. fact : the law does not cover or materially impact employment decisions made by \u201c a junk mail filter, firewall, antiviral software, calculator, spreadsheet, databases, data set, or other compilation of data. \u201d 2in legislation, aedt or automated employment decision tools are defined as \u201c computational process, derived from machine learning, statistical modeling, data analytics, or artificial intelligence, that issues simplified output, including a score, classification, or", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as \u201c computational process, derived from machine learning, statistical modeling, data analytics, or artificial intelligence, that issues simplified output, including a score, classification, or recommendation, that is used to substantially assist or replace discretionary decision making for making employment decisions that impact natural persons. \u201d myth # 3 : an individual can file a court complaint if they think they \u2019 ve been discriminated against by an enterprise \u2019 s aedt. fact : the law makes no mention of \u201c private right of action. \u201d if nyc finds an employer \u2019 s aedt was discriminatory, there is a path for a federal court class action complaint. myth", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "right of action. \u201d if nyc finds an employer \u2019 s aedt was discriminatory, there is a path for a federal court class action complaint. myth # 4 : employers must provide notices to all employees and candidates that an aedt tool will be used in connection with employee and / or candidate assessments / evaluations. fact : non - residents of new york city are not required to receive this notice, even when applying to a city - based position. the notice only needs to be sent to individuals who live in new york city. it must be made no less than 10 business days before use of aedt, plus it must list", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to be sent to individuals who live in new york city. it must be made no less than 10 business days before use of aedt, plus it must list the job qualifications / characteristics that the aedt used for decision making. employers have the option of publicly disclosing the type of data used, the data source, and data retention policy for the aedt on their company website or providing the above information to employees or candidates upon written request within 30 days of receiving the request. upon notice, candidates can opt out and request an alternative selection process or accommodation. after the law goes into effect on january 1, 2023, regulators know", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". upon notice, candidates can opt out and request an alternative selection process or accommodation. after the law goes into effect on january 1, 2023, regulators know enterprises won \u2019 t be able to comply overnight \u2014 so naturally there will be an interim grace period to give companies enough time to put processes and tools in place to meet regulation fine print before fines are levied. concerns around bias in the automated hiring process have pre - dated the passage of the nyc ai bias law. readers who are interested in taking a deeper dive into algorithmic hiring, equity, and bias should check out work by the non - profit group upturn. for additional", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "who are interested in taking a deeper dive into algorithmic hiring, equity, and bias should check out work by the non - profit group upturn. for additional reading, reference \u201c we need fairness and explainability in algorithmic hiring \u201d co - authored by john dickerson, arthur \u2019 s chief scientist. or, delve into the academic paper \u201c mitigating bias in algorithmic hiring : evaluating claims and practices. \u201d in summary, the nyc bias law requires an independent third - party audit to assess aedt \u2019 s disparate impact on candidates or employees of a particular gender or race / ethnicity. but, how can you determine", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- party audit to assess aedt \u2019 s disparate impact on candidates or employees of a particular gender or race / ethnicity. but, how can you determine if your aedt \u2019 s model outcomes result in disparate impact year - round and not just a point in time for employment decision making? arthur \u2019 s bias tab empowers human resources teams to view predictions ( or outcomes ) of your aedt model, segmented by relevant subgroups in the population. an adjustable fairness threshold lets you quickly identify if your model is causing disparate impact for protected classes. with arthur, you can proactively and continuously measure di", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "adjustable fairness threshold lets you quickly identify if your model is causing disparate impact for protected classes. with arthur, you can proactively and continuously measure disparate impact with algorithmic bias model monitoring \u2014 and act on it to improve the outcomes for both future and current employees you serve. interested in seeing the arthur platform in action? schedule a demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentation", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / 4 - myths - about - the - nyc - ai - bias - law", "metadata": {"source": "https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law", "row": 190, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 191 text : making ai work for even more people solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnl", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesmaking ai work for even more peopleby : adam wenchelseptember 27, 2022at arthur our mission is simple : make ai work for everyone. make it work for our partners building their businesses on ai. make it work for all of us who are impacted by ai", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "simple : make ai work for everyone. make it work for our partners building their businesses on ai. make it work for all of us who are impacted by ai every day through healthcare, social media, finances, and so many other areas of our lives. make it work for the data scientists, ml engineers, product owners, and executives who need these systems to be reliable, ethical, and highly accurate. simple does not mean easy. ai in the real world is in its infancy ; it is just getting started. ai is fundamentally a step change in mankind \u2019 s ability to automate. big leaps like this require relentless invention and reinvent", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "; it is just getting started. ai is fundamentally a step change in mankind \u2019 s ability to automate. big leaps like this require relentless invention and reinvention. i am continually amazed by the way our team at arthur embraces this challenge every day \u2014 literally building the future in the process. today, i \u2019 m thrilled to announce the latest recognition of their great work. arthur has raised $ 42m in series b funding, which will accelerate our mission to make ai work for everyone. the investment is the industry \u2019 s largest - ever in a machine learning observability platform. over the last year we have averaged 58 %", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for everyone. the investment is the industry \u2019 s largest - ever in a machine learning observability platform. over the last year we have averaged 58 % growth per quarter, a total of 445 % over the whole period. more than anything, this is a significant recognition of the last 3 + years of category - defining work by our world - class group of researchers and engineers. we would be nowhere without the amazing partnerships we have with some of the largest, most forward - thinking companies in the world. they are the ones who are leaning in to the change and reaping the rewards that well - considered ai efforts bring.", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- thinking companies in the world. they are the ones who are leaning in to the change and reaping the rewards that well - considered ai efforts bring. thank you for sharing our vision and the journey. we are excited to work with theresia gouw and asad khaliq from acrew capital and dylan pearce and bryan subijano from greycroft, our co - leads for the round. we \u2019 ve collaborated with theresia, asad, and the team at acrew for the last 18 months and could not be more excited to take the next step in our partnership. dylan, bryan, and the greycroft team", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "at acrew for the last 18 months and could not be more excited to take the next step in our partnership. dylan, bryan, and the greycroft team join as new investors, bringing invaluable experience growing and operating successful software companies. we welcome new investor bam elevate, and all of our excellent existing investors who are doubling down including index ventures, work - bench, and plexo capital. what will we do with this funding? first and foremost, we have scaled up our investment in fundamental research. our research team has produced a number of breakthroughs that not only advance the state of ai but also create significant", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we have scaled up our investment in fundamental research. our research team has produced a number of breakthroughs that not only advance the state of ai but also create significant value for our customers. it is this spirit of innovation, deeply rooted in arthur \u2019 s dna, that has driven our growth. we are also scaling up many client functions like customer success and our team of field data scientists to make sure that our quickly growing user base has the world - class support they deserve. tomorrow, we will roll up our sleeves and be back at it \u2014 there is still much to be done. i feel lucky to be a part of this team and look", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we will roll up our sleeves and be back at it \u2014 there is still much to be done. i feel lucky to be a part of this team and look forward to the journey to make ai work for everyone. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https :", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / making - ai - work - for - even - more - people", "metadata": {"source": "https://www.arthur.ai/blog/making-ai-work-for-even-more-people", "row": 191, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 192 text : will ai solve climate change? it \u2019 s not that simple solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai research & innovationwill ai solve climate change? it \u2019 s not that simpleby : arthur teamaugust 23, 2022with the biggest commitment to mitigating climate change in u. s. history having recently been signed into law, climate is \u2014 even more", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "23, 2022with the biggest commitment to mitigating climate change in u. s. history having recently been signed into law, climate is \u2014 even more so than usual \u2014 top of mind for many. from ai - based forecasts around extreme weather and co\u2082 emissions to identification of climate - vulnerable regions, ai and machine learning technologies have been touted as the ultimate solution or silver bullet to overcoming climate change. unsurprisingly, it \u2019 s not quite that simple. keep reading to find some use cases for ai and climate change, as well as the ways in which ai is simultaneously solving and contributing to the phenomenon. the", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". keep reading to find some use cases for ai and climate change, as well as the ways in which ai is simultaneously solving and contributing to the phenomenon. the prosin so many ways, these technologies can be absolute game - changers when it comes to mitigating the effects of climate change. in fact, 87 % of private and public sector ceos with decision - making power in ai and climate believe ai is an essential tool in the fight against climate change. below are just a few of the more common use cases. mitigationthis is one of the most crucial use cases for ai : the measurement, reduction, and", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "just a few of the more common use cases. mitigationthis is one of the most crucial use cases for ai : the measurement, reduction, and removal of emissions and ghg effects. measurement : measuring emissions, both in the overall environment and at the level of individual products and actions, allows us to take stock of our current situation and forecast future trends. it also allows us to prioritize the abatement efforts with the highest potential to reduce emissions, at both the macro level and the micro level. reduction : ai solutions can reduce emissions intensity by improving supply forecasts for intermittent renewable energy sources or creating more efficient electric", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "both the macro level and the micro level. reduction : ai solutions can reduce emissions intensity by improving supply forecasts for intermittent renewable energy sources or creating more efficient electric - grid balancing. additionally, they can reduce emissions - generating activities by optimizing supply chains, such as through improved demand forecasting or more efficient transportation of goods. removal : ai - based solutions can be used to support and more accurately quantify the environmental removal process by analyzing satellite images to detect deforestation and estimate ecosystem carbon sequestration. they can also be used for technological removal efforts such as direct air capture ( dac ) or carbon capture by assessing capture and storage", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ecosystem carbon sequestration. they can also be used for technological removal efforts such as direct air capture ( dac ) or carbon capture by assessing capture and storage locations, monitoring leakage, and optimizing the industrial processes and materials used for carbon capture. adaptation & resilienceadapting to climate change is a critical undertaking for policymakers and the public, as it boosts resilience to the effects of long - term climate trends and extreme events. solutions can apply ai to promote adaptation and resilience, particularly in hazard forecasting and in vulnerability and risk - exposure management. hazard forecasting : ai is well suited", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "apply ai to promote adaptation and resilience, particularly in hazard forecasting and in vulnerability and risk - exposure management. hazard forecasting : ai is well suited to help in forecasting and projecting climate - related hazards, whether in the form of improved long - term projections of regionalized events, such as sea - level rise, or in the form of early warning systems for extreme weather events such as hurricanes or droughts. vulnerability & risk - exposure management : ai can be used here to enable public - and private - sector leaders to make more informed decisions during crises, strengthen infrastructure through predictive maintenance of structures, protect populations through the prediction", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "here to enable public - and private - sector leaders to make more informed decisions during crises, strengthen infrastructure through predictive maintenance of structures, protect populations through the prediction and monitoring of things like large - scale migration patterns or food insecurity risk, and preserve biodiversity through systems like intelligent irrigation. fundamentalsai can be used to support research and education efforts on climate change, helping stakeholders understand the risks and implications and share what they learn. all such efforts reinforce and magnify progress toward mitigation and toward adaptation and resilience. ai can be used to bolster climate research and modeling ; climate finance and analytics ; and education, nu", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "mitigation and toward adaptation and resilience. ai can be used to bolster climate research and modeling ; climate finance and analytics ; and education, nudging, and behavior change ( such as in the form of recommendations for environmentally friendly purchases ). the conson the other hand, the carbon footprint and computational costs of training large machine learning models can \u2019 t and shouldn \u2019 t be overlooked. in a recent publication, researchers from the university of massachusetts at amherst conducted a life cycle analysis for training several typical big ai models. they discovered that the process may produce over 626, 000 pounds of co\u2082 equivalent \u2014 which is 5x", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "life cycle analysis for training several typical big ai models. they discovered that the process may produce over 626, 000 pounds of co\u2082 equivalent \u2014 which is 5x more than a u. s. car produces in its entire lifetime and a whopping 57x more than one human produces in a year. artificial intelligence algorithms, which power some of technology \u2019 s most cutting - edge applications, such as producing logical stretches of text or creating visuals from descriptions, may need massive amounts of computational power to train \u2014 and their energy demands are rapidly increasing. the computing resources required to create a best - in - class ai model have doubled every 3 to 4", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "power to train \u2014 and their energy demands are rapidly increasing. the computing resources required to create a best - in - class ai model have doubled every 3 to 4 months in the deep learning era. this, in turn, necessitates a vast quantity of power, prompting many to fear that the carbon footprint of these increasingly popular ultra - large ai systems will render them environmentally unsustainable. here are a few ways that the carbon footprint of ai can be reduced : make use of computationally efficient machine learning algorithms. don \u2019 t train a model from scratch if it is not necessary. when you can, use automated ml. since", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "use of computationally efficient machine learning algorithms. don \u2019 t train a model from scratch if it is not necessary. when you can, use automated ml. since the model inference accounts for 80 - 90 % of the model cost, use customized processors that increase the speed and efficiency of training and testing neural networks. ai is a tool that is already helping us and will continue to help us build a more resilient future \u2014 but, like all tools, it should be used wisely. additionally, solving the climate change crisis requires not just technological innovation, but also the will of decision makers to take action and make the necessary changes toward a", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ly. additionally, solving the climate change crisis requires not just technological innovation, but also the will of decision makers to take action and make the necessary changes toward a better future. interested in learning how arthur can help your organization monitor, measure, and improve ai efforts? schedule a demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##pcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / will - ai - solve - climate - change - its - not - that - simple", "metadata": {"source": "https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple", "row": 192, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 193 text : what to know as you consider the next step in your tech journey solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlife at arthurwhat to know as you consider the next step in your tech journeyby : reid champlinaugust 3, 2022after an unprecedented period of growth, the tech industry appears to be entering a stretch of profound uncertainty, leaving many professionals wondering what their", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ust 3, 2022after an unprecedented period of growth, the tech industry appears to be entering a stretch of profound uncertainty, leaving many professionals wondering what their next step should be. in just the first half of 2022, over 24, 000 people were affected by layoffs at some of america \u2019 s top tech companies, including netflix, coinbase, tesla, twitter, and more. as fears of a potential recession grow, industry titans are pulling back on new hiring, with many saying the industry \u2019 s meteoric growth is being brought back to earth. but for talent professionals and hiring managers throughout the industry, it \u2019 s clear", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with many saying the industry \u2019 s meteoric growth is being brought back to earth. but for talent professionals and hiring managers throughout the industry, it \u2019 s clear the market remains red hot. despite some high - profile companies rethinking their hiring strategies, tens of thousands of tech jobs remain unfilled as new opportunity emerges. having your career shaken by factors beyond your control can be daunting, but your future is still bright. as you look forward to your next opportunity, here are three critical factors to keep in mind that \u2019 ll help you navigate the new job market and ensure your future success. 1. know the industry", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "opportunity, here are three critical factors to keep in mind that \u2019 ll help you navigate the new job market and ensure your future success. 1. know the industry as you consider new companies and organizations to further your career, ask yourself : what opportunities exist in this field, and how will those opportunities change and grow into the future? finding life - changing opportunities often means being in the right industry at the right time. not only are these opportunities less likely to be affected by uncertainty in the market, but they also offer the biggest reward as promising fields grow and occupy larger and larger niches within the tech world. propelled by the limitless potential", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the market, but they also offer the biggest reward as promising fields grow and occupy larger and larger niches within the tech world. propelled by the limitless potential of ai, the machine learning industry is poised to explode in value to nearly $ 200 billion by 2030. the nascent field is already revolutionizing finance, healthcare, transportation, and countless other industries while employing more and more people every year \u2014 ml jobs have grown by 75 % in the past four years alone. as the leader in the ml operations industry, arthur has already experienced tremendous growth and will only continue to grow into the future. \u201c as the ai industry continues to auto", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the leader in the ml operations industry, arthur has already experienced tremendous growth and will only continue to grow into the future. \u201c as the ai industry continues to automate and mature the model development and deployment process, we \u2019 re going to witness the next 5 - 10 years intensely focused on bringing the same level of sophistication and ease to model performance management and optimization once these models go live in production, \u201d says victoria vassileva, arthur \u2019 s commercial accounts lead. \u201c this is where all of the value \u2014 and risk \u2014 is for any organization. model performance and business performance, including regulatory and reputational risk, will", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". \u201c this is where all of the value \u2014 and risk \u2014 is for any organization. model performance and business performance, including regulatory and reputational risk, will be increasingly linked and so having the necessary oversight and controls will be the differentiator between success and failure, between positive impact and harm. \u201d 2. know the companyafter considering the broader industry outlook, ask yourself : which company is best poised to take the lead in creating that new future? knowing your prospective organizations inside and out is crucial to identifying which have the strengths needed to reach their goals \u2014 and to help you reach yours. proven leadership is indispensable to", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "inside and out is crucial to identifying which have the strengths needed to reach their goals \u2014 and to help you reach yours. proven leadership is indispensable to any sound organization. the best indicator of future success is past success, and leaders who know what it takes to survive and realize their goals despite the odds are the most likely to do it again. no company can predict the future, and often the road to success is forged in difficult economic times. what separates the companies that float and the companies that sink is a focus on sustainable, responsible growth and commitment to take the steady route to success. look into or ask about your prospective companies", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "float and the companies that sink is a focus on sustainable, responsible growth and commitment to take the steady route to success. look into or ask about your prospective companies \u2019 fundraising, annual recurring revenue ( arr ), burn rate, and other key financial indicators. are they prepared for whatever the future holds? research has proven again and again that diverse organizations are higher - performing organizations and that fostering an inclusive company culture has a significant impact on a company \u2019 s fate. are the companies you \u2019 re considering doing everything they can to foster a workplace where people of all backgrounds and identities are welcomed, included, and supported? at arthur, we", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "companies you \u2019 re considering doing everything they can to foster a workplace where people of all backgrounds and identities are welcomed, included, and supported? at arthur, we \u2019 ve built a culture - driven organization run by experienced leaders with an eye towards a more responsible future for the machine learning world. our ceo adam wenchel has over 20 years of industry experience and found success both as ceo of anax security, a dc - based ai startup, and as vp of ai & data innovation at capital one. we \u2019 re built by ai experts and backed by world - class, diverse investors, led by mike volpi at index ventures, lo tone", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "at capital one. we \u2019 re built by ai experts and backed by world - class, diverse investors, led by mike volpi at index ventures, lo toney, theresia gouw, work - bench, and homebrew. and we \u2019 re even more proud to foster an environment where everyone feels welcome and invested in the arthur mission. \u201c it \u2019 s refreshing that we regularly and thoughtfully talk about how diversity and belonging are important to arthur \u2019 s mission to make ai work for everyone, \u201d says genesis whitlock, arthur \u2019 s head of talent. \u201c the time, energy, and focus it takes to find amazing talent", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "work for everyone, \u201d says genesis whitlock, arthur \u2019 s head of talent. \u201c the time, energy, and focus it takes to find amazing talent from different backgrounds, experiences, and identities is so rewarding when you have leaders and colleagues who are actively working alongside you to create a diverse and inclusive environment. \u201d 3. know your valueswhat matters the most to you? ultimately, only you can decide the right move for yourself. as you look forward to the next adventure in your career, take some time to think about what would make your next job the perfect one. are you looking to grow and expand your skill set", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "next adventure in your career, take some time to think about what would make your next job the perfect one. are you looking to grow and expand your skill set? in an industry that changes as rapidly as tech, staying up to date with the latest skills is critical to a long and impactful career. do your prospective companies value mentoring and continuing education both in principle and in practice? are you looking to take on leadership responsibilities and have your voice be heard? if so, it \u2019 s critical to know whether your new organization regularly encourages employees to take on new challenges and rewards them with opportunities to do so. are you looking to work for", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 s critical to know whether your new organization regularly encourages employees to take on new challenges and rewards them with opportunities to do so. are you looking to work for a company with a mission you can believe in? people are far more likely to stay longer and achieve more if they believe in their organization \u2019 s mission. consider what a truly good business looks like to you, and compare that vision against the companies you \u2019 re considering. arthur \u2019 s values are at the heart of everything we do, and we work tirelessly to make sure we provide every opportunity our people could want. even as we grow and scale, we remain a small team", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we do, and we work tirelessly to make sure we provide every opportunity our people could want. even as we grow and scale, we remain a small team at heart, elevating employees to new opportunities and rewarding excellence with a platform to have your voice be heard. \u201c with startups, building a great product doesn \u2019 t happen without a great culture as the foundation, \u201d ceo adam wenchel says. \u201c when i co - founded arthur three years ago, i wanted to make sure that everyone on our team loved coming to work. we \u2019 ve spent a lot of time and energy fostering a company culture that reflects our", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "wanted to make sure that everyone on our team loved coming to work. we \u2019 ve spent a lot of time and energy fostering a company culture that reflects our core values of integrity, transparency, and inclusion. \u201d we \u2019 re on a mission to make a more responsible ai - driven world \u2014 and we hope you \u2019 ll join us in making that vision a reality. interested in learning more about a career at arthur? check out our job openings. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllm", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / what - to - know - as - you - consider - the - next - step - in - your - tech - journey", "metadata": {"source": "https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey", "row": 193, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 194 text : data drift detection part ii : unstructured data in nlp and cv solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##servabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringdata drift detection part ii : unstructured data in nlp and cvby : karthik rao and rowan cheungaugust 2, 2022note : this is the second part of a series where we take a deeper dive into the", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "karthik rao and rowan cheungaugust 2, 2022note : this is the second part of a series where we take a deeper dive into the question of data drift detection. if you haven't yet, check out the first part where we discussed data drift in the context of tabular data! introductiondata drift detection is a key component of a machine learning monitoring system. so far, we \u2019 ve discussed what data drift can look like in the context of tabular data, as well as some approaches to measuring drift. to recap, let \u2019 s revisit a simple example of data drift in a single", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "tabular data, as well as some approaches to measuring drift. to recap, let \u2019 s revisit a simple example of data drift in a single feature : comparing distributions. in this diagram, we examine a single input feature ( age ) and look at the distribution of this variable at two time points. in the training data ( green distribution ) and in today \u2019 s production data ( purple distribution ). in this case, the distribution of age in the training dataset is different from its distribution in a production environment. over time, the performance of a model using age as an input feature can decay in response to the change in", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is different from its distribution in a production environment. over time, the performance of a model using age as an input feature can decay in response to the change in the environment the model is deployed in. there are a variety of metrics we can use for measuring the difference in these two distributions, but how do we measure drift without structured features? systems trained on unstructured data, like text or images, face the same risks when deployed in production. however, detecting drift in these scenarios is more subtle, as we cannot use common divergence metrics on the raw data. in this post, we \u2019 ll walk through a general framework", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these scenarios is more subtle, as we cannot use common divergence metrics on the raw data. in this post, we \u2019 ll walk through a general framework for data drift detection with unstructured data and we \u2019 ll highlight the two example use cases of nlp and computer vision. specifically, we aim to identify datapoints that are anomalous, or belonging to a distribution different than the training data. formally, we would like to surface incoming datapoints that are likely to have been drawn from a distribution q ( x ) that is different from the training distribution p ( x ). we \u2019 ll rely on two common use cases", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to have been drawn from a distribution q ( x ) that is different from the training distribution p ( x ). we \u2019 ll rely on two common use cases to illustrate the out - of - distribution detection problem and evaluate our solution. our first example will be a computer vision use case, where the goal is to classify images based on the objects depicted in the image. for this setting, we used the stl - 10 dataset from stanford which provides high - resolution images from ten different possible classes including airplane, bird, dog, truck, and so on. example images from the stl - 10 dataset. we see images of", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ten different possible classes including airplane, bird, dog, truck, and so on. example images from the stl - 10 dataset. we see images of cars, planes, trucks, dogs, etc. the stl - 10 dataset contains 8, 000 images. our second use case will be in nlp and we used a news headline dataset which contains news headlines along with their respective topics such as crime, entertainment, world news, comedy, etc. here, our objective is to classify headline text to the correct category. example datapoint from the news headline dataset. we see that we receive information about the news category", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "our objective is to classify headline text to the correct category. example datapoint from the news headline dataset. we see that we receive information about the news category, news headline, authors, etc. the news headline dataset contains 200, 000 + records. algorithm / approachas with measuring multivariate drift in tabular data, the core motivation of the approach is to model the density, or distribution, of the reference dataset. overviewthere are several different approaches for finding anomalies in unstructured data. for any given approach, the three main aspects to determine anomalies in unseen data require : vector representation :", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "finding anomalies in unstructured data. for any given approach, the three main aspects to determine anomalies in unseen data require : vector representation : convert the unstructured data to a vector embedding. density model : define a density model for the reference dataset. scoring : create a method for scoring new datapoints against the reference density model. in this section, we will discuss the variety of different techniques used for each of these three different components. further, we will highlight example results with nlp and computer vision datasets. vector representationwe must convert our image or text data into a meaningful vector representation in", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", we will highlight example results with nlp and computer vision datasets. vector representationwe must convert our image or text data into a meaningful vector representation in order to understand the underlying distribution of the reference dataset. these vector representations are a type of feature extraction that can capture a useful representation of our unstructured data. transfer learning is one approach for creating these representations by extracting embeddings of each image or text sequence from a large pre - trained model. these large - scale models are generally trained on millions of different datapoints and use state of the art architectures ( cnn \u2019 s for image data or transformers for", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these large - scale models are generally trained on millions of different datapoints and use state of the art architectures ( cnn \u2019 s for image data or transformers for text data ) that can take unseen datapoints and produce a meaningful vector representation. for images, pre - trained models such as resnet, vgg, or similar will be appropriate. for nlp data, we need to extract document embeddings and turn to pre - trained ( or fine - tuned ) large language models. while these are just a few examples of large - scale pre - trained models, there exist several others which are trained on different neural - network architecture", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "language models. while these are just a few examples of large - scale pre - trained models, there exist several others which are trained on different neural - network architectures and different datasets. this approach can be used with any type of vector embedding as long as it is meaningful for the context of your machine learning task. density modelonce we have meaningful vector abstractions for every point in our reference dataset, we must now create a density model that can model the underlying distribution. we can train a flexible density model to these embedding vectors. this could be accomplished with many possible techniques such as an auto - en", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the underlying distribution. we can train a flexible density model to these embedding vectors. this could be accomplished with many possible techniques such as an auto - encoder, a vae, a normalizing flow, a gan, etc. in each case, this density model learns the structure and distribution of the reference set images or text ( as represented in the embedding space ). example of an auto - encoder architecture. as an example, auto - encoders are frequently used for unsupervised anomaly detection. auto - encoders learn the latent representations of the reference set ( consisting of vector", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##coders are frequently used for unsupervised anomaly detection. auto - encoders learn the latent representations of the reference set ( consisting of vector embeddings ) by encoding the vector to a lower dimensional vector and then decoding that representation back to its original dimension. we refer to the error measurement between the original input vector and the output vector as the reconstruction loss. datapoints that are similar to points from the reference distribution will have a lower reconstruction error than points that are very different from the reference distribution. this property is useful for finding outliers as points that are outside the distribution of the reference set will have", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "points that are very different from the reference distribution. this property is useful for finding outliers as points that are outside the distribution of the reference set will have a high reconstruction error. taking a look at our news headline example, we can inspect the space learned by our auto - encoder. we first train the model on news headlines categorized as crime, which we treat as our in - distribution data. below is a visualization of held - out crime headlines, as well as entertainment headlines. umap visualization of in - distribution crime headlines ( blue ) and out - of - distribution entertainment headlines ( red ) as encoded by the", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as entertainment headlines. umap visualization of in - distribution crime headlines ( blue ) and out - of - distribution entertainment headlines ( red ) as encoded by the auto - encoder. scoringonce we have trained our density model on our reference set, we must find a way to convert the reconstruction loss values from the model to actionable anomaly scores. our approach is outlined below : after training the model, we compute the reconstruction error of a holdout set ( subset of the reference set ) to use as a proxy distribution. for every unseen datapoint, we compute the reconstruction error after being fed through our trained density model. we", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the reference set ) to use as a proxy distribution. for every unseen datapoint, we compute the reconstruction error after being fed through our trained density model. we find the percentile that our reconstruction error falls into relative to the reconstruction errors of the holdout set. the motivation for our approach is twofold : a lower reconstruction error means that the point is less likely to be anomalous ( because the auto - encoder has seen many examples like it ). therefore, if an unseen datapoint yields a high reconstruction error ( larger than anything from the holdout set ), it is likely to be anomalous. because we", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "if an unseen datapoint yields a high reconstruction error ( larger than anything from the holdout set ), it is likely to be anomalous. because we rank in terms of percentiles, all our scores are normalized between 0 and 1. this makes it user - friendly and interpretable. points that are close to 1 are more likely to be anomalous than points close to 0. evaluation / resultsthere are very few open - source datasets that have labeled data to measure anomaly detection for unstructured data types. therefore, we constructed a few different test cases with our example datasets introduced earlier in", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "have labeled data to measure anomaly detection for unstructured data types. therefore, we constructed a few different test cases with our example datasets introduced earlier in this paper to measure the efficacy of our anomaly detection algorithm for unstructured data. for each dataset ( news headlines and stl - 10 ), we broke up our test cases as follows : we segment our datapoints into in - distribution and out - of - distribution sets based on the labeled classes ( e. g. all images as airplanes would be the reference set and all images as cars would be the out - of - distribution test set ). we furthermore segment the", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "g. all images as airplanes would be the reference set and all images as cars would be the out - of - distribution test set ). we furthermore segment the in - distribution dataset into 80 % as the training set, and 20 % as the holdout set. we use this to determine if our model would classify previously unseen datapoints as \u201c anomalous \u201d or \u201c non - anomalous \u201d based on the image object. for example, we would expect images from the in - class set should have low anomaly scores ( near 0 ) and images from the out - of - class set should have high anomaly scores ( near 1", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the in - class set should have low anomaly scores ( near 0 ) and images from the out - of - class set should have high anomaly scores ( near 1 ). we run this test across different pairs of classes available in our dataset. we compute the auc scores with relation to the \u201c classification \u201d of each datapoint as anomalous or non - anomalous ( based on our scoring methodology ). we highlight two graphics below showcasing the results of our experiments. cv ood : roc curves for stl - 10 data where the in - distribution dataset is the class planes, and the out - of - distribution class", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "cv ood : roc curves for stl - 10 data where the in - distribution dataset is the class planes, and the out - of - distribution class on the left is ships and the out - of - distribution class on the left is birds. the figures above are showcasing the roc curves for two specific experiments we ran using the stl - 10 dataset. the graph on the left is measuring the auc when the in - distribution dataset ( non - anomalous ) was taken from a set of ship images while the out - of - distribution dataset ( anomalous ) was taken from a set of plane images", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##lous ) was taken from a set of ship images while the out - of - distribution dataset ( anomalous ) was taken from a set of plane images. similarly, the graph on the right shows the roc curve where the in - distribution dataset was taken from a set of bird images and the out - of - distribution dataset was taken from a set of plane images. we notice that for both experiments, the anomaly detector does a very good job ( auc scores of 0. 804 and 0. 996 ) of being able to differentiate between in - distribution and out - of - distribution datapoints. nlp o", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of 0. 804 and 0. 996 ) of being able to differentiate between in - distribution and out - of - distribution datapoints. nlp ood : news headlines dataset divided by each category as the reference set. compares cross - class anomaly accuracies ( where another class is all \u201c anomalous \u201d ). the heatmap above is reporting the auc scores for all possible pairwise experiments between possible classes in the news headline dataset ( such as crime, entertainment, etc. ). for any given cell in the heatmap, we are reporting the auc score where the category on the x", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as crime, entertainment, etc. ). for any given cell in the heatmap, we are reporting the auc score where the category on the x - axis is the in - distribution ( non - anomalous ) dataset while the category on the y - axis is the out - of - distribution ( anomalous ) dataset. we reported an average auc score ( across all crosswise pairs ) to be 0. 83, which is quite impressive given this task is difficult even for humans. conclusionthis approach to out - of - distribution detection is especially powerful because it is completely unsupervised.", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "this task is difficult even for humans. conclusionthis approach to out - of - distribution detection is especially powerful because it is completely unsupervised. in a production environment, we often don \u2019 t have prior knowledge of what kind of distribution shifts to expect or access to labeled data. additionally, while we have considered two classification problems in this post, this technique can be applied to any type of machine learning task, as it only considers the input data and is therefore independent of the underlying ml task. detection of out - of - distribution samples is only the first step in maintaining a robust machine learning system. monitor your ml model for", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of the underlying ml task. detection of out - of - distribution samples is only the first step in maintaining a robust machine learning system. monitor your ml model for drift with arthur. at arthur, we \u2019 re helping data scientists and machine learning engineers detect, understand, and respond to unforeseen production environments. faqhow do changes in the external environment, unrelated to the main features of the model, affect the process of data drift detection in nlp and cv applications within ai and ml frameworks, and how can one adjust the detection mechanisms to accommodate such changes? changes in the external environment can significantly impact the effectiveness of ai applications", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai and ml frameworks, and how can one adjust the detection mechanisms to accommodate such changes? changes in the external environment can significantly impact the effectiveness of ai applications, particularly in nlp and cv, by introducing new patterns or visual trends not present in the training data, leading to higher rates of misclassification or irrelevant results. this indicates a data drift. to adjust the detection mechanisms within ai and ml frameworks, one could incorporate adaptive learning strategies, allowing the model to periodically update its parameters based on new data. additionally, implementing a robust anomaly detection framework capable of identifying and adapting to sudden shifts in data distribution without human intervention might help", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "update its parameters based on new data. additionally, implementing a robust anomaly detection framework capable of identifying and adapting to sudden shifts in data distribution without human intervention might help. regularly updating the datasets with recent examples and employing domain adaptation techniques are also effective strategies to mitigate the effects of external changes on the performance of ml models. what are the specific computational costs associated with implementing the described data drift detection methods for unstructured data in real - world applications within ai and ml domains, and how do these costs compare with traditional data drift detection methods used for structured data? implementing data drift detection methods for unstructured data in real -", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "domains, and how do these costs compare with traditional data drift detection methods used for structured data? implementing data drift detection methods for unstructured data in real - world ai and ml applications can be significantly more computationally intensive than for structured data. this is primarily due to the complexity of processing and analyzing unstructured data, such as images and text, which requires advanced algorithms and increased computational power typical of ai systems. techniques like vector representation, density model training, and anomaly scoring, integral to ml workflows, are resource - intensive, especially when handling large datasets. in comparison, traditional data drift detection in structured data,", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", integral to ml workflows, are resource - intensive, especially when handling large datasets. in comparison, traditional data drift detection in structured data, often found in classical ml scenarios, relies on less computationally demanding statistical methods or simpler models. however, the exact computational costs can vary significantly depending on the specifics of each ai and ml implementation, the frequency of model updates, and the volume of data being analyzed. how does the data drift detection framework integrate with existing machine learning pipelines in ai systems, particularly in automated environments where continuous monitoring and instant decision - making are essential? the data drift detection framework can be integrated into", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning pipelines in ai systems, particularly in automated environments where continuous monitoring and instant decision - making are essential? the data drift detection framework can be integrated into existing ml pipelines within ai systems as a dedicated monitoring layer that functions in parallel with the main data processing workflow. in automated ai environments, this involves the continuous, real - time analysis of incoming data to assess its conformity to the model's initial training distribution, a cornerstone in ml operations. the framework should trigger alerts or initiate a retraining cycle if significant drift is detected, maintaining the ml model's accuracy and relevance. for effective integration, apis", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "trigger alerts or initiate a retraining cycle if significant drift is detected, maintaining the ml model's accuracy and relevance. for effective integration, apis could be developed to direct data from operational activities straight into the drift detection system and automate responses based on the outcomes, thereby enhancing the ai system's responsiveness and reliability. this ensures that the ml models remain accurate and relevant without disrupting the overall operational flow of ai - driven systems. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatl", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / data - drift - detection - part - ii - unstructured - data - in - nlp - and - cv", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv", "row": 194, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 195 text : arthur research : equalizing credit opportunity in algorithms solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai research & innovationarthur research : equalizing credit opportunity in algorithmsby : lizzie kumarjuly 26, 2022for decades, financial institutions and lenders have used statistical models to make credit - related decisions. while these models improve efficiency and reduce variability, they have the potential to simultaneously per", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "financial institutions and lenders have used statistical models to make credit - related decisions. while these models improve efficiency and reduce variability, they have the potential to simultaneously perpetuate and accelerate historical patterns of discrimination. in the u. s., legislation like the equal credit opportunity act specifically bans discrimination in lending by forbidding credit score systems from using information like sex, race, marital status, national origin, and religion \u2014 and agencies are charged with enforcing this. however, credit invisibility and historical injustice mean that labeled credit data is limited on protected groups, which could negatively impact the accuracy of models trained on that data. additionally, a", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##visibility and historical injustice mean that labeled credit data is limited on protected groups, which could negatively impact the accuracy of models trained on that data. additionally, a vast body of research has demonstrated that even with sufficient training data, machine learning algorithms can encode many different versions of \u201c unfairness. \u201d this means that financial institutions could \u2014 potentially unwittingly \u2014 engage in illegal discrimination through the use of this technology. two conversations exist in parallel here : one about u. s. discrimination law / policy, and the other about machine learning fairness research. yet, policymakers and researchers in this space seem to talk past each other when it", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". discrimination law / policy, and the other about machine learning fairness research. yet, policymakers and researchers in this space seem to talk past each other when it comes to data access, usage of input features, and the definition of \u201c discrimination \u201d ( intent - based vs. outcome - based ). next tuesday, at the aaai / acm conference on artificial intelligence, ethics, and society ( aies ), we will be presenting a paper called equalizing credit opportunity in algorithms : aligning algorithmic fairness research with u. s. fair lending regulation. the paper provides an overview of the following : the current landscape of credit -", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "algorithms : aligning algorithmic fairness research with u. s. fair lending regulation. the paper provides an overview of the following : the current landscape of credit - specific u. s. anti - discrimination law as it pertains to algorithms for fair lending researchersfair ml research results, contextualized to the realities of credit data to identify \u201c discrimination risks \u201d in the credit settingregulatory opportunities to address those risksthe areas of lending regulation and of ml research are constantly evolving. we hope this paper is a useful tool for ml practitioners to understand the landscape and potential future directions. interested in attending aies? check out the conference", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "evolving. we hope this paper is a useful tool for ml practitioners to understand the landscape and potential future directions. interested in attending aies? check out the conference website. also, learn more about arthur \u2019 s r & d team here. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of servicepriva", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - research - equalizing - credit - opportunity - in - algorithms", "metadata": {"source": "https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms", "row": 195, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 196 text : data drift detection part i : multivariate drift with tabular data solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##vabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringdata drift detection part i : multivariate drift with tabular databy : keegan hines and reese hydejuly 12, 2022note : this is the first in a series of posts where we take a deeper dive into the question of data", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "hines and reese hydejuly 12, 2022note : this is the first in a series of posts where we take a deeper dive into the question of data drift detection. we explore not only why it is an important part of model monitoring, but we also discuss regimes and approaches to keep in mind. in the first part of the series, we discuss drift in the context of tabular data and describe univariate and multivariate techniques for tackling these problems. in the follow - on posts, we \u2019 ll dive into unstructured data, such as images and documents, and discuss how we can build data drift detection systems", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the follow - on posts, we \u2019 ll dive into unstructured data, such as images and documents, and discuss how we can build data drift detection systems in these more challenging regimes. part i : multivariate data drift with tabular datamonitoring the ongoing success of a machine learning model requires monitoring the data coming into that model. this means ensuring the data coming through today looks exactly how you expect it to look. ultimately, you want to make sure the data looks typical : does it look the same way it did when the model was first trained? if the data has changed significantly, your trained model is likely stale and", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "looks typical : does it look the same way it did when the model was first trained? if the data has changed significantly, your trained model is likely stale and resulting in inaccurate predictions. whether you \u2019 re talking about tabular numeric data, image data, or nlp data, the data monitoring problem remains the same. in all cases, we will have some sense of what the data ought to look like and then alert when things go astray. in technical terminology, this is often referred to as out - of - distribution detection : we want to find when the data no longer adheres to the shape and distribution that it used to", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is often referred to as out - of - distribution detection : we want to find when the data no longer adheres to the shape and distribution that it used to ( back when the model was trained ). there are many ways of thinking about data drift detection, and in this post, we \u2019 ll describe the benefits of a high - dimensional and multivariate approach. a handy approach to begin thinking about data drift detection is to measure the distributional similarity between the data coming through a model now versus how the data is supposed to look, such as in the training set. a great starting approach is to separately look at each input variable to", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a model now versus how the data is supposed to look, such as in the training set. a great starting approach is to separately look at each input variable to a model ( and outputs as well ). this so - called univariate drift approach can be tackled with many technical implementations. common approaches include hypothesis tests, such as ks test and chisquared test, and the so - called f - divergences, such as the kldivergence, jsdivergence, or similar. common to all of these approaches, we would typically apply them in a univariate way to each input feature to a model (", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ivergence, or similar. common to all of these approaches, we would typically apply them in a univariate way to each input feature to a model ( see figure 1 ). figure 1 : comparing distributions. in this diagram, we examine a single input feature ( age ) and look at the distribution of this variable at two time points : in the training data ( green distribution ), and in today \u2019 s production data ( purple distribution ). it is clear that the general shape of this distribution has changed quite a bit. this could lead to model inaccuracy. a higher - dimensional variant can be calculated in theory, but", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "shape of this distribution has changed quite a bit. this could lead to model inaccuracy. a higher - dimensional variant can be calculated in theory, but these methods are ineffective in high - dimensional applications due to data sparsity. primarily, by using univariate measures for drift detection, we make an implicit assumption of feature independence. while this might be approximately true in some cases, most generally, our dataset likely has some complex interactions between features and other significant structures. importantly, this can lead to missed events when we consider only one feature at a time. therefore, we must consider the high - dimensional joint distribution of the", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". importantly, this can lead to missed events when we consider only one feature at a time. therefore, we must consider the high - dimensional joint distribution of the data. in a multivariate approach, we fit a multi - dimensional ancillary model to the full joint distribution of the training set. this ancillary model will act as a density model and learn the patterns and structure in the dataset. this model can quantify how much any given datapoint is typical or atypical relative to the reference dataset. in implementation, there are many potential approaches for this density model \u2014 examples include things like a variational auto", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or atypical relative to the reference dataset. in implementation, there are many potential approaches for this density model \u2014 examples include things like a variational autoencoder, normalizing flow models, density models, isolation forest, and so on. any technique which is flexible should be able to work effectively. then, we can use this learned density model to evaluate future datapoints on how similar they are to the training data. this approach is explained further in the sketch below, which shows a simplified view of the process. on the left, imagine we have a training dataset ; in this case, it entails only a couple", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", which shows a simplified view of the process. on the left, imagine we have a training dataset ; in this case, it entails only a couple of continuous variables ( x1 and x2 ). we have a small number of sample datapoints scattered around this space of x1 and x2. the dataset has some particular patterns and structure to it ( the curved relationship between x1 and x2 ), perhaps unknown to us. in step two, we fit a density model to this dataset. for brevity, we omit implementation details here, but the overall goal is to quantify where in the", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "fit a density model to this dataset. for brevity, we omit implementation details here, but the overall goal is to quantify where in the x1 - x2 space we saw lots of data and where we saw no data. that is indicated in this sketch by the shaded contours : the darkest shading suggests areas where we were very likely to see some data, and blank areas show where we didn \u2019 t see any data at all. in step 3, we can use this trained density model to score any new datapoints in terms of how likely they would have been seen, per the training data. another", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "3, we can use this trained density model to score any new datapoints in terms of how likely they would have been seen, per the training data. another way to think about this is to score a new datapoint based on whether it adheres to the typical shapes / patterns in the training set or if it is abnormal. as an example, one of the datapoints is green because it falls right in line with the \u201c typical \u201d regions of the density model. this datapoint is very similar to other data in the training set. in contrast, the red datapoint is found in a region where none of the training data was ever", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##point is very similar to other data in the training set. in contrast, the red datapoint is found in a region where none of the training data was ever seen. in this way, this datapoint is an anomaly and is unlike anything in the training set. in technical terminology, this point is said to be \u201c out of distribution \u201d relative to the training set. in the example sketched here, note that the univariate drift measures would likely fail to notice the anomalous datapoint. when viewed in a univariate sense ( against either x1 or x2 ), this anomalous datapoint is quite typical", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##omalous datapoint. when viewed in a univariate sense ( against either x1 or x2 ), this anomalous datapoint is quite typical. however, because x1 and x2 have a complex structure, we find that the red datapoint is quite different from the training data. when we fail to consider the multivariate case, we can miss many subtle shifts where the production data falls off the data manifold. this form of out - of - distribution detection is an important part of monitoring the health of a machine learning system. it is becoming increasingly important that ml models can have understanding of their own uncertainty and confidence", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "detection is an important part of monitoring the health of a machine learning system. it is becoming increasingly important that ml models can have understanding of their own uncertainty and confidence. in many cases, this amounts to uncertainty over its predictions, given an input. however, with out - of - distribution detection, we can understand what the model thinks the world looks like and we can flag when things are quite a bit different. this is useful because complex ml models are often overconfident in their predictions, especially for data that is unlike what they were trained on. by considering whether each input is in - sample or out - of - sample, we", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "their predictions, especially for data that is unlike what they were trained on. by considering whether each input is in - sample or out - of - sample, we can better quantify when to trust a model \u2019 s prediction and when to be leerier. in this post, we introduced ideas about out - of - distribution detection for the context of tabular data. but this problem is pervasive for all types of machine learning. in the next post, we dive into these ideas for computer vision models and nlp models. see how easy the arthur platform enables you easily to detect and react to data drift in our product deep dive.", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ideas for computer vision models and nlp models. see how easy the arthur platform enables you easily to detect and react to data drift in our product deep dive. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / data", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##press inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / data - drift - detection - in - high - dimensional - and - unstructured - data - part - i - multivariate - data - drift - with - tabular - data", "metadata": {"source": "https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data", "row": 196, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 197 text : what \u2019 s missing from your model governance strategy? solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringwhat \u2019 s missing from your model governance strategy? by : arthur teamjuly 8, 2022if you \u2019 re an enterprise with ai models in production across multiple lines of business, chances are you already have a model governance plan in place to comply with business, operational", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 re an enterprise with ai models in production across multiple lines of business, chances are you already have a model governance plan in place to comply with business, operational, and regulatory requirements. but governance goes beyond access controls, fancy arrow flow charts, policy pdfs, and checklists. how can you ensure your resulting model is accurate, robust, and reliable over time? model monitoringmodel monitoring is a critical part of the ai lifecycle that enables data science teams to detect \u2014 and ultimately address \u2014 issues like data drift and algorithmic bias, while providing the necessary tools for correcting performance issues in the real world. according to harvard business", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and ultimately address \u2014 issues like data drift and algorithmic bias, while providing the necessary tools for correcting performance issues in the real world. according to harvard business review, nearly 80 % of ai projects won \u2019 t scale beyond proof of concept. of the 20 % remaining, it can take from 3 months to 2 years to roll a model into production. given both the significant time and investment poured into ml model development, experimentation, training, and deployment, why wouldn \u2019 t you want to ensure that your model performs well post - production? what is worth monitoring in a model? 1. performancemodel performance metrics can be a hybrid", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "want to ensure that your model performs well post - production? what is worth monitoring in a model? 1. performancemodel performance metrics can be a hybrid depending on model complexity. regression : mae, mse, rmse, residual histogram, predicted vs. true, forecast horizonclassification : accuracy, confusion matrix, precision / recall tradeoff, f - 1 score, au - roc, lift curve, cumulative gains curve, calibration curve, etc. these measures of accuracy are only useful, however, if you have access to the ground truth of your model. in a production environment, ground truth can sometimes take", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these measures of accuracy are only useful, however, if you have access to the ground truth of your model. in a production environment, ground truth can sometimes take days, weeks, or even years to acquire ( if it is available at all ). in order to assess the performance of models without these prohibitive delays, proxies such as data drift can be used as a real - time barometer of your model \u2019 s activity. drift metrics can be a meaningful leading indicator of the need for models to be retrained or restructured. 2. drift and degradationdata driftover time, data changes. data drift is", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "indicator of the need for models to be retrained or restructured. 2. drift and degradationdata driftover time, data changes. data drift is defined as a variation in the production data from the data that was used to train, test, and validate the model before putting it into production. the first two questions we often ask ourselves when we are faced with data drift are \u201c when did this happen? \u201d and \u201c how did this happen? \u201d coincidentally, those are also some of the best ways to characterize different types of data drift. we can start to characterize how drift occurs by looking at the distributional makeup", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "are also some of the best ways to characterize different types of data drift. we can start to characterize how drift occurs by looking at the distributional makeup of the drift ( i. e. what pieces of our data are drifting ). we can then look at when the drift occurs by characterizing drift by intensity or timeline. covariate driftcovariate drift is one of our most common types of real - world drift. it occurs when there is a change in the feature space of the model \u2014 or, in other words, when the distribution of one or more of the features has changed. concept drift concept drift is a", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the feature space of the model \u2014 or, in other words, when the distribution of one or more of the features has changed. concept drift concept drift is a change in the relationship between input and output data variables over time. this change can be gradual, reoccurring, or sudden. oftentimes in production, we may not have access to the ground truth immediately. for example, if we are predicting whether or not a customer will default on a loan, we may not know if we are correct for months \u2014 or even years. in cases like these, it can be useful to also evaluate the relationship between features and your predicted", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "not know if we are correct for months \u2014 or even years. in cases like these, it can be useful to also evaluate the relationship between features and your predicted values, instead of just the true ( or ground truth ) target variables. distribution drift common in real - world model scenarios is distribution drift. a key feature of distributional drifts in production is that they are silent. arthur provides the ability to both monitor and set alerts to detect for different data distributional drifts, using both statistical and model - based methodologies. model degradation, or model decay, happens when a model \u2019 s performance becomes less reliable over time due to", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", using both statistical and model - based methodologies. model degradation, or model decay, happens when a model \u2019 s performance becomes less reliable over time due to changes in the environment. when data drift occurs, the production - time assumption we made about the similarity between our training dataset and our production dataset is no longer true, which can cause the model \u2019 s decay. 3. explainability end users, data scientists, business leaders, and regulators need to understand how models make decisions. improving model transparency can reduce model development and debugging time, highlight areas of concern for data drift and bias, and increase overall trust in the", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "make decisions. improving model transparency can reduce model development and debugging time, highlight areas of concern for data drift and bias, and increase overall trust in the model. explainability vs. interpretabilitymodel interpretability refers to models that are inherently understandable to humans. these models are simple enough that a human looking at the logic and internals of the model can understand how the model makes an inference given a particular input. in practice, few models are truly interpretable. instead, there is often a tradeoff between interpretability and performance \u2014 especially for models performing complex tasks. high - performing models that do complex tasks are often the", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "instead, there is often a tradeoff between interpretability and performance \u2014 especially for models performing complex tasks. high - performing models that do complex tasks are often the least interpretable models. the goal of model explainability is to provide visibility into models that are too complex to be inherently interpretable. this often requires additional models and other techniques to generate explanations that are comprehensible to humans. arthur \u2019 s platform offers powerful explainability techniques to provide prediction - level and whole model - level visibility into any model, including advanced \u201c what if \u201d analysis and feature importance ranking. local vs. global explainersglobal explainers provide ho", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- level visibility into any model, including advanced \u201c what if \u201d analysis and feature importance ranking. local vs. global explainersglobal explainers provide holistic model - level explanations. global explanations are often presented as a summary of feature importance across the entire model. these explanations show which input features make the greatest impact on the output predictions of the model. since global explanations serve as simplified summaries of model behavior, they may not be accurate for specific data samples. however, they can help data scientists contextualize data drift to understand when a model needs to be retrained. this is especially useful when ground truth labels are", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", they can help data scientists contextualize data drift to understand when a model needs to be retrained. this is especially useful when ground truth labels are unavailable. global explainers are also useful to identify differences between groups for bias / fairness or debugging purposes. local explainers provide a hypothesis of why a model made the prediction it did given a specific input sample. these explanations are useful in providing specific explanations to end users. they can also be helpful to data scientists when trying to identify and understand the cause of specific production issues. local explanations can be aggregated across many samples to form global explanations. types of explanationsfor explain", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "when trying to identify and understand the cause of specific production issues. local explanations can be aggregated across many samples to form global explanations. types of explanationsfor explainers to be useful, they must present explanations in a way that is comprehensible and intuitive to humans. this is often presented in the form of data visualizations for feature importance. explanations for tabular data can be intuitively represented as a bar plot of feature importances. explanations for computer vision models and image data are provided by highlighting the most significant regions of an image, while natural language processing models can be explained by highlighting significant words and phrases. explainer algorithmsarth", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data are provided by highlighting the most significant regions of an image, while natural language processing models can be explained by highlighting significant words and phrases. explainer algorithmsarthur leverages the industry standard lime and shap algorithms to provide local and global explanations for tabular, computer vision, and natural language processing models. both algorithms create simplified surrogate models to provide local explanations, which can be aggregated into global explanations. lime and shap are model - agnostic explainers, meaning that they can generate explanations for any type of model, without accessing the internal logic and parameters of the model. depending on the particular model and use case,", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that they can generate explanations for any type of model, without accessing the internal logic and parameters of the model. depending on the particular model and use case, a data scientist may favor one of these algorithms over the other. on the arthur platform, lime is used for image and text data, while shap is used for tabular data. 4. bias / fairnesslocal and federal regulations around detecting and addressing bias are in the works ( see nyc hiring law, algorithmic accountability act of 2022 ). we have learned that traditional approaches which equate to \u201c fairness through unawareness \u201d simply do not work. in this case", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "accountability act of 2022 ). we have learned that traditional approaches which equate to \u201c fairness through unawareness \u201d simply do not work. in this case, ignorance is not bliss, and not enough to address existing / upcoming regulations. this unawareness - based approach may meet the requirements of avoiding the discriminatory practice known as disparate treatment, but does not address the possibility of the discriminatory practice known as disparate impact ( this distinction is rooted in the civil rights act of 1964 ). essentially, a model that does not take into account membership of a protected class can still have adverse effects on members of that", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the civil rights act of 1964 ). essentially, a model that does not take into account membership of a protected class can still have adverse effects on members of that protected class. detecting bias and discriminatory practices requires actively probing your data to see if groups are being treated unfairly. arthur does this active probing for you and makes it easy to detect bias by making comparisons between subgroups, even if that group identity is not being used as an input to your model. there are a number of different metrics to quantify fairness. the three most common ones are demographic parity, equality of opportunity, and equalized odds. arthur allows", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a number of different metrics to quantify fairness. the three most common ones are demographic parity, equality of opportunity, and equalized odds. arthur allows you to quickly identify, quantify, and visualize the degree of bias / fairness ( using standard or custom fairness metrics ) in your model \u2019 s outputs. if bias is identified, arthur can help mitigate that bias based on post - processing techniques which do not require fundamentally changing your training data or model architecture. summarydon \u2019 t forget that an essential part of model governance is tracking model health post - production through model monitoring. automated monitoring of performance, drift, model degradation", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "summarydon \u2019 t forget that an essential part of model governance is tracking model health post - production through model monitoring. automated monitoring of performance, drift, model degradation, explainability, bias, and fairness as well as alerts / notifications of potential issues is an important aspect of ensuring responsible ai in your mlops lifecycle. want to learn more about arthur? schedule a demo with one of our experts to see the ai monitoring, explainability, and bias analytics platform in action. photo by tingey injury law firm on unsplashprevious postsharenext post we make ai better for everyone. sign up for our newsletter to", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". photo by tingey injury law firm on unsplashprevious postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / whats - missing - from - your - model - governance - strategy", "metadata": {"source": "https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy", "row": 197, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 198 text : arthur recognized in 2022 gartner\u00ae hype cycle\u2122 for data and analytics governance solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##firewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur recognized in 2022 gartner\u00ae hype cycle\u2122 for data and analytics governanceby : arthur teamjuly 1, 2022new york, ny \u2013 arthur, the ai performance company, was listed as a", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for data and analytics governanceby : arthur teamjuly 1, 2022new york, ny \u2013 arthur, the ai performance company, was listed as a sample vendor for both ai governance and responsible ai in 2022 gartner hype cycle report for data and analytics governance. this gartner report highlights how \" data and analytics leaders must understand the hype and progress of governance practice and technology innovations so their adoption delivers organizational value at the right time. \" \u201c with ai now delivering value in practical enterprise application, data and analytics leaders see that scaling ai without governance is ineffective and dangerous, \u201d the gartner report states.", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai now delivering value in practical enterprise application, data and analytics leaders see that scaling ai without governance is ineffective and dangerous, \u201d the gartner report states. as production ai continues to drive meaningful business value, arthur is working with top enterprises to monitor, measure, and improve ml to drive better, more responsible results. arthur is deployed at leading fortune 100 enterprises across financial services, healthcare, retail, and technology, working together to accelerate mlops and optimize for accuracy, explainability, and fairness. \u201c responsible ai means a deliberate approach in many directions at once, \u201d the gartner report states. \u201c data science \u2019 s", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "explainability, and fairness. \u201c responsible ai means a deliberate approach in many directions at once, \u201d the gartner report states. \u201c data science \u2019 s responsibility to deliver unbiased, trusted and ethical ai is just the tip of the iceberg. responsible ai helps ai participants develop, implement, utilize and resolve the dilemmas they face. \u201d \u201c we \u2019 re working with data scientists, product owners, and business leaders to ensure that their ml models are delivering accurate, transparent, and fair results \u2014 not only for the businesses themselves, but for the people they serve. \" - adam wenchel, arthur co - founder and ceo", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "transparent, and fair results \u2014 not only for the businesses themselves, but for the people they serve. \" - adam wenchel, arthur co - founder and ceothe report underscores the importance of model monitoring, explainability, and fairness in building truly ai - native enterprises. \u201c arthur is proud to be deployed across business - critical use cases at leading enterprises at an especially essential time in the ai industry \u2019 s maturity, \u201d said adam wenchel, co - founder and ceo of arthur. \u201c we \u2019 re working with data scientists, product owners, and business leaders to ensure that their ml models are delivering accurate, transparent, and fair", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of arthur. \u201c we \u2019 re working with data scientists, product owners, and business leaders to ensure that their ml models are delivering accurate, transparent, and fair results \u2014 not only for the businesses themselves, but for the people they serve. \u201d as more and more businesses are turning to ml capabilities like computer vision and natural language processing to solve key challenges, arthur is the only ai performance solution for cv and the industry leader in nlp performance management \u2014 in addition to exclusive support for multi - class, multi - label, and regression tabular models. arthur is model - agnostic and platform - agnostic, providing a centralized dashboard and a", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "multi - class, multi - label, and regression tabular models. arthur is model - agnostic and platform - agnostic, providing a centralized dashboard and a flexible query api that allow enterprises to democratize ml performance and tie it directly to business kpis. earlier this year, arthur was mentioned in 2022 gartner innovation insight for bias detection / mitigation, explainable ai and interpretable ai report [ 2 ], and in infographic : heat index : gartner \u2019 s 2021 connecting hot, hype and cool [ 3 ]. arthur was named a 2021 gartner cool vendor in ai governance and responsible ai", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##rtner \u2019 s 2021 connecting hot, hype and cool [ 3 ]. arthur was named a 2021 gartner cool vendor in ai governance and responsible ai [ 4 ]. if you are a gartner client you can access the 2022 hype cycle report here. learn more about arthur and sign up for a demo at www. arthur. ai. references [ 1 ] gartner, \u201c hype cycle for artificial intelligence, 2021 \u201d, shubhangi vashisth, svetlana sicular, 29 july 2021 [ 2 ] gartner, \u201c innovation insight for bias detection / mitigation,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "vashisth, svetlana sicular, 29 july 2021 [ 2 ] gartner, \u201c innovation insight for bias detection / mitigation, explainable ai and interpretable ai \u201d, van baker, svetlana sicular, avivah litan, erick brethenoux, 9 february 2022 [ 3 ] gartner, \u201c infographic : gartner \u2019 s 2021 heat index : connecting hot, hype and cool \u201d, michael woodbridge, thomas bittman, sam olyaei, rita sallam, 16 february 2022 [ 4 ] gartner, \u201c cool vendors\u2122 in ai", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bridge, thomas bittman, sam olyaei, rita sallam, 16 february 2022 [ 4 ] gartner, \u201c cool vendors\u2122 in ai governance and responsible ai \u201d, svetlana sicular, moutusi sau, et al, 10 june 2021disclaimer : gartner, cool vendors and hype cycle are a registered trademark and service mark of gartner, inc. and / or its affiliates in the u. s. and internationally and are used herein with permission. gartner does not endorse any vendor, product or service depicted in its research publications, and does not", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "internationally and are used herein with permission. gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. gartner research publications consist of the opinions of gartner's research organization and should not be construed as statements of fact. gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. photo by milad fakurian on unsplashprevious postsharenext", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", including any warranties of merchantability or fitness for a particular purpose. photo by milad fakurian on unsplashprevious postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - featured - in -", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##3 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - featured - in - gartner - 2022 - hype - cycle - for - data - and - analytics - governance", "metadata": {"source": "https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance", "row": 198, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 199 text : why only 12 % of companies have achieved \u2018 ai maturity \u2019 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai research & innovationwhy only 12 % of companies have achieved \u2018 ai maturity \u2019 by : arthur teamjune 27, 2022a recent accenture report called \u201c the art of ai maturity : advancing from practice to performance \u201d went into detail about the journey some companies have taken", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", 2022a recent accenture report called \u201c the art of ai maturity : advancing from practice to performance \u201d went into detail about the journey some companies have taken from just testing the waters of ai to achieving a level of sophistication that is having massive positive impacts on their business. precisely, \u201c some companies \u201d refers to the 12 % of firms that, according to the report, \u201c have advanced their ai maturity enough to achieve superior performance and growth. \u201d the report also refers to these companies as \u201c ai achievers. \u201d another 25 % of firms are \u201c somewhat advanced in their level of ai maturity, \u201d the report states,", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "refers to these companies as \u201c ai achievers. \u201d another 25 % of firms are \u201c somewhat advanced in their level of ai maturity, \u201d the report states, while the remaining 63 % are still in the early stages. so, what exactly is ai maturity and why does it matter? accenture defines ai maturity as \u201c the degree to which organizations have mastered ai - related capabilities in the right combination to achieve high performance for customers, shareholders, and employees. \u201d it \u2019 s become increasingly clear in recent years that harnessing the power of ai is crucial for businesses to have a competitive advantage. in fact, among executives of the world \u2019 s", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "increasingly clear in recent years that harnessing the power of ai is crucial for businesses to have a competitive advantage. in fact, among executives of the world \u2019 s 2, 000 largest companies, those who discussed ai on their earnings calls were 40 % more likely to see their firms \u2019 share prices increase. if most organizations are racing to embrace ai, why are some seeing more value than others? ai achievers, according to the report, are not defined by the sophistication of any one capability, but by their ability to combine strengths across strategy, processes, and people. here are five key success factors for achievers : their top", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of any one capability, but by their ability to combine strengths across strategy, processes, and people. here are five key success factors for achievers : their top leaders champion ai as a strategic priority for the entire organization. companies can create strong ai strategies, but unless those strategies receive enthusiastic support from the ceo and the rest of the c - suite, they \u2019 re likely to flounder. when it comes to achievers, 83 % of these companies have ceo and senior sponsorship. they invest heavily in talent to get more from their ai investments. this senior sponsorship allows organizations to invest heavily in creating data and ai fluency across their workforce", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". they invest heavily in talent to get more from their ai investments. this senior sponsorship allows organizations to invest heavily in creating data and ai fluency across their workforces. 78 % of achievers have mandatory ai trainings for most employees, from product development engineers to c - suite executives. they industrialize ai tools and teams to create a strong ai core. an ai core is an operational data and ai platform that taps into companies \u2019 talent, technology, and data ecosystems, allowing firms to balance experimentation and execution. they design ai responsibly, from the start. as companies deploy ai for a growing range of tasks, adhering", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "firms to balance experimentation and execution. they design ai responsibly, from the start. as companies deploy ai for a growing range of tasks, adhering to laws, regulations, and ethical norms is critical to building a sound data and ai foundation. achievers are 53 % more likely than other companies to be responsible by design : designing, developing, and deploying ai with good intention to empower employees and businesses, and to fairly impact customers and society \u2014 allowing companies to engender trust and scale ai with confidence. they prioritize long - and short - term ai investments. one reason achievers get more out of ai", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "engender trust and scale ai with confidence. they prioritize long - and short - term ai investments. one reason achievers get more out of ai is simply because they invest more in it. achievers also understand that their ai investment journey doesn \u2019 t have a finish line and that there is no \u201c peak ai. \u201d these companies know they have only scratched the surface of their ai transformations and that the quality of their investments matters just as much as the quantity. echoing the report \u2019 s findings, arthur \u2019 s customers self - identify as ai experimenters ( 63 % ) or ai innovators ( 13 % ), with", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the report \u2019 s findings, arthur \u2019 s customers self - identify as ai experimenters ( 63 % ) or ai innovators ( 13 % ), with longer - term aspirations of evolving into the ai builder or achiever categories. advancing from practice to performance is a roadmap typically spanning a two - year time horizon for most enterprises. maturity isn \u2019 t a one - size - fits - all path, either. while organizations may be farther along in ml / ai development and management maturity, we \u2019 ve discovered these same organizations are still in infancy for model monitoring and validation maturity. they \u2019 re still logging inferences, manually jug", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and management maturity, we \u2019 ve discovered these same organizations are still in infancy for model monitoring and validation maturity. they \u2019 re still logging inferences, manually juggling python notebooks, and facing problems from fragmented or restricted data stacks for day - to - day model data science workflows. the report says it best : \u201c advancing ai maturity is no longer a choice. it \u2019 s an opportunity facing every industry, every organization, and every leader. \u201d the good news is that the share of ai achievers will increase rapidly and significantly in the next few years, more than doubling from the current 12 % to 27 % by 2024", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that the share of ai achievers will increase rapidly and significantly in the next few years, more than doubling from the current 12 % to 27 % by 2024. will your organization be one of them? previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / /", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##tationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / why - only - 12 - of - companies - have - achieved - ai - maturity", "metadata": {"source": "https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity", "row": 199, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 200 text : how arthur \u2019 s tech stack is built for scalability solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedproduct featureshow arthur \u2019 s tech stack is built for scalabilityby : arthur teamjune 13, 2022with the increasing number of enterprise ml models in production, the rising demand for localized models, and the focus on more robust model monitoring, scalability is more important", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##with the increasing number of enterprise ml models in production, the rising demand for localized models, and the focus on more robust model monitoring, scalability is more important than ever. consequently, ml model monitoring platforms need to be built with a highly scalable architecture that can do everything your organization needs it to do at production grade. an ml model monitoring architecture that isn \u2019 t scalable can result in a number of technical challenges, including lack of responsiveness, increased infrastructure cost, and platform inelasticity. if your model monitoring system isn \u2019 t built to scale, your models \u2014 and ultimately, your business \u2014 will suffer the consequences.", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "platform inelasticity. if your model monitoring system isn \u2019 t built to scale, your models \u2014 and ultimately, your business \u2014 will suffer the consequences. unlike less resilient ml model monitoring and observability solutions, the arthur platform is built for high performance and scalability from the ground up. keep reading to find out why arthur \u2019 s tech stack makes it the leading platform for enterprises that want to run high - performing ml models at scale. 1. database managementarthur leverages the strengths of both clickhouse and postgres to handle different types of workloads. clickhouse, used for olap work", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "managementarthur leverages the strengths of both clickhouse and postgres to handle different types of workloads. clickhouse, used for olap workloads, is a horizontally scalable database management system that allows for high insert rates and fast serving of complex queries against very large data for multi - dimensional analysis. postgres, used for oltp workloads, is one of the best relational databases. with its reliable transactional mechanism, it organizes the users and the models to support the recording and the querying of respective model metrics. clickhouse and postgres work together to provide the arthur platform and", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "users and the models to support the recording and the querying of respective model metrics. clickhouse and postgres work together to provide the arthur platform and its end users with an optimal level of flexibility. 2. auto - scaler mechanism & streaming - first architectureas previously mentioned, components in arthur \u2019 s platform are independently and horizontally scalable. the platform \u2019 s auto - scaler mechanism self - manages and optimizes resource utilization on a kubernetes cluster, automatically scaling up and down based on platform activities as well as the lag observed in the data pipeline queue. whether the data is coming from streaming or batch", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", automatically scaling up and down based on platform activities as well as the lag observed in the data pipeline queue. whether the data is coming from streaming or batch, arthur \u2019 s streaming - first architecture allows a very large volume of data to be ingested reliably and efficiently in a non - blocking fashion. for queuing, arthur uses apache kafka, which was built for streaming big data and is ideal for mlops use cases such as high - throughput activity tracking, stream processing, event sourcing, and log aggregation. 3. high - performance programming languagearthur \u2019 s platform core is written in the go programming", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "tracking, stream processing, event sourcing, and log aggregation. 3. high - performance programming languagearthur \u2019 s platform core is written in the go programming language, which was developed by google and is used by leading enterprises like uber and dropbox. go was chosen for a few reasons : it \u2019 s compiled to machine code, its runtime performance is up to 30x faster than languages that are interpreted or have virtual runtime, and it \u2019 s built for concurrency and parallelism. at the end of the day, scalability is far more than just a \u201c nice to have. \u201d as your organization grows and ml projects out", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "parallelism. at the end of the day, scalability is far more than just a \u201c nice to have. \u201d as your organization grows and ml projects outpace their original deployment, your model monitoring system must be resilient and flexible to adjust to the ever - growing volume of data. the arthur platform was not only built with all of this in mind, but it also continues to be optimized to ensure models are proactively scaling and ultimately maximizing value for your business. read more about the importance of scalability and performance in our whitepaper here. want to dig into arthur \u2019 s tech stack even more? check", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "your business. read more about the importance of scalability and performance in our whitepaper here. want to dig into arthur \u2019 s tech stack even more? check out our dev docs and resources. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##companyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / how - arthurs - tech - stack - is - built - for - scalability", "metadata": {"source": "https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability", "row": 200, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 201 text : 3 strategies for maintaining your ml talent solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnl", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedbest practices3 strategies for maintaining your ml talentby : arthur teamjune 16, 2022a 2021 365 data science ltd study revealed that data science professionals remain with an employer for only 1. 7 years on average. in fact, the sudden impulse to write a data science resignation letter doesn \u2019 t", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data science professionals remain with an employer for only 1. 7 years on average. in fact, the sudden impulse to write a data science resignation letter doesn \u2019 t change depending on where you work in the world. the study data sampled data science professionals from the us ( 35 % ), the uk ( 25 % ), the eu ( 25 % ), and india ( 15 % ). so, when it comes to maintaining talent, what can employers do to combat data scientists \u2019 desire to change jobs and keep them fulfilled in their roles? # 1 pay competitive salaries and incentivize with annual bonuses and equity refreshes. currently", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "desire to change jobs and keep them fulfilled in their roles? # 1 pay competitive salaries and incentivize with annual bonuses and equity refreshes. currently, demand outstrips supply. it \u2019 s simple economics. according to the u. s. bureau of labor statistics, the expected job market growth rate for data science and related fields is 31 %, far greater than the overall average job growth rate of 4 %. when there are more jobs than candidates, data scientists can easily jump jobs and get a pay bump because they have more salary bargaining power. # 2 carve out and create dedicated weekly innovation project time for team members", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "can easily jump jobs and get a pay bump because they have more salary bargaining power. # 2 carve out and create dedicated weekly innovation project time for team members. in a recent medium blog post authored by ian xao, he noted that \u201c data science teams at big companies are facing a serious retention problem \u201d as scientists are quitting enterprise companies for new jobs at startups. xao pointed out that many data scientists have an expectation - reality gap, leading to day - to - day job disillusion. data scientists desire to work with advanced algorithms and troubleshoot models \u2014 but they end up spending up to 40 % of their time", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- day job disillusion. data scientists desire to work with advanced algorithms and troubleshoot models \u2014 but they end up spending up to 40 % of their time on data collection, data cleaning, and tedious administrative manual data export - import activities to keep up with reporting demands. data scientist natassha selvaraj echoed xao \u2019 s sentiment on a recent kdnuggets post : \u201c at the end of the day, you \u2019 re no longer building complex algorithms and predictive models like you imagined. you now spend all your time brushing up on sql and data preparation skills to pull data out of the system into different formats", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and predictive models like you imagined. you now spend all your time brushing up on sql and data preparation skills to pull data out of the system into different formats, and present this data to stakeholders so they can use it to make business decisions. although your job title has the word \u2018 data science \u2019 in it, you aren \u2019 t in a role you \u2019 ve always pictured yourself in. you \u2019 re unhappy being the company \u2019 s data janitor, and want to work on projects that will actually utilize the skills you \u2019 ve spent so long to gain. \u201d giving data scientists autonomy to work on model innovation or experimentation ( i. e", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "projects that will actually utilize the skills you \u2019 ve spent so long to gain. \u201d giving data scientists autonomy to work on model innovation or experimentation ( i. e. passion projects ) for a set percentage of time ( 10 % to 20 % ) each week keeps your employees engaged and empowered because they feel a sense of ownership and control over their role. # 3 automate documentation workflows and reporting activities to reduce admin burdens. documentation, documentation, documentation ( cue jaws theme ). as ai / ml regulation matures and places greater pressure on higher up model risk managers to document rationale and explainability behind data sets and model", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "theme ). as ai / ml regulation matures and places greater pressure on higher up model risk managers to document rationale and explainability behind data sets and model decision making, repetitive reporting asks often trickle down to data scientists and become a dreaded chore. this can range from design docs to writing model cards for communicating to external or non - technical stakeholders deployed model context, performance evaluation criteria and other relevant information. or it could be a more in - depth report to satisfy an dpia ( data protection impact assessment ), internal, or third - party audit requirement. investigate mlops platforms that can automate a team \u2019 s manual", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "an dpia ( data protection impact assessment ), internal, or third - party audit requirement. investigate mlops platforms that can automate a team \u2019 s manual, repeatable activities. by automating univariate and multivariate data drift detection, event logging, data exporting for monthly / quarterly reporting requirements and more, your team can break the chains from day - in, day - out data science \u201c janitorial \u201d maintenance and focus on complex model diagnosis and mitigation solutions. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribe", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "mitigation solutions. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / 3 - strategies - for - maintaining - your - ml - talent", "metadata": {"source": "https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent", "row": 201, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 202 text : learnings from ttc summit & good tech fest solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedeventslearnings from ttc summit & good tech festby : daniel nissanimay 25, 2022earlier this month, i attended the trust, transparency, and control labs ( ttc labs ) summit on trustworthy ai experiences and good tech fest. each conference offered sessions", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "month, i attended the trust, transparency, and control labs ( ttc labs ) summit on trustworthy ai experiences and good tech fest. each conference offered sessions that centered around one central theme : how do we make technology that is in service of humankind? they wrestled with tensions in regulatory practices that make it hard for governments to enforce ai protections, discussed the nuance and meaning necessary for good explanations in ai systems, and dissected the privilege present in ai ethics today and how we can change it by engaging communities that will be harmed at the onset. the conference left me thinking that transparent ai systems, standardized international regulatory mechanisms, and", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "how we can change it by engaging communities that will be harmed at the onset. the conference left me thinking that transparent ai systems, standardized international regulatory mechanisms, and impact assessments can help ai be good for everyone. read on to see how i arrived at these learnings. tensions around regulatory practicesduring the transparency & explainability in ai regulation panel, several tensions arose around the ai regulatory space. eu laws like gdpr and the forthcoming ai act are focused on citizen rights, whereas u. s. federal laws around data, such as hipaa, are focused on sectoral regulation. this nuance in the intention of the laws makes it", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "u. s. federal laws around data, such as hipaa, are focused on sectoral regulation. this nuance in the intention of the laws makes it hard to come up with frameworks and guidance for organizations to follow, as more and more organizations operate internationally. and even if these frameworks provided good guidelines ( as some do, such as the ai and data protection risk toolkit ico ), creating tooling to automate and manage these regulations throughout an organization is still a challenging and cumbersome problem. i think my favorite part of the panel was an acknowledgement that until society writ large has an intuitive understanding of ai", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "still a challenging and cumbersome problem. i think my favorite part of the panel was an acknowledgement that until society writ large has an intuitive understanding of ai concepts, enforcing regulations is going to be extremely difficult. even something as simple as defining an ai system is quite challenging. according to one of the speakers during the ai & society : demonstrating accountability panel, the ai act actually provides some guidance on what counts as an ai system. via this guidance, a manually coded decision tree, with rules hard coded, is not considered an ai system \u2014 but a trained decision tree, that learns the same exact decision rules based on a training dataset", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", with rules hard coded, is not considered an ai system \u2014 but a trained decision tree, that learns the same exact decision rules based on a training dataset, does. this implies that the former does not need to abide by the laws that would be enacted by the ai act. although definitions will always contain loopholes, acknowledging them is a first step to refining definitions over time. going past explainable modelsresearchers from google walked us through some of their explainability case studies, a tool they use to help them think through the nuances behind explainability in ai systems. what fascinated me most was that the case study we", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "case studies, a tool they use to help them think through the nuances behind explainability in ai systems. what fascinated me most was that the case study we did together made us think past the conventional idea of explaining why a model had a certain prediction. we discussed the amount of information presented, the way the information was framed, and how the urgency or necessity of such information should prompt different types of responses from ai systems. these are fine - grained ideas to grapple with, and start to move into the territory of design or hci work around explainability. one of the panelists on the where control happens panel emphasized on multiple", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", and start to move into the territory of design or hci work around explainability. one of the panelists on the where control happens panel emphasized on multiple occasions that explanations need to be meaningful. it isn \u2019 t just that an explanation needs to happen, but it needs to be useful for a user, possibly giving them agency that they otherwise would not have had. tenets of ai ethicswhen trying to design ethical ai systems, we need to think about those who are involved in creating the system and those who are impacted by it. at both conferences, this idea was brought up repeatedly as something that needs to be addressed. specifically", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "involved in creating the system and those who are impacted by it. at both conferences, this idea was brought up repeatedly as something that needs to be addressed. specifically, we need to focus on communities that do not have a voice in the creation of ai systems because they are usually the ones most susceptible to harm. during the explainable to all : designing ai experiences panel, several of the panelists discussed how bringing those voices that are traditionally underrepresented, if represented at all, will allow us to at least mitigate some of the harm that ai systems may impose on a community. but this is much easier said than done. as", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", will allow us to at least mitigate some of the harm that ai systems may impose on a community. but this is much easier said than done. as brought up on the panel, ideas of fairness, transparency, and consent in ai systems is primarily a notion in the global north. even using frameworks like participatory design to address these ideas proves challenging because the notion of participatory design itself comes from the global north. thus, those communities that are impacted in the global south may have no good way to give consent or may not have the cultural understanding to participate effectively in a participatory design framework.", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "impacted in the global south may have no good way to give consent or may not have the cultural understanding to participate effectively in a participatory design framework. addressing the cultural issues behind our notions first will help us start to actively include these communities. this should also be considered when collecting data. as discussed at good tech fest \u2019 s session on missing voices in natural language processing, the majority of languages used in large language models comes from primarily privileged communities that have access to technology. thus, we need to be conscious about our language data collection mechanisms and how we deploy and assess large language models. the presenter even cited one of our blog posts", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "thus, we need to be conscious about our language data collection mechanisms and how we deploy and assess large language models. the presenter even cited one of our blog posts as a good guide to ethical natural language processing practices. takeaways and resourcesoverall, these conferences expanded my thinking on a lot of topics and reminded me of the nuance and complexity that fairness and explainability hold. below are some takeaways coupled with some resources that can help you continue your own journey with these concepts. transparency is key to solving the challenge of trustworthy ai. by explicitly showing how systems work, providing agency to individuals, and attempting to participate with", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these concepts. transparency is key to solving the challenge of trustworthy ai. by explicitly showing how systems work, providing agency to individuals, and attempting to participate with communities being impacted, we can develop ai systems that will benefit humanity. some of my favorite papers on transparency are documentation - based, such as datasheets for datasets or model cards. creating standardized regulations for ai systems and having consistent auditing frameworks are also effective. realizing these notions is easier said than done, but being able to enforce regulations, while also providing tools for organizations to be successful under this enforcement, will create more ethical uses of ai. starting from", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "done, but being able to enforce regulations, while also providing tools for organizations to be successful under this enforcement, will create more ethical uses of ai. starting from impact assessments will help organizations realize who will be harmed most by ai systems. once identified, one of the panelists on the explainable to all : designing ai experiences panel expressed that inviting representatives from those communities and achieving a consensus with them will help address harms, while also gaining an effective proxy for consent. lastly, part of my attendance at good tech fest was as a datakind volunteer. during datakind \u2019 s session, we helped nonprofits start to think about their data", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "part of my attendance at good tech fest was as a datakind volunteer. during datakind \u2019 s session, we helped nonprofits start to think about their data scienceable problem. datakind offers their playbook for how to design and scope out data sciencable problems ethically. conclusionfairness and explainability are extremely nuanced and important fields because, if done right, we can make the world a better place. it \u2019 s part of the reason why i am so excited to be part of the arthur team. the tools we are building help organizations start to operationalize the latest research in fairness and explainability, allowing", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "am so excited to be part of the arthur team. the tools we are building help organizations start to operationalize the latest research in fairness and explainability, allowing them to realize their aspirations to be ethical. looking forward to continuing the journey! interested in learning more about what arthur can do for you? get in touch. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnews", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##benchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / learnings - from - ttc - summit - good - tech - fest", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest", "row": 202, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 203 text : arthur launches custom rbac to strengthen data privacy & reduce compliance risks for enterprises in highly regulated industries solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedproduct featuresarthur launches custom rbac to strengthen data privacy & reduce compliance risks for enterprises in highly regulated industriesby : caryn lusinchimay 11, 2022arthur is the very first mlops observability platform", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "risks for enterprises in highly regulated industriesby : caryn lusinchimay 11, 2022arthur is the very first mlops observability platform to launch custom rbac ( role - based access control ) to support today \u2019 s enterprise security and data privacy demands. as organizational structures evolve and grow, companies need the flexibility to review and adjust permissions frequently to protect model data and mlops lifecycle access management. additionally, specific roles within mrm ( model risk management ) and third - party independent auditors require strict confidentiality, privacy, and access management controls. when businesses use highly sensitive personal identifiable information for privacy -", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "management ) and third - party independent auditors require strict confidentiality, privacy, and access management controls. when businesses use highly sensitive personal identifiable information for privacy - sensitive data analysis in supervised learning environments, they face increased regulatory risk. to reduce privacy and compliance risks, segregation of duties is essential for different roles across the enterprise that are involved in ml monitoring and validation activities. to uphold the integrity of trust frontiers necessary in ai systems and provide transparency to higher - level stakeholders who are tasked with management / governance kpis as data flows through the ml development lifecycle, arthur now offers a custom role - based access control ( rbac ) system.", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with management / governance kpis as data flows through the ml development lifecycle, arthur now offers a custom role - based access control ( rbac ) system. enterprise organizations cannot rely on offerings with inflexible preset / pre - defined roles and provisioning access. arthur provides the ability to set up a fully customizable rbac for on - premises customers using oidc authentication, and the flexibility and freedom to review and adjust permissions associated with each role as their org structures evolve and grow. with this unique new capability, arthur allows organizations to create groups and assign employees using any tool that speaks open id connect language (", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "their org structures evolve and grow. with this unique new capability, arthur allows organizations to create groups and assign employees using any tool that speaks open id connect language ( okta, microsoft azure active directory, duo, secureauth, autho, ping identity, and more ). organizations can leverage arthur \u2019 s group mapping api for enterprise secure user authentication between data scientists, ml model engineers, systems integrators, domain practitioners, risk analysts, internal auditors, and external auditors. here \u2019 s an example managing users \u2019 permissions in arthur using a third - party identity provider, such as okta. each okta", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "external auditors. here \u2019 s an example managing users \u2019 permissions in arthur using a third - party identity provider, such as okta. each okta group corresponds to a role within an organization in arthur \u2019 s platform. in the future, we plan to extend custom rbac to our existing support of saml 2. 0 - based sso identity providers. when building trustworthy ai systems, it \u2019 s critical for enterprises to identify clear roles, requirements, and responsibilities for teams building, monitoring, and optimizing machine learning models. arthur \u2019 s rbac - oidc authentication solution was specifically built to support enterprises that", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for teams building, monitoring, and optimizing machine learning models. arthur \u2019 s rbac - oidc authentication solution was specifically built to support enterprises that are subject to eu, uk, us or state regulation that mandates algorithmic transparency and auditing. as such, our custom rbac feature offers full - fledged flexibility across standard roles, plus organization - level, model - level, and alert - related permissioning. fully customizable permissions here \u2019 s an excerpt of some of the different permissions arthur supports for use with custom roles. for a full list, you can check out our dev docs. discover how arthur", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of some of the different permissions arthur supports for use with custom roles. for a full list, you can check out our dev docs. discover how arthur \u2019 s custom rbac fits with your enterprise oauth. schedule a demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of servicepr", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - launches - custom - rbac - to - strengthen - data - privacy - reduce - compliance - risks - for - enterprises - in - highly - regulated - industries", "metadata": {"source": "https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries", "row": 203, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 204 text : learnings from odsc east 2022 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedeventslearnings from odsc east 2022 by : arthur teamapril 27, 2022the open data science conference, or odsc, is one of the largest gatherings of professional data scientists, and its attendees, presenters, and companies are shaping the present and future of", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", or odsc, is one of the largest gatherings of professional data scientists, and its attendees, presenters, and companies are shaping the present and future of the field. the goal of the conference is to bring together the global data science community in an effort to encourage the exchange of innovative ideas and the growth of open source software. last week, three members of arthur \u2019 s ml engineering team headed up to odsc east in boston as both attendees and as speakers. keep reading for some recaps of their favorite talks that they attended as well as the ones they presented themselves based on their own research. unsolved ml safety", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "reading for some recaps of their favorite talks that they attended as well as the ones they presented themselves based on their own research. unsolved ml safety problemsdan hendrycks, phd candidate at uc berkeleythough most business - critical machine learning systems today are relatively simple models, the research community has produced astounding results with large models such as gpt - 3 and dall - e, which often have surprising capabilities. in this talk, dan hendrycks, a phd student at uc berkeley, discussed some high - level approaches to the problem of ml safety. hendrycks described three buckets for future research :", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", a phd student at uc berkeley, discussed some high - level approaches to the problem of ml safety. hendrycks described three buckets for future research : robustness, or the ability for ml systems to perform well even in unexpected or adversarial settings ; monitoring, or the ability to understand and anticipate new challenges ; and alignment, or the ability to coordinate the behavior of ml systems with human goals, values, and intentions. while the highest - stakes motivations for the talk, and the largest open questions, generally refer to large deep learning models, these challenges and goals are still relevant for simpler models. operationalizing", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s for the talk, and the largest open questions, generally refer to large deep learning models, these challenges and goals are still relevant for simpler models. operationalizing fair ml : from industry to research and backjessica dai, machine learning engineer at arthur ( co - presented with laura mariano, lead ethical ai data scientist at humana ) in both research and industry, discussion of \u201c fair machine learning \u201d has exploded in the past few years \u2014 yet there is often a gap between what is available in academia and the constraints and needs of a real - world organization. this talk was co - presented by jessica dai of arthur and laura mariano of", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "what is available in academia and the constraints and needs of a real - world organization. this talk was co - presented by jessica dai of arthur and laura mariano of humana, which has been an incredible partner of arthur \u2019 s for years now. the two discussed humana \u2019 s journey towards achieving informed, responsible use of machine learning to improve health outcomes. first, humana implemented organizational and process - based tools for governance. having set the stage for actively improving models, however, humana \u2019 s data scientists then realized that none of the popular, published approaches to achieving fairness were applicable to their goals : the way humana deployed and used", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "humana \u2019 s data scientists then realized that none of the popular, published approaches to achieving fairness were applicable to their goals : the way humana deployed and used machine learning violated assumptions made by many available \u201c fair ml \u201d methods. in the latter half of this talk, jessica showed how these constraints motivated novel research questions and guided the development of an academic research project ; explained and demonstrated the method we came up with ; and discussed considerations as we folded this research work back into the product so that it was ultimately usable in a real - world production setting. utilizing nlp in the context of comp360 psilocybin therapy for treatment", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "so that it was ultimately usable in a real - world production setting. utilizing nlp in the context of comp360 psilocybin therapy for treatment - resistant depressiongregory ryslik, executive vice president, ai, engineering, digital health research & technology at compass pathwaysin this talk, gregory ryslik of compass pathways discussed how nlp techniques have been used in the process of setting up clinical trials for using psilocybin as an intervention against treatment - resistant depression. the magnitude of the problem is, of course, enormous ; the talk focused on how this specific application setting, as well as the constraints it", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- resistant depression. the magnitude of the problem is, of course, enormous ; the talk focused on how this specific application setting, as well as the constraints it brought \u2014 such as working with nontechnical clinicians, privacy and regulatory concerns, and following the clinical trial process \u2014 all shaped the development of nlp approaches on the technical side. drift detection in structured and unstructured datakeegan hines, vice president of machine learning at arthurmachine learning systems in production are subject to performance degradations due to many external factors and it is vital to actively monitor system stability and integrity. a common source of model degradation is due the inherent", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "subject to performance degradations due to many external factors and it is vital to actively monitor system stability and integrity. a common source of model degradation is due the inherent non - stationarity of the real - world environment, commonly referred to as data drift. in this presentation, keegan described how to reliably quantify data drift in a variety of different data paradigms including tabular data, computer vision data, and nlp data. attendees of this talk came away with a conceptual toolkit for thinking about data stability monitoring in their own models, with example use cases in common settings as well as in more challenging regimes. simplify", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a conceptual toolkit for thinking about data stability monitoring in their own models, with example use cases in common settings as well as in more challenging regimes. simplifying mlops by taking storage out of the equationmiroslav klivansky, principal data architect, ai & analytics at purestorageone of the biggest issues that enterprise companies face when trying to design and scale machine learning pipelines is worrying about the raw storage of the data. in this talk, speaker miroslav klivansky discussed the work he was doing at purestorage, a flash storage company based in palo alto. he discussed how we need to think about", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "klivansky discussed the work he was doing at purestorage, a flash storage company based in palo alto. he discussed how we need to think about handling large - scale data pipelines and having a data storage system that can handle the many different kinds of tasks from different machine learning models. the key takeaway from the talk was how we must differentiate between data and storage, and how they are not the same thing. he showcased a workflow ( done at purestorage ) where we think about the different parts of the data pipeline abstracted away from the storage system, which yields cleaner and more efficient pipelines. fastc", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##age ) where we think about the different parts of the data pipeline abstracted away from the storage system, which yields cleaner and more efficient pipelines. fastcfe : a distributed deep reinforcement learning counterfactual explainerkarthik rao, machine learning engineer at arthurcounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps to move a data point from one side of a decision boundary to another. these explanations have a clear use case for several applications ranging from loan decisions to healthcare diagnosis, where they need to advise stakeholders about actions they can take to achieve a", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these explanations have a clear use case for several applications ranging from loan decisions to healthcare diagnosis, where they need to advise stakeholders about actions they can take to achieve a different outcome. individuals not provided loans want steps they can take to achieve a loan, and similarly, patients want to know how they can achieve a better diagnosis. in this presentation, karthik rao showcased fastcfe, an algorithm and feature that uses reinforcement learning to provide real - time counterfactual explanations. the presentation was broken down as follows : overview of counterfactuals and reinforcement learning ( rl ) deep distributed reinforcement learning using openai gym and ray", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the presentation was broken down as follows : overview of counterfactuals and reinforcement learning ( rl ) deep distributed reinforcement learning using openai gym and ray + rllibbenchmarks and resultsa unified view of trustworthy ai with the 360 toolkitsdr. kush varshney, distinguished research staff member and manager at ibm researchthe final talk of the conference was given by dr. kush varshney of ibm research, discussing the open source initiatives that ibm has taken to develop trustworthy ai toolkits. this was a particularly enlightening talk and it is definitely beneficial to the ai community that large players", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ibm has taken to develop trustworthy ai toolkits. this was a particularly enlightening talk and it is definitely beneficial to the ai community that large players such as ibm are committed to trustworthy initiatives and are even willing to open source these new technologies to help companies build trusted ai systems. if your organization has data scientists who can afford to invest the time and want to be truly hands - on, these ibm toolkits will work well for your team. dr. varshney explained three specific ai toolkits discussing : ai fairness 360 : different methods to detect bias and use bias mitigation techniquesai explainability : a", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##shney explained three specific ai toolkits discussing : ai fairness 360 : different methods to detect bias and use bias mitigation techniquesai explainability : a toolkit providing out of the box explainable tools to explain modelsai adversarial robustness : how to detect and handle adversarial data attacked on modelsodsc east was an incredibly rewarding experience for the arthur team, and we \u2019 re thrilled that our engineers were able to share the results of the hard work they \u2019 ve been doing to advance the field of machine learning. we \u2019 re feeling grateful to have finally attended a conference in person after so long \u2014", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "hard work they \u2019 ve been doing to advance the field of machine learning. we \u2019 re feeling grateful to have finally attended a conference in person after so long \u2014 and especially to have been able to co - present with one of our long - time partners, humana, to amplify the story of their leading work. see you at odsc 2023! interested in learning more about arthur \u2019 s research - led approach to product development? read about it here. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldben", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / learnings - from - odsc - east - 2022", "metadata": {"source": "https://www.arthur.ai/blog/learnings-from-odsc-east-2022", "row": 204, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 205 text : mining for proxies in machine learning systems solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessmining for proxies in machine learning systemsby : keegan hinesapril 18, 2022in deployed machine learning systems that affect people \u2019 s lives, it is important to ensure that your model is not resulting in disparate impact for some sub - populations of", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning systems that affect people \u2019 s lives, it is important to ensure that your model is not resulting in disparate impact for some sub - populations of your data. the typical populations of concern might be the so - called protected basis variables such as race, age, or sex, but can also be any subgroup which is relevant to your industry. creating ml systems that are fair can be tricky due to the complexities of data used for training. if you want to ensure that your model isn \u2019 t resulting in disparate impact across sex, it might be tempting to simply not use sex as an input variable into the model.", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that your model isn \u2019 t resulting in disparate impact across sex, it might be tempting to simply not use sex as an input variable into the model. this tactic, often referred to as fairness - through - unawareness, is insufficient. even though the model doesn \u2019 t know about sex explicitly, it is highly likely that some other input feature ( or combination of features ) can act as a highly effective proxy for what you were trying to hide from the model. many approaches to fairml have focused on mitigating unfairness either by augmenting the training data, the training algorithm, or the model \u2019 s outputs. while", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to fairml have focused on mitigating unfairness either by augmenting the training data, the training algorithm, or the model \u2019 s outputs. while these approaches help to obtain a more fair model for any given set of inputs, they don \u2019 t illuminate for the data scientist what factors were resulting in the model bias. as the model developer, it is helpful to have full awareness of which features are acting as proxies for sensitive attributes. in this post, we describe a technique for effectively surfacing complex combinations of features which act as proxies for a sensitive attribute. proxiesbeing able to identify", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", we describe a technique for effectively surfacing complex combinations of features which act as proxies for a sensitive attribute. proxiesbeing able to identify proxies can aid the model developer in being judicious about feature selection and model development. at a first pass, identifying proxy variables is as simple as determining which variables have high correlation with a sensitive attribute, and then choosing whether or not to omit those variables. the downside of this approach is that measures of correlation ( perhaps pearson correlation or mutual information ) will marginalize over the full domain, perhaps averaging out important effects only seen for some specific values. further,", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of correlation ( perhaps pearson correlation or mutual information ) will marginalize over the full domain, perhaps averaging out important effects only seen for some specific values. further, it could be the case that pairwise combinations ( or higher - order ) of features are able to form highly effective proxies, more so than any univariate features. as a fictitious example, suppose we have a dataset where one variable is a binary sex ( female or male ), and another variable is a boolean is _ pregnant. while the is _ pregnant variable does provide some overall correlation with the sex variable, the strongest proxying effects are seen only for", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a boolean is _ pregnant. while the is _ pregnant variable does provide some overall correlation with the sex variable, the strongest proxying effects are seen only for some combinations of the two variables. that is, if we know that is _ pregnant = false for a particular person, then we can \u2019 t necessarily conclude much about their sex, and likely p ( sex = female ) ~ = p ( sex = male ). however, if is _ pregnant = true, then we \u2019 ve likely gained significant certainty about sex and p ( sex = female ) > > p ( sex = male ). this example illustrates a desirable property when identifying", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 ve likely gained significant certainty about sex and p ( sex = female ) > > p ( sex = male ). this example illustrates a desirable property when identifying proxies. it is of only limited utility to know that is _ pregnant is correlated with our sensitive attribute sex. it is much more useful to know that is _ pregnant = true is a strong proxy for sex = female, whereas is _ pregnant = false is uninformative. the difference here is that we aim to identify small pockets in the data ( and combinations of features ) that form highly predictive proxies. to accomplish this, we \u2019 ll rely on", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "aim to identify small pockets in the data ( and combinations of features ) that form highly predictive proxies. to accomplish this, we \u2019 ll rely on the techniques of frequent pattern mining and association rule mining. this will allow us to surface exactly which values of a variable, and higher - order combinations thereof, are acting as strong proxies for any sensitive attributes. association rulesassociation rule mining is a rather classic technique in unsupervised pattern discovery, and it can be especially useful in the context of fairml and proxies. the ideas often stem from \u201c market basket analysis \u201d, which imagines we", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and it can be especially useful in the context of fairml and proxies. the ideas often stem from \u201c market basket analysis \u201d, which imagines we have a large database of purchases at a supermarket. for each shopper, we know what items they bought together, such as { milk, eggs, cheese } for one shopper and { diapers, beer } for another. from the co - occurrences of items in baskets, our goals are to ( 1 ) identify commonly co - occurring items and ( 2 ) identify directional correlations. for our first goal, we can use frequent pattern mining techniques such as fpgrowth", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "commonly co - occurring items and ( 2 ) identify directional correlations. for our first goal, we can use frequent pattern mining techniques such as fpgrowth, which will result in a list of item combinations that were highly probable in the dataset. going a step further, we can apply association rule mining to identify not just correlated items, but items correlated with a particular directionality. for example, if a person bought sunglasses and a bathing suit, then maybe they also bought sunscreen. but if all we know is that they bought sunscreen, they \u2019 re not necessarily in the market for new shades. association rules are of the", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "sunscreen. but if all we know is that they bought sunscreen, they \u2019 re not necessarily in the market for new shades. association rules are of the form \u201c if a, then b \u201d. the antecedent a can be one or more items, and the consequent b can be one or more items. importantly, we \u2019 re not looking solely for relationships that are 100 % fulfilled in our dataset. instead, we \u2019 re looking for consequents that follow antecedents with higher probability than we might expect. quantifying those base - level expectations means we have to introduce some new terminology and measures", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s that follow antecedents with higher probability than we might expect. quantifying those base - level expectations means we have to introduce some new terminology and measures. measuresfor a particular itemset ( whether it is an antecedent, consequent, or neither ), the support for that itemset is a measure of how frequently the itemset occurs in the data. we can think of it as the overall probability of occurrence for an itemset. note that for a particular itemset, we need to tabulate not just exact matches to that itemset, but also all other larger itemsets that include at least that one.", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "particular itemset, we need to tabulate not just exact matches to that itemset, but also all other larger itemsets that include at least that one. an itemset with high support ( like 0. 80 ) would mean that 80 % of the dataset is exactly this itemset. as you might imagine, itemsets with fewer items tend to have more support than itemsets with more items. for example, the itemset { milk } probably occurred in many of our shoppers \u2019 baskets, whereas the combination of { milk, lettuce, hat } was probably more rare. for a particular rule \u201c if a, then", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "shoppers \u2019 baskets, whereas the combination of { milk, lettuce, hat } was probably more rare. for a particular rule \u201c if a, then b \u201d, the confidence quantifies how often that rule was true. another way to think about it is : whenever you saw a, what \u2019 s the probability that you would see b? naturally, if a rule were a really strong correlation, then confidence in the consequent would be close to 100 %. in contrast, for a weaker relationship, knowing about antecedent a perhaps doesn \u2019 t give us much certainty at all about b. other useful concepts such as", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in contrast, for a weaker relationship, knowing about antecedent a perhaps doesn \u2019 t give us much certainty at all about b. other useful concepts such as lift and conviction will help us quantify the extent to which a helps us predict b more than we \u2019 d be able to predict b otherwise. mining for proxies identifying association rules in a dataset can be a useful technique for uncovering proxies for sensitive variables. in this case, we \u2019 ll be looking for association rules of the form \u201c if a, then b \u201d where the consequent b is one of the values of our sensitive attributes. for example", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "looking for association rules of the form \u201c if a, then b \u201d where the consequent b is one of the values of our sensitive attributes. for example, if we want to find proxies for female in our dataset, we can limit ourselves to association rules where the consequent is the itemset { female } and nothing else. then, the antecedent of such a rule would be a feature value, or combination, that is highly predictive of a person being female. the confidence of the rule tells us how strong the association is : a confidence near 50 % means that the antecedent isn \u2019 t", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a person being female. the confidence of the rule tells us how strong the association is : a confidence near 50 % means that the antecedent isn \u2019 t actually a very effective proxy for female, whereas a confidence near 100 % means we have found a combination of feature values that can identify females with near certainty. let \u2019 s try this idea out and see how it works. identifying proxies for sexwe \u2019 ll use the uci adult dataset, which includes financial and demographic data about a group of 30, 000 adults. while this dataset is often used for classification tasks ( predicting if a person has high income ), we", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "demographic data about a group of 30, 000 adults. while this dataset is often used for classification tasks ( predicting if a person has high income ), we \u2019 ll ignore for now the supervised learning aspect. instead we \u2019 ll focus on some of the demographic variables ( such as sex or race ) and investigate whether these can be proxied by other variables ( such as education, occupation, maritalstatus and so on ). we \u2019 ll focus on sex and use association rule mining to identify strong proxies. for generating candidate itemsets, we \u2019 ll focus first on categorical variables. continuous variables can be easily incorporated by", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "rule mining to identify strong proxies. for generating candidate itemsets, we \u2019 ll focus first on categorical variables. continuous variables can be easily incorporated by discretizing their domains, either manually ( such as quartiles ) or by dynamically identifying bin splits ( in a similar spirit to decision tree algorithms ). it is up to us to identify how many input features we might include for itemsets and what order of combinations we want to consider. for ease of interpretability, we might constrain ourselves to first -, second -, and third - order combinations. that is, in the antecedents of our", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "interpretability, we might constrain ourselves to first -, second -, and third - order combinations. that is, in the antecedents of our rules, we \u2019 ll only want to consider itemsets with one, two, or three items. this choice is up to us, depending on what level of rule complexity we want to access. as we increase this complexity, the combinatorics of itemset and rule generation expands. luckily, algorithms such as fpgrowth, which are based on tree structures, will allow us to apply these ideas to large datasets and complex itemsets, if we desire. let", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##rowth, which are based on tree structures, will allow us to apply these ideas to large datasets and complex itemsets, if we desire. let \u2019 s see whether the variables maritalstatus, occupation, and education can form proxies for sex. remember we \u2019 re not just looking at the variables holistically, but instead identifying whether certain values and combinations of those values can be highly predictive of certain values of sex. we \u2019 ll generate rules up to second - order and see which rules can predict male with high confidence. a few top examples are below. we see that some ( but not all ) occupations", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to second - order and see which rules can predict male with high confidence. a few top examples are below. we see that some ( but not all ) occupations and educations are strong proxies for males. for example, for all the rows in the data where occupation was repair and maritalstatus was married, those people were male 99 % of the time. this combination is also fairly prevalent, as the antecedent support is 8 %, a sizable chunk of the data. so for this pocket of the data, we have a near - perfect proxy for sex. the same applies for numerous other combinations, though they are less", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data. so for this pocket of the data, we have a near - perfect proxy for sex. the same applies for numerous other combinations, though they are less prevalent. there are many association rules generated with confidence greater than 80 % for predicting male. they typically entail small pockets of the data ( support of 1 % to 5 % ). the effect of each is small, but the aggregate is that for large volumes of the data space, we can predict males with high confidence. similar results are returned if we look for proxies for female. here, we see that maritalstatus can be a strong proxy for female in some", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". similar results are returned if we look for proxies for female. here, we see that maritalstatus can be a strong proxy for female in some instances. especially strong is the antecedent \u201c if maritalstatus is widowed \u201d, an unsurprising rule since the term \u201c widowed \u201d is fairly gendered to begin with. in combination with occupation, maritalstatus can become an even more precise predictor of female. take for example the rule \u201c if occupation is admin - clerical, then sex is female \u201d. this rule \u2019 s confidence is 67 %, so a bit higher than the base - rate of", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "occupation is admin - clerical, then sex is female \u201d. this rule \u2019 s confidence is 67 %, so a bit higher than the base - rate of female in the dataset, but it does have a support of over 10 %. the higher order rule \u201c if occupation is admin - clerical and education is high school, then sex is female \u201d, has slightly higher confidence ( at 70 % ) and slightly lower support ( at 5 % ). as the antecedent becomes more precise, we can find smaller pockets and subsets of the data that yield more confident proxies of the sensitive attribute. intuitively, these", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##dent becomes more precise, we can find smaller pockets and subsets of the data that yield more confident proxies of the sensitive attribute. intuitively, these results are unsurprising : many jobs are highly correlated with males or females. but we now have a more precise way to understand this effect. the support of each rule tells us how prevalent a particular combination is in the data, and the confidence tells us how certain the proxying can be. this analysis is easy to apply to any sensitive attributes we want to explore. we could identify proxies for certain races, age groups, health conditions, and so on. next", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to apply to any sensitive attributes we want to explore. we could identify proxies for certain races, age groups, health conditions, and so on. next stepsgiven how informative some of these combinations can be, it would be unsurprising for an ml model to pick up on these proxies even if it didn \u2019 t have direct access to sensitive attributes. any sufficiently complex algorithm ( even just a decision tree or tree ensemble ) should easily be able to capture the simple combinations highlighted here, if they are helpful in predicting the target variable. surfacing these relationships should be an early part of the model development process and", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "capture the simple combinations highlighted here, if they are helpful in predicting the target variable. surfacing these relationships should be an early part of the model development process and should play a role in feature selection. ultimately, our goal must be to mitigate disparate impact in the final system. with this clarity into proxies, we could choose to ( i ) omit certain variables from a model, ( ii ) omit / recode certain values of a variable from the model, or ( iii ) turn to training - time and post - hoc methods for bias mitigation. want to further explore arthur \u2019 s bias detection capabilities", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model, or ( iii ) turn to training - time and post - hoc methods for bias mitigation. want to further explore arthur \u2019 s bias detection capabilities? click here to learn more. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www.", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##panyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / mining - for - proxies - in - machine - learning - systems", "metadata": {"source": "https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems", "row": 205, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 206 text : predicting the future with machine learning solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpc", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai monitoring & performancepredicting the future with machine learning by : arthur teammarch 11, 2022the goal of machine learning is to predict the future, based on data from the past. it \u2019 s more important than ever to make predictions that match reality, but as the world changes around us,", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the future, based on data from the past. it \u2019 s more important than ever to make predictions that match reality, but as the world changes around us, so does the data that is used to generate new predictions. machine learning models fail silently, which means they will make predictions even if the incoming data looks nothing like the data they were trained against. they will make predictions for scenarios and situations they were never trained for. they will be inaccurate and incorrect ; and worst of all, they will be confidently incorrect. and these incorrect predictions will influence business decisions, impacting both dollars and human lives. using ground truth to calculate model performance the", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", they will be confidently incorrect. and these incorrect predictions will influence business decisions, impacting both dollars and human lives. using ground truth to calculate model performance the most straightforward way to measure model success in production is to compare how closely predictions match reality, using performance metrics like accuracy, precision, f1, recall and more. performance metrics provide insights into how the model is holding up in the production environment, and when the model needs re - training ( or tuning ). these metrics also help data scientists and model owners calculate the delta between predicted truth and ground truth. machine learning teams rely on ground truth to test predictions that algorithms are", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "metrics also help data scientists and model owners calculate the delta between predicted truth and ground truth. machine learning teams rely on ground truth to test predictions that algorithms are making against the real world. no ml model guarantees 100 % accuracy, but the goal is to get it as close as possible to this target. each business area responsible for oversight of ml models in production sets their own tolerance when it comes to performance metrics. this threshold is carefully defined based on many aspects, including potential impact to revenue ( both positive and negative ). depending on the use case and the nature of the data, model performance can start degrading as soon as", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "impact to revenue ( both positive and negative ). depending on the use case and the nature of the data, model performance can start degrading as soon as the model is deployed to production without a monitoring solution in place. after weeks in production, if the accuracy of that model has dropped to 70 %, it could be well below what \u2019 s acceptable by business leadership, causing a measurable impact to business kpis, such as revenue. however, with an ai performance solution like arthur that provides continuous model monitoring, it \u2019 s easy to detect when the tolerance threshold set for model accuracy falls into an unacceptable range and correct for it.", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "like arthur that provides continuous model monitoring, it \u2019 s easy to detect when the tolerance threshold set for model accuracy falls into an unacceptable range and correct for it. measuring performance is straightforward if data is generated from a known model, the ground truth. however, all models trained are limited by the ground truth quality used to train and test them, and by the timing and availability of ground truth data in production. ground truth challenges 1. accounting for time series and seasonal variabilitydelayed ground truth is quite common when there \u2019 s a calendar delay between model prediction and when the ground truth information is ready. an example of this is in the financial", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ground truth is quite common when there \u2019 s a calendar delay between model prediction and when the ground truth information is ready. an example of this is in the financial services industry where customers have up to 3 months after a suspect transaction to flag it as fraudulent. there \u2019 s a 90 day lag ; problems don \u2019 t manifest in real time when the original transaction occurs. 2. definitions for ground truth vary across the organization supervised learning requires a large volume of diverse data with corresponding correct ground truth labels. enterprise datasets are siloed in systems across the organization and often complex in nature. these systems are often not interoperable.", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ground truth labels. enterprise datasets are siloed in systems across the organization and often complex in nature. these systems are often not interoperable. ground truth consistency suffers when there are missing, inconsistent or edge case annotations. 3. computer vision and nlp models require humans - in - the - loop labelingwhen you are working with model types like computer vision ( cv ) or natural language processing ( nlp ), ground truth labels are not readily available without manual annotation which requires labor - intensive processing. with nlp, you cannot always rely on the literal word meaning but someone must infer customer intent", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "without manual annotation which requires labor - intensive processing. with nlp, you cannot always rely on the literal word meaning but someone must infer customer intent or satisfaction. solving for ground truth while some ground truth challenges must be addressed earlier in the ml lifecycle at the organizational level or during model development, accounting for delayed ground truth can be solved with arthur \u2019 s performance technology. in some use cases, the ground truth will be available seconds after the prediction ; in others, it could take months or years. arthur \u2019 s platform allows for ground truth data to be updated at any moment, regardless of when the inference was recorded and with", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "it could take months or years. arthur \u2019 s platform allows for ground truth data to be updated at any moment, regardless of when the inference was recorded and with no data duplication. performance metrics are then recalculated on the fly. ground truth data can be updated individually ( for every inference, as the data becomes available ), or in bulk. with full support through the sdk and api, there are many different options to automate ground truth updates. very simple scripts can be used to retrieve data from log files, databases or other sources and leverage the arthur api to update ground truth data. working with models when ground", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "simple scripts can be used to retrieve data from log files, databases or other sources and leverage the arthur api to update ground truth data. working with models when ground truth is delayedin most production applications, there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. leveraging labeling teams or services can help close this lag, but it will not completely remove it. instead of monitoring metrics based on outputs, arthur can alternatively monitor inputs based on data drift metrics with automating data drift thresholding. arthur automatically creates relevant thresholds for detecting data drift", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "outputs, arthur can alternatively monitor inputs based on data drift metrics with automating data drift thresholding. arthur automatically creates relevant thresholds for detecting data drift, driving speed to value in optimizing ml models, while other ml observability and monitoring solutions rely on users to manually define thresholds for each attribute, which is slow and labor - intensive. \u201c it is a capital mistake to theorize before one has data. insensibly one begins to twist facts to suit theories, instead of theories to suit fact. \u201d - sherlock holmesremember, without ground truth data, the value of your predictive algorithms can", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to suit theories, instead of theories to suit fact. \u201d - sherlock holmesremember, without ground truth data, the value of your predictive algorithms can be called into question. using arthur with ground truth to calculate model performance or using automated data drift thresholding to combat delayed ground truth, will build greater trust in model predictions and drive better business outcomes for everyone. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdoc", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / predicting - the - future - with - machine - learning - ground - truth", "metadata": {"source": "https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth", "row": 206, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 207 text : arthur selected to provide critical ai performance capabilities for department of defense solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur selected to provide critical ai performance capabilities for department of defenseby : arthur teamfebruary 16, 2022for immediate releasenew york, ny / / february 16, 2022the joint artificial intelligence center ( jaic ) has selected arthur to", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "16, 2022for immediate releasenew york, ny / / february 16, 2022the joint artificial intelligence center ( jaic ) has selected arthur to deliver its industry - leading ai performance management solution as part of the test and evaluation ( t & e ) blanket purchase agreement ( bpa, ) a $ 249m effort to accelerate production ai capabilities across the department of defense ( dod ). this new vehicle will allow the dod to leverage the arthur platform for ai monitoring and optimization to deliver better results with accuracy, explainability, and fairness. arthur \u2019 s world - class research team has pioneered exclusive capabilities in computer vision, natural language", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "optimization to deliver better results with accuracy, explainability, and fairness. arthur \u2019 s world - class research team has pioneered exclusive capabilities in computer vision, natural language processing ( nlp ), bias mitigation, and other key functional areas. with the rollout of the dod \u2019 s ethical ai principles \u2014 responsible, equitable, traceable, reliable, governable \u2014 in february 2020, ensuring the appropriate foundation is in place for production ai capabilities is paramount. the arthur team brings considerable expertise and commitment to the federal government, having successfully worked with darpa, the ic, nsf, nist, arpa - e, treasury", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "brings considerable expertise and commitment to the federal government, having successfully worked with darpa, the ic, nsf, nist, arpa - e, treasury, air force, army, and navy. interested federal agencies should contact publicsector @ arthur. ai for more information and to schedule a briefing. about arthurarthur is the ai performance company. our platform measures and improves machine learning models to deliver better results. we help data scientists, product owners, and business leaders accelerate model operations to optimize for accuracy, explainability, and fairness. arthur \u2019 s research - led approach to product development drives exclusive capabilities in computer vision", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "accelerate model operations to optimize for accuracy, explainability, and fairness. arthur \u2019 s research - led approach to product development drives exclusive capabilities in computer vision, nlp, bias mitigation, and other critical areas. at arthur, we \u2019 re on a mission to make ai work for everyone, and we are deeply passionate about building ml technology to drive responsible results. about the dod joint artificial intelligence centerthe department of defense ( dod ) joint artificial intelligence center ( jaic ) was established in 2018 to be the focal point of the dod artificial intelligence ( ai ) strategy to accelerate scaling ai and its impact across the department. the", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##c ) was established in 2018 to be the focal point of the dod artificial intelligence ( ai ) strategy to accelerate scaling ai and its impact across the department. the mission of the jaic is \u201c to accelerate the delivery of ai - enabled capabilities, scale the department - wide impact of ai, and synchronize dod ai activities to expand joint force advantages. \u201d working closely with the services, combatant commands, and other components, jaic identifies appropriate use cases for ai across dod, rapidly pilots solutions, and scales impact across its enterprise. previous postsharenext post we make ai better for everyone. sign up for our newsletter to", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "dod, rapidly pilots solutions, and scales impact across its enterprise. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - selected - to - provide - critical - ai - performance - capabilities - for - department -", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "https : / / www. arthur. ai / blog / arthur - selected - to - provide - critical - ai - performance - capabilities - for - department - of - defense", "metadata": {"source": "https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense", "row": 207, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 208 text : fast counterfactual explanations using reinforcement learning solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml explainabilityfast counterfactual explanations using reinforcement learningby : karthik rao and sahil vermamarch 18, 2022introductioncounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "2022introductioncounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps to move a data point from one side of a decision boundary to another. these explanations have a clear use - case for several applications ranging from loan decisions ( example shown below ) to healthcare diagnosis. problem : a binary classifier, being used by a financial services company, predicts if an individual should be approved ( 1 ) or denied for a loan ( 0 ). individuals want to know how to get approved for a loan if the model predicts that they should be rejected.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") or denied for a loan ( 0 ). individuals want to know how to get approved for a loan if the model predicts that they should be rejected. counterfactual explanation : we can now provide a series of steps ( as changes in the input features ) that can help an individual get approved for a loan, e. g. add $ 10, 000 to your salary and gain 2 more years of education. in practice, we might have many more requirements of cfes beyond just finding the other side of the decision boundary. for example, we might need the features to be constrained to only certain actionable sets, or we", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##es beyond just finding the other side of the decision boundary. for example, we might need the features to be constrained to only certain actionable sets, or we might need the resultant counterfactual to be realistic and similar to the training data. recent work has summarized these various desiderata and the common research themes in the field. additionally, we need to be able to compute counterfactuals in way that is computationally efficient for high - data - volume uses cases. in this article, we present a distributed and scalable reinforcement learning framework that can produce real - time counterfactual explanations for any binary classifier", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "cases. in this article, we present a distributed and scalable reinforcement learning framework that can produce real - time counterfactual explanations for any binary classifier. we provide an overview of the algorithm used to find counterfactual explanations in real - time, and implementation details of how we've used open ai gym and ray ( rllib ) to put this work into practice. counterfactuals overviewwe define a counterfactual explanation ( cfe ) as follows : definition. given a model \\ ( f \\ ), and a data point \\ ( x \\ ), where \\ ( f ( x )", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") as follows : definition. given a model \\ ( f \\ ), and a data point \\ ( x \\ ), where \\ ( f ( x ) = y \\ ), we want to find a point \\ ( x'\\ ) where \\ ( f ( x') = 1 - y \\ ) such that we minimize \\ ( dist ( x, x') \\ ) for any distance function \\ ( dist \\ ). this definition states that we want to find a point \\ ( ( x') \\ ) where our binary classifier returns the inverse of the original classification \\ ( ( y ) \\ ).", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to find a point \\ ( ( x') \\ ) where our binary classifier returns the inverse of the original classification \\ ( ( y ) \\ ). this definition is also malleable, as we can model different causal relationships of features in order to produce more realistic counterfactual explanations. for example, we can define a set of immutable features which cannot be changed ( such as gender and marital status ), and a set of non - decreasing features ( such as age and education level ). we also want to find a counterfactual explanation \\ ( ( x') \\ ), such that \\ (", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "such as age and education level ). we also want to find a counterfactual explanation \\ ( ( x') \\ ), such that \\ ( x'\\ ) is close to the original training data. such additional constraints allow us to produce counterfactual explanations that are realistic and well - defined. one of the key shortcomings of most counterfactual approaches is that they are computed on each individual instance in a data set. this means that for any set of inferences, we must solve an optimization problem to find a counterfactual explanation that has the following properties : correct output : we need the", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "any set of inferences, we must solve an optimization problem to find a counterfactual explanation that has the following properties : correct output : we need the new data point \\ ( x'\\ ) to have the desired output from the model ( aka moving from a classification of 0 to 1 ). minimize data manifold distance : we want to minimize the distance between our new counterfactual data - point \\ ( ( x') \\ ) and our original training dataset ( through knn distance ). adhere to feature constraints : we have to respect the properties of features especially if they are immutable ( cannot be changed )", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "( through knn distance ). adhere to feature constraints : we have to respect the properties of features especially if they are immutable ( cannot be changed ) or non - decreasing ( must only increase ) solving a new optimization problem for each data point can be expensive, preventing us from creating real - time counterfactual explanations. our approach creates a model that allows us to pay for this expensive training upfront for the entire dataset and produce fast inferences for any new data point that needs a counterfactual explanation. reinforcement learning framework for cfesbased on our goal of achieving near real - time explanations, we considered", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data point that needs a counterfactual explanation. reinforcement learning framework for cfesbased on our goal of achieving near real - time explanations, we considered a reinforcement learning framework [ verma, hines, dickerson ] that allows us to do a one - time training of the model and produce explanations in real - time. in this section, we will present a brief overview of reinforcement learning and how we applied it to counterfactual explanations. reinforcement learning overviewreinforcement learning is a machine learning framework that allows an agent to interactively \" learn \" a best set of actions for a desired state in a given environment.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##forcement learning is a machine learning framework that allows an agent to interactively \" learn \" a best set of actions for a desired state in a given environment. reinforcement learning has had the most success in robotics and game - playing scenarios ( we have seen rl models beat the best players in the world at poker and go ). a few key terms will be used through this article as we model our counterfactuals problem : agent : the agent operates in the entire state space ( environment ) for a problem. the agent aims to make a right series of actions through the state space to perform some task. it will sample the environment", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "space ( environment ) for a problem. the agent aims to make a right series of actions through the state space to perform some task. it will sample the environment in order to learn the best actions in any state. environment : a generic set of states that produce a reward when an agent moves from state to state. the agent will only learn about the reward from the environment once it has arrived at a certain state. action : at any given state \\ ( s _ t \\ ), an agent needs an action to move to state \\ ( s _ { t + 1 } \\ ). the goal of any rl problem is to find", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", an agent needs an action to move to state \\ ( s _ { t + 1 } \\ ). the goal of any rl problem is to find a series of actions that move the agent from an undesirable state to a desirable state ( by maximizing expected reward through all the states ). for the purposes of this article, we do not provide a more comprehensive overview of a reinforcement learning framework. i would highly recommend reading [ this blog ], which provides such an overview. reinforcement learning for cfes our recent work ( appearing soon at aaai ) shows how to use a reinforcement learning framework to generate real - time", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "such an overview. reinforcement learning for cfes our recent work ( appearing soon at aaai ) shows how to use a reinforcement learning framework to generate real - time counterfactual explanations. let us consider a very simple dataset : \\ ( x _ 1, x _ 2, y \\ ). we have some binary classifier that creates a decision boundary shown in figure 1. our goal is to move any arbitrary point on the left side of the boundary, namely points in blue, across the boundary to be classified as the opposite class ( orange points ). figure 1 : decision boundary for arbitrary dataset. note the binary classifier", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "blue, across the boundary to be classified as the opposite class ( orange points ). figure 1 : decision boundary for arbitrary dataset. note the binary classifier is not perfect and mislabels a few points, namely the blue points on the right side of the graph. to frame this problem as a reinforcement learning problem, we define the following : agent : in our counterfactual scenario, our agent can be considered to be located on any point on the left side of the decision boundary ( any of the blue points ). the agent needs to take a series of actions ( which we will define below ), to move from the", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the decision boundary ( any of the blue points ). the agent needs to take a series of actions ( which we will define below ), to move from the left side of the decision boundary to the right side. environment : our environment ( which represents our state - space ) is defined as the entire grid ( which is discretized into a finite number of states for every input in our feature space ). our environment will send a reward to agents at state \\ ( s \\ ). the reward function is defined as the sum of two different components : classification reward : we want to reward agents that are on the correct side of the decision", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "). the reward function is defined as the sum of two different components : classification reward : we want to reward agents that are on the correct side of the decision boundary. therefore, we give a high reward once an agent is across the decision boundary. we use the binary classifier \u2019 s predict function to determine if an agent is close to the boundary ( if the class probability is close to the decision threshold ). therefore the closer to the decision boundary the agent is in, the higher reward it will receive and vice - versa. this value will always be between 0 and 1. dataset manifold reward : we want our final counterfact", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "higher reward it will receive and vice - versa. this value will always be between 0 and 1. dataset manifold reward : we want our final counterfactual point to be similar to the training dataset. therefore, we give higher negative reward for points that are dissimilar to the training data. computing this similar - ness of states can be implemented in several ways, such as auto - encoders, k - nearest - neighbors, etc. for this implementation we used knn distance ( normalized between 0 and 1 ) to measure the similar - ness of the final counterfactual point. action : in our", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we used knn distance ( normalized between 0 and 1 ) to measure the similar - ness of the final counterfactual point. action : in our discretized environment, we allow our agent to move a small distance along any one of the feature dimensions. in our example above, an action \\ ( a \\ ) for an agent at state \\ ( s \\ ), will be a small \\ ( ( \u00b10. 05 ) \\ ) change for either feature \\ ( x _ 1 \\ ) or \\ ( x _ 2 \\ ). we opted for these small discretized movement to limit to number of movements an agent", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "( x _ 1 \\ ) or \\ ( x _ 2 \\ ). we opted for these small discretized movement to limit to number of movements an agent can perform at any given state. we also believe that an agent will incrementally learn the right movements in order to move towards the decision boundary. figure 2 shows a possible path that a point can take to find a cfe. figure 2 : sample path that an agent can take in our environment. open ai gymopen ai gym provides a framework that allows us to create an environment for an agent to interact with. this is the default standard for defining rl environments (", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "gymopen ai gym provides a framework that allows us to create an environment for an agent to interact with. this is the default standard for defining rl environments ( and already comes with a set of pre - defined environments for different tasks ), however you have the ability to define your own environment. in order for us to learn a model that achieves our rl task, we must fit it into the open ai gym framework. 1. observation space : you must define what every state will look like for your rl environment. in python, all observation spaces must be defined by one of the following types : tuple, discrete, box", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "state will look like for your rl environment. in python, all observation spaces must be defined by one of the following types : tuple, discrete, box, dict, multi - binary, multi - discrete ( view here ). # we create an observation space as a box ( np array ) with limits ( - 1, 1 ) self. obeservation _ space = gym. spaces. box ( low = np. ones ( shape = len ( num _ features ) ) * - 1, high = np. ones ( shape = len ( num _ features ) ) ) 2. action space : you must also", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "features ) ) * - 1, high = np. ones ( shape = len ( num _ features ) ) ) 2. action space : you must also define what an action will look for any given agent. it is similarly defined by the same types as observation spaces ( view here ). # we create an action space as a tuple, where the first value is a feature # index and the second value is binary ( increasing or decreasing ) self. action _ space = gym. spaces. tuple ( ( gym. spaces. discrete ( num _ features ), gym. spaces. discrete ( 2 ) ) ) 3.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "gym. spaces. tuple ( ( gym. spaces. discrete ( num _ features ), gym. spaces. discrete ( 2 ) ) ) 3. step function : we define a function that when given an action ( from the action space ) and a state, it will be able to return the reward produced by taking this action. it will also return the new state the agent moved because of this action ( this may or may not be deterministic ). def step ( self, action : tuple [ int, int ] ) - > tuple [ tuple, float, bool, dict ] : \" \"", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "( self, action : tuple [ int, int ] ) - > tuple [ tuple, float, bool, dict ] : \" \" \" step function for the gym environment : param : action : action in the gym environment ( transformed _ feature _ index, increase / decrease ) : return : state : an observation space defined above : return : reward : reward for moving to that state : return : done : ( bool ) is the process complete : return : info : dict info about the environment at every step ( useful for debugging ) \" \" \" # get action and if we should increase or", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "return : info : dict info about the environment at every step ( useful for debugging ) \" \" \" # get action and if we should increase or decrease feature _ index = action [ 0 ] decrease = bool ( action [ 1 ] ) # set default reward to be negative reward = - 10. 0 done = false constant _ cost = 0. 0 # checks to make sure we are not changing an immutable feature if feature _ index in immutable _ features : return self. state, reward, done # check to make sure we are not decreasing a non - decreasing features if feature _ index in non _", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "_ features : return self. state, reward, done # check to make sure we are not decreasing a non - decreasing features if feature _ index in non _ decreasing _ features and decrease : return self. state, reward, done # move the agent ( x _ train should be normalized between 0 and 1 ) new _ state = self. state amount = - 0. 05 if decrease else 0. 05 new _ state [ feature ] + = amount self. state = new _ state # compute classifier reward ( if we crossed the decision boundary this number # will be very large ) classifier _ reward _, done = self.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "_ state # compute classifier reward ( if we crossed the decision boundary this number # will be very large ) classifier _ reward _, done = self. classifier _ reward ( self. state ) # compute knn reward manifold _ dist _ loss = self. distance _ to _ data _ manifold ( self. state ) # compute total reward reward = classifier _ reward - manifold _ dist _ loss return self. state, reward, done, info4. reset function : once the agent has reach a desired state ( or we want to start over ), we need to be able to reset to some starting state.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "reset function : once the agent has reach a desired state ( or we want to start over ), we need to be able to reset to some starting state. we must define the policy for determining the starting state in the reset function. def reset ( self, initial _ state : pd. series = none ) - > tuple : \" \" \" reset methods for gym environment. called after an episode is complete or called for starting evaluation with a specified starting initial state : param : initial _ state : initial starting state ( used for evaluation ) \" \" \" if initial _ state is not none : # this is done for inference self.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": initial _ state : initial starting state ( used for evaluation ) \" \" \" if initial _ state is not none : # this is done for inference self. state = initial _ state else : # randomly get a data point from our training dataset self. state = self. x _ train. sample ( ) return self. statewe can now formally define our fastcfe ( fast counterfactual explanations ) class, which has all the above components defined in one python class. class fastcfe ( gym. env ) : def _ _ init _ _ ( self, classifier _ predict : callable [ [", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "class. class fastcfe ( gym. env ) : def _ _ init _ _ ( self, classifier _ predict : callable [ [ pd. series ], np. ndarray ], x _ train : pd. dataframe, categorical _ features : list [ str ], immutable _ features : list [ str ] = none, non _ decreasing _ features : list [ str ] = none ) : \" \" \" initializes the gym environment to train the rl model for counter - factual - explanations ( cfe ) ( https : / / arxiv. org /", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "initializes the gym environment to train the rl model for counter - factual - explanations ( cfe ) ( https : / / arxiv. org / pdf / 2106. 03962. pdf ) \" \" \" # initialize all global class variables self. classifier = classifier _ predict self. x _ train = x _ train self. categorical _ features = categorical _ features self. immutable _ features = immutable _ features self. non _ decreasing _ features = non _ decreasing _ features # create the state and action space # we create an observation space as a box ( np array", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "self. non _ decreasing _ features = non _ decreasing _ features # create the state and action space # we create an observation space as a box ( np array ) with limits ( - 1, 1 ) self. obeservation _ space = gym. spaces. box ( low = np. ones ( shape = len ( x _ train. columns ) ) * - 1, high = np. ones ( shape = len ( x _ train. columns ) ) ) # we create an action space as a tuple, where the first value is a feature # index and the second value is binary ( increasing or decreasing ) self. action", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "create an action space as a tuple, where the first value is a feature # index and the second value is binary ( increasing or decreasing ) self. action _ space = gym. spaces. tuple ( ( gym. spaces. discrete ( len ( x _ train. columns ) ), gym. spaces. discrete ( 2 ) ) ) def step ( self, action : tuple [ int, int ] ) - > tuple [ tuple, float, bool, dict ] : \" \" \" step function for the gym environment : param : action : action in the gym environment ( transformed _ feature _ index,", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", dict ] : \" \" \" step function for the gym environment : param : action : action in the gym environment ( transformed _ feature _ index, increase / decrease ) : return : state : an observation space defined above : return : reward : reward for moving to that state : return : done : ( bool ) is the process complete : return : info : dict info about the environment at every step ( useful for debugging ) \" \" \" # get action and if we should increase or decrease feature _ index = action [ 0 ] decrease = bool ( action [ 1 ] ) # set default reward to be", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "action and if we should increase or decrease feature _ index = action [ 0 ] decrease = bool ( action [ 1 ] ) # set default reward to be negative reward = - 10. 0 done = false constant _ cost = 0. 0 # checks to make sure we are not changing an immutable feature if feature _ index in immutable _ features : return self. state, reward, done # check to make sure we are not decreasing a non - decreasing features if feature _ index in non _ decreasing _ features and decrease : return self. state, reward, done # move the agent ( x _ train should be normal", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "if feature _ index in non _ decreasing _ features and decrease : return self. state, reward, done # move the agent ( x _ train should be normalized between 0 and 1 ) new _ state = self. state amount = - 0. 05 if decrease else 0. 05 new _ state [ feature ] + = amount self. state = new _ state # compute classifier reward ( if we crossed the decision boundary this number # will be very large ) classifier _ reward _, done = self. classifier _ reward ( self. state ) # compute knn reward manifold _ dist _ loss = self. distance _", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "reward _, done = self. classifier _ reward ( self. state ) # compute knn reward manifold _ dist _ loss = self. distance _ to _ data _ manifold ( self. state ) # compute total reward reward = classifier _ reward - manifold _ dist _ loss return self. state, reward, done, info def reset ( self, initial _ state : pd. series = none ) - > tuple : \" \" \" reset methods for gym environment. called after an episode is complete or called for starting evaluation with a specified starting initial state : param : initial _ state : initial starting state ( used", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "environment. called after an episode is complete or called for starting evaluation with a specified starting initial state : param : initial _ state : initial starting state ( used for evaluation ) \" \" \" if initial _ state is not none : # this is done for inference self. state = initial _ state else : # randomly get a data point from our training dataset self. state = self. x _ train. sample ( ) return self. statetraining the model using ray + rllib now that we have created our environment, actions, reward, state - space, and have properly defined our openai gym environment, we must now", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "rllib now that we have created our environment, actions, reward, state - space, and have properly defined our openai gym environment, we must now produce a model that produces real - time counterfactual explanations. we must produce an optimal policy, \\ ( { \\ pi } ( s ) \\ ), which will produce an action for an agent in state \\ ( s \\ ) that will maximize future expected reward ( reaching our desirable state across the decision boundary ). in reinforcement learning, there are several ways to find the optimal policy \\ ( { \\ pi } \\ ), ranging from model - free to model - based", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". in reinforcement learning, there are several ways to find the optimal policy \\ ( { \\ pi } \\ ), ranging from model - free to model - based optimization techniques. we recommend this link to review more about training reinforcement learning algorithms. ray and rllib we were focused on finding an optimization algorithm and framework that is fast, scale - able, and easy to use. much of the recent work has been focused on distributed deep reinforcement learning, which uses neural networks to implicitly learn the optimal policy \\ ( { \\ pi } \\ ). one such framework is [ ray + rllib ] : ray is a new distributed framework", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ly learn the optimal policy \\ ( { \\ pi } \\ ). one such framework is [ ray + rllib ] : ray is a new distributed framework in python designed to distribute training tasks across any cluster. rllib is a specific package within ray that is designed to train different rl agents for different environments. rllib has a variety of different optimization algorithms, and provides a configurable dictionary that allows us to distribute training and evaluation to different cores and machines very easily. furthermore, it provides an easy api that allows use to modify the internal of the deep rl optimizer. rllib is maintained", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machines very easily. furthermore, it provides an easy api that allows use to modify the internal of the deep rl optimizer. rllib is maintained by anyscale ( founded out of berkeley riselab ) and it one of the state - of - the art frameworks for distributed computing / machine learning. we opted to use the proximal policy optimizer ( a deep rl algorithm ) because of its favorable balance of faster training times and simplicity. we needed an algorithm that would train relatively fast and that could distribute fairly simply, both of which ppo provides out of the box. we provide our pseudo - code", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "needed an algorithm that would train relatively fast and that could distribute fairly simply, both of which ppo provides out of the box. we provide our pseudo - code that provides a fastcfe specific wrapper around a native rllib ppo optimizer. for our implementation, we opted to use the proximal policy optimizer ( a deep rl algorithm ), which outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall - time [ schulman et. al ]. rllib provides an out of the box way to use ppo and distribute", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", simplicity, and wall - time [ schulman et. al ]. rllib provides an out of the box way to use ppo and distribute it across a cluster and on your laptop. below, we showcase some pseudo for how we wrapped our optimizer around rllib to provide an easy package to use rllib and our fastcfe model : class raylib ( ) : def _ _ init _ _ ( self, env : fastcfe = none ) - > none : \" \" \" initializes a raylib optimizer for ppo ( can add functionally for other optimi", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cfe = none ) - > none : \" \" \" initializes a raylib optimizer for ppo ( can add functionally for other optimizers ) : param : env : the fastcfe class which contains our gym environment \" \" \" if env is not none : super ( ). _ _ init _ _ ( env ) self. env = env self. model : optional [ ppo. ppotrainer ] = none self. config : optional [ dict [ str, any ] ] = none def train ( self ) - > none : \" \"", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "= none self. config : optional [ dict [ str, any ] ] = none def train ( self ) - > none : \" \" \" train the rl agent for the cfe environment ( with hyperparameters ) view more of the features here : ( https : / / docs. ray. io / en / master / rllib - training. html ) \" \" \" hyperparameters = ppo. default _ config. copy ( ) # register the cfe environment to be trained in ray def env _ creator ( env : fastcfe ) : return en", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##g. copy ( ) # register the cfe environment to be trained in ray def env _ creator ( env : fastcfe ) : return env register _ env ( \" my _ env \", env _ creator ) # set the environment config variables hyperparameters ['env _ config'] = self. env # create the ppo trainer with the configuration and train self. model = ppo. ppotrainer ( env = \" my _ env \", config = self. config ) print ( \" training rl agent.", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##otrainer ( env = \" my _ env \", config = self. config ) print ( \" training rl agent..... \" ) for i in range ( 10 ) : result = self. model. train ( ) print ( f \" training iteration : { i + 1 } / { num _ epochs } \" ) print ( f \" total number of datapoints sampled : { result ['episodes _ total'] } \" ) print ( f \" mean reward ( max of 100 ) : { result ['episode _ reward _ mean'] } \" ) print (", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "] } \" ) print ( f \" mean reward ( max of 100 ) : { result ['episode _ reward _ mean'] } \" ) print ( f \" avg. steps to produce counterfactual ( min : 1, max : 20 ) : { result ['episode _ len _ mean'] } \" ) print ( \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \" ) return none def predict ( self, initial _ state : pd. series, max _ steps : int = none ) - > tuple [ list [ pd. series ], float, bool ] : \" \" \" make a counterfactual prediction : param : initial _ state : initial _ state : para", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "series ], float, bool ] : \" \" \" make a counterfactual prediction : param : initial _ state : initial _ state : param : mode : scaled or unscaled path \" \" \" if self. model : max _ steps = 50 num = 1 next _ state : tuple = self. env. reset ( initial _ state = initial _ state ) done : bool = false reward : float = 0. 0 steps = [ ] while num < max _ steps : action = self. model. compute _ action ( next _ state ) next _ state, reward, done,", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "[ ] while num < max _ steps : action = self. model. compute _ action ( next _ state ) next _ state, reward, done, info = self. env. step ( action ) steps. append ( next _ state ) if done : break num + = 1 return steps, reward, done else : raise valueerror ( \" model not loaded \" ) benchmarkswe want to showcase two major benchmarks for this algorithm and implementation : performance metrics : we want to understand how our fastcfe approach using rl compares against other methods for performance metrics ( described later ). training time", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "performance metrics : we want to understand how our fastcfe approach using rl compares against other methods for performance metrics ( described later ). training time : we want to see how much savings we achieved when using a distributed rl framework like ray against a single - threaded research implementation. these benchmarks for implemented for a variety of combinations of the following credit risk datasets. all of these datasets contained some from of credit risk data and the models are binary classifiers that predict a single applicant ( one row ) should be accepted or rejected for a loan. we want to find counterfactual explanations for all the", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "classifiers that predict a single applicant ( one row ) should be accepted or rejected for a loan. we want to find counterfactual explanations for all the rejected applicants. the sizes of the datasets are shown below ( number of rows by number of columns ) : german credit : 1, 000 data - points x 21 featuresadult credit : 48, 842 data - points x 14 featurescredit default : 30, 000 data - points x 25 featuressmall encoded credit dataset : 332, 000 data - points x 44 featureslarge encoded credit dataset : 332, 000 data - points x 652 featuresthe", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "encoded credit dataset : 332, 000 data - points x 44 featureslarge encoded credit dataset : 332, 000 data - points x 652 featuresthe first three datasets ( german credit, adult credit, credit default ) are all open source datasets with the links provided above. the last two datasets are proprietary datasets with obfuscated column names. these datasets were larger and tested the scaleable of our implementation. performance metricswe want to see how our fastcfe model compares to other well - known methods. specifically, we are focusing on the following two metrics : validity :", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##e want to see how our fastcfe model compares to other well - known methods. specifically, we are focusing on the following two metrics : validity : this is the total number of counterfactual explanations found divided by the total number of data points. this is represented as a percentage. mean inference time : this is the mean time it takes to calculate a batch of inferences. namely is the time it takes to compute \\ ( n \\ ) inferences divided by \\ ( n \\ ). the results shown below are fastcfe against a number of state of the art counterfactual explanation methods : adult credit validity", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\\ ( n \\ ). the results shown below are fastcfe against a number of state of the art counterfactual explanation methods : adult credit validity ( % ) mean inference time ( s ) dice - genetic 98. 1 1. 71 dice - random 100 0. 17 mace lr 100 38. 45 mace rf 100 101. 29 fastcfe 97. 3 0. 07 german credit validity ( % ) mean inference time ( s ) dice - genetic 89. 5 3. 45 dice - random 100 0. 22 dice - gradient 84 59. 75 fastcfe 100 0. 015 credit default validity ( % )", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "5 3. 45 dice - random 100 0. 22 dice - gradient 84 59. 75 fastcfe 100 0. 015 credit default validity ( % ) mean inference time ( s ) dice - genetic 92. 6 3. 58 dice - random 100 0. 39 dice - gradient 81. 0 479. 17 fastcfe 99. 9 0. 051 as we see here, we perform nearly as close as the best method ( dice - random ) across these three different datasets. furthermore, we have inference times of up to 20x faster than dice - random. training timethe first implementation of this project was done", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "different datasets. furthermore, we have inference times of up to 20x faster than dice - random. training timethe first implementation of this project was done using a package called stablesbaseline3, and was naively computed trained on a single machine. this section wants to showcase the change in training time of our scaleable and distributed rllib implementation against a naive implementation. the results are shown below : rllib train time ( hrs ) naive train time ( hrs ) german credit. 25 1. 5 credit default 1 6 small encoded 1. 5 8 large encoded 8 dnf we achieve a nearly 6x", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "hrs ) german credit. 25 1. 5 credit default 1 6 small encoded 1. 5 8 large encoded 8 dnf we achieve a nearly 6x savings in train time and can handle much larger datasets than we could through our naive implementation. this shows the promise and power of using a scaleable and distributed reinforcement learning framework \u2014 we can significantly reduce training times which is a major bottleneck for several reinforcement learning applications. conclusionwe hope this article provided an overview into the following ideas / concepts : counterfactual explanations : what they are and how they are useful for industrial and explainability applicationsreinforcement learning implementation", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the following ideas / concepts : counterfactual explanations : what they are and how they are useful for industrial and explainability applicationsreinforcement learning implementation : how we implement a production level reinforcement learning model. power of distribution : how we can achieve tremendous savings by using scalable and distributed reinforcement learning frameworks such as rllib. we hope that we provided some interesting ideas and some starter code to help you make your own reinforcement learning model. if you would like to learn more about this article, please reach out! previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "learn more about this article, please reach out! previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / fast - counterfactual - explanations - using - reinforcement - learning", "metadata": {"source": "https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning", "row": 208, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 209 text : two commitments every employer should make in 2022 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedlife at arthurtwo commitments every employer should make in 2022by : adam wencheljanuary 27, 2022 \u201c i really want to work at a startup. but, i care about my mental health. \u201d if you want to see an animated reaction, make a comment like that to", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "want to work at a startup. but, i care about my mental health. \u201d if you want to see an animated reaction, make a comment like that to someone on our team. yes, startups are hard work ; audacious goals always are. but they don \u2019 t have to come at the expense of anybody \u2019 s well - being. the last couple of years have been challenging. people are reassessing their priorities, craving meaning in relationships, in free time pursuits, and importantly, in our work. at the same time, people want to feel supported \u2013 and employers need to step up and use their unique position to", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "pursuits, and importantly, in our work. at the same time, people want to feel supported \u2013 and employers need to step up and use their unique position to create community and care for people more deeply than they ever have before. at arthur, we commit to both : fulfilling and challenging work at a company that provides community and care. people join arthur because they are excited to solve complex problems at the intersection of technology and humanity, and to do it as part of a diverse, creative, and passionate team. the same team that is also raising children, caring for loved ones, taking classes, running marathons, exploring cities, and doing", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "creative, and passionate team. the same team that is also raising children, caring for loved ones, taking classes, running marathons, exploring cities, and doing a myriad of other things that make us each happy and whole. it \u2019 s awesome to see both our work and our community get strengthened as our team grows and people bring new and unique passions with them! this belief that we need both fulfilling work and genuine community centers our culture at arthur and we feel the impact of it every day. here are 7 ways we are creating meaning, connection, and caring at arthur : work that has a huge impact on the world. we \u2019 re", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "every day. here are 7 ways we are creating meaning, connection, and caring at arthur : work that has a huge impact on the world. we \u2019 re working to make ai more transparent and equitable, changing how technology affects real humans for the better. flexible work locations. we have open offices for those that want to safely work and hang in the same room ; and we support fully remote work across the country. a diverse team that cares & celebrates. we practice empathy, kindness, and inclusion in our work together. we shout out each other \u2019 s wins and take the time to help each other solve problems. team offsites.", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "kindness, and inclusion in our work together. we shout out each other \u2019 s wins and take the time to help each other solve problems. team offsites. we meet somewhere cool as a company twice a year for fresh air and time to be creative, think big - picture, and have some laughs together. a learning mindset and budget. our work on the cutting edge of ai technology keeps us learning all the time, and we offer a generous education and professional development budget. true time away. our flexible vacation policy includes mandatory minimums, and we fully close the office over the holidays. we also believe in \u201c no weekend messages \u201d", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "true time away. our flexible vacation policy includes mandatory minimums, and we fully close the office over the holidays. we also believe in \u201c no weekend messages \u201d and encourage \u201c roam the earth \u201d remote summer work to explore our awesome planet. a big vision \u2013 and the chance to own a piece of it. there \u2019 s a ton of opportunity for ownership and growth along the road to achieving our big goals for arthur. every member of our team is critical in delivering on that vision. it \u2019 s neat to see word spread about the positive culture we are creating \u2013 we \u2019 ve recently been named one of business insider \u2019 s 2022 enterprise", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". it \u2019 s neat to see word spread about the positive culture we are creating \u2013 we \u2019 ve recently been named one of business insider \u2019 s 2022 enterprise - tech startups to bet your career on and we made built in nyc \u2019 s best places to work. it \u2019 s an important formula that has an outsized impact \u2013 care about people, create connection, do meaningful work and build something really special in the process. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvta", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / i - really - want - to - work - at - a - startup - but", "metadata": {"source": "https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but", "row": 209, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 210 text : built in honors arthur in its esteemed 2022 best places to work awards solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##servabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesbuilt in honors arthur in its esteemed 2022 best places to work awardsby : arthur teamjanuary 5, 2022arthur earns 15th place on built in \u2019 s list of 50 best small companies in new york cityfor", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##by : arthur teamjanuary 5, 2022arthur earns 15th place on built in \u2019 s list of 50 best small companies in new york cityfor immediate release new york, ny / / january 5, 2022 \u2014 built in today announced that arthur is a winner in their 2022 best places to work awards. specifically, arthur earned a place on the 50 best small places to work in nyc and 50 best paying companies in nyc lists. the annual awards program includes companies of all sizes, from startups to enterprise, and honors both remote - first employers as well as companies in the eight largest tech markets across the u. s", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of all sizes, from startups to enterprise, and honors both remote - first employers as well as companies in the eight largest tech markets across the u. s. \u201c we \u2019 re thrilled to be recognized alongside the best companies in new york, \u201d says adam wenchel, arthur \u2019 s co - founder & ceo. \u201c i \u2019 m constantly inspired by our team \u2019 s focus on our mission to make ai work for everyone. we \u2019 ve clearly seen and felt the challenges the last two years have presented for people \u2013 and so creating a caring culture built on integrity, equity, diversity, and compassion is the best thing we can do for", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "last two years have presented for people \u2013 and so creating a caring culture built on integrity, equity, diversity, and compassion is the best thing we can do for all of us. 2022 will be a big year for us and we are excited to welcome many new voices! \u201d check out open roles at arthur here. built in determines the winners of best places to work based on an algorithm, using company data about compensation, benefits and companywide programming. to reflect the benefits candidates are searching for more frequently on built in, the program also weighs criteria like remote and flexible work opportunities, programs for dei and other people - first cultural offerings.", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "are searching for more frequently on built in, the program also weighs criteria like remote and flexible work opportunities, programs for dei and other people - first cultural offerings. \u201c it is my honor to extend congratulations to the 2022 best places to work winners, \u201d says sheridan orr, chief marketing officer, built in. \u201c this year saw a record number of entrants \u2014 and the past two years fundamentally changed what tech professionals want from work. these honorees have risen to the challenge, evolving to deliver employee experiences that provide the meaning and purpose today \u2019 s tech professionals seek. \u201d about built inbuilt in is creating the largest platform", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", evolving to deliver employee experiences that provide the meaning and purpose today \u2019 s tech professionals seek. \u201d about built inbuilt in is creating the largest platform for technology professionals globally. monthly, more than three million of the industry \u2019 s most in - demand professionals visit the site from across the world. they rely on our platform to stay ahead of tech trends and news, develop their careers and find opportunities at companies whose values they share. built in also serves 1, 800 innovative companies of all sizes, ranging from startups to the fortune 100. by putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "companies of all sizes, ranging from startups to the fortune 100. by putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard - to - reach tech professionals, locally, nationally or remotely. www. builtin. com about built in \u2019 s best places to workbuilt in \u2019 s esteemed best places to work awards, now in its fourth year, honor companies across numerous categories : 100 best places to work, 50 best small places to work, 100 best midsize places to work, 50 companies with the best benefits and 50 best paying companies, 100 best large companies to work for,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to work, 100 best midsize places to work, 50 companies with the best benefits and 50 best paying companies, 100 best large companies to work for, and 100 best remote - first places to work. about arthurat arthur, we are deeply passionate about building technology to make ai work for everyone. arthur is the ai performance company that delivers on the full potential of equitable ai for enterprise customers through performance monitoring and optimization, explainability, and bias detection. learn more at www. arthur. ai contacts : tory marlin, director of marketing tory @ arthur. ai previous postsharenext post we make ai better for everyone.", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "www. arthur. ai contacts : tory marlin, director of marketing tory @ arthur. ai previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - named - a - best - place - to - work -", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "serviceprivacy source : https : / / www. arthur. ai / blog / arthur - named - a - best - place - to - work - in - 2022", "metadata": {"source": "https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022", "row": 210, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 211 text : a crash course in fair nlp for practitioners solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessa crash course in fair nlp for practitioners by : jessica dainovember 14, 2021the topic of fairness in natural language processing ( nlp ) is exceptionally broad ; in this post, we hope to distill some of the key points from academic literature for an", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in natural language processing ( nlp ) is exceptionally broad ; in this post, we hope to distill some of the key points from academic literature for an audience of technical practitioners. in section 1, we outline a general framework for thinking about fairness ; in section 2, we survey some notable academic work ; and in section 3, we outline a set of questions that may be useful when considering specific applications. 1. what does fairness in nlp even mean? the core idea we hope to illustrate in this post is that there is no panacea for magically achieving \" fair nlp \" \u2014 though many of the core problems are", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we hope to illustrate in this post is that there is no panacea for magically achieving \" fair nlp \" \u2014 though many of the core problems are intuitive, the complexity of human language means that measuring, much less mitigating, \" unfairness \" is a difficult task. \" bias \" and \" fairness \" are exceptionally broad terms that span wide range of possible behaviors. there is no single definition of desirable \" fair \" behavior ; to the extent that nlp systems model the world ( or a particular worldview ), there is no single perfectly neutral, unbiased model. in other words, any nlp system involves", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model the world ( or a particular worldview ), there is no single perfectly neutral, unbiased model. in other words, any nlp system involves some proposition about both what the world does look like and what the would should look like ; for practitioners, it's critical to think deeply and with precision about what exactly desired behavior looks like, and why. that being said : the first component of any approach to fairness is defining who and what exactly we want to be fair with respect to. while social groups in the real world are fluid, in artificial intelligence ( ai ) and machine learning ( ml ) we typically define discrete groups", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "fair with respect to. while social groups in the real world are fluid, in artificial intelligence ( ai ) and machine learning ( ml ) we typically define discrete groups along the axes of gender, race / ethnicity, and religion. ( fair ml, especially in language, unfortunately tends to treat gender as binary \u2014 both because it is mathematically convenient and because the data on which language models are trained often reflect a binary. ) for fairness in a typical tabular data setting, we generally assume that each data point reflects information about a single person, and that the values of these demographic attributes are generally known or accessible for each datapoint. for fairness", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "assume that each data point reflects information about a single person, and that the values of these demographic attributes are generally known or accessible for each datapoint. for fairness in nlp, however, there isn't always a clear mapping between text and demographic information. more specifically, social groups might be inferred, but demographic information may be labelled based on author demographic \u2014 that is, text generated by particular groups, which covers things like dialect, accent, or writing style \u2014 or subject demographic \u2014 that is, text about particular groups. crucially, author and subject demographic are distinct approaches to defining fairness. which method to use for demographic label", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "subject demographic \u2014 that is, text about particular groups. crucially, author and subject demographic are distinct approaches to defining fairness. which method to use for demographic labelling is context - dependent and varies based on the task at hand. similarly, there are many dimensions in nlp settings across which fairness can be measured. when the end - goal of the nlp model is something like classification or regression, we might be able to apply existing metrics for fairness in these applications by measuring the group - conditional performance ( e. g. positivity rate, tpr, fpr, etc. ). in language, particularly text generation,", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the group - conditional performance ( e. g. positivity rate, tpr, fpr, etc. ). in language, particularly text generation, additional harms arise \u2014 most prominently, language models which propagate harmful societal stereotypes. measuring stereotypes is a murkier task : existing ( academic ) approaches have focused on either investigating the trained model artifact itself ( i. e. the word embeddings ), or evaluating the model outputs on some specially - curated datasets. however, both of these approaches have known issues, and should not be considered to be any conclusive or concrete standards. 2. a", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "curated datasets. however, both of these approaches have known issues, and should not be considered to be any conclusive or concrete standards. 2. a ( non - exhaustive ) survey of relevant workordered by year. starred entries are worth reading in full! 1. man is to computer programmer as woman is to homemaker? debiasing word embeddings ( 2016 ) one of the first works on \" bias \" in language models. measures / illustrates bias by using the word embeddings to generate analogies via vector addition / substraction, showing that embeddings confirm stereotypes ; demographic groups", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "bias by using the word embeddings to generate analogies via vector addition / substraction, showing that embeddings confirm stereotypes ; demographic groups are therefore determined with respect to text content. debiasing approach involves identifying the \" gender subspace \" and rotating the word embeddings such that they are orthogonal to the subspace. 2. gender and dialect bias in youtube automatic captions ( 2017 ) this is an illustrative example of the many moving parts in what is casually referred to as \" fair nlp \". this is a speech - to - text task : i. e., one where the", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "parts in what is casually referred to as \" fair nlp \". this is a speech - to - text task : i. e., one where the output itself is text. however, there is some notion of performance that summarizes the goodness of the text output in a single number. demographic groups are determined with respect to speaker ( author ), not content. bias is observed here because of differential model performance across groups. 3. social bias in elicited natural language inferences ( 2017 ) this work analyzes a dataset that was popular at the time ( the stanford natural language inference corpus ; with the rise of larger", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "language inferences ( 2017 ) this work analyzes a dataset that was popular at the time ( the stanford natural language inference corpus ; with the rise of larger language models trained on the web it's unclear the extent to which this is still used ). the approach is similar to \" man is to computer programmer \" \u2014 they use a mathematical measurement of similarity between words ( in this case mutual information ) to find ( gendered ) associations across words, with demographic groups determined by text content. in my opinion, this does exhibit some of the pitfalls outlined in \" language ( technology ) is power \" \u2014 there is no explicit discussion of", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "text content. in my opinion, this does exhibit some of the pitfalls outlined in \" language ( technology ) is power \" \u2014 there is no explicit discussion of what comprises a harmful association. 4. neural user factor adaptation for text classification : learning to generalize across author demographics ( 2019 ) here, demographic groups are determined via authorship rather than text content \u2014 this work explores gender, age, country, and region ; finds the existence of performance disparities across groups ; and introduces a novel approach to learn text classifiers which reduce those performance disparities. 5. black is to criminal as caucasian is to police : detecting and removing multi", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a novel approach to learn text classifiers which reduce those performance disparities. 5. black is to criminal as caucasian is to police : detecting and removing multiclass bias in word embeddings ( 2019 ) an extension of the 2016 \" man is to computer programmer \" paper to the multiclass setting ; the original work made use of \" binary \" gender in calculating a \" gender direction / subspace \". 6. mitigating gender bias in natural language processing : literature review ( 2019 ) a lit review of approaches to gender bias x nlp ( at this point a few years old ) ; mostly useful for a high - level", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "review ( 2019 ) a lit review of approaches to gender bias x nlp ( at this point a few years old ) ; mostly useful for a high - level overview of many possible tasks and approaches. 7. lipstick on a pig : debiasing methods cover up systematic gender biases in word embeddings but do not remove them ( 2019 ) this work shows limitations of the approaches to debiasing word embeddings in \" man is to computer programmer \" and \" black is to criminal \". in short, while the approaches enumerated in those papers do successfully debias with respect to their original definitions of \"", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\" black is to criminal \". in short, while the approaches enumerated in those papers do successfully debias with respect to their original definitions of \" bias, \" they ultimately preserve most relationships between words in the corpus : \" gendered \" words still cluster together. as a result, it is possible to recover the original \" biased \" relationships, and they may persist in downstream applications of those embeddings even if not detected according to the original metric. this work is a clear example of why \" debiasing \" in general but especially in language must be evaluated with a critical / skeptical eye, and why specifications of desired", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is a clear example of why \" debiasing \" in general but especially in language must be evaluated with a critical / skeptical eye, and why specifications of desired \" unbiased \" behavior must be careful and precise. 8. * * language ( technology ) is power : a critical survey of \" bias \" in nlp ( 2020 ) this survey paper is worth reading ( or at least skimming ) in full. this paper is motivated by the idea that there is no single definition of \" desirable behavior, \" and no such thing as a \" completely unbiased \" model or dataset ; instead, any specification of desired behavior is", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "definition of \" desirable behavior, \" and no such thing as a \" completely unbiased \" model or dataset ; instead, any specification of desired behavior is inherently value laden. the survey conducted of work on \" fairness / bias in nlp \" finds that most such work does not state clearly what comprises \" bias \" and how to conceptualize algorithm behavior with respect to broader societal power structures \u2014 to whom the harm is done and how those groups are defined ; whether the harm is primarily representational or allocational ; what behavior is deemed harmful and what is not, and why \u2014 and more. 9. crows - pairs : a challenge data", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "primarily representational or allocational ; what behavior is deemed harmful and what is not, and why \u2014 and more. 9. crows - pairs : a challenge dataset for measuring social biases in masked language models ( 2020 ) introduces a dataset for evaluating the performance of masked language models ( models trained on data like this is a masked sentence ; the [ mask ] is to determine what word is behind the masked token. \u2014 the paper reports results on bert and bert + models ). several axes of discrimination / bias are included \u2014 gender, race, sexual orientation, nationality, religion, age, dis / ability, appearance, and socio", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". several axes of discrimination / bias are included \u2014 gender, race, sexual orientation, nationality, religion, age, dis / ability, appearance, and socioeconomic status. 10. realtoxicityprompts : evaluating neural toxic degeneration in language models ( 2020 ) this paper focuses on toxicity, specifically the generation of toxic ( racist, sexist, etc ) text by pre - trained language models. this is a slightly different paradigm than the typical \" bias \" approach \u2014 rather than considering harms against specific groups, this work groups all harmful / derogatory generated text as \" toxic. \" the authors find that even", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "approach \u2014 rather than considering harms against specific groups, this work groups all harmful / derogatory generated text as \" toxic. \" the authors find that even surface - level innocuous prompts can trigger highly problematic output, and that existing methods are insufficient to prevent this ; upon inspection of training corpora, they find high volumes of toxic content in the training data. 11. * * on the dangers of stochastic parrots : can language models be too large? ( 2020 ) this is the infamous stochastic parrots paper that ultimately led to the ousting of drs. timnit gebru &", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "( 2020 ) this is the infamous stochastic parrots paper that ultimately led to the ousting of drs. timnit gebru & margaret mitchell from google research. this is a broader survey paper about the harms of large language models, including the centralization of power, cultural homongenization / flattening, environmental harms, among others. worth a read for broad context in responsible nlp. 12. redditbias : a real - world resource for bias evaluation and debiasing of conversational language models ( 2021 ) introduces a dataset for evaluating the bias of conversational language tasks based", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- world resource for bias evaluation and debiasing of conversational language models ( 2021 ) introduces a dataset for evaluating the bias of conversational language tasks based on reddit data, as well as evaluation frameworks for conversational model performance after debiasing \u2014 includes four axes ( gender, race, religion, queerness ). they benchmark dialogpt and several debiasing approaches on this dataset and find evidence of bias with respect to religion that can be mitigated with some methods ( though not all ). 13. rooting out anti - muslim bias in popular language model gpt - 3 ( 2021 ) this", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##igated with some methods ( though not all ). 13. rooting out anti - muslim bias in popular language model gpt - 3 ( 2021 ) this is a blog post ( with a link to the full paper ) summarizing some explorations of anti - muslim bias in gpt - 3's text generation. in short, gpt - 3 exhibits substantial anti - muslim bias in its generated text, which is only slightly reduced in existing mitigation methods. 14. * * stereotyping norwegian salmon : an inventory of pitfalls in fairness benchmark datasets ( 2021 ) this paper surveys existing benchmark data", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "14. * * stereotyping norwegian salmon : an inventory of pitfalls in fairness benchmark datasets ( 2021 ) this paper surveys existing benchmark datasets for evaluating \" fairness \" in nlp tasks. the authors \u2014 who also wrote the 2020 \" language ( technology ) is power \" paper \u2014 apply a social science approach ( \" measurement modeling \" ), and find that the benchmark datasets themselves have unclear definitions and specifications behind both what constitutes \" biased \" or \" stereotyping \" behavior and what constitutes desirable model behavior. if evaluating models on the datasets covered in this paper, the results should be taken", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or \" stereotyping \" behavior and what constitutes desirable model behavior. if evaluating models on the datasets covered in this paper, the results should be taken with a grain of salt. worth a skim, especially the illustrative example on the first page, to understand the gist of the criticism. 15. challenges in automated debiasing for toxic language detection ( 2021 ) this paper focuses on text classifiers, specifically toxicity detection. demographic groups are explored both in terms of text content ( swear words, slurs, identity mentions ) and text authorship ( aave dialectical markers ). bias here is defined by the un", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "terms of text content ( swear words, slurs, identity mentions ) and text authorship ( aave dialectical markers ). bias here is defined by the unjustified flagging of toxic text ( in conventional classification terms, high false positive rates ). the authors find that existing methods are generally unsuccessful in debiasing toxicity detectors, and propose a proof of concept approach which synthetically relabels the training data ; this approach ( modifying the training data ) is more effective than attempting to modify a pretrained model.'additionally, some scholars who actively work in this area are jieyu zhao, su lin blodgett", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "attempting to modify a pretrained model.'additionally, some scholars who actively work in this area are jieyu zhao, su lin blodgett, amandalynne paullada ; their future work is likely to be relevant as well. 3. a worksheet for practitioners1. defining the language task and model setting : assuming the model takes in some amount of text, does it generate a single output ( e. g. a probability, a classification, or multiple classifications ), or text output? 2. defining the sensitive attribute : do you care about author demographic or subject demographic, or both? are you able", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or multiple classifications ), or text output? 2. defining the sensitive attribute : do you care about author demographic or subject demographic, or both? are you able to come up with or access sensitive feature values for each data point? for example, can you come up with a vector that looks like [ < string input >, < demographic info > ]? 3. defining and measuring the harm : if the model generates a single output, we can check typical measures of fairness ( disparate accuracies or tprs or fprs or positivity rates etc ). if the model generates text output : \u2014 is there any", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##sparate accuracies or tprs or fprs or positivity rates etc ). if the model generates text output : \u2014 is there any notion of performance that is used to measure the \" goodness \" of the text output? you may be able to measure the performance of the generated text and determine whether there are group - wise performance disparities if you already have a means for evaluating generated text. \u2014 what sorts of representational or stereotyping harms do you anticipate? in other words, what is the best - case expected output, and what does a \" bad \" output look like? 4.", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "do you anticipate? in other words, what is the best - case expected output, and what does a \" bad \" output look like? 4. mitigating the harm : if the model generates a single output : existing classification / regression postprocessing approaches to fairness may be worth attempting, though they will be limited in that they cannot make use of the the text input. see annotated bibliography for some examples of bias work in text classification. if the model generates text output : \u2014 for concerns around representational harms ( such as stereotyping ), do you have a sense of what prompts might trigger", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model generates text output : \u2014 for concerns around representational harms ( such as stereotyping ), do you have a sense of what prompts might trigger \" bad \" output? \u2014 most mitigation techniques for language models rely on adjusting model internals, and even then, have varied degrees of success ( see annotated bibliography ). everything you need to know about model monitoring for natural language processing. discover arthur \u2019 s features for ensuring fairness, including detecting and mitigating bias in nlp. photo by jason leung on unsplashprevious postsharenext post we make ai better for everyone. sign up for", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##igating bias in nlp. photo by jason leung on unsplashprevious postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / crash - course - in - fair - nlp - for - practitioners", "metadata": {"source": "https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners", "row": 211, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 212 text : hotspots : automating underperformance regions surfacing in machine learning systems solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##firewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringhotspots : automating underperformance regions surfacing in machine learning systemsby : kenneth chennovember 1, 2021isolating problematic data for remediation and retraining ml modelswhen a system has", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning systemsby : kenneth chennovember 1, 2021isolating problematic data for remediation and retraining ml modelswhen a system has high dimensional data, troubleshooting the right data input regions becomes a difficult problem. hotspots automates identifying regions associated with poor ml performance to significantly reduce time and error of finding such regions. overviewwe might have a ml model deployed in production and some monitoring in place. we might notice that performance is degrading from classic performance metrics or from drift monitoring combined with explainability techniques. we \u2019 ve identified that our model is failing, and the next step is to", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ing from classic performance metrics or from drift monitoring combined with explainability techniques. we \u2019 ve identified that our model is failing, and the next step is to identify why our model is failing. this process would involve slicing and dicing our input data that caused model degradation. that is, we want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks or retraining our models on those regions. this basically boils down to a time - consuming task of finding needles in a haystack. what if we could reverse engineer the process and surface all of the needles", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "boils down to a time - consuming task of finding needles in a haystack. what if we could reverse engineer the process and surface all of the needles, i. e. input regions associated with poor performance, directly to the user? we can! the steps we \u2019 ll take are ( 1 ) train a decision tree on a proper partitioning objective. ( 2 ) create and store hotspot tree artifact. ( 3 ) retrieve hotspots from the hotspot tree at query time. fig1 : a general framework for training dataset and using it to create hotspot tree, which is then queried to", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ot tree at query time. fig1 : a general framework for training dataset and using it to create hotspot tree, which is then queried to retrieve hotspots. each phase outputs the artifact that is used in part i of the following row. in the toy example below that we \u2019 ll use throughout this post, we have two ground truth regions separated by a parabolic function, with blue above and red below the parabola. the color of the datapoints represent the predictions. we want to isolate the hotspot regions where the prediction color does not match the region color, which we do so in the pale boxes", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "represent the predictions. we want to isolate the hotspot regions where the prediction color does not match the region color, which we do so in the pale boxes for two different accuracy thresholds. fig2 : example of hotspots retrieval on two inputs. the boxes are the hotspots, where blue points in red regions and red points in blue regions are incorrect classifications. as the threshold ( accuracy, in this case ) decreases, the algorithm targets node candidates with more incorrect classifications and the hotspot regions are narrower / purer. ( 1 ) train a decision tree on a proper partitioning objectivedecision treeas soon as", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "classifications and the hotspot regions are narrower / purer. ( 1 ) train a decision tree on a proper partitioning objectivedecision treeas soon as we think about partitioning data into regions of interest, we should think about tree models, and specifically a decision tree. remember that our task is ultimately an inference task and not a prediction task, so there is no need to use an ensemble of trees model like random forests or xgboost because ( a ) we \u2019 re not trying to perform predictions and ( b ) ensembles introduce noise and non - deterministic decision paths for splitting our data. recall that the premise of", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 re not trying to perform predictions and ( b ) ensembles introduce noise and non - deterministic decision paths for splitting our data. recall that the premise of decision tree splits are based on selecting a feature and split value, among an enumeration over input features and their possible values, that minimizes impurity to create children that are more pure, based on the output labels. in simple speak, let \u2019 s say output was color and we have blue and red marbles. all of the marbles have varying diameters across both groups, but blue marbles are textured while red marbles are smooth. if we had to", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". all of the marbles have varying diameters across both groups, but blue marbles are textured while red marbles are smooth. if we had to choose between diameter size or texture to partition our marbles, we \u2019 d choose to partition our marbles based on texture, i. e. textured or smooth, since that would perfectly separate blue marbles into one group and red marbles into another, effectively reducing impurity to 0 in each group in this case. in reality, a dataset would need multiple splits in order to reduce impurity to 0 in the leaf nodes. partitioning objectiveso what is exactly the", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". in reality, a dataset would need multiple splits in order to reduce impurity to 0 in the leaf nodes. partitioning objectiveso what is exactly the equivalent of the the blue and red marbles example above? we ultimately want to separate the bad predictions from the good predictions and need some metric as the output, i. e. partitioning objective, in our decision tree. for classification, we can encode correct classifications as 1s and incorrect classifications as 0s. if we want more granularity while partitioning, we could also encode classifications as 1, 2, 3, or 4 for true positive, false positive", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "if we want more granularity while partitioning, we could also encode classifications as 1, 2, 3, or 4 for true positive, false positive, true negative, and false negative, respectively. for regression, we actually need to encode the regression outputs, i. e. rmse between ground truths and predictions, as classification outputs, e. g. a datapoint is encoded as 1 if rmse is greater than median rmse + 2 median average deviation of rmse and 0 otherwise. we could also use a percentile rule, e. g. datapoints with rmse over the 80 % percentile are", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of rmse and 0 otherwise. we could also use a percentile rule, e. g. datapoints with rmse over the 80 % percentile are 1s and 0s otherwise. the reason why we do not use mean and standard deviation is because those values are skewed by high rmse outliers, and the entire point is to partition datapoints with high rmse compared to the average. this mimics the behavior we want in the classification case, and we \u2019 ll dive more in the mathematics for why we cannot use regression outputs, after the methodology is explained for the classification case. ( 2 ) create and", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and we \u2019 ll dive more in the mathematics for why we cannot use regression outputs, after the methodology is explained for the classification case. ( 2 ) create and store hotspot tree artifactif we feed the 500 datapoints from our toy example into a decision tree, using our encoding method discussed above for four classes, the tree looks like this : fig3 : example of [ iv. trained decision tree ] on two inputs, x [ 0 ] and x [ 1 ]. the boxes are nodes, where each node has an associated set of metrics ( i. e. accuracy, precision, recall, f1 score ) that can be", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the boxes are nodes, where each node has an associated set of metrics ( i. e. accuracy, precision, recall, f1 score ) that can be used as thresholds in querying for hotspots. each node represents a collection of datapoints, which are filtered by the accumulated ranges on the input features on the path from the top root node to the particular node in question. a hotspot is a node that violates the threshold specified by a user when [ iii. extract hotspot nodes ] step is performed. hotspots can be any node in the tree and not only the terminal leaf nodes at the", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "[ iii. extract hotspot nodes ] step is performed. hotspots can be any node in the tree and not only the terminal leaf nodes at the bottom of the tree. here, we only have two features : x [ 0 ] and x [ 1 ]. at each node in a decision tree, the data is split into two children nodes based on a feature and cutoff value. as an example, at the root node, the 500 datapoints are split into two groups, a left child group and right child group, where x [ 1 ] \u2264 - 2. 258 and x [ 1 ] > - 2. 258", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "two groups, a left child group and right child group, where x [ 1 ] \u2264 - 2. 258 and x [ 1 ] > - 2. 258, respectively. we can accumulate the rules along any particular path from the root node to any child node. we can also compute performance metrics like accuracy, precision, recall, and f1 on the data in the node. ( 3 ) retrieve hotspots from the hotspot tree at query timenow that we have our hotspot tree, let \u2019 s pick some hotspots! notice in fig2 that we have accuracy thresholds of. 3 and. 5.", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "our hotspot tree, let \u2019 s pick some hotspots! notice in fig2 that we have accuracy thresholds of. 3 and. 5. in the latter case, the hotspot regions are wider and accidentally capture more correctly classified points. as the threshold decreases, we are less tolerant with wider regions that accidentally capture correct classifications. normally, we might think that we always want lower thresholds to capture only misclassified datapoints, but that does run the risk of ( a ) making the regions incredibly small and not interpretable and ( b ) isolating many regions that might not contain many datapoints, requiring lots", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "risk of ( a ) making the regions incredibly small and not interpretable and ( b ) isolating many regions that might not contain many datapoints, requiring lots of manual work to investigate. so how were those hotspots retrieved? when an accuracy threshold is sent in a query to extract hotspots from the hotspot tree, we traverse along all possible paths from the root node. at any node in the traversal, if a node violates the threshold, that node is defined as a hotspot and all of the information in that node is appended to a list of hotspots returned to the user. specifically", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "node is defined as a hotspot and all of the information in that node is appended to a list of hotspots returned to the user. specifically, in our example, accuracy is our metric. at any particular node, if that node \u2019 s accuracy is less than the threshold, we know that the datapoints in that node collectively have violated the threshold and that node is a hotspot. whenever a node is identified as a hotspot, the traversal along that path stops, since downstream nodes would be more pure and the nodes with poor performance are in even more extreme violation of the user - provided metric and threshold", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "along that path stops, since downstream nodes would be more pure and the nodes with poor performance are in even more extreme violation of the user - provided metric and threshold. of course, what \u2019 s great about defining the metric and threshold at query time is that a user can requery with different metrics and threshold combinations, depending on the question and tolerance level for the threshold. hotspots found! using our example with the. 5 threshold, our three hotspots arehotspots contain the filters on the input data regions that can be applied in fetching the entire dataset for further examination, research, and model development.", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##hotspots contain the filters on the input data regions that can be applied in fetching the entire dataset for further examination, research, and model development. deep diveas promised, answers to the hanging question about regression above, plus some deeper aspects! deep dive : ( 1 ) train a decision tree on a proper partitioning objectiveso why not regression? the naive setup would be to take the rmse between ground truths and predictions in our regression task and use them as our partitioning objective. however, the regression criterion to split the data in a node is based on some distance metric, e. g. mse, to the", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "partitioning objective. however, the regression criterion to split the data in a node is based on some distance metric, e. g. mse, to the mean output value of the data the node, which in this case are the rmse values themselves. when we traverse the hotspot tree, we \u2019 d flag nodes with mse above a certain threshold as hotspots. let \u2019 s say we have a majority of rmse values around. 05 and a few rmse values around. 10, where we want to flag and isolate the latter. it \u2019 s totally possible that a regression tree might group more of the.", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "rmse values around. 10, where we want to flag and isolate the latter. it \u2019 s totally possible that a regression tree might group more of the. 10 rmse datapoints lower in the tree and we \u2019 d never be able to find them according to the stopping role once a hotspot is detected. for example, one path might be. 06 mse,. 09 mse,. 04 mse. given a user threshold between. 06 and. 09 mse, we would never reach the third node in this case since. 09 >. 04, and that third node could contain a lot of. 10 rms", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "mse, we would never reach the third node in this case since. 09 >. 04, and that third node could contain a lot of. 10 rmse datapoints ( the low. 04 value comes from the fact that there are many. 10 rmse datapoints and only a few. 05 rmse points, so the node is \u201c pure \u201d in the sense that the rmse values are close together ). we could fix this with a custom regression criterion that uses distances against 0, i. e. the raw rmse values, instead of the mse distance against the average rmse in a node, but that introduces", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "distances against 0, i. e. the raw rmse values, instead of the mse distance against the average rmse in a node, but that introduces an issue of standardization across models and data rollups. what if a rmse value of. 10 really isn \u2019 t bad for one model but is for another? as such, it \u2019 s more robust to convert the regression setting into a classification setting. data rollupif we have a streaming ( or batch ) model, how much data do we put into creating a hotspot tree at one time? if we created a hotspot tree on last week \u2019 s (", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", how much data do we put into creating a hotspot tree at one time? if we created a hotspot tree on last week \u2019 s ( or a previous batch \u2019 s ) data and now see incoming data, do we append those datapoints and retrain the previous tree or do we create a new hotspot tree for this week \u2019 s ( or current batch \u2019 s ) data? there \u2019 s certainly no wrong way to implement this, but at arthur ai, we take the latter approach. if we have an orange tree and now have some apples that arrive in a box, we \u2019 ll probably be interested", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur ai, we take the latter approach. if we have an orange tree and now have some apples that arrive in a box, we \u2019 ll probably be interested in the apple tree that generated those apples to figure out why we received some rotten apples, not the orange tree that we \u2019 ve already inspected last week ( or batch ). metadata galore! notice how the inputs to the decision tree can really be anything, including metadata not used as inputs to the model that created the original predictions! this means surfacing insights via hotspots is not constrained only to model inputs, which can have benefits if we are tracking sensitive non - input", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "original predictions! this means surfacing insights via hotspots is not constrained only to model inputs, which can have benefits if we are tracking sensitive non - input attributes in a model, like race or gender. deep dive : ( 2 ) create and store hotspot tree artifactwhy all the metrics? accuracy is not king and users need different metrics depending on the task their ml model is solving. this is a good introduction to other performance metrics like precision and recall. after giving it a read, here are two practical examples that helped me when i first learned about precision and recall. ( 1 ) consider an anomaly detection", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "recall. after giving it a read, here are two practical examples that helped me when i first learned about precision and recall. ( 1 ) consider an anomaly detection system for a bot that scrapes financial data from financial reports. it would be terrible if we have many false positives from a ml model, which means that the model would predict that wrong information ( positives ) exists for many documents that actually are correct ( false ), leading to a remediation team wasting time on tasks that are not incorrect ( and it really wouldn \u2019 t be highly problematic if some false negatives slipped through the cracks ). in this case, high", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "on tasks that are not incorrect ( and it really wouldn \u2019 t be highly problematic if some false negatives slipped through the cracks ). in this case, high precision is important. ( 2 ) consider a cancer detection model. it would be terrible if we have many false negatives from a ml model, which means that the model would predict no cancer ( negatives ) for many people who indeed have cancer ( false ), leading to a lack of recommended treatment and further health complications for those individuals. in this case, high recall is important. multiclassification and micrometricshow do we generalize to multiclassification tasks? accuracy", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for those individuals. in this case, high recall is important. multiclassification and micrometricshow do we generalize to multiclassification tasks? accuracy is the still the same, and we can use weighted precision, recall, and f1 score, weighted by ground truth size, under global metrics in the toy example below. notice how instead of just macrometrics, we also can now define micrometrics for precision, recall, and f1 score, with respect to each ground truth class. this can be powerful, as now hotspot trees can be traversed on specific classes if, e. g. the bird class is", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "each ground truth class. this can be powerful, as now hotspot trees can be traversed on specific classes if, e. g. the bird class is causing a lot of model failures for an object detection model and we want to figure out what \u2019 s going on for bird images in particular. fig4 : a multiclassification exampleconclusionthat \u2019 s it for today! hope you gained some insights about how to implement hotspots surfacing for your particular use case! we implement these kinds of systems at arthur, and automation is an important product category in ml monitoring for the customer experience. happy monitoring! photo by", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "use case! we implement these kinds of systems at arthur, and automation is an important product category in ml monitoring for the customer experience. happy monitoring! photo by designecologist on unsplashheader photo by nareeta martin on unsplashprevious postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "& dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / hotspots - automating - underperformance - regions - surfacing - in - machine - learning - systems", "metadata": {"source": "https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems", "row": 212, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 213 text : automating data drift thresholding in machine learning systems solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringautomating data drift thresholding in machine learning systemsby : kenneth chennovember 1, 2021monitoring production ml model inputs practically and efficiently in the absence of ground truth datain real life ml monitoring applications, we want to detect whether or not a ml", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##itoring production ml model inputs practically and efficiently in the absence of ground truth datain real life ml monitoring applications, we want to detect whether or not a ml model is performing well or is breaking. not picking up on poor model performance can translate into poor or even biased predictions that can lead to lost revenue and even pr fires that we see year over year from even the large tech companies, from facial recognition systems that fail to pick up on certain minorities to hate speech in autocomplete for search engines. in this article, we \u2019 ll go over a brief overview of how data drift thresholding helps capture poor model performance, and the", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##plete for search engines. in this article, we \u2019 ll go over a brief overview of how data drift thresholding helps capture poor model performance, and the majority of the post will focus on two versions of implementing automated data drift detection in production level ml monitoring systems. overviewwhy do we need data drift? the standard approach to monitoring models is based on performance metrics, i. e. accuracy / precision / recall / f - score, over some time period or from batch to batch. in order to produce these metrics, we need both predictions and ground truth labels for the datapoints, e. g. a credit risk", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "batch to batch. in order to produce these metrics, we need both predictions and ground truth labels for the datapoints, e. g. a credit risk model predicts that a person will pay their loan on time in 1 year and thus should be approved for a credit card, and we know whether or not that person paid their loan on time in 1 year. already, we have an issue because we do not know ground truth until 1 year later. in most production applications, there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. leveraging labeling", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. leveraging labeling teams or services can help close this lag, but it will not completely remove it. as such, instead of monitoring metrics based on outputs, we can instead monitor inputs based on data drift metrics. what is data drift? data drift fundamentally measures the change in statistical distribution between two distributions, usually the same feature but at different points in time. as an example, in the univariate case where we \u2019 re looking at one input feature, we should reasonably expect that", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "but at different points in time. as an example, in the univariate case where we \u2019 re looking at one input feature, we should reasonably expect that, if the shape of the feature shifts significantly between training time and prediction time, the model outputs will degrade in quality. as a toy example, if we train a ml model to solve math problems only on algebra questions, and all of a sudden geometry questions are fed into the model at prediction time, we \u2019 d expect the predictions to be pretty bad since the model hasn \u2019 t been trained on geometry questions. in essence, data drift is a proxy for our classical performance metric", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "expect the predictions to be pretty bad since the model hasn \u2019 t been trained on geometry questions. in essence, data drift is a proxy for our classical performance metrics in the absence of ground truth labels. the next natural question is how to formally quantify data drift. overview of data drift metricsthere are many different kinds of metrics we could use for quantifying data drift. here, we \u2019 ll focus on two popular families of metrics : f - divergence and hypothesis test metrics. for the former, we \u2019 ll look at kl divergence and psi. for the latter, we \u2019 ll look at", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##gence and hypothesis test metrics. for the former, we \u2019 ll look at kl divergence and psi. for the latter, we \u2019 ll look at chi - squared and ks test statistics. for any drift metrics, p is the training data ( reference set ) on which the ml model was trained and q is the data on which the model is performing predictions ( inference set ), which can be defined on a rolling time window for streaming models or a batch basis for batch models. kl divergenceif you need a quick overview, i \u2019 ve found this introductory post very helpful. kl divergence from p to q", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "batch models. kl divergenceif you need a quick overview, i \u2019 ve found this introductory post very helpful. kl divergence from p to q is interpreted as the nats of information we expect to lose in using q instead of p for modeling data x, discretized over probability space k. kl divergence is not symmetrical, i. e. the value is different if p and q are swapped, and should not be used as a distance metric. discretized kl divergencepopulation stability index ( psi ) while kl divergence is well - known, it \u2019 s usually used as a regular", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "discretized kl divergencepopulation stability index ( psi ) while kl divergence is well - known, it \u2019 s usually used as a regularizing penalty term in generative models like vaes. a more appropriate metric that can be used as a distance metric is population stability index ( psi ), which measures the roundtrip loss of nats of information we expect to lose from p to q and then from q returning back to p. discretized psihypothesis testshypothesis testing uses different tests depending on whether a feature is categorical or continuous. for a categorical feature with k categories, i.", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##pothesis testshypothesis testing uses different tests depending on whether a feature is categorical or continuous. for a categorical feature with k categories, i. e. k\u22121 are the degrees of freedom, where n _ pk and n _ qk are the count of occurrences of the feature being k, with 1\u2264k\u2264k, for p and q respectively, then the chi - squared test statistic is the summation of the standardized squared differences of expected counts between p and q. chi - squared statistic with k - 1 degrees of freedomfor a continuous features with f _ p and f _ q being the empirical", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "between p and q. chi - squared statistic with k - 1 degrees of freedomfor a continuous features with f _ p and f _ q being the empirical cumulative densities, for p and q respectively, the kolmogorov - smirnov ( ks ) test is a nonparametric, i. e. distribution - free, test that compares the empirical cumulative density functions f _ p and f _ q. kolmogorov - smirnov test statisticwhy automated drift thresholds? photo by scott rodgerson on unsplashhaving humans manually look at drift metrics produced by a model over time or", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##why automated drift thresholds? photo by scott rodgerson on unsplashhaving humans manually look at drift metrics produced by a model over time or over batches is obviously tedious and not ideal, and the immediate use case for drift metrics would be to set alerts based on some threshold, e. g. psi jumps over. 3 for this batch and should create an alert for a person to examine. remediation actions could be checking if a pipeline that \u2019 s feeding that feature into the model is broken, if there was a recent buggy code or calculation change, or if the feature really did drift and", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that \u2019 s feeding that feature into the model is broken, if there was a recent buggy code or calculation change, or if the feature really did drift and suggests that the model needs to be retrained. but where did that. 3 come from? setting arbitrary handpicked thresholds is not a good solution. if the threshold is too high, alerts that should be brought up are now ignored ( more false negatives ). if the threshold is too low, alerts that should not be brought up are now present ( more false positives ). a universal constant threshold is not robust because that threshold should depend on the", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "alerts that should not be brought up are now present ( more false positives ). a universal constant threshold is not robust because that threshold should depend on the shape of the the training data p. if p is uniform and we see a unimodal q, that drift value is going to be significantly less than if p was highly unimodal and centered at some mean far away from that same q, since the former p that is uniform is less certain if q did not come from p. an example is shown below. in order to set robust thresholds that make sense for an alerting system, we \u2019 ll dive into how", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "come from p. an example is shown below. in order to set robust thresholds that make sense for an alerting system, we \u2019 ll dive into how to automate calculation of such thresholds. that is, we want to recommend data drift thresholds to create alerts, so users ( a ) don \u2019 t have to manually set alerts and ( b ) don \u2019 t have to determine what a good threshold value is. automated drift thresholds for hypothesis test metricsfor hypothesis test metrics, the trivial solution would be setting thresholds at the the proper critical values for each test using the traditional \u03b1 =. 05,", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##sfor hypothesis test metrics, the trivial solution would be setting thresholds at the the proper critical values for each test using the traditional \u03b1 =. 05, i. e. 95 % confident that any hypothesis metric above the respective critical value suggests significant drift where q [UNK] p is likely false. hypothesis tests, however, come with limitations, from sample sizes influencing significance for the chi - squared test to sensitivity in the center of the distribution rather than the tails for the ks test. for those reasons, it \u2019 s important to explore other classes of drift metrics such as f - divergence metrics, and we \u2019 ll now explore ways", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "for those reasons, it \u2019 s important to explore other classes of drift metrics such as f - divergence metrics, and we \u2019 ll now explore ways to automate f - divergence drift thresholds. automated drift thresholds v1 : mc simulationoverviewsystem for mc simulated automated drift thresholds ( image by author ) ( 1 ) for a large number of simulations indexed by i, for a given input feature, mc ( monte carlo ) simulate m times, where m is sufficiently large, from the reference set p and construct an inference distribution q\u1d62 *, which represents what we \u2019 d expect had our data truly", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", where m is sufficiently large, from the reference set p and construct an inference distribution q\u1d62 *, which represents what we \u2019 d expect had our data truly come from p via simulation. then we calculate and keep track of data drift f ( p, q\u1d62 * ) for each simulation. ( 2 ) we set the data drift threshold to the conservative value of max _ i ( f ( p, q\u1d62 * ) ), which is essentially a critical value / cutoff where \u03b1 \u2192 0 in a traditional statistics setting, i. e. we are 99. 999 % confident that a data drift value from an actual q, i", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "where \u03b1 \u2192 0 in a traditional statistics setting, i. e. we are 99. 999 % confident that a data drift value from an actual q, i. e. inference time slice or batch, that is above the threshold signifies that the inference data was not produced from the same underlying distribution as p. if incurring false positives is not expensive, then we could set the threshold instead to the 95 % percentile of f ( p, q\u1d62 * ) in the traditional \u03b1 =. 05 one - sided setting. limitations of mc simulationmc methods are attractive because they are simulation - based and distribution - free, but this", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the traditional \u03b1 =. 05 one - sided setting. limitations of mc simulationmc methods are attractive because they are simulation - based and distribution - free, but this method is not scalable due to two issues. ( 1 ) running simulations for every feature per model is computationally expensive. specifically, runtime on a per model basis would be : o ( n _ features * n _ categories _ per _ feature * n _ metrics * n _ simulations ) ( 2 ) the assumption of sampling stability is broken with lower sample sizes. as a toy example, let \u2019 s say we have red, white, blue, and orange marbles", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "assumption of sampling stability is broken with lower sample sizes. as a toy example, let \u2019 s say we have red, white, blue, and orange marbles in p, distributed uniformly. let \u2019 s now present q of size 3 ; there is no way to tell if the q came from p or not since there is no way for us to represent all four colors. even if we had sampled 6 marbles, we could not closely approximate 1 / 4 for each category given those 6 marbles. let m be the number of simulated datapoints on each simulation to create q\u1d62 *. in the same toy example of marbles,", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "those 6 marbles. let m be the number of simulated datapoints on each simulation to create q\u1d62 *. in the same toy example of marbles, sampling instability causes artificially high uncertainty ( and therefore artificially high data drift thresholds ) when m is low, as p has not been well mc - sampled and represented in q\u1d62 *. kl divergence mc sampling instability ( image by author ) the way to fix issue ( 2 ) is to create simulated threshold curves for a given feature by varying m, but now we have exacerbated issue ( 1 ) by multiplying runtime by m. we also", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "curves for a given feature by varying m, but now we have exacerbated issue ( 1 ) by multiplying runtime by m. we also now have issues with space complexity, since now we would have to store thresholds in a lookup ( database or cache ) for use in downstream applications, like an alert service creating alert rules or a query service cache. forgoing the extra storage and simulating the threshold curve at query time is also not an option since the query would take too long for most user - facing applications. in short, the trade - off of using a flexible simulation setup is not worth the computational expenses", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the query would take too long for most user - facing applications. in short, the trade - off of using a flexible simulation setup is not worth the computational expenses when taking into account correction curves that would either ( a ) need to be stored, which would cost a lot in storage fees or blow up a cache, or ( b ) be simulated at query time, which is too slow for user - facing applications. automated drift thresholds v2 : closed - form statisticsphoto by edge2edge media on unsplashit turns out that we can upper bound data drift thresholds in one shot using probability theory. we \u2019 ll make", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "by edge2edge media on unsplashit turns out that we can upper bound data drift thresholds in one shot using probability theory. we \u2019 ll make use of standard probability theory, the dirichlet distribution, and first - order taylor series expansions. remember that we are still deriving the thresholds on a per feature basis. kl divergencefirst, letreference set p is known and inference set q is unknown. intuitively, this simply means that we have observed the training data used in creating our ml model and that we \u2019 re asking a hypothetical question of what we could expect data drift to be for", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that we have observed the training data used in creating our ml model and that we \u2019 re asking a hypothetical question of what we could expect data drift to be for some hypothetical inference set q in the future. formally, 2nd line : linearity of expectation and conditional probability. 3rd line : q [UNK] dirchlet ( \u03b1 ), i. e. q models a probability distribution and is a multivariate generalization of the beta distribution. as such, we can use the digamma \u03c8 function in this case. \u03b1 = ( \u03b1 _ 1, \u2026, \u03b1 _ k ) traditionally corresponds to the observed probabilities in the k", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##amma \u03c8 function in this case. \u03b1 = ( \u03b1 _ 1, \u2026, \u03b1 _ k ) traditionally corresponds to the observed probabilities in the k categories for the feature from q, offset by a small constant, i. e. prior counts. however, q is unknown, so how do we derive the correct counts? we do so from p. after all, the entire point of this expectation statement is to quantify on average the drift we \u2019 d expect to see if q [UNK] p, and an amount above that value in an actual inference set signifies that we likely have significant drift, i. e. q [UNK] p", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "see if q [UNK] p, and an amount above that value in an actual inference set signifies that we likely have significant drift, i. e. q [UNK] p is false. back to \u03b1, we can use a bayesian uniform prior q [UNK] dirichlet ( 1 _ k ) and update such that qp [UNK] dirichlet ( 1 _ k + n _ q * p _ k ), where n _ q is the number of datapoints in the inference set q and p _ k corresponds to the known k - dimensional probability vector for the feature from p. this is the multivariate generalization of the the beta -", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and p _ k corresponds to the known k - dimensional probability vector for the feature from p. this is the multivariate generalization of the the beta - binomial conjugacy. so we thus have, population stability indexpsi gets a bit more tricky and we need to upper - bound the expectation. letthenhere are the steps : 2nd line : linearity of expectation and substituting the result for kl divergence above. 3rd line : linearity of expectation and conditional probability. 4th line : expectation of a dirichlet distribution. 5th line : delta method approximation ( see appendix ). 6th line :", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ity of expectation and conditional probability. 4th line : expectation of a dirichlet distribution. 5th line : delta method approximation ( see appendix ). 6th line : expectation of a dirichlet distribution. 7th line : simplifying. v1 issues solved! notice how we now have solved for the two issues from mc simulation. ( 1 ) we do not have to worry about runtime, as this form is a one shot solution that only involves counts and we can use an approximation to the digamma function. ( 2 ) we handle varying sample sizes by using a bayesian plus pseudocount setup on the parameters of q.", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "approximation to the digamma function. ( 2 ) we handle varying sample sizes by using a bayesian plus pseudocount setup on the parameters of q. implementationin production, how do we implement this dynamic thresholding? although the thresholds are dynamic depending on the metric and the incoming inference set, all of the expectations above, conditioned on p, can be expressed as sql queries or custom but simple functions. discretizing all continuous features, instead of smoothing discrete features, would be a reasonable trade - off : although we may not capture the full continuous nature of p and q, we \u2019 ll avoid computationally expensive kernel density", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", would be a reasonable trade - off : although we may not capture the full continuous nature of p and q, we \u2019 ll avoid computationally expensive kernel density estimation in representing p and q as continuous distributions. wait, what about variance? we can calculate variances too and apply delta method approximations, as we would run into variances and covariances on logarithmic terms that cannot be solved using standard probability theory. those tools are below in the appendix. however, as it turns out, most metrics will not come out as cleanly as kl divergence, so most of the conditional expectations and variances", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "however, as it turns out, most metrics will not come out as cleanly as kl divergence, so most of the conditional expectations and variances are upper - bounded via cauchy - schwarz. it \u2019 s certainly doable to include an additive upper - bounded variance term, or to even use second - order approximations, but the extra computation might not be worth the time gained in exchange for a stricter bound. for another time \u2026 we can also use taylor series expansions to upper - bound other metrics, like js divergence and hellinger distance. conclusionthat \u2019 s it for today! hope", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "taylor series expansions to upper - bound other metrics, like js divergence and hellinger distance. conclusionthat \u2019 s it for today! hope you gained some insights about how to implement automated thresholds in a computationally and probabilistically sound manner! we implement these kinds of systems at arthur, and a few equations can go a long way for the customer experience. check out our data drift detection part ii : unstructured data in nlp and cv blog post for more. happy monitoring! photo by pablo heimplatz on unsplashappendix : delta method approximationsnote that the first -", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "blog post for more. happy monitoring! photo by pablo heimplatz on unsplashappendix : delta method approximationsnote that the first - order approximations for both sections below are upper bounds because the log function is concave. log ( x ) approximationby a first - order taylor series expansion around the mean, using this approximation, we haveandwith inequality by cauchy - schwarz. xlog ( x ) by a first - order taylor series expansion around the mean, using this approximation, we haveand lettingwe havewith inequality by cauchy - schwarz. previous postsharenext post we make ai", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "around the mean, using this approximation, we haveand lettingwe havewith inequality by cauchy - schwarz. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / automating - data - drift - thresholding", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / automating - data - drift - thresholding - in - machine - learning - systems", "metadata": {"source": "https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems", "row": 213, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 214 text : arthur \u2014 rapidly growing amidst surging interest in model monitoring \u2014 identified as a sample vendor in 2021 gartner \u00ae hype cycle \u2122 for ai report solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first fire", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##eamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur \u2014 rapidly growing amidst surging interest in model monitoring \u2014 identified as a sample vendor in 2021 gartner \u00ae hype cycle \u2122 for ai reportby : arthur", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2014 rapidly growing amidst surging interest in model monitoring \u2014 identified as a sample vendor in 2021 gartner \u00ae hype cycle \u2122 for ai reportby : arthur teamseptember 15, 2021new york, ny \u2013 arthur, the leading machine learning ( ml ) monitoring platform, is identified as a sample vendor for the ai governance category. gartner released its annual \u201c hype cycle for artificial intelligence 2021 \u201d report, highlighting the growing importance of ai governance practices in ai - enabled companies as it states, \u201c data and analytics leaders must leverage this research to successfully navigate ai - specific innovations that are in various phases of maturation,", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- enabled companies as it states, \u201c data and analytics leaders must leverage this research to successfully navigate ai - specific innovations that are in various phases of maturation, adoption and hype. \u201d [ 1 ] \u201c with ai now delivering value in practical enterprise application, data and analytics leaders see that scaling ai without governance is dangerous. when each ai output is replicated millions of times, they ask how to balance the business value promised by ai against the need for appropriate oversight, risk management, and investment decisions. \u201d as digital transformation continues to be a driving force in operations across industries, gartner highlights the necessity to invest in technology to support", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", and investment decisions. \u201d as digital transformation continues to be a driving force in operations across industries, gartner highlights the necessity to invest in technology to support ai governance, responsible ai, and modelops within an organization. the gartner report states : \u201c with ai now delivering value in practical enterprise application, data and analytics leaders see that scaling ai without governance is dangerous. when each ai output is replicated millions of times, they ask how to balance the business value promised by ai against the need for appropriate oversight, risk management, and investment decisions. \u201d [ 1 ] \u201c over the past three years, we \u2019 ve successfully enabled", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "by ai against the need for appropriate oversight, risk management, and investment decisions. \u201d [ 1 ] \u201c over the past three years, we \u2019 ve successfully enabled our customers to put in place robust ai monitoring and governance at companies of all sizes. we believe that this gartner recognition of the importance of ai governance is further proof of its foundational importance in the enterprise. our team is uniquely positioned to help customers put in place this governance and maximize their ai investments. \u201d arthur believes the report underpins the transformative impact that ai has on all enterprises. as a result, the need for explainable ai and fairness is paramount. \u201c", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "believes the report underpins the transformative impact that ai has on all enterprises. as a result, the need for explainable ai and fairness is paramount. \u201c as enterprises become ai - enabled, the performance of their models drives the performance of their business. we are seeing an increased focus on ai governance all the way up to the board level, \u201d said adam wenchel, founder and ceo of arthur. \" over the past three years, we \u2019 ve successfully enabled our customers to put in place robust ai monitoring and governance at companies of all sizes. we believe that this gartner recognition of the importance of ai governance is further proof of", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "put in place robust ai monitoring and governance at companies of all sizes. we believe that this gartner recognition of the importance of ai governance is further proof of its foundational importance in the enterprise. our team is uniquely positioned to help customers put in place this governance and maximize their ai investments. \u201d since its inception in 2018, arthur has enabled customers like humana, zesty. ai, and truebill to accelerate their business transformation through ai. customers use arthur to monitor their ml models, mitigate, analyze and mitigate unwanted biases, and explain how the models are making decisions. the platform continues to innovate \u2014 arthur", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ml models, mitigate, analyze and mitigate unwanted biases, and explain how the models are making decisions. the platform continues to innovate \u2014 arthur has been the first to market in enabling the adoption of computer vision ( cv ) and natural language processing ( nlp ) models. in april, it was named one of cb insight \u2019 s ai 100 list, recognizing promising private ai companies globally for the second consecutive year. in june, it was named a gartner \u2018 cool vendor \u2019 in ai governance and responsible ai [ 2 ]. if you \u2019 re a gartner client, you can access the full report here :", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2018 cool vendor \u2019 in ai governance and responsible ai [ 2 ]. if you \u2019 re a gartner client, you can access the full report here : hype cycle for artificial intelligence 2021learn more about arthur at arthur. ai / product. to schedule a demo of arthur, please contact info @ arthur. ai. [ 1 ] gartner, \u201c hype cycle for artificial intelligence, 2021 \u201d, shubhangi vashisth, svetlana sicular, 29 july 2021 [ 2 ] gartner, \u201c cool vendors \u2122 in ai governance and responsible ai \u201d, svetlana sicular, mo", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "sicular, 29 july 2021 [ 2 ] gartner, \u201c cool vendors \u2122 in ai governance and responsible ai \u201d, svetlana sicular, moutusi sau, et al, 10 june 2021 required disclaimer : gartner, cool vendors and hype cycle are a registered trademark and service mark of gartner, inc. and / or its affiliates in the u. s. and internationally and are used herein with permission. gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. gartner research publications consist of the opinions of gartner's research organization and should not be construed as statements of fact. gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. note : additional information may be included in the press release, but must be in compliance with the gartner copyright and quote policy : http :", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "particular purpose. note : additional information may be included in the press release, but must be in compliance with the gartner copyright and quote policy : http : / / www. gartner. com / technology / about / policies / copyright. jsp industry - general excerpts from the report are permitted ( e. g., evaluation criteria ) ; no company - specific excerpts permitted as they may appear endorsing ; vendor company information may be included provided it is clearly differentiated from the gartner report. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news!", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "from the gartner report. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - rapidly - growing - amidst - surging - interest - in - model - monitoring - identified - as - a - sample", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur. ai / blog / arthur - rapidly - growing - amidst - surging - interest - in - model - monitoring - identified - as - a - sample - vendor - in - 2021 - gartner - r - hype - cycle - tm - for - ai - report", "metadata": {"source": "https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report", "row": 214, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 215 text : arthur's response to nist guidance on bias risk in ai solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-nist-response", "row": 215, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur's response to nist guidance on bias risk in aiby : lizzie kumaraugust 4, 2021in june, the national institute of standards and technology ( nist ) released a draft of a document outlining an approach for identifying and", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-nist-response", "row": 215, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##gust 4, 2021in june, the national institute of standards and technology ( nist ) released a draft of a document outlining an approach for identifying and managing bias in artificial intelligence. the team driving responsible ai research at nist has requested feedback on the document \" from people whom ai affects, both from those who create ai systems and also those who are not directly involved in its creation. \" at arthur, we are excited to see nist embrace and elevate the importance of these issues. in our response, we express our support for nist's efforts as well as address potential gaps in their framework. read it here", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-nist-response", "row": 215, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "importance of these issues. in our response, we express our support for nist's efforts as well as address potential gaps in their framework. read it here. figure 1 from nist \u2019 s proposal. note the arrow from \u201c deployment \u201d back to \u201c pre - design. \u201d previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-nist-response", "row": 215, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##pcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthurs - nist - response", "metadata": {"source": "https://www.arthur.ai/blog/arthurs-nist-response", "row": 215, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 216 text : google \u2019 s dermatology app announcement highlights promises and potential perils of computer vision technology solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##firewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessgoogle \u2019 s dermatology app announcement highlights promises and potential perils of computer vision technologyby : connor toupsjune 28, 2021last week, google announced a new dermatology app that will use computer", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "perils of computer vision technologyby : connor toupsjune 28, 2021last week, google announced a new dermatology app that will use computer vision to automatically classify skin conditions from images and provide \u201c dermatologist - reviewed information and answers to commonly asked questions, along with similar matching images from the web \u201d for the matching skin condition. why it \u2019 s important : google first published their model in nature in 2020. moving to embed that technology into a public - facing app highlights the continued impact of computer vision in the healthcare industry. concerns around bias : google \u2019 s announcement prompted concerns that the model would be less accurate", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- facing app highlights the continued impact of computer vision in the healthcare industry. concerns around bias : google \u2019 s announcement prompted concerns that the model would be less accurate for darker - skinned people. according to the nature paper google published, fitzpatrick type v and vi skin, which broadly correlates to brown and dark brown skin, accounted for under 3. 5 % of the train set. that class imbalance could cause the model to underperform on darker - skinned people - - raising concerns about disparate impact bias. things we noticed in google \u2019 s validation set numbersas others have noted, just 2. 7 % of the validation", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "raising concerns about disparate impact bias. things we noticed in google \u2019 s validation set numbersas others have noted, just 2. 7 % of the validation set examples from the nature publication used type v or vi skin, so it \u2019 s difficult to assess how well the model generalizes to dark skin. we also noticed that the topline accuracy numbers google reported for ethnicity - level accuracy might obscure racial disparities in the validation set. in slides shared with motherboard ( vice \u2019 s tech publication ), google reported that its model had an 87. 9 percent accuracy rate for black patients - - the highest of any ethnicity group.", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "vice \u2019 s tech publication ), google reported that its model had an 87. 9 percent accuracy rate for black patients - - the highest of any ethnicity group. the nature publication also reported a relatively high accuracy rate for black patients ( though we couldn \u2019 t find the 87. 9 percent figure ) that were on par with accuracy rates for white patients. just looking at the accuracy rate, however, can obscure potential racial disparities. in the supplementary figures, google \u2019 s nature publication reported its average sensitivity, which they calculate by averaging sensitivity across the 26 conditions the model considers. this has the advantage of weighting each condition \u2019 s performance", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "publication reported its average sensitivity, which they calculate by averaging sensitivity across the 26 conditions the model considers. this has the advantage of weighting each condition \u2019 s performance equally, so common conditions don't skew the accuracy rates as much. looking at average sensitivity, google \u2019 s model performed worst on black patients. the difference between accuracy and sensitivity rates for black patients could suggest that the black patients in their validation set were more likely to have a common condition ( eg. acne ) that the model can perform very well on, but when it comes time to detect a more serious but less common condition like skin cancer, the model under", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ne ) that the model can perform very well on, but when it comes time to detect a more serious but less common condition like skin cancer, the model underperforms for black patients. non skin - cancer conditions could be overrepresented in the black sample because the prevalence of skin cancer in non - white people is lower than the prevalence for white people ( however, skin cancer diagnoses in non - white people often come at a more advanced stage - - increasing the lethality rate ). imagine we randomly sample a group of black patients with some type of skin condition ( not exclusively skin cancer ) and an equally sized group of", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "lethality rate ). imagine we randomly sample a group of black patients with some type of skin condition ( not exclusively skin cancer ) and an equally sized group of white patients with some type of skin condition : if the prevalence of skin cancer is higher in the white population than the black population, a random sampling process will result in the white sample having more patients with skin cancer and the black sample having more patients with some non - cancerous skin condition. the code below demonstrates this phenomenon. whatever the underlying cause for the disparities, low sensitivity rates for black patients could exacerbate existing racial inequalities in healthcare. a", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". whatever the underlying cause for the disparities, low sensitivity rates for black patients could exacerbate existing racial inequalities in healthcare. a brief history of bias in computer vision modelsas computer vision research has exploded, there \u2019 s been an increased interest in auditing these models for potential racial and gender bias. in 2018, gebru and buolamwini found three major commercial facial recognition systems performed significantly worse on darker - skinned people and women than lighter - skinned people and men. that landmark study inspired further research into auditing other computer vision models that might exhibit these pernicious biases. studies have", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "lighter - skinned people and men. that landmark study inspired further research into auditing other computer vision models that might exhibit these pernicious biases. studies have also examined cv models in a healthcare context : last year, a study looked at chest x - ray diagnosis computer vision models and found that they exhibited tpr ( true positive rate ) disparities with respect to gender, race, and age. tpr disparities with respect to race were especially prominent with the models performing best on white patients and worst on hispanic patients. difficulties in auditing dermatology cv modelsafter google \u2019 s announcement, our team looked into conducting", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "performing best on white patients and worst on hispanic patients. difficulties in auditing dermatology cv modelsafter google \u2019 s announcement, our team looked into conducting an audit of a dermatology computer vision model using an external test set to look for possible racial or gender bias. however, as we explored the feasibility of such a study, it became clear that there were no open source dermatology datasets that included metadata on each patient \u2019 s race or skin type. external audits are important methods of ensuring that models in production are equitable and inclusive : balanced datasets that include patient skin type and race metadata are critical", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "external audits are important methods of ensuring that models in production are equitable and inclusive : balanced datasets that include patient skin type and race metadata are critical to enabling these audits, and we at arthur hope that the healthcare and tech industries work together to create these datasets and conduct these audits. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##chscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / googles - dermatology - app - announcement - highlights - promises - and - potential - perils - of - computer - vision - technology", "metadata": {"source": "https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology", "row": 216, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 217 text : arthur named a 2021 gartner cool vendor in ai governance and responsible ai solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##servabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur named a 2021 gartner cool vendor in ai governance and responsible aiby : arthur teamjune 29, 2021new york, ny \u2014 arthur, the ai monitoring and governance company, is announcing today that it has been named a", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": arthur teamjune 29, 2021new york, ny \u2014 arthur, the ai monitoring and governance company, is announcing today that it has been named a \u201c cool vendor \u201d as featured in the report titled \u201c cool vendors in ai governance and responsible ai \u201d by expert analysts at gartner, inc. the report \u201c is designed to highlight interesting, new and innovative vendors, products and services \u201d, and highlights the \u201c adoption of artificial intelligence is pushing organizations from declarations of their responsible and ethical ai principles to actions. \u201d according to gartner, \u201c responsible artificial intelligence ( ai ) is in the spotlight because of the increased organizational", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "their responsible and ethical ai principles to actions. \u201d according to gartner, \u201c responsible artificial intelligence ( ai ) is in the spotlight because of the increased organizational and societal exposure to ai bias, distrust and lack of fairness that stand in the way of ai adoption. the more ubiquitous ai becomes, the more ai practitioners recognize in their everyday work that data and algorithms amplify and perpetuate human biases. \u201d according to the same report, gartner predicts that \u201c through 2025, 80 % of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance. \u201d through", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u201c through 2025, 80 % of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance. \u201d through 2025, 80 % of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance. ( gartner ) since its inception in 2018, arthur has cemented its position as the leading model monitoring and governance solution in the market with many industry - exclusive features such as comprehensive support for natural language processing ( nlp ) and computer vision models. in april, it was named one of cb insight \u2019 s ai 100 list, which", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "comprehensive support for natural language processing ( nlp ) and computer vision models. in april, it was named one of cb insight \u2019 s ai 100 list, which recognizes promising private ai companies globally, for the second consecutive year. \u201c we consider it a great honor to be named a gartner \u2018 cool vendor, \u2019 \u201d says adam wenchel, ceo of arthur. \u201c we strongly believe in the importance of ai governance and responsibility and are glad to see gartner highlighting trends in this industry. we believe this designation recognizes our commitment to creating the most comprehensive set of model monitoring tools possible to enable any company to operationalize ai governance seam", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in this industry. we believe this designation recognizes our commitment to creating the most comprehensive set of model monitoring tools possible to enable any company to operationalize ai governance seamlessly. \u201d if you \u2019 re a gartner client, you can access the full report here : cool vendors in ai governance and responsible ailearn more about arthur at arthur. ai / product. to schedule a demo of arthur, please contact info @ arthur. ai. gartner disclaimergartner does not endorse any vendor, product or service depicted in our research publications, and does not advise technology users to select only those vendors with the highest ratings", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "does not endorse any vendor, product or service depicted in our research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. gartner research publications consist of the opinions of gartner's research organization and should not be construed as statements of fact. gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. about arthurarthur is a proactive machine learning model monitoring platform that gives you the confidence that your ai deployments are performing as expected and the", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "particular purpose. about arthurarthur is a proactive machine learning model monitoring platform that gives you the confidence that your ai deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. with performance monitoring, bias detection, and customizable alerts, arthur makes sure that you never miss an issue, and arthur \u2019 s explainability engine makes runtime debugging effortless. arthur \u2019 s customers, including fortune 100 companies like humana and ai - driven startups like expel and truebill, are using the platform to ensure that they can catch and fix", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "including fortune 100 companies like humana and ai - driven startups like expel and truebill, are using the platform to ensure that they can catch and fix any issues with their production ai systems before they become billion - dollar problems. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / 2021 - gartner - cool - vendor", "metadata": {"source": "https://www.arthur.ai/blog/2021-gartner-cool-vendor", "row": 217, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 218 text : introducing monitoring for computer vision models solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpc", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringintroducing monitoring for computer vision modelsby : arthur teamjune 17, 2021as computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. the adoption of computer vision into industry applications promises", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. the adoption of computer vision into industry applications promises enormous potential upside ; however, computer vision models, like any ml model, must be carefully monitored. a promising model that has gone off the rails can quickly become a dangerous liability. today, arthur is excited to provide the first model monitoring support for computer vision ( cv ) models. with arthur, you can launch cv models into production, and rest assured that you \u2019 ll be immediately notified when something warrants your attention. arthur supports both image classification and object detection, providing monitoring for", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "into production, and rest assured that you \u2019 ll be immediately notified when something warrants your attention. arthur supports both image classification and object detection, providing monitoring for performance and data drift, as well as in - depth explainability. in this post, we describe the key components for successful monitoring of cv models and how arthur helps ensure your models are performing as expected \u2013 maximizing success for your organization while mitigating risk. understanding data drift in computer vision applicationsa critical aspect of monitoring any ml model is to ensure that the data coming into the model continues to look as expected. in computer vision models, this means ensuring that the images we", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "monitoring any ml model is to ensure that the data coming into the model continues to look as expected. in computer vision models, this means ensuring that the images we see today are similar to those used to train the model. in technical terminology, we refer to this as out - of - distribution detection or anomaly detection, and it is a field of burgeoning research in the ml scientific community. using a reference dataset of images, we can perform ongoing monitoring of all new images to understand which ones are similar to the training data and which ones seem like anomalies. it is essential to know exactly when your model won \u2019 t general", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to understand which ones are similar to the training data and which ones seem like anomalies. it is essential to know exactly when your model won \u2019 t generalize to new settings. for example, if your object detection model was trained primarily on images from outdoor locations in good weather and good lighting, it will likely underperform in rainy and dark conditions. arthur \u2019 s data drift detection tool can automatically detect if these dissimilar images start coming in so that your data scientists can get ahead of any model issues. in another example, let \u2019 s say you \u2019 ve trained a computer vision model that examines chest x - rays to", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "scientists can get ahead of any model issues. in another example, let \u2019 s say you \u2019 ve trained a computer vision model that examines chest x - rays to diagnose tuberculosis. despite your best efforts, you \u2019 re only able to collect training examples from a small number of x - ray machines, each of which tends to have its own set of artifacts and nuances. once you deploy your model into the wider world, you \u2019 re nervous that these artifacts will prevent your model from generalizing well to image data drawn from a much larger set of machines. in this case, it would be helpful to quantify how much each production", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model from generalizing well to image data drawn from a much larger set of machines. in this case, it would be helpful to quantify how much each production image adheres to the attributes of the training dataset. luckily, arthur \u2019 s data drift detection tool can identify if your model fails to generalize and prompts you to take action before your model derails and causes adverse impacts. the diagram above demonstrates how anomaly detection works. here, we trained our computer vision model using images that consist of typical street scenes. most of the images involve transportation, outdoor lighting, people, buildings, and so on. at production time,", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model using images that consist of typical street scenes. most of the images involve transportation, outdoor lighting, people, buildings, and so on. at production time, some of the incoming data is visually ( and semantically ) similar to the training data, such as the top image of a train platform. however, the bottom image of the toy robot and cat is quite different from anything in the training set. therefore, our tool would flag this image as an anomaly. this kind of drift detection for image data will ensure that your model \u2019 s output predictions remain trustworthy and reliable. furthermore, we allow for sorting and searching through images based on", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "drift detection for image data will ensure that your model \u2019 s output predictions remain trustworthy and reliable. furthermore, we allow for sorting and searching through images based on their anomaly score : how dissimilar are they to the training data. this classification model was trained on aerial imagery primarily of green landscapes. however, the model occasionally sees dense urban images and we are able to quickly identify that these images are likely going to lead to misclassifications. this dynamic filtering gives data scientists the tool to quickly find representative examples of anomalous images. whatever your computer vision model does, it \u2019 s easy to imagine how data drift could cause", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "scientists the tool to quickly find representative examples of anomalous images. whatever your computer vision model does, it \u2019 s easy to imagine how data drift could cause issues. it \u2019 s critical to monitor data drift to prevent unwanted surprises and wasted time debugging your model. explainability in cv modelsone concern with complex ml models, especially computer vision models, is that they can be \u201c right for the wrong reasons. \u201d when models are in production, we want to ensure that they \u2019 re looking at the \u2018 right things \u2019 to make decisions. for example, if we have a model for identifying cancerous cells in micrographs, we", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "they \u2019 re looking at the \u2018 right things \u2019 to make decisions. for example, if we have a model for identifying cancerous cells in micrographs, we would want to ensure that the model picks up on medically important aspects of cells instead of some artifacts that happened to be present in the training data. using arthur \u2019 s local explainability techniques, you can visualize saliency maps over images to reveal which image components were particularly important for the model \u2019 s decision. the importance scores are associated with a class, so a positive score indicates that a region strongly contributed to a positive class prediction. a negative score indicates that a region", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the importance scores are associated with a class, so a positive score indicates that a region strongly contributed to a positive class prediction. a negative score indicates that a region was negatively associated with a target class. the arthur platform makes image explanations easy to use - it shades regions of the original image with green or red to indicate the importance of that region. green shading indicates a region that positively contributed to the selected class, while red shading indicates a region that negatively contributed to the selected class. a user can drag a slider bar interactively, indicating how many regions are shown, sorted by overall importance. in this example, data scientists,", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "class. a user can drag a slider bar interactively, indicating how many regions are shown, sorted by overall importance. in this example, data scientists, researchers, and business analysts alike can utilize the arthur platform to guarantee peak model performance. arthur \u2019 s computer vision explainability tool is simple, easy to understand, and provides cross - functional teams with key insights. monitoring your computer vision model for algorithmic biasas with any machine learning model, it \u2019 s necessary to ensure that algorithmic bias hasn \u2019 t seeped into your computer vision model. unfortunately, a growing body of research demonstrates that some of the most popular computer vision models", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that algorithmic bias hasn \u2019 t seeped into your computer vision model. unfortunately, a growing body of research demonstrates that some of the most popular computer vision models are biased. in 2018, researchers gebru and buolamwini found three major commercial facial recognition systems performed significantly worse on darker - skinned people and women than lighter - skinned people and men. a year later, a national institute of standards and technology study that evaluated 189 facial recognition models found pervasive bias across facial recognition models. that same nist study, however, found that some models performed equitably across all demographic groups. while the study didn \u2019", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "across facial recognition models. that same nist study, however, found that some models performed equitably across all demographic groups. while the study didn \u2019 t evaluate causal reasons for this outcome, it suggests that the model you use \u2014 and the data you train on \u2014 affects the degree of bias in computer vision models. this study further emphasizes the importance of continuously monitoring and evaluating each of your models to see where it falls on the fairness spectrum. as computer vision continues to offer new opportunities for innovation and growth, we must ensure that its applications are equitable and inclusive to avoid encoding dangerous systemic biases. arthur has built - in bias", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "new opportunities for innovation and growth, we must ensure that its applications are equitable and inclusive to avoid encoding dangerous systemic biases. arthur has built - in bias monitoring so you can easily compare equity across various groups, and maintain high standards of fairness. monitor your cv models with arthurthe arthur platform has recently released extensive monitoring support for cv models, including performance monitoring, data drift and bias detection, and explainability features. unlike other model monitoring solutions, arthur has you covered for tabular data, nlp data, and image data, so you can be assured that your monitoring platform can grow easily with your ambitious ai agenda. if you \u2019", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ular data, nlp data, and image data, so you can be assured that your monitoring platform can grow easily with your ambitious ai agenda. if you \u2019 re deploying cv models into production and are looking for a solution for monitoring those models over time, we \u2019 d love to connect and show you how arthur can help. request a demo today. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcom", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / introducing - monitoring - for - computer - vision - models", "metadata": {"source": "https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models", "row": 218, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 219 text : arthur releases the first computer vision model monitoring solution for enterprise solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesarthur releases the first computer vision model monitoring solution for enterpriseby : arthur teamjune 17, 2021new york, ny \u2014 today, arthur, a machine learning model monitoring company, is releasing a suite of new tools and features that enable organizations to monitor the", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##new york, ny \u2014 today, arthur, a machine learning model monitoring company, is releasing a suite of new tools and features that enable organizations to monitor the performance of their computer vision ( cv ) models, including image classification and object detection models. arthur \u2019 s new cv feature set allows companies to : monitor cv model pipelines for data anomaliesreview potential biases in image modelsunderstand model predictions and decisionsexplore, investigate, and triage model predictions and errorsview model predictions and important regions for determining a class output in a particular imageperformance monitoring : arthur now offers performance monitoring of cv models, including", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and errorsview model predictions and important regions for determining a class output in a particular imageperformance monitoring : arthur now offers performance monitoring of cv models, including tracking data drift and anomaly detection ( out - of - distribution detection for images ). this helps users identify any images being sent to the model where the model is likely to be underperforming. in addition to supporting all tabular and nlp models, arthur \u2019 s cv functionality allows companies to monitor all of their machine learning models on one centralized dashboard, regardless of model types. bias detection : the new cv feature set enables companies to identify biases in their cv models by", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning models on one centralized dashboard, regardless of model types. bias detection : the new cv feature set enables companies to identify biases in their cv models by evaluating image classification outputs for particular classes that users are interested in evaluating. by examining outputs for a specific class, users can better understand where their model misclassifies and perpetuates biases. explainability : arthur \u2019 s cv feature set offers insights into how cv models are making decisions. this capability gives users a clear visualization of which regions of an image are impactful for a model \u2019 s decision, making it easy to identify when models perform unexpectedly. additionally, users", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "clear visualization of which regions of an image are impactful for a model \u2019 s decision, making it easy to identify when models perform unexpectedly. additionally, users can explore the results of their object detection models with an interactive interface that displays the model outputs for pipeline images. the release of the cv feature set comes just a few weeks after arthur released the first natural language processing ( nlp ) model monitoring solution, furthering arthur \u2019 s position as the most comprehensive machine learning model monitoring solution in the market. view object detection model outputs on arthur's interactive interfacethe growing need for cv model monitoringas computer vision technology has grown more sophisticated and", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "solution in the market. view object detection model outputs on arthur's interactive interfacethe growing need for cv model monitoringas computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. according to stanford hai \u2019 s 2021 ai index report, companies are investing a larger amount of computational resources in training computer vision algorithms than ever before. the adoption of computer vision into industry applications promises enormous potential upside ; however, computer vision models, like any ml model, must be carefully monitored. a promising model that has gone awry can quickly become a dangerous liability.", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "; however, computer vision models, like any ml model, must be carefully monitored. a promising model that has gone awry can quickly become a dangerous liability. in 2018, researchers gebru and buolamwini found three major commercial facial recognition systems performed significantly worse on darker - skinned people and women than lighter - skinned people and men. a year later, a national institute of standards and technology study that evaluated 189 facial recognition models found pervasive bias across facial recognition models. without the proper tools to audit cv models, it \u2019 s difficult to pinpoint the issues they may cause. armed with arthur \u2019 s cv model monitoring", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "models. without the proper tools to audit cv models, it \u2019 s difficult to pinpoint the issues they may cause. armed with arthur \u2019 s cv model monitoring functionality, companies can maximize the performance of their cv models and minimize any risks associated with them. - - - to schedule a demo of arthur, visit arthur. ai / demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteam", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthur - releases - the - first - computer - vision - model - monitoring - solution - for - enterprise", "metadata": {"source": "https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise", "row": 219, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 220 text : reinforcement learning for counterfactual explanations solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llm", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedexplainable aireinforcement learning for counterfactual explanationsby : sahil vermajune 10, 2021authors : sahil verma, keegan hines, john dickersonin the field of explainable ai, a recent area of exciting and rapid development has been", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s : sahil verma, keegan hines, john dickersonin the field of explainable ai, a recent area of exciting and rapid development has been counterfactual explanations. in this approach, we aim to understand the decisions of a black - box machine learning model by quantifying what would have needed to have been different in order to get a different decision. a common example is that of credit lending : if i am denied for a loan ( by an algorithm ), then a counterfactual explanation of that decision could inform me that if my income were $ 10k higher and my credit score were 30 points higher", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "), then a counterfactual explanation of that decision could inform me that if my income were $ 10k higher and my credit score were 30 points higher, then i would have been approved. this basic premise is intriguing, but it also comes along with several subtle constraints and desirable add - ons. these might include things like, causal constraints, realism constraints, actionability, sparsity, computational efficiency, and so on. if you \u2019 re interested in learning more about these overall areas of research, feel free to read our recent review paper from the neurips workshop on ml retrospectives, which won a best paper award", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "about these overall areas of research, feel free to read our recent review paper from the neurips workshop on ml retrospectives, which won a best paper award. we \u2019 ve recently posted a new approach to these problems which allows us to solve for many of these constraints. by framing the problem of generating counterfactual explanations as a markov decision problem, we can associate many of the desiderata to various components of an agent and environment ( such as the transition function or reward function ). then, using common techniques from reinforcement learning, we can train an agent to calculate counterfactuals for a given model. once", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "or reward function ). then, using common techniques from reinforcement learning, we can train an agent to calculate counterfactuals for a given model. once this agent is trained, we have amortized the computation of new counterfactuals, since any new counterfactual is calculated by simply evaluating the agent \u2019 s policy for any starting point. this makes the method extremely computationally efficient for calculating new counterfactuals. as you can see in the table below, our approach, which we termed fastcfe, is able to accommodate all of the major desiderata / constraints that have been brought forth recently", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "table below, our approach, which we termed fastcfe, is able to accommodate all of the major desiderata / constraints that have been brought forth recently in the counterfactual explainability community. this is an exciting development in operationalizing counterfactual explainability for real - world and high - volume use cases. we hope you enjoy the paper. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdoc", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / reinforcement - learning - for - counterfactual - explanations", "metadata": {"source": "https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations", "row": 220, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 221 text : serving, hosting and monitoring of an xgboost model : ubiops and arthur solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##firewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesserving, hosting and monitoring of an xgboost model : ubiops and arthur by : arthur teammay 25, 2021making ml ops observable and explainable : serving, hosting and monitoring combined. maximizing", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ubiops and arthur by : arthur teammay 25, 2021making ml ops observable and explainable : serving, hosting and monitoring combined. maximizing performance and minimizing risk are at the heart of model monitoring. in mission - critical ai models, real - time visibility is crucial. respond quickly by retraining your model, replacing the current version or tweaking the pipelines. additionally, setting up your serving and hosting infrastructure in a scalable way can be time consuming and complex. for these reasons, arthur and ubiops are pleased to announce an exciting integration that brings best - in - class tooling for", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "be time consuming and complex. for these reasons, arthur and ubiops are pleased to announce an exciting integration that brings best - in - class tooling for model serving and model monitoring. ubiops is the easy - to - use serving and hosting layer for data science code. ubiops makes it easier than ever to use a top - notch deployment, serving, and management layer on top of your preferred infrastructure. accessible via the ui, client library, or cli, it \u2019 s suitable for every type of data scientist, without the need for in - depth engineering knowledge. arthur is the proactive model monitoring platform that gives", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", it \u2019 s suitable for every type of data scientist, without the need for in - depth engineering knowledge. arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their ai deployments are performing at peak. arthur provides a layer of performance monitoring, algorithmic bias detection, and explainability, even for black box models, so data science teams can detect, diagnose, and fix any issues in production. together, arthur and ubiops provide a powerful set of tools that give you complete control over your production ai. this blog post will demonstrate just how easy it is to get started with the", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ps provide a powerful set of tools that give you complete control over your production ai. this blog post will demonstrate just how easy it is to get started with the arthur and ubiops integration, so you can complete your ai stack and stay in control. 1. use case descriptionwith the recent increases in house prices across the world, a model to predict what the price of a specific house would be could come in handy. but how can we keep track of the model \u2019 s performance, and how can we explain why one house may be priced higher than another? using a publicly available dataset from king county, usa, we trained a", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "performance, and how can we explain why one house may be priced higher than another? using a publicly available dataset from king county, usa, we trained a xgboost model to predict the house prices in that region. using the ubiops cli, the model is easily containerised, deployed on a kubernetes pod, and served behind an automatic api endpoint. via the api, arthur picks up the model \u2019 s inputs and predictions and calculates data drift, model stability, and explanations. 2. notebook walkthrough and instructionsthis notebook is based on the xgboost recipe from the", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data drift, model stability, and explanations. 2. notebook walkthrough and instructionsthis notebook is based on the xgboost recipe from the ubiops cookbook. please check out the full notebook for everything you need to set this up. compared to the xgboost recipe there are 2 big differences. the first adjustment is the following : we need to register the ( pre - trained ) model to the arthur platform. we do that using the following code snippets : from arthurai import arthurai from arthurai. common. constants import stage, inputtype, outputtype, valuetype arthur", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "code snippets : from arthurai import arthurai from arthurai. common. constants import stage, inputtype, outputtype, valuetype arthur _ url = \" https : / / app. arthur. ai \" arthur _ access _ key = \" \" # fill this in connection = arthurai ( url = arthur _ url, access _ key = arthur _ access _ key ) # define the model schema arthur _ model = connection. model ( partner _ model _ id = \" ubiops house prices \", input _ type = inputtype. tabular, output _ type = outputtype. regression", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "partner _ model _ id = \" ubiops house prices \", input _ type = inputtype. tabular, output _ type = outputtype. regression, is _ batch = true ) arthur _ model. from _ dataframe ( train _ data [ input _ columns ], stage. modelpipelineinput ) arthur _ model. add _ regression _ output _ attributes ( { \" price \" : \" price _ gt \" }, value _ type = valuetype. float ) arthur _ model. review ( ) the dataframe above represents how the model will look to arthur, and the format of the data it will expect.", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "float ) arthur _ model. review ( ) the dataframe above represents how the model will look to arthur, and the format of the data it will expect. notice how it detected some columns as categorical ( such as waterfront and view ). now we can save the model to arthur, and store the arthur model id to be used by our deployment arthur _ model _ id = arthur _ model. save ( ) with open ( \" xgboost - deployment / arthur - model - id. txt \",'w') as f : f. write ( arthur _ model _ id ) then, we \u2019 ll upload", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model - id. txt \",'w') as f : f. write ( arthur _ model _ id ) then, we \u2019 ll upload the data we used to train the model as a reference set. future data sent to the model will be compared to this reference set, to measure how much it has drifted from the types of inputs the model was built from. ref _ df = train _ data [ ['price'] + input _ columns ]. rename ( columns = {'price':'price _ gt'} ) ref _ df ['price'] = xgb. predict", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". rename ( columns = {'price':'price _ gt'} ) ref _ df ['price'] = xgb. predict ( ref _ df [ input _ columns ]. to _ numpy ( ) ) ref _ df arthur _ model. set _ reference _ data ( data = ref _ df ) the second addition is sending the inference data from within a deployment to the arthur platform. you can see it in a snippet from the actual deployment file below ( again look at our full notebook for the complete code ). it is as simple as using the ` send _ inference", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##nippet from the actual deployment file below ( again look at our full notebook for the complete code ). it is as simple as using the ` send _ inference ` method of the arthur client library. def request ( self, data ) : \" \" \" method for deployment requests, called separately for each individual request. : param dict / str data : request input data. in case of deployments with structured data, a python dictionary with as keys the input fields as defined upon deployment creation via the platform. in case of a deployment with plain input, it is a string. : return dict / str : request output", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "defined upon deployment creation via the platform. in case of a deployment with plain input, it is a string. : return dict / str : request output. in case of deployments with structured output data, a python dictionary with as keys the output fields as defined upon deployment creation via the platform. in case of a deployment with plain output, it is a string. in this example, a dictionary with the key : output. \" \" \" print ('loading data') batch = pd. read _ csv ( data ['data'] ). astype ( {'id': str } ) batch _", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ") batch = pd. read _ csv ( data ['data'] ). astype ( {'id': str } ) batch _ id = str ( uuid4 ( ) ). split ('-') [ - 1 ] print ( \" predictions being made \" ) batch ['price'] = self. model. predict ( batch. drop ( columns = ['id'] ). to _ numpy ( ) ) print ( \" sending batch to arthur \" ) inference _ data = [ {'inference _ timestamp': datetime. datetime. now ( p", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "print ( \" sending batch to arthur \" ) inference _ data = [ {'inference _ timestamp': datetime. datetime. now ( pytz. utc ),'partner _ inference _ id': row ['id'],'batch _ id': batch _ id,'inference _ data': { k : row [ k ] for k in row. keys ( ) if k! ='id'} } for row in batch. to _ dict ( orient ='records') ] self. arthur _ model. send _ inferences ( inference _ data ) now the arthur", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "batch. to _ dict ( orient ='records') ] self. arthur _ model. send _ inferences ( inference _ data ) now the arthur platform can compare training data to actual inferences. you can explore the data in data explorer and view drift metrics for the input and output variables, as well as accuracy metrics such as root mean square error ( rmse ). 3. wrap upnow that you \u2019 ve seen how ubiops and arthur. ai integrate to make serving, hosting and monitoring easily accessible, we hope that this helps you in your day - to - day projects. for any questions or suggestions", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai integrate to make serving, hosting and monitoring easily accessible, we hope that this helps you in your day - to - day projects. for any questions or suggestions and the latest product updates please join the ubiops community slack channel and the arthur slack community. download the full notebook here. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##nlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / serving - hosting - and - monitoring - of - an - xgboost - model - ubiops - and - arthur", "metadata": {"source": "https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur", "row": 221, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 222 text : cb insights recognizes arthur as one of the most innovative ai startups in the world solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##observabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatescb insights recognizes arthur as one of the most innovative ai startups in the worldby : arthur teamapril 7, 2021today, cb insights released their \u201c ai 100 \u201d ranking for 2021, highlighting the 100 most promising and innovative private", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##by : arthur teamapril 7, 2021today, cb insights released their \u201c ai 100 \u201d ranking for 2021, highlighting the 100 most promising and innovative private ai companies from around the world \u2014 and we are thrilled to be included in the list for the second year in a row. arthur is one of just two companies that cb insights selected in the model monitoring space, and we are the only model monitoring company to make the list two years in a row. last year \u2019 s ai 100 companies went on to raise a total of $ 5. 2b in additional funding ( including us! ), and many companies on the list last year", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ai 100 companies went on to raise a total of $ 5. 2b in additional funding ( including us! ), and many companies on the list last year went on to exit via m & a, spac, or ipo. we \u2019 re honored that cb insights has recognized arthur alongside some of our favorite companies in the space, including our partners at algorithmia. with so many notable startups lined up next to us on this year \u2019 s ai 100 list, the future looks bright. we believe in ai \u2019 s amazing potential to transform the world and are excited to be doing our part to ensure this transformation is safe, reliable", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "future looks bright. we believe in ai \u2019 s amazing potential to transform the world and are excited to be doing our part to ensure this transformation is safe, reliable, and fair. cb insights'ai 100 is a ranking of the 100 most innovative and promising ai startups around the world \u2014 and we're thrilled to be included for the second year in a row. about arthurarthur is a proactive machine learning model monitoring platform that gives you the confidence that your ai deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. with performance monitoring, bias detection", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##s are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. with performance monitoring, bias detection, and customizable alerts, arthur makes sure that you never miss an issue, and arthur \u2019 s explainability engine makes runtime debugging effortless. arthur \u2019 s customers, including fortune 100 companies like humana and ai - driven startups like expel and truebill, are using the platform to ensure that they can catch and fix any issues with their production ai systems before they become billion - dollar problems. previous postsharenext post we make ai better", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ensure that they can catch and fix any issues with their production ai systems before they become billion - dollar problems. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / cb - insights - ai - 100 - 2021", "metadata": {"source": "https://www.arthur.ai/blog/cb-insights-ai-100-2021", "row": 222, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 223 text : interactive analysis with petabytes of model data solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringinteractive analysis with petabytes of model databy : keegan hinesapril 6, 2021machine learning models in production can degrade in lots of unexpected ways. there are certain questions, concepts, and metrics that we might know ahead of time that we \u2019", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "models in production can degrade in lots of unexpected ways. there are certain questions, concepts, and metrics that we might know ahead of time that we \u2019 ll want to check in on week over week. and then, of course, there are unknown unknowns - problems, questions, and calculations that we are unable to anticipate but will become critical in 6 - 12 months from now. for this reason, the arthur platform has been built to be as configurable as possible, to not be prescriptive, and to allow our users full and flexible interfaces for interrogating and understanding model performance. in this post", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##able as possible, to not be prescriptive, and to allow our users full and flexible interfaces for interrogating and understanding model performance. in this post, we \u2019 ll describe some of the architectural choices that underlie this design philosophy, and share a couple of the powerful modes of interaction that are made possible by these choices. optimized storageas your model data is ingested into the arthur platform, it is indexed into a distributed columnar datastore. not only is this highly scalable ( can easily handle petabytes of model data ), but it allows for rapid aggregations and queries, thus allowing ex", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "only is this highly scalable ( can easily handle petabytes of model data ), but it allows for rapid aggregations and queries, thus allowing exploratory and ad - hoc access to all the information about historical model performance. you don \u2019 t have to make the difficult choice to subsample or summarize your data, and you don \u2019 t need to decide your favorite metrics ahead of time. you \u2019 ll have at your fingertips all the historical data about your model \u2019 s inputs, predictions, performance, explanations, and other insights. this enables a powerful ability to slice - and - dice model performance across", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data about your model \u2019 s inputs, predictions, performance, explanations, and other insights. this enables a powerful ability to slice - and - dice model performance across any facets or subpopulations that are relevant to you. for arthur users, this capability results in two particularly powerful ways of monitoring data and models. data explorerthe arthur platform provides an interactive data visualization suite that allows you to explore and understand the data pertaining to your model. our backend architecture allows us to compute the necessary aggregations, groupings, and filters in milliseconds, even over hundreds of millions of data points. in the examples below,", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to compute the necessary aggregations, groupings, and filters in milliseconds, even over hundreds of millions of data points. in the examples below, we can visualize the distributions and correlations amongst model inputs, outputs, and even explanations. we can quickly navigate through different time slices, facet the data by groupings of different variables, and understand your model \u2019 s predictions and data landscape. query enginein addition to the rich set of visualizations and metrics available in the arthur ui, you can also fetch any and all of this underlying data ( and computation ) through our api. our api - first approach means that", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "available in the arthur ui, you can also fetch any and all of this underlying data ( and computation ) through our api. our api - first approach means that data scientists can quickly check - in on model performance using a familiar tool, such as a jupyter notebook. our query engine exposes a sql - like language that will be quickly familiar to data scientists, so that they can compute and visualize ad - hoc summaries and aggregations on large sets of data. as an example, one day we might be curious to know how our model is performing for males versus females, and if that has been changing over time.", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". as an example, one day we might be curious to know how our model is performing for males versus females, and if that has been changing over time. we construct a query with familiar group - by \u2019 s and filters and the arthur backend computes aggregations over millions of inferences in just a few milliseconds. the result of this query is easy to drop into a pandas dataframe for quick visualization. you can use arthur's query engine to explore and visualize your monitoring data \u2014 for example, plotting average prediction value by sex over time. in addition to model evaluation metrics, it is a", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to explore and visualize your monitoring data \u2014 for example, plotting average prediction value by sex over time. in addition to model evaluation metrics, it is a snap to get a quick view of the distribution ( s ) of a model \u2019 s input or outputs, to understand how they may be shifting over time. arthur's query engine makes it easy to visualize how your model input and output distributions are shifting over time. from a notebook, we can quickly and easily dive into any subpopulations and assess model performance and data stability. once we are alerted to issues with model performance or data drift, having this data at our", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "into any subpopulations and assess model performance and data stability. once we are alerted to issues with model performance or data drift, having this data at our fingertips empowers us to conduct an investigation and drill down to its root cause. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of servicepr", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / interactive - analysis - with - petabytes - of - model - data", "metadata": {"source": "https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data", "row": 223, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 224 text : everything you need to know about model monitoring for natural language processing solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringeverything you need to know about model monitoring for natural language processingby : keegan hinesmarch 29, 2021from simple chatbots to document classifiers to generative models like gpt - 3, natural language processing models are seemingly everywhere these days.", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "29, 2021from simple chatbots to document classifiers to generative models like gpt - 3, natural language processing models are seemingly everywhere these days. nlp models are powerful tools for processing unstructured text data \u2014 but with great power comes great responsibility. if you \u2019 re not monitoring your nlp models just as you would your tabular models, you can overlook a number of sticky issues that could quickly become billion - dollar problems. here are a few things that any organization deploying nlp models into production should be doing to ensure that those models continue to perform as expected. monitor for data drift to prevent drops in performance", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "any organization deploying nlp models into production should be doing to ensure that those models continue to perform as expected. monitor for data drift to prevent drops in performance. an nlp model, just like any other machine learning model, is trained on a training dataset that represents the world at a point in time \u2014 and as the world changes and the input data coming into your language model starts to change, your model will start to run into performance issues. over time, your nlp model performance will degrade if you \u2019 re not proactively monitoring for and correcting data drift as it occurs. monitoring nlp models for data drift", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##p model performance will degrade if you \u2019 re not proactively monitoring for and correcting data drift as it occurs. monitoring nlp models for data drift involves comparing the statistical similarity of new input documents to your training set documents. as input documents shift in their typical word - use, you might need to update your model to account for the linguistic patterns. understanding when and where drift is occurring is essential to maintaining the integrity of your nlp models over time. the arthur platform monitors nlp models for data drift and automatically alerts you when your input documents start drifting beyond acceptable bounds. monitor for bias to protect against discrimination. algorithmic", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "monitors nlp models for data drift and automatically alerts you when your input documents start drifting beyond acceptable bounds. monitor for bias to protect against discrimination. algorithmic bias is a critical issue in any machine learning system, but issues of bias can be especially pernicious and difficult to solve in language models. there are several methods of detecting and mitigating bias, which we cover extensively in blog posts, making models more fair and a crash course in fair nlp for practioners. in addition to the techniques laid out in those blog posts, there are a few additional ways to track potential bias in your machine learning models. first", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ioners. in addition to the techniques laid out in those blog posts, there are a few additional ways to track potential bias in your machine learning models. first, you can track the performance of your nlp models on input documents partitioned by sensitive attributes that you may know. for example, you can track whether your medical document classifier is more or less accurate for documents from medical visits of men versus women, or black patients versus white patients. uncovering differences in accuracy and other performance measures across different subgroups can help you identify \u2014 and fix \u2014 unfair model bias. check out our data drift detection part ii : unstructured data", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "other performance measures across different subgroups can help you identify \u2014 and fix \u2014 unfair model bias. check out our data drift detection part ii : unstructured data in nlp and cv blog post. the arthur platform offers this type of performance - bias analysis for natural language and tabular models, as well as the ability to partition by multiple attributes at a time to provide you more granular insights into potential biasesanother step is to take proactive steps to debias language outputs from your models. we at arthur are working on building out more features to do this in our platform \u2014 stay tuned for more! use token - level", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "language outputs from your models. we at arthur are working on building out more features to do this in our platform \u2014 stay tuned for more! use token - level explanations to understand black box nlp model behavior. whether your nlp model is a \u201c bag of words \u201d model ( the position of the words doesn \u2019 t matter ) or a sequence - based model ( the context and position of words do matter ), getting a token - level explanation of your model output is incredibly useful and important for understanding why your model might be getting something wrong, and understanding how to take the right steps to fix it. for example, if we are using", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and important for understanding why your model might be getting something wrong, and understanding how to take the right steps to fix it. for example, if we are using a medical document classifier to predict different document types in a given set of medical records, we can use this technique to understand the specific words that resulted in a given document being classified as a document about cardiovascular and pulmonary issues. on the far right - hand side, we see the top words contributing to the predicted class in green and words contributing against it in red. we see this illustrated in the document itself. this is useful for a domain expert to understand in human context the \u201c", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "green and words contributing against it in red. we see this illustrated in the document itself. this is useful for a domain expert to understand in human context the \u201c real world \u201d value of the highlighted words. for example, we see the word \u201c procedure \u201d appears at the top of the list of explanations, meaning it most significantly impacted the ( correctly ) predicted classification of cardiovascular / pulmonary. if we click through, on the left - hand side, some of the other incorrect predictions, we begin to notice this word is also a top influencer in those predictions as well. see urology, below : arthur's nlp explainability tools", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we begin to notice this word is also a top influencer in those predictions as well. see urology, below : arthur's nlp explainability tools provide token - level explanations for model outputs. above, we see that \u201c procedure \u201d was the second most influential word in this classification prediction. this is where we see the importance of domain expertise and human involvement. given the context of a healthcare system, it is easy for us to imagine how the word \u201c procedure \u201d might appear frequently across medical specialties. perhaps it \u2019 s worth exploring a different model that accounts for the positioning of particular words, or perhaps we simply need to re", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "appear frequently across medical specialties. perhaps it \u2019 s worth exploring a different model that accounts for the positioning of particular words, or perhaps we simply need to retrain how it treats certain words based on what we know is most influencing predictions. in any case, we see how providing this analysis for each prediction and word is crucial information. monitor your nlp models with arthur. the arthur platform has recently released extensive monitoring support for nlp models, including nlp data drift detection, token - level explainability that provides insight into the key drivers in nlp classification model predictions, and bias detection for nlp models. if you \u2019 re deploy", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "token - level explainability that provides insight into the key drivers in nlp classification model predictions, and bias detection for nlp models. if you \u2019 re deploying nlp models into production and are looking for a solution for monitoring those models over time, we \u2019 d love to connect and show you how arthur can help. request a demo today. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompan", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##scribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / nlp - model - monitoring - with - arthur", "metadata": {"source": "https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur", "row": 224, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 225 text : making models more fair : everything you need to know about algorithmic bias mitigation solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe,", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##wallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessmaking models more fair : everything you need to know about algorithmic bias mitigationby : jessica daimarch 23, 2021as ml models affect our lives more and more, machine learning practitioners need to ensure that our models are not", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##igationby : jessica daimarch 23, 2021as ml models affect our lives more and more, machine learning practitioners need to ensure that our models are not creating harm for end - users. it is especially important to make sure that our models are not unfairly harming any subgroups of our population. the first step is identifying and quantifying any potential bias in a model, and many different definitions of group fairness have been proposed. the arthur platform provides proactive bias monitoring and alerting, so you know exactly when, where, and how algorithmic bias is occurring in your models. at arthur, we \u2019 re also interested", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "monitoring and alerting, so you know exactly when, where, and how algorithmic bias is occurring in your models. at arthur, we \u2019 re also interested in the question of what we can do to help make your models more fair. in this post, we briefly describe families of techniques for bias mitigation : ways to improve an unfair model. while this is an active area of research, current mitigation techniques target specific parts of the model development lifecycle : preprocessing, or adjustments on the training data ; in - processing, or algorithms specifically intended to be fair ; and post - processing, or adjusting the outputs", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##rocessing, or adjustments on the training data ; in - processing, or algorithms specifically intended to be fair ; and post - processing, or adjusting the outputs of the model. the right solution might depend on the use case, the industry, and the model deployment ; in this post, we outline several families of approaches to fair ml. why do we observe unfair classifiers? intuitively, training classifiers on historical datasets \u201c bakes in \u201d bias into the model : if your hiring algorithm uses data from the 1960s, it \u2019 s likely to think that women are great at being secretaries, while men should be ceos.", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": if your hiring algorithm uses data from the 1960s, it \u2019 s likely to think that women are great at being secretaries, while men should be ceos. however, actually characterizing how \u201c biased data \u201d might be related to a \u201c biased model \u201d is a more complicated task. one commonly discussed cause of bias in a learned model is an imbalance between subgroups in the data. by definition, if we have a subgroup that is a minority represented in our training data, then that means we have fewer observations of them compared to majority groups. when the classifier is training, it is optimizing a loss function over the whole", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "then that means we have fewer observations of them compared to majority groups. when the classifier is training, it is optimizing a loss function over the whole dataset. if the major class is truly dominant, then it is possible that the best way to achieve high overall accuracy on the training data is to be as accurate as possible on the majority group while incurring errors on the minority group. therefore, if the majority group and minority group have any differences in their properties and relationship to the target variable, the model is likely adhering primarily to the patterns of the major data and potentially ignoring the contributions of the minority. this could", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and relationship to the target variable, the model is likely adhering primarily to the patterns of the major data and potentially ignoring the contributions of the minority. this could mean the model is fairly accurate for the majority groups, but much less accurate for the smaller subgroups. of course, it \u2019 s worth mentioning that not all data biases result from undersampling. in many cases where the data in question represents human behaviors, as in the cases of loan performance, hiring, or crime data, historical biases from human personalities can also show up in the information. hiring data from the 1960s made predictively, for instance, might suggest that", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "crime data, historical biases from human personalities can also show up in the information. hiring data from the 1960s made predictively, for instance, might suggest that women are best suited as secretaries rather than executives. no matter what technique you choose to ameliorate bias among these options, close attention should be paid to the historical context through which the data collection practices and societal influence can be better understood. pre - processing bias mitigationpre - processing techniques for bias mitigation tend to be all about the data. as described in the previous section, particular characteristics of the training data may directly cause the problematic performance of a learned model", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##tion tend to be all about the data. as described in the previous section, particular characteristics of the training data may directly cause the problematic performance of a learned model. for this reason, many techniques for pre - processing focus on modifying the training set to overcome versions of dataset imbalance. this could be achieved in many ways including resampling rows of the data, reweighting rows of the data, flipping the class labels across groups, and omitting sensitive variables or proxies. other techniques consider learning direct modifications and transformation functions that achieve desired fairness constraints. in all cases, the strategy is to change the underlying training data", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "proxies. other techniques consider learning direct modifications and transformation functions that achieve desired fairness constraints. in all cases, the strategy is to change the underlying training data, and then proceed with training using any classification algorithm desired. by modifying the training data in these specific ways, the outputs of the learned classifier will be less biased. in - processing bias mitigationwith in - processing techniques, we want to create a classifier that is explicitly aware of our fairness goals. that is, in training the classifier, it is not enough to simply optimize for accuracy on the training data. instead, we modify the loss function to", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is, in training the classifier, it is not enough to simply optimize for accuracy on the training data. instead, we modify the loss function to account simultaneously for our two goals : our model should be both accurate and fair. this modification can be achieved in many ways such as using adversarial techniques, ensuring underlying representations are fair, or by framing constraints and regularization. in each case, the goal is that the underlying classifier is directly taking fairness into consideration. as a result, the outcomes of that trained classifier will be less biased as compared to a classifier that knew nothing about fairness. post - processing", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "consideration. as a result, the outcomes of that trained classifier will be less biased as compared to a classifier that knew nothing about fairness. post - processing bias mitigationfinally, there is a family of techniques that aim to only adjust the outputs of a model and leave the underlying classifier and data untouched. the benefit here is appealing in its simplicity - in using post - processing methods, we allow the model development team to use any modeling algorithms they wish, and they don \u2019 t need to modify their algorithm or retrain a new model to make it more fair. instead, post - processing methods center on the idea of", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "they don \u2019 t need to modify their algorithm or retrain a new model to make it more fair. instead, post - processing methods center on the idea of adjusting the outputs of an unfair model such that the final outputs become fair. as an example, early works in this area have focused on modifying outcomes and thresholds in a group - specific manner. suppose we build a classification model to assist in credit risk decisions. after much hyperparameter tuning, we arrive at a model that is accurate and generalizes well, but we notice that it tends to favor older loan applicants over younger applicants. with post - processing techniques, we would keep", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that is accurate and generalizes well, but we notice that it tends to favor older loan applicants over younger applicants. with post - processing techniques, we would keep the classifier as is, but adjust the outcomes so that the overall acceptance rates are more equitable. we would pick a definition of fairness ( say, demographic parity ), and adjust the treatments across the groups such that the final outcomes are as desired. this means we might have group - specific thresholds instead of a single threshold for the classifier. detecting unintended bias is the first step to mitigating it. arthur gives you the power to detect and analyze", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a single threshold for the classifier. detecting unintended bias is the first step to mitigating it. arthur gives you the power to detect and analyze bias in your ml models. it \u2019 s important to note that in this scenario, there remains a lot of legal ambiguity around bias mitigation. with so much unknown about how courts will handle algorithmic discrimination, many organizations are leaning heavily on their legal teams for how to navigate this complexity! many post - processing techniques have this basic structure in common : they leave the classifier and the data alone, and only adjust the outcomes in a group - dependent way. and while binary", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "have this basic structure in common : they leave the classifier and the data alone, and only adjust the outcomes in a group - dependent way. and while binary classification has been a focus in the past, recent work has sought to extend these ideas to regression models as well. the overall framework can be effective for achieving fairness in an ml system, though in some use cases, treating groups differently could be an uncomfortable proposition, or even an illegal one. accuracy / fairness tradeoffwhen we embark on deploying ml models that are more fair, we have to acknowledge that this fairness does not come for free ; in fact, in many cases", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##hen we embark on deploying ml models that are more fair, we have to acknowledge that this fairness does not come for free ; in fact, in many cases, it may conflict with model accuracy. consider one extreme : a model that is as accurate as possible ( relative to available ground truth ) is potentially quite unfair and discriminates against at least one subpopulation. consider the other extreme : a model which is perfectly fair and is equitable across all populations, this model must be less accurate than a model that did not consider fairness as a constraint. ( though some recent work suggests the tradeoff may not necessarily always occur, the", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "must be less accurate than a model that did not consider fairness as a constraint. ( though some recent work suggests the tradeoff may not necessarily always occur, the behavior of \" fair algorithms \" when deployed in the real world will not always match the results demonstrated in theory ; as a result, understanding the relationship between fairness and accuracy is critical to being confident in the models we choose to use. ) between these two extremes live a broad family of possible models that balance the concerns of accuracy and fairness. this set of models forms a pareto frontier ( efficient frontier ) in the space of accuracy vs fairness. the figure below, from a 2019 survey", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and fairness. this set of models forms a pareto frontier ( efficient frontier ) in the space of accuracy vs fairness. the figure below, from a 2019 survey paper characterizing the performance of many popular fair ml algorithms, illustrates this tradeoff empirically : on the x - axis is disparate impact, a measure of fairness, while the y - axis is accuracy. ( the entire paper is worth reading ; it is an excellent introduction to many common considerations for the performance of fair ml models. ) as practitioners and stakeholders, we must confront questions about this tradeoff : for each use case, we must weigh the costs of potential harm", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "fair ml models. ) as practitioners and stakeholders, we must confront questions about this tradeoff : for each use case, we must weigh the costs of potential harm through unfairness against costs of potential harm through reduced accuracy. the following charts illustrate the trade - off between fairness and accuracy among some of the more popular fair ml algorithms. these are challenging questions that have no single right answer. instead, ml practitioners must work together with stakeholders such as business leaders, humanities experts, compliance, and legal teams and formulate a program for how to best treat your population. the arthur platform brings together performance monitoring and algorithmic bias monitoring into a unified view", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and legal teams and formulate a program for how to best treat your population. the arthur platform brings together performance monitoring and algorithmic bias monitoring into a unified view for all of your stakeholders, so you can make informed decisions about how to make your models more fair and effective. if you \u2019 d like to learn more about how arthur can help you combat bias in your ai systems, please reach out to schedule a demo. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / ai - bias - mitigation - 101", "metadata": {"source": "https://www.arthur.ai/blog/ai-bias-mitigation-101", "row": 225, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 226 text : deploy, serve, monitor, and maintain ai at scale with arthur and algorithmia solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##observabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatesdeploy, serve, monitor, and maintain ai at scale with arthur and algorithmiaby : arthur teamfebruary 22, 2021at arthur, we are obsessed with making sure that our customers have complete ai observability for", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "algorithmiaby : arthur teamfebruary 22, 2021at arthur, we are obsessed with making sure that our customers have complete ai observability for all of their machine learning models, no matter where they are deployed. but we know that production ai systems need more than just monitoring \u2014 and that \u2019 s why today, we are excited to announce our partnership with algorithmia, the industry leader in machine learning operations ( mlops ) and model management. algorithmia \u2019 s enterprise mlops platform manages all stages of the machine learning lifecycle within existing operational processes, enabling you to put models into production quickly, securely, and cost -", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "enterprise mlops platform manages all stages of the machine learning lifecycle within existing operational processes, enabling you to put models into production quickly, securely, and cost - effectively. unlike inefficient and expensive do - it - yourself mlops management solutions that lock users into specific technology stacks, algorithmia automates ml deployment, optimizes collaboration between operations and development, leverages existing sdlc and ci / cd systems, and provides enterprise - grade security and governance across the lifecycle. arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their ai deployments are performing at peak. arthur provides", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "lifecycle. arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their ai deployments are performing at peak. arthur provides a layer of performance monitoring, algorithmic bias detection, and explainability, even for black box models, so data science teams can detect, diagnose, and fix any issues in production. we are working with the algorithmia team to make it easier than ever to use their top - notch deployment, serving, and management tools and our leading monitoring capabilities. together, arthur and algorithmia provide a powerful set of tools that give you complete control over your production ai. this blog", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "management tools and our leading monitoring capabilities. together, arthur and algorithmia provide a powerful set of tools that give you complete control over your production ai. this blog post will demonstrate just how easy it is to get started with the arthur and algorithmia integration, so you can complete your ai stack. arthur and algorithmia : helping you reach ai maturity fasteralgorithmia \u2019 s 2021 enterprise trends in machine learning report revealed that 56 % of organizations struggle with governance, security, and auditability issues \u2014 making it the # 1 challenge for model deployment and management. arthur extends algorithmia \u2019 s monitoring capabilities to give you rich insights into model performance", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "auditability issues \u2014 making it the # 1 challenge for model deployment and management. arthur extends algorithmia \u2019 s monitoring capabilities to give you rich insights into model performance, data integrity, and ai fairness, helping you manage ai governance more completely. if you \u2019 re already using algorithmia for mlops, it takes just minutes to add arthur monitoring. with our integration, algorithmia customers can take full advantage of advanced ml lifecycle management workflows enabled by arthur \u2019 s runtime monitoring capabilities \u2014 think automated refits based on data drift, horse racing ( a / b testing ) different versions of a model, and more. let \u2019 s", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "monitoring capabilities \u2014 think automated refits based on data drift, horse racing ( a / b testing ) different versions of a model, and more. let \u2019 s walk through what it looks like to use arthur and algorithmia together to monitor and maintain ai at scale. deploy your model behind an endpoint with algorithmiawe \u2019 ll use algorithmia to serve our trained model behind an algorithmia endpoint. here is an example algorithmia project where we \u2019 re doing just that : we have a saved model ( pkl file ), and we \u2019 re loading this model into memory. our implementation of algorithmia \u2019 s apply ( )", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we have a saved model ( pkl file ), and we \u2019 re loading this model into memory. our implementation of algorithmia \u2019 s apply ( ) function is to invoke the predict _ proba ( ) method for any new inputs to the endpoint. deploying your machine learning model behind an endpoint with algorithmia's apply ( ) function. this model is now deployed behind an endpoint that we can hit with rest, or with any of algorithmia \u2019 s supported sdks in several languages. log your model inputs and outputs with arthurnext, let \u2019 s integrate this project with arthur so we can", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ia \u2019 s supported sdks in several languages. log your model inputs and outputs with arthurnext, let \u2019 s integrate this project with arthur so we can monitor this model. let \u2019 s assume we \u2019 ve already registered a model with arthur, and will fetch this object by its model id. we \u2019 ll import the arthur ai python library and set up a connection with the platform. we will use that connection to fetch our model by its id. the last step here is that we \u2019 ll want to wrap our model \u2019 s predict _ proba ( ) method using the loginferences decorator provided in the arthurai library", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that we \u2019 ll want to wrap our model \u2019 s predict _ proba ( ) method using the loginferences decorator provided in the arthurai library. to do this, we \u2019 ll define a simple function called * * modelpredict ( ) * * for this wrapping, and we \u2019 ll decorate that method as shown below. finally, our implementation of algorithmia \u2019 s apply ( ) function is simply to call our model _ predict ( ) wrapper for any inputs to the endpoint. each time the endpoint is invoked, the model \u2019 s input and output are logged with arthur for automated analytics and alerting.", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "inputs to the endpoint. each time the endpoint is invoked, the model \u2019 s input and output are logged with arthur for automated analytics and alerting. just a few lines of python, and you're all set to monitor your algorithmia - deployed model with arthur's ml monitoring platform. with a couple of extra lines of python, we can easily add arthur \u2019 s monitoring tools to any models we have deployed with algorithmia. unlock model optimization, versioning, and fairness with arthurai and algorithmiathe arthur - algorithmia integration enables you to deploy, serve, manage, and maintain your ai at scale with advanced monitoring", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and fairness with arthurai and algorithmiathe arthur - algorithmia integration enables you to deploy, serve, manage, and maintain your ai at scale with advanced monitoring and governance. ready to get started today? if you \u2019 re already using algorithmia and arthur, it takes minutes to implement the integration as demonstrated above. if you \u2019 re not already using algorithmia, sign up to watch a demo today and learn how enterprise mlops can accelerate your time - to - value for ml. and if you \u2019 re not already using arthur, get in touch with us to schedule a demo and learn more about how we can help you get started with", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". and if you \u2019 re not already using arthur, get in touch with us to schedule a demo and learn more about how we can help you get started with proactive model monitoring. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur.", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##amcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / algorithmia - partnership", "metadata": {"source": "https://www.arthur.ai/blog/algorithmia-partnership", "row": 226, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 227 text : our top takeaways from neurips 2020 on responsible machine learning solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##vabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedeventsour top takeaways from neurips 2020 on responsible machine learningby : jessica daijanuary 8, 2021no longer the niche sub - field it was several years ago, \" fair \" ml and related topics have now become a core part of the field", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "8, 2021no longer the niche sub - field it was several years ago, \" fair \" ml and related topics have now become a core part of the field of machine learning. at neurips 2020, the broader theme of responsibility ( even beyond fairness ) in ml has a much larger footprint at the conference ; on a more granular level, fair ml as a field overall has also developed substantially, asking a broader range of questions critical to operationalizing fairness. from the start of the conference, neurips 2020 emphasized the importance of thinking deeply about the implications of work in machine learning research : the conference kicked off with a keynote from", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of the conference, neurips 2020 emphasized the importance of thinking deeply about the implications of work in machine learning research : the conference kicked off with a keynote from georgia tech's charles isbell, titled \" you can't escape hyperparameters and latent variables : machine learning as a software engineering enterprise. \" despite its somewhat dry title, the talk is anything but. isbell makes the case for machine learning researchers to approach their work in the same way as software engineers must, considering how the usage of software must shape its development. isbell dispells the myth of bias being \" only due to biased dataset", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "engineers must, considering how the usage of software must shape its development. isbell dispells the myth of bias being \" only due to biased datasets \" and therefore something that individual scientists need not be concerned about ; this keynote set the tone for how the rest of the conference unfolded. ( you can watch the full video of the keynote ; this is nothing like any pre - recorded talk i've ever seen, and i cannot recommend it more strongly. ) the growing community around fair / responsible ml is clear not only in the papers accepted to the main conference, but also in the workshops, several of which were dedicated to", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "growing community around fair / responsible ml is clear not only in the papers accepted to the main conference, but also in the workshops, several of which were dedicated to discussing concerns related to developing fair ml \u2014 resistance ai, dataset curation and security, fair ai in finance, algorithmic fairness through the lens of causality and interpretability, consequential decisions in dynamic environments \u2014 and several more where retrospectives, surveys, and meta - analyses, ml for economic policy, broader impacts of ai research, human and machine - in - the - loop evaluation and training strategies. in the rest of this piece, i will be highlighting contributions", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "broader impacts of ai research, human and machine - in - the - loop evaluation and training strategies. in the rest of this piece, i will be highlighting contributions from both workshops and the main conference. at arthur, we \u2019 re really excited about the development of these research directions : fairness for a broader range of algorithms and applications. most early work in fair ml focused on binary classifiers with a single protected attribute ; today, the vast majority of available open - source implementations are also only available for binary classifiers with a single protected attribute. of course, there are many other problem settings where fairness may be a concern, and this year '", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "only available for binary classifiers with a single protected attribute. of course, there are many other problem settings where fairness may be a concern, and this year's neurips saw the introduction of many new fair ml algorithms for many more scenarios : clustering, streaming, online learning, regression models, overlapping group membership, multiple classes, and scenarios without access to information about demographics. more nuanced ways of measuring, understanding, and communicating fairness. while metrics of fairness based on output distributions and group - conditional error rates have been common for some time now, some exciting lines of work have been developing around more nuanced approaches", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "based on output distributions and group - conditional error rates have been common for some time now, some exciting lines of work have been developing around more nuanced approaches to measurement. can i trust my fairness metric? emphasizes the high variance of fairness metrics when batch or dataset sizes are small, and introduces a way to generate more accurate and lower - variance estimates of metric values ; meanwhile, measuring bias with wasserstein distance, from the dataset curation and security workshop, proposes an alternate metric for fairness that captures inequity that may be missed by current standard metrics. opportunities for a more interdisciplinary approach to perceptions of fairness", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "proposes an alternate metric for fairness that captures inequity that may be missed by current standard metrics. opportunities for a more interdisciplinary approach to perceptions of fairness in ml, from the ml retrospectives, surveys, and analyses workshop ( ml - rsa ), draws insight from the field of psychology to discuss how common fairness metrics are interpreted by human end - users. analyzing the qualities of fair algorithms, such as robustness and downstream implications. complementary to the development of fair algorithms themselves, a substantial amount of new work discusses how fair algorithms might perform under a variety of conditions and situating the algorithms in the broader context of deployment.", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "algorithms themselves, a substantial amount of new work discusses how fair algorithms might perform under a variety of conditions and situating the algorithms in the broader context of deployment. for example, how do fair decisions fare in long - term qualification? considers the impact of static fairness constraints on long - term well - being ; the workshop on consequential decision making in dynamic environments was dedicated to work in this area. similarly, fair multiple decisionmaking through soft interventions considers the scenario where there are multiple, potentially - interacting algorithms. in a parallel thread, ensuring fairness beyond the training data proposes the first ( to my knowledge ) algorithm to train a fair class", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "multiple, potentially - interacting algorithms. in a parallel thread, ensuring fairness beyond the training data proposes the first ( to my knowledge ) algorithm to train a fair classifier that is provably robust to a set of possible distribution shifts. more broadly, the workshop on algorithmic fairness through the lens of causality and interpretability has work contextualizing fair algorithms in many ways, such as fairness and robustness in invariant learning, which connects topics from causal inference and domain generalization to fairness. the fair ai in finance workshop also touches on related issues, particularly with regards to the application. concerns of fairness and bias complement concerns of model robust", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to fairness. the fair ai in finance workshop also touches on related issues, particularly with regards to the application. concerns of fairness and bias complement concerns of model robustness writ large. indeed, discussion of the \u201c robustness \u201d of a machine - learning - based system necessarily includes discussions of dataset and model security, policy and privacy, dataset and model bias, data ingest via scraping and labeling, amongst numerous other considerations. toward that end, joint with colleagues at cmu, ibm, illinois, maryland, and ttic, our chief scientist john dickerson hosted the workshop on dataset curation and security, which brought together researchers", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", ibm, illinois, maryland, and ttic, our chief scientist john dickerson hosted the workshop on dataset curation and security, which brought together researchers from the adversarial ml and fairness in ml communities as well as policy wonks from the brookings institute and other \u201c tech - adjacent \u201d bodies for a full day of discussion of what it means to claim, and what it might take to improve, \u201c robustness \u201d of machine learning models. in short, it is hard to make statements about model behavior ( vis - a - vis, for example, fairness ) without also deeply considering other dimensions such as security and the", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "it is hard to make statements about model behavior ( vis - a - vis, for example, fairness ) without also deeply considering other dimensions such as security and the legal landscape. our vp of responsible ai, liz o \u2019 sullivan, also gave an invited talk at this workshop on some of the dangers of scraping data from the internet, focusing primarily on how this can take agency from unknowing humans and may result in otherwise unexpected or undefined behavior. similar concerns and sentiments were raised at other neurips workshops, in papers, and in the aforementioned invited talk by charles isbell ; we expect this trend to continue in the coming months", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "raised at other neurips workshops, in papers, and in the aforementioned invited talk by charles isbell ; we expect this trend to continue in the coming months and years. beyond fairness : bigger - picture views of algorithmic ( in ) justice. it's been clear for a while now that it is not enough for algorithms to simply be \" fair \" \u2014 there are several related topics of technical interest, as well as broader considerations that must be made when analyzing the impact of algorithms in the world. the ml - rsa workshop has several works in this category, such as arthur's own counterfactual explanations for machine learning", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "algorithms in the world. the ml - rsa workshop has several works in this category, such as arthur's own counterfactual explanations for machine learning, as well as a survey of algorithmic recourse. ml - rsa also has some more critical work, such as data and its ( dis ) contents : a survey of dataset development and use in ml research. finally, it behooves everyone working in fair ml to take a look at the work from the resistance ai workshop, which is full of thought - provoking work, both technical and non - technical, that questions the way power is arranged", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the work from the resistance ai workshop, which is full of thought - provoking work, both technical and non - technical, that questions the way power is arranged and rearranged by ml systems. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##companyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / neurips - responsible - ml", "metadata": {"source": "https://www.arthur.ai/blog/neurips-responsible-ml", "row": 227, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 228 text : an overview of counterfactual explainability solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedexplainable aian overview of counterfactual explainabilityby : keegan hinesdecember 10, 2020within the field of explainable ai ( xai ), the technique of counterfactual explainability has progressed rapidly, with many exciting developments just in the past", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the field of explainable ai ( xai ), the technique of counterfactual explainability has progressed rapidly, with many exciting developments just in the past couple years. to help crystallize and understand the major development areas, we \u2019 ll be presenting a new paper at the neurips workshop on ml retrospectives, surveys, and meta - analyses. this post will provide a high - level summary, but if you \u2019 re interested in getting started in this area, we encourage you to check out the full paper. what are counterfactual explanations? counterfactual explanations ( cfes ) are an emerging technique for", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "we encourage you to check out the full paper. what are counterfactual explanations? counterfactual explanations ( cfes ) are an emerging technique for local, example - based post - hoc explanations methods. given a datapoint a and its prediction p from a model, a counterfactual is a datapoint close to a, such that the model predicts it to be in a different class q ( p = q ). a close datapoint is considered a minimal change that needs to be made to get a different prediction. this can help in scenarios like rejection of a loan or credit card request, where the applicant is", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "minimal change that needs to be made to get a different prediction. this can help in scenarios like rejection of a loan or credit card request, where the applicant is willing to know about the smallest change in the feature set that can lead to acceptance of the request. explicitly, consider a person who applies for a loan and is rejected. in our simple example, say that the person is represented by a length - 3 feature vector x : liquid assets of $ 10k ; outstanding debt of $ 50k ; and annual income of $ 45k. so, x = ( $ 10k, $ 50k, $ 45k ). our fictitious", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "$ 50k ; and annual income of $ 45k. so, x = ( $ 10k, $ 50k, $ 45k ). our fictitious loan agency uses a pre - trained binary \u201c loan acceptance \u201d classifier, f, that takes as input length - 3 feature representations of applications and returns one of two labels : y = 0 ( reject ), or y = 1 ( accept ). here, then, the applicant is rejected because f ( x = ( $ 10k, $ 50k, $ 45k ) ) = reject. roughly speaking, a counterfactual explanation for this decision ( reject ) would", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "$ 10k, $ 50k, $ 45k ) ) = reject. roughly speaking, a counterfactual explanation for this decision ( reject ) would describe changes, call them x \u2019, to the applicant \u2019 s initial feature vector x such that f ( x \u2019 ) = accept. there may be many possible counterfactual explanations : for example, lowering outstanding debt from $ 50k to $ 25k to form x \u2019 = ( $ 10k, $ 25k, $ 45k ) ; or, increasing liquid assets from $ 10k to $ 20k to form x \u2019 \u2019 = ( $ 20k, $", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##k, $ 45k ) ; or, increasing liquid assets from $ 10k to $ 20k to form x \u2019 \u2019 = ( $ 20k, $ 50k, $ 45k ). some may be easier or harder to attain for the applicant - - and some may be completely impossible to achieve - - motivating research into the creation of \u201c the best \u201d counterfactual explanations for a particular use case, which we discuss in greater depth below. fig1 : illustrative diagram counterfactual explanations. the datapoint labeled x ( blue ) got classified in the negative class. cf1 ( red ) and", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": illustrative diagram counterfactual explanations. the datapoint labeled x ( blue ) got classified in the negative class. cf1 ( red ) and cf2 ( green ) are two counterfactuals for x, which the model classifies in the positive class. several counterfactuals can be generated for a datapoint, which differ in closeness to the original datapoint and other desirable properties. themes of research in cfesmuch of the literature in counterfactual explanations have proposed algorithms to address additional aspects of the problem. we categorize recent research into the following major themes : actionability :", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in counterfactual explanations have proposed algorithms to address additional aspects of the problem. we categorize recent research into the following major themes : actionability : a cfe is only useful if it prescribes changes to features that can actually change. it would be unhelpful if i were told to change my birthplace in order to receive a loan. sparsity : a useful cfe should modify only a few features in order to be simple and easy to use. proximity : a useful cfe should be the smallest possible change that achieves the desired outcome. causality : a useful cfe must be able to adhere", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". proximity : a useful cfe should be the smallest possible change that achieves the desired outcome. causality : a useful cfe must be able to adhere to any causal constraints that a domain expert specifies. for example, i should not have to decrease my age in order to get a loan. data manifold : a useful cfe should result in a datapoint that is similar to other datapoints seen in the training data. it would be less trustworthy if the resulting datapoint is utterly unlike anything the classifier has ever seen. speed : cfes should be generated quickly for new, incoming datapoints. model access : some cf", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##point is utterly unlike anything the classifier has ever seen. speed : cfes should be generated quickly for new, incoming datapoints. model access : some cfe approaches require detailed knowledge of model internals and gradients. others can work in a black - box fashion and are model - agnostic. in our survey paper, we collect, review, and categorize 39 recent papers that propose algorithms to solve the counterfactual explanation problem. we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently - proposed algorithms against that rubric. this provides easy comparison and comprehension of the", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently - proposed algorithms against that rubric. this provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. we also identify gaps and discuss promising research directions in the space of counterfactual explainability. conclusionscfes present a compelling form of xai, providing users with understandable and actionable feedback. the additional constraints and desiderata explored in recent years seek to ensure that these explanations are always reasonable and useful. many exciting open questions remain, and we close our paper", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "constraints and desiderata explored in recent years seek to ensure that these explanations are always reasonable and useful. many exciting open questions remain, and we close our paper by proposing research challenges for the community to tackle in the coming years. we firmly believe that cfes will form a long - lasting part of the ml explainability toolkit. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcare", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ctshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / counterfactual - explainability - neurips", "metadata": {"source": "https://www.arthur.ai/blog/counterfactual-explainability-neurips", "row": 228, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 229 text : recommendation engines need fairness too! solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpc", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessrecommendation engines need fairness too! by : keegan hinesjuly 16, 2020as we turn to digital sources for news and entertainment, recommendation engines are increasingly influencing the daily experience of life, especially in a world where folks are encouraged to stay indoors. these systems are not just", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "news and entertainment, recommendation engines are increasingly influencing the daily experience of life, especially in a world where folks are encouraged to stay indoors. these systems are not just responsible for suggesting what we read or watch for fun, but also for doling out news and political content, and for surfacing potential connections with other people online. when we talk about bias in ai systems, we often read about unintentional discrimination in ways that apply only to simple binary classifiers ( e. g. in the question \u201c should we let this prisoner out on parole? \u201d, there are only two potential predictions yes, or no ). thinking about mit", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "g. in the question \u201c should we let this prisoner out on parole? \u201d, there are only two potential predictions yes, or no ). thinking about mitigating bias in recommendation engines is much more complex. in this post, we \u2019 ll briefly describe how these systems work, then surface some examples of how they can go wrong, before offering suggestions on how to detect bias and improve your users \u2019 experience online, in a fair and thoughtful way. part 1 the anatomy of a recommender systemif you \u2019 re coming to this article as someone who regularly builds or works on recommender systems, feel free to skip this part. for", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a recommender systemif you \u2019 re coming to this article as someone who regularly builds or works on recommender systems, feel free to skip this part. for those of you needing a refresher or primer on the topic, read on! recommender engines help companies predict what they think you \u2019 ll like to see. for netflix, youtube and other content providers, this might happen in the format of choosing which video cues next in auto - play. for a retailer like amazon, it could be picking which items to suggest in a promotional email. at their core, recommender systems take as input two \u201c sides \u201d of a problem", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "amazon, it could be picking which items to suggest in a promotional email. at their core, recommender systems take as input two \u201c sides \u201d of a problem - - users and items. in the case of netflix, each user is an account, and each item is a movie. for amazon, users are shoppers, and items are things you can buy. for youtube, users are viewers, items are videos, and a third component are the users that create the content. you can imagine analogues with newspapers and other media sources such as the new york times and the wall street journal, music streaming services such as spotify and pandora", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you can imagine analogues with newspapers and other media sources such as the new york times and the wall street journal, music streaming services such as spotify and pandora, as well as social networking services such as twitter and facebook. users rate some items, but not all of them. for example, even if you binge watch shows on netflix, it \u2019 s unlikely that you have rated even a small fraction of netflix \u2019 s vast content catalogue, much less so when it comes to youtube \u2019 s library, where over 300 hours of content are uploaded every minute. a recommender system \u2019 s goal is, given a user, find the items", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "youtube \u2019 s library, where over 300 hours of content are uploaded every minute. a recommender system \u2019 s goal is, given a user, find the items or items that will be of greatest interest to that user, under the assumption that most items have not been rated by most users. how is this done? by learning from other, similar items, similar users, and combinations of the two. recommender systems recommend content based on inductive biases. one common inductive bias is that users who seem similar in the past will continue to seem similar in the present and future. in the context of recommender systems, this", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##uctive bias is that users who seem similar in the past will continue to seem similar in the present and future. in the context of recommender systems, this means that users who have, for example, rated videos similarly on youtube in the past will probably rate videos similarly moving forward. recommendations based on this intuition might try to find similar users to a particular user, and similar pieces of content to a particular piece of content, and then combine learnings from those two neighborhoods into an individual score for that particular pairing of a user and item. by doing this for every user - content pair, the recommender system can \u201c fill in all the", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "individual score for that particular pairing of a user and item. by doing this for every user - content pair, the recommender system can \u201c fill in all the blanks \u201d, that is, predict a rating for each combination of user and piece of content. after that, it is simply a matter of picking the most highly - rated pieces of content for that customer, and serving those up as you might see in a sidebar on youtube or a \u201c view next \u201d carousel on amazon shopping. part 2 what could go wrong? as we \u2019 ve discussed above, recommender engines attempt to \u201c fill in the blanks \u201d for a particular", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "amazon shopping. part 2 what could go wrong? as we \u2019 ve discussed above, recommender engines attempt to \u201c fill in the blanks \u201d for a particular user by guessing at their level of interest in other topics when we only know how they feel about things they \u2019 ve already seen or read. most recommender engines are a blend of \u201c nearest neighbor \u201d calculations and active rating elicitation, using a combination of supervised and unsupervised learning alongside deterministic rules that modify the selection process among the content that you could potentially recommend. to discuss some of the issues that often arise in recommender engine bias, we \u2019", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "rules that modify the selection process among the content that you could potentially recommend. to discuss some of the issues that often arise in recommender engine bias, we \u2019 ll look at a couple of examples from industry that illustrate the nuance and complexity involved. one of the more common issues we see in industry can be illustrated by youtube \u2019 s spectacularly named \u201c gangnam style problem \u201d. the problem is this no matter what content you recommend to your user, when one looks at the potential pathways they could take from one recommendation to the next, they all lead back to whatever happens to be the most popular video that day. while this may be", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "potential pathways they could take from one recommendation to the next, they all lead back to whatever happens to be the most popular video that day. while this may be good news for psy and k - pop stans worldwide, gaining traction within a recommender engine can make or break the experience for someone creating content on these platforms, where they need their content to be seen in order to survive. more so every day, we hear about complaints from within the youtube creator community, claiming that their channels suffer from this disparity, and that youtube is biased against emerging artists. thinking this through from a business perspective, it \u2019 s easy to", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "that their channels suffer from this disparity, and that youtube is biased against emerging artists. thinking this through from a business perspective, it \u2019 s easy to see why this might be the case youtube wants to keep users on the page, and they \u2019 re more likely to do that if they can show you content that they know you \u2019 ll actually like. in fact, the less youtube knows about how users will interact with your particular brand of content, the more risky it becomes to promote it. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribepro", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to promote it. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / recommender - systems - need - fairness - too", "metadata": {"source": "https://www.arthur.ai/blog/recommender-systems-need-fairness-too", "row": 229, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 230 text : arthurai fintech innovation lab : class of 2020 recap solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmo", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##products the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedeventsarthurai fintech innovation lab : class of 2020 recapby : victoria vassilevajuly 1, 2020the labevery year since 2010, the fintech innovation lab - a joint program between the partnership fund for new york city and accenture - chooses", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", 2020the labevery year since 2010, the fintech innovation lab - a joint program between the partnership fund for new york city and accenture - chooses just 10 emerging enterprise tech startups developing innovative solutions for financial institutions. this year, over 250 early - and growth - stage companies applied. over 40 participating financial institutions and vc firms across the globe choose each class. needless to say, we \u2019 re proud to have been selected alongside the rest of our 2020 class. the experiencethe lab provides access to top executives at the world \u2019 s leading financial institutions who, throughout our experience, routinely extended exceptionally generous and open feedback. the", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the experiencethe lab provides access to top executives at the world \u2019 s leading financial institutions who, throughout our experience, routinely extended exceptionally generous and open feedback. the added exposure to procurement and security leadership for insight into their requirements and timelines is a priceless check on our enterprise - readiness. passing these checks is crucial to success and reflects how thoroughly the lab works to prepare us. arthur was born out of the pain points that our founders personally experienced within the financial industry. the opportunity to quickly and repeatedly validate our product and market fit directly with key decision makers and core users alike was invaluable. this is table stakes for producing", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to quickly and repeatedly validate our product and market fit directly with key decision makers and core users alike was invaluable. this is table stakes for producing a best - of - breed, turnkey observability platform. model performance, explainability, and fairness are board - level, company - wide concerns. we built arthur to serve as a single pane of glass for the entire organization - serving data scientists and engineers as well as operations, risk teams, and business stakeholders. this made the lab the perfect opportunity to refine our messaging across dozens of high - leverage use cases. the challenge - 2020 and beyond2020", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "business stakeholders. this made the lab the perfect opportunity to refine our messaging across dozens of high - leverage use cases. the challenge - 2020 and beyond2020 has presented no shortage of tough challenges. for this reason we feel particularly privileged for the chance to join forces with leading financial institutions on the joint mission of not only triaging the immediate and ongoing impact of covid - 19, but also investing in proactive production ai monitoring fit for the advanced technologies driving their customers \u2019 financial health. the profound changes to our society immediately showed up in data and financial models. the impact proved to flow both ways. model performance, explainability, and", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "health. the profound changes to our society immediately showed up in data and financial models. the impact proved to flow both ways. model performance, explainability, and bias required a renewed urgency, with the pandemic underscoring how they are connected. we have been inspired by the cultures of care and innovation we witnessed in so many of our partners to address these needs. the regulators and governance & compliance leaders were no different. at arthur some of our relationships with regulators go back decades and we remain active in the policy space. while we regularly participate as advisors in this arena, we were nonetheless very motivated by the enthusiasm from finra,", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "decades and we remain active in the policy space. while we regularly participate as advisors in this arena, we were nonetheless very motivated by the enthusiasm from finra, the nys dfs, the occ, and the cftc for open dialogue as partners toward industry model fairness. we are excited by their interest to collaborate on solutions. covid - 19 will certainly carry a lasting impact - it has been promising to engage with our peers on the financial and customer challenges at hand not only today, but ensuring they are also prepared for tomorrow. going virtualas a result of the ongoing pandemic, the lab, like many, pivot", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "only today, but ensuring they are also prepared for tomorrow. going virtualas a result of the ongoing pandemic, the lab, like many, pivoted to a first - time - ever, entirely virtual experience - we \u2019 re happy to report that this came with its own advantages. meeting with senior executives is typically a fairly formal event in a corporate office, but now we were entering each other \u2019 s homes - admiring background art, scoping home libraries, and grinning as a few kids made surprise appearances. it was a wonderfully leveling experience that enabled genuine relationships when it mattered most. we \u2019 ll see you over drinks", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "as a few kids made surprise appearances. it was a wonderfully leveling experience that enabled genuine relationships when it mattered most. we \u2019 ll see you over drinks in 2021. the figureswe mutually paired with 6 premier financial institutions40 + meetings with executives and counting7 meetings with regulatory bodies such as finra, nys dfs, the occ, and the cftcdozens of meetings with fintech vcs, insurance leaders, entrepreneurs, and media and marketing advisorswhat \u2019 s nextthough the program has come to a formal close, the opportunities and support have not. having tailored and proven our enterprise - grade,", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##what \u2019 s nextthough the program has come to a formal close, the opportunities and support have not. having tailored and proven our enterprise - grade, production model observability platform and its value with some of the most advanced financial technology providers, we are excited about the discussions of pilot programs and upcoming members of our select customer advisory board. stay tuned for more on that! we know we have made lasting and ongoing partnerships with the institutions and organizers alike - for that alone we \u2019 d advise anyone to pursue a spot in the lab. special thanks to maria gotsch, sunny parikh, and jeff kalski for their", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "alone we \u2019 d advise anyone to pursue a spot in the lab. special thanks to maria gotsch, sunny parikh, and jeff kalski for their ever - present positivity and support! your turnget in touch to see for yourself. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / arthurai - fintech - innovation - lab - class - of - 2020 - recap", "metadata": {"source": "https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap", "row": 230, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 231 text : how to build a production - ready model monitoring system for your enterprise solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for business", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##bilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringhow to build a production - ready model monitoring system for your enterpriseby : keegan hinesjune 15, 2020whether your team has recently deployed their first or their 100th ml model to production, you likely appreciate the importance of proactively monitoring your", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "15, 2020whether your team has recently deployed their first or their 100th ml model to production, you likely appreciate the importance of proactively monitoring your models \u2019 inputs and outputs. in an ever - changing world, the model you trained in the lab is almost certainly performing differently out in the wild. in order to ensure that all stakeholders can trust your ml systems, you need to ensure that your models continue to perform well, that they can be understood, and that they don \u2019 t discriminate unfairly. these ideas form the major pillars for ai observability : performance, explainability, and fairness. in this", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "don \u2019 t discriminate unfairly. these ideas form the major pillars for ai observability : performance, explainability, and fairness. in this post, we \u2019 ll walk through the components of building an enterprise - wide system for model monitoring, so that you can provide an always - on visibility to stakeholders across your company, including data scientists as well as risk officers, legal advisers, and business stakeholders. this guide will help you understand what you \u2019 d need to build for your enterprise, or, you can reach out to us at arthurai to learn more about our turnkey system for enterprise model monitoring. platform agnostic", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "build for your enterprise, or, you can reach out to us at arthurai to learn more about our turnkey system for enterprise model monitoring. platform agnostic integrationat a large organization, you might have many different data science teams, all of whom use their favorite tools for model development and deployment. perhaps some corners of your org are big fans of tensorflow, others love sagemaker, others love h20, and yet some others have in - house tooling built with open source stacks. we see this pattern all the time and we think it \u2019 s great : let data scientists use the tools that they love so they can be as", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "source stacks. we see this pattern all the time and we think it \u2019 s great : let data scientists use the tools that they love so they can be as effective as possible. when it comes to model development and deployment, this heterogeneity can be a sign of a diverse and creative data science community. however, from the perspective of technology risk management, all these disparate systems might make you ( and your legal team ) a little uneasy. to answer a simple question like \u201c are all the models doing ok today? \u201d, you \u2019 d have to go check many different systems and hope they all offer comparable information.", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "simple question like \u201c are all the models doing ok today? \u201d, you \u2019 d have to go check many different systems and hope they all offer comparable information. what you \u2019 d really want is a single go - to place for seeing how all your models are performing, and a way to be notified as soon as something goes wrong. this is why it is important that across your ml ecosystem you need to create a centralized and standardized layer for model monitoring and governance. with this goal in mind, your monitoring system needs to be platform agnostic and allow drop - in integration across all those different stacks. fortunately, much of model monitoring can be", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in mind, your monitoring system needs to be platform agnostic and allow drop - in integration across all those different stacks. fortunately, much of model monitoring can be achieved without tight coupling to the specific environments for model development and deployment. as we \u2019 ll describe below, monitoring the stability of a model \u2019 s outputs is a process that can be completely ignorant of the model \u2019 s development process or deployment environment. the same is true for monitoring a model \u2019 s inputs and for ensuring a model is fair. and with many methods for model explainability, the specifics of the model training procedure are irrelevant once we have a trained model to probe.", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "model is fair. and with many methods for model explainability, the specifics of the model training procedure are irrelevant once we have a trained model to probe. therefore, it is possible to build out a platform - agnostic monitoring system that is accessed through rest apis so that all the models in your ecosystem are consistently monitored in real - time, no matter how they were built or deployed. even better, it \u2019 ll be important to provide data scientists and engineers with easy - to - use client libraries in their favorite languages ( mostly python, but possibly also java, scala, or r ). performance and data monitoringmodel outputsthe", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to - use client libraries in their favorite languages ( mostly python, but possibly also java, scala, or r ). performance and data monitoringmodel outputsthe most salient question you want to answer is this : is my model performing as well today as i expect it to? notions of \u201c good \u201d performance will vary depending on different business cases, but there are many general themes you can bring to bear. in the ideal setting, your models will have access to a timely ground - truth so that you can effortlessly compute things like accuracy, confusion matrices, roc curves, and so on. monitoring a model \u2019 s accuracy can make you aware", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- truth so that you can effortlessly compute things like accuracy, confusion matrices, roc curves, and so on. monitoring a model \u2019 s accuracy can make you aware of a performance issue as soon as it happens. in fact, it would be great if you could make any such metrics extensible, because surely someday you \u2019 ll find a data scientist who wants something slightly different than the metrics you are calculating in your system. allowing users to onboard custom metrics calculations will ensure that no data scientists are left out. in a use case such as product recommendation or targeted advertising, you \u2019 ll know instantly whether your model \u2019 s outputs", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ensure that no data scientists are left out. in a use case such as product recommendation or targeted advertising, you \u2019 ll know instantly whether your model \u2019 s outputs resulted in a click or not. however, there are many cases where ground - truth is hard to come by, and what can we do then? an example is issuing credit cards - if you approve someone for a credit card, it will likely be months or years before you decide that was a bad decision. in these cases, you \u2019 ll need to generate proxy metrics for accuracy by monitoring the stability of a model \u2019 s inputs and outputs. the stability of a model \u2019", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "cases, you \u2019 ll need to generate proxy metrics for accuracy by monitoring the stability of a model \u2019 s inputs and outputs. the stability of a model \u2019 s output predictions is a useful proxy in the absence of ground truth. since a model, once fitted and deployed, never changes it \u2019 s view of the world ( its decision surface ), any notable changes to its output can be attributed to significant changes in its inputs - something your data scientists will want to know about. you should consider monitoring the distribution of output predictions from a model, whether a regression model, classification model, multilabel model, multimodal model, or", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you should consider monitoring the distribution of output predictions from a model, whether a regression model, classification model, multilabel model, multimodal model, or anything else. there are many ways to quantify similarity / stability of distributions, depending on the type of model and dimensionality ( more on that below ). when it comes to monitoring the stability of distributions, you might quantify changes through time, or changes relative to baseline ( such as the training set ), or both. data driftjust as important as monitoring a model \u2019 s outputs, it is vital to monitor the stability of a model \u2019 s inputs : often referred to", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "both. data driftjust as important as monitoring a model \u2019 s outputs, it is vital to monitor the stability of a model \u2019 s inputs : often referred to as data drift or concept drift. your model is trained on a fixed snapshot of the world - the training data. once your model is deployed, the world will inevitably change and depending on how severe that change is, the learned decision boundary of your model might become irrelevant. by regularly quantifying the similarity of the data coming through the model today to the data the model was trained on, you \u2019 ll be able to quickly identify when things start to go off the rails.", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "data coming through the model today to the data the model was trained on, you \u2019 ll be able to quickly identify when things start to go off the rails. there are many ways to quantify distributional similarity and to build a system for data drift detection. your first approach might be to monitor each of the incoming features to a model and quantify similar to the training set. for example, if a model has age as an input feature, then you \u2019 d want to look at the training set and develop a statistical profile of the data in that training set. then going forward, you can calculate the similarity of the data coming through,", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "at the training set and develop a statistical profile of the data in that training set. then going forward, you can calculate the similarity of the data coming through, as compared to the training data. measuring the similarity of two distributions can be achieved in many ways including non - parametric hypothesis tests, kl divergence, jensen - shannon divergence, population stability index, wasserstein distance, and many more. what these have in common is that they will take in two empirical distributions as input and will result in a similarity score as output. when that similarity score starts to have big changes, it is an indication that the data today is", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "distributions as input and will result in a similarity score as output. when that similarity score starts to have big changes, it is an indication that the data today is quite different than the training set, and maybe it \u2019 s time for a retrain and redeploy. computing distributional similarity for each input feature independently will generally tackle a large proportion of the problem, but is not without limitations. an early thing to consider is that we \u2019 re implicitly looking for drift only in the marginal distributions of each feature and not in the higher - dimensional joint distribution over the data. for high dimensional datasets, and especially for unst", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in the marginal distributions of each feature and not in the higher - dimensional joint distribution over the data. for high dimensional datasets, and especially for unstructured data such as imagery and text, it will be important to consider multivariate methods for quantifying distributional similarity. while there is generally not an easily - computed high - dimensional analog for kl divergence, we can take a model - based approach to quantify data drift. in this approach, we will train a model of some kind on the training set. this model isn \u2019 t a classification or regression model, but instead is some kind of density model (", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "will train a model of some kind on the training set. this model isn \u2019 t a classification or regression model, but instead is some kind of density model ( or similar ) which is trying to understand how the data is distributed in the high dimensional space. once we have fitted such a model to the training set, we have a snapshot of what \u201c normal \u201d data looks like. going forward, we can collect new inferences and query this model to quantify how well the new inferences adhere to the distribution of the training dataset. for each single inference, this gives us a powerful mechanism for anomaly detection, since we can now", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "new inferences adhere to the distribution of the training dataset. for each single inference, this gives us a powerful mechanism for anomaly detection, since we can now identify incoming points that the model has scored but yet don \u2019 t really look anything like the training data. further, as we aggregate over larger groups of inferences, we have an holistic view into multivariate data drift in a high dimensional space. that general framework can be accomplished with many different modeling techniques. one approach might be using an isolation forest or kdtree to fit to your dataset. more recent techniques show promise for fitting properly - normalized probability models to", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "approach might be using an isolation forest or kdtree to fit to your dataset. more recent techniques show promise for fitting properly - normalized probability models to high dimensional datasets, including variational autoencoders and normalizing flows. additionally, preprocessing methods such as dimensionality reduction have been shown to be helpful for high - scale problems. in all cases, you \u2019 d need to also build a system for training, storing, containerizing, and serving each one of these density models so that streaming inferences can be scored as they come in. finally, it is worth noting that not all data drift is", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "serving each one of these density models so that streaming inferences can be scored as they come in. finally, it is worth noting that not all data drift is created equal. this idea is sometimes referred to as \u2018 virtual \u2019 concept drift and denotes instances where the data has drifted but in a direction that doesn \u2019 t materially affect the model \u2019 s outputs. thus, it is very helpful to combine data drift monitoring for each feature with a simultaneous quantification of how important each feature is for model decisioning. combining data drift modeling with model explainability ( more on that below ) is a powerful way to prioritize your teams \u2019", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "feature is for model decisioning. combining data drift modeling with model explainability ( more on that below ) is a powerful way to prioritize your teams \u2019 time and attention. explainability as a serviceunderstanding a model \u2019 s decisions is an important part of building trust for the adoption of ml across your organization. with increasingly - complex ml techniques, their flexibility is often accompanied by a difficulty in understanding why they make their predictions. the field of explainable ai has put forth many valuable techniques for calculating explanations of model predictions. in your model - monitoring system, if you are able to provide these explanations for every prediction your models make,", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "many valuable techniques for calculating explanations of model predictions. in your model - monitoring system, if you are able to provide these explanations for every prediction your models make, this can go a long way toward building trust and comfort among a broad class of stakeholders. there are many great techniques for calculating local explanations of ml models. these methods could be model agnostic ( like lime ), model - based ( like deeplift ), or both ( like shap ). in all cases, you need to access a model \u2019 s predict function in order to probe the relationships between a model \u2019 s inputs and outputs. for your monitoring platform, this means", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you need to access a model \u2019 s predict function in order to probe the relationships between a model \u2019 s inputs and outputs. for your monitoring platform, this means you don \u2019 t need to be tightly coupled to the model training environment. instead, you only need access to the final trained model and ability to probe it. you might hook into existing model deployment apis, or you might build and replicate a model microservice solely for computing explanations on the fly. in this case, you \u2019 ll need to think about containerization for replicating the model \u2019 s dependencies and environment. and if your model requires high throughput or", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "case, you \u2019 ll need to think about containerization for replicating the model \u2019 s dependencies and environment. and if your model requires high throughput or high dimensional data ( or both ) you \u2019 ll want to think about ways for autoscaling the computation of explanations in order to keep up with the inference load. additionally, you might put some thought into refactoring some of those favorite explainability methods to make them more performant for your use cases. once you \u2019 ve computed explanations for every inference coming through your models, it opens many exciting possibilities for helping your organization. first, it provides your data scientists with a", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you \u2019 ve computed explanations for every inference coming through your models, it opens many exciting possibilities for helping your organization. first, it provides your data scientists with a granular view in a model \u2019 s decision surface, allowing them to identify areas where a model might be underperforming and helping to debug models in production. second, these explanations form a useful audit trail for your modeling system, ensuring that every decision that a model makes is logged and can be understood at a later time. and finally, considering local and global feature importance will help you understand and prioritize data drift and the emergence of new patterns in your data", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a later time. and finally, considering local and global feature importance will help you understand and prioritize data drift and the emergence of new patterns in your data. algorithmic fairnessit is important to not only ensure your models are making the \u201c right \u201d decision from a statistical standpoint, but also the \u201c right \u201d decisions from an ethical standpoint. it has become clear across many industries that systems for automated decision making can exacerbate disparate impact and discrimination against specific groups of people. ensuring that your models are fair is tantamount to ensuring that they result in similar predictions / outputs for all relevant subpopulations", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "specific groups of people. ensuring that your models are fair is tantamount to ensuring that they result in similar predictions / outputs for all relevant subpopulations of your customer base. traditionally, fairness analysis is conducted over protected classes as defined by race, age, and sex. but for your business, there could be many other dimensions and factors over which you want to ensure your model is resulting in comparable outcomes. for example, you might want to understand model disparities by geography, income, spending level, or any business - driven segmentation of your customer base. many researchers have attempted to quantify notions of algorithmic fairness,", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "geography, income, spending level, or any business - driven segmentation of your customer base. many researchers have attempted to quantify notions of algorithmic fairness, with many such definitions proposed in recent years. for your business, you should decide which definitions and fairness metrics are most aligned with your company \u2019 s goals for responsible ai. the next step is to build a fairness audit framework into all modeling done in your organization. in this pursuit, you would examine all models \u2019 inputs and outputs per any sensitive / protected classes and arrive at a quantification of disparate impact. naturally, this kind of fairness audit would not be a", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and outputs per any sensitive / protected classes and arrive at a quantification of disparate impact. naturally, this kind of fairness audit would not be a static process but would be an ongoing analysis that is continually conducted as new inferences go through your models. ideally, you could provide an easy - to - use dashboard for identifying, exploring, and mitigating disparate impact in your models. this dashboard would make these fairness metrics accessible to all important stakeholders in your organization, and ensure that this information is not just residing with data scientists. apis, storage, and computewith inferences stream into your monitoring platform,", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "your organization, and ensure that this information is not just residing with data scientists. apis, storage, and computewith inferences stream into your monitoring platform, and real - time metrics being computed for drift, accuracy, and more, you \u2019 ll need to put some careful thought into data storage, access, and streaming analytics. some data science uses cases operate in large batches while some might operate in a streaming fashion, so a combination architecture with kafka and spark might prove useful. many of the metrics and analytics we \u2019 ve described can be computed in a streaming context and autoscaled to meet load requirements. once", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "might prove useful. many of the metrics and analytics we \u2019 ve described can be computed in a streaming context and autoscaled to meet load requirements. once these metrics, analytics, and explanations have been computed over the inferences, it would nice to make all of this data available to data science teams to explore on their own. you might consider storing the metrics in a datastore appropriate for fast access and large scale, so that your data scientists can quickly slice and dice this data. you could even connect this backend store to an interactive data visualization dashboard, allowing your teams to explore a model \u2019 s decision space", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and dice this data. you could even connect this backend store to an interactive data visualization dashboard, allowing your teams to explore a model \u2019 s decision space and better understand areas to improve and debug. real - time alertingonce you \u2019 re calculating all the previously described metrics ( and housing them in a scalable data store ), you \u2019 ve got everything you need for real - time alerting. you can let data scientists know the moment a model \u2019 s accuracy starts to drop too much or an important feature seems to have drifted significantly away from the training set. you can build alert integrations directly to where", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 s accuracy starts to drop too much or an important feature seems to have drifted significantly away from the training set. you can build alert integrations directly to where people are spending their time, including email, slack, or servicenow. apologies in advance when you get a 2am wakeup call about your model \u2019 s performance plummeting. user interfacethe great thing about instrumenting a platform - agnostic model monitoring tool, is that it suddenly enables disparate stakeholders to have effortless access to a model \u2019 s outputs and behaviors. concepts that might have typically been stuck inside a data scientist \u2019 s notebooks ( things like model", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "have effortless access to a model \u2019 s outputs and behaviors. concepts that might have typically been stuck inside a data scientist \u2019 s notebooks ( things like model explanations ) are now readily available for a broader audience to consume. you now have a few different personas to think about when designing interfaces. the first might be data science practitioners who are hands - on with model development and will want a very specific and technical view into the data surrounding each model. the second might be data science leaders and risk management leaders, who primarily will be concerned with ensuring that everything is healthy and nothing needs escalation. finally, you \u2019 ll want to", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "science leaders and risk management leaders, who primarily will be concerned with ensuring that everything is healthy and nothing needs escalation. finally, you \u2019 ll want to think about less - technical business stakeholders who are using these models to accomplish their business goals ( and accepting these operational risks ). you \u2019 ll want to make sure they have easy - to - understand access to major insights around model performance and risk. the other important stufflet \u2019 s not forget about things like role - based access control ( rbac ), single sign on ( sso ) integration with enterprise user directories, end - to - end encryption and other policy compliance.", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "access control ( rbac ), single sign on ( sso ) integration with enterprise user directories, end - to - end encryption and other policy compliance. you \u2019 ve got to get this stuff right the first try, so be sure to move very carefully and thoughtfully through these topics. putting it all togetherwith these pieces in place, your data science teams should be able to effortlessly onboard new ( and old ) models into your monitoring system and begin sending inferences and telemetry. your monitoring system will aggregate this information and compute real - time metrics for stability, performance, fairness, and anything else important to your organization. alert", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##try. your monitoring system will aggregate this information and compute real - time metrics for stability, performance, fairness, and anything else important to your organization. alerting will provide real - time awareness so that data scientists can begin solving problems before it \u2019 s too late. the dashboarding you \u2019 ve built will provide access to these concepts to a whole new suite of stakeholders across your business - not just for data scientists anymore. good luck! monitor your models with arthurof course, you can build! or you can trust arthur. arthur is a proactive machine learning model monitoring platform that gives you the confidence that your ai deployments are performing", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you can build! or you can trust arthur. arthur is a proactive machine learning model monitoring platform that gives you the confidence that your ai deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. with performance monitoring, bias detection, and customizable alerts, arthur makes sure that you never miss an issue, and arthur \u2019 s explainability engine makes runtime debugging effortless. if you \u2019 re deploying any ( i. e., tabular, nlp, computer vision, etc. ) models into production and are looking for a", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "you \u2019 re deploying any ( i. e., tabular, nlp, computer vision, etc. ) models into production and are looking for a solution for monitoring those models over time, we \u2019 d love to connect and show you how arthur can help. request a demo today. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 202", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##lmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / how - to - build - a - production - ready - model - monitoring - system - for - your - enterprise", "metadata": {"source": "https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise", "row": 231, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 232 text : ai during black swan events solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcv", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoringai during black swan eventsby : liz o'sullivanjune 8, 2020the world has been turned upside down by covid - 19 and civil demonstrations during the last few months. the effects on our society, the overall economy, and just about every industry are unprecedented. april and may look absolutely", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and civil demonstrations during the last few months. the effects on our society, the overall economy, and just about every industry are unprecedented. april and may look absolutely nothing like january or february, and with the situation still unfolding, june and july will be completely different as well. one of the less obvious impacts of this period of rapid change is to the behavior of ai models that play critical roles in our society. at arthur, we \u2019 ve seen models across many industries affected. trading algorithms are one clear example, as recently covered in the wall street journal. \u201c hedge funds that use artificial intelligence models to suggest trades and stock picks declined when", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "trading algorithms are one clear example, as recently covered in the wall street journal. \u201c hedge funds that use artificial intelligence models to suggest trades and stock picks declined when stock markets unraveled in late february... market experts warn the unpredictable nature of the economy these days could trip up some algorithms that continue to rely on data gathered during better times. \u201d - wsj, ai funds decline \u2014 then recover \u2014 during market turmoil, may 1, 2020let \u2019 s examine why these models are so vulnerable. the shortcomings of machine learningmachine learning ( ml ) is the most prevalent form of ai today, to the point that", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "these models are so vulnerable. the shortcomings of machine learningmachine learning ( ml ) is the most prevalent form of ai today, to the point that the two terms are often used synonymously. ml works by learning patterns in existing data ( aka the training set ) and then using those learned patterns to reason about future data. it performs quite well when that future data is similar to the training set. as that similarity disappears, the lessons the ml model has learned become less relevant and the model becomes less able to \u201c reason \u201d correctly about the new data it is seeing. in a recent mit technology review interview, gary marcus, a", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "relevant and the model becomes less able to \u201c reason \u201d correctly about the new data it is seeing. in a recent mit technology review interview, gary marcus, a vocal critic of deep learning - based approaches to ai said \u201c i think that we \u2019 re living in a weird moment in history where we are giving a lot of trust to software that doesn \u2019 t deserve that trust. \u201d so what are these shortcomings and how do they relate to our current situation? consider the ancient game of go. we all watched as alphago proved that algorithms had finally surpassed humans on this extraordinarily complex challenge. but what if we were to", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "game of go. we all watched as alphago proved that algorithms had finally surpassed humans on this extraordinarily complex challenge. but what if we were to replay those games, adding in a little twist. in this new game, every few moves the rules get altered slightly - - additional rows of dots added, the ability to place more than one stone per turn, etc. this time around, the human would best the algorithm because it would never have seen any data like this in the past. the human, while faced with a more difficult task than a routine game of go, would still be able to play relatively effectively. simply stated", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in the past. the human, while faced with a more difficult task than a routine game of go, would still be able to play relatively effectively. simply stated, current approaches to ai do not adapt well to changing situations. ai in the age of covid - 19right now, the automated systems affecting our finances, healthcare, travel, purchasing intents, and many other aspects of our life are making decisions based on patterns they learned prior to covid - 19. so are models that make business decisions in capital markets, high frequency trading, supply chain management, and manufacturing. but those patterns are much less applicable now ; the world has", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "models that make business decisions in capital markets, high frequency trading, supply chain management, and manufacturing. but those patterns are much less applicable now ; the world has shifted. these shifts and models \u2019 inability to adapt will result in companies losing money at a time when they are already being strained. meanwhile, consumers and end users who interact with these systems will be on the receiving end of less reliable decisions. is there a solution? there are avenues of research that seek to solve this problem. reinforcement and online learning focus on models that continue to learn as new data flows in, so they aren \u2019 t limited to patterns learned from a \u201c point in", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". reinforcement and online learning focus on models that continue to learn as new data flows in, so they aren \u2019 t limited to patterns learned from a \u201c point in time \u201d snapshot training set. but these approaches are often hard or impossible to apply to real world scenarios. symbolic ai is an alternative to deep learning that seeks to explicitly model truths about the real world and then allow machines to programmatically apply formal logic to these truths to draw conclusions. it has the huge advantage of being able to reason about scenarios it has never seen before. while it has been out of fashion for many years due to reliability concerns, it has had a resurgence", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "being able to reason about scenarios it has never seen before. while it has been out of fashion for many years due to reliability concerns, it has had a resurgence in the last couple of years as researchers grapple with the shortcomings of deep learning. these types of scientific advances are a ways off and will require time to mature to the point they can be utilized in the real world. the immediate, pragmatic solution is to make your ai observable. systematic model monitoring allows you to immediately assess the shifts in the data feeding your models, and understand the degree of impact it is having on model decisioning. with this knowledge", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "monitoring allows you to immediately assess the shifts in the data feeding your models, and understand the degree of impact it is having on model decisioning. with this knowledge you can respond immediately, smartly prioritizing model performance improvements to ensure your business maintains maximum continuity. going forwardpeople who have worked in data science long enough have battle scars from model performance loss. sometimes it \u2019 s quick, such as when you encounter a black swan event. more frequently, it \u2019 s gradual. often the gradual degradation is worse because it flies under the radar for a long time. a lot will change as a result of covid - 19. people", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". often the gradual degradation is worse because it flies under the radar for a long time. a lot will change as a result of covid - 19. people are already questioning social norms that go back centuries - - handshakes, sporting events, working in an office, etc. for those of us in the ml community this is a wake up call to build systems that are resilient. that starts with building ai that is observable. knowing when your ai is struggling to adapt to rapid change and which models are the most impacted is critical for real world systems. and longer term, continued research into more adaptable approaches to", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "struggling to adapt to rapid change and which models are the most impacted is critical for real world systems. and longer term, continued research into more adaptable approaches to ai such as symbolic ai and reinforcement learning will be paramount. automated systems are creating extraordinary value - - making them more observable will ensure that continues even when the world shifts. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteam", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / ai - during - black - swan - events", "metadata": {"source": "https://www.arthur.ai/blog/ai-during-black-swan-events", "row": 232, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 233 text : how explainable ai and bias are interconnected solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels ll", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnesshow explainable ai and bias are interconnectedby : keegan hinesapril 24, 2020in the world of artificial intelligence, explainability has become a contentious topic. one view among machine learning experts is that the less a model can be interpreted, the more accurate it will", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "artificial intelligence, explainability has become a contentious topic. one view among machine learning experts is that the less a model can be interpreted, the more accurate it will be. some fear that adoption of explainable ai will slow down the adoption of machine learning. however, explainability doesn \u2019 t seek to slow down advancements in ai - it seeks to make that advancement fairer and safer for both everyday people and businesses implementing the ai. explainability also goes hand - in - hand with decreasing bias in ai. how important is the interpretability of your taxi driver? yann lecun, vp & chief ai strategist at facebook", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "with decreasing bias in ai. how important is the interpretability of your taxi driver? yann lecun, vp & chief ai strategist at facebook, once asked the question, \u201c how important is the interpretability of your taxi driver? \u201d on the surface, this thought experiment might seem to be an explainability killer. as long as you get from point a to point b safely, who cares how and why the taxi driver got you there? but this is a red herring - - of course you want to understand your taxi driver \u2019 s choices! otherwise, what \u2019 s to stop someone from taking a longer route that makes", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "red herring - - of course you want to understand your taxi driver \u2019 s choices! otherwise, what \u2019 s to stop someone from taking a longer route that makes you late or costs your more money? also, the taxi driver needs to be able to explain their actions to protect themselves against any legal issues. explainable ai aims to address the lack of information around how ai decisions are made. it is important because when an uninterpreted model fails, those in vulnerable groups are most likely to be negatively impacted, as we saw in the gender shades study. in the study, it was revealed that three top ai companies performed better on", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "are most likely to be negatively impacted, as we saw in the gender shades study. in the study, it was revealed that three top ai companies performed better on men than women and especially performed the worst on women with dark skin tones. without explainability, discriminatory decisions go unchecked. there are also growing legal issues surrounding ai that explainability seeks to address. the european union, for example, has expanded guidelines around automated decision making rights under gdpr. meanwhile, in the us, certain decisions like those based on creditworthiness are also subject to right to explanation laws. as regulations catch up with technologies, the need to", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in the us, certain decisions like those based on creditworthiness are also subject to right to explanation laws. as regulations catch up with technologies, the need to be able to explain ai decisions will become increasingly important. plus, when you do get anomalous decisions from a model, that often does reflect something in the world that wasn \u2019 t picked up on during model development. it \u2019 s important that when this happens data scientists know what data or new features are needed to have a properly trained model. a lack of clarity around ai decisions can lead to frustrations from users who aren \u2019 t able to understand why a decision was made and", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a properly trained model. a lack of clarity around ai decisions can lead to frustrations from users who aren \u2019 t able to understand why a decision was made and even legal implications if black box decisions further discrimination of protected groups. this underscores the importance of understanding ai predictions better to develop ai that are more fair and helpful for society. ignorance is not bliss. there \u2019 s a real and very valid concern around the legality of implementing black box models in sensitive areas. if a police officer catches someone based on an algorithm, how do they explain that that wasn \u2019 t based on prior bias? if an algorithm tells a doctor a patient", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a police officer catches someone based on an algorithm, how do they explain that that wasn \u2019 t based on prior bias? if an algorithm tells a doctor a patient is dying, but the ai is wrong, who \u2019 s liable for the incorrect diagnosis - - the doctor using the algorithm? the company that sold the software? or the architects of the original algorithm? for example, look at the now - scrapped hr system amazon built : amazon \u2019 s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10 - year period. most came from men, a reflection of male dominance across the tech industry.", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "by observing patterns in resumes submitted to the company over a 10 - year period. most came from men, a reflection of male dominance across the tech industry. in effect, amazon \u2019 s system taught itself that male candidates were preferable. it penalized resumes that included the word \u201c women \u2019 s, \u201d as in \u201c women \u2019 s chess club captain. \u201d and it downgraded graduates of two all - women \u2019 s colleges, according to people familiar with the matter. they did not specify the names of the schools. ( reuters ) even though amazon edited the programs to make them neutral to these particular terms, there was still", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". they did not specify the names of the schools. ( reuters ) even though amazon edited the programs to make them neutral to these particular terms, there was still no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory. this led to amazon ultimately disbanding the team with recruiters not able to trust the system enough to rely on it solely, leading executives to lose hope for the project. and herein is one of biggest arguments for explainability : it allows for the safe, responsible adoption of ai. explainability could \u2019 ve helped these developers catch this disparity prior to deployment", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "arguments for explainability : it allows for the safe, responsible adoption of ai. explainability could \u2019 ve helped these developers catch this disparity prior to deployment, and consequently, a lot more women would have had opportunities to be screened. explainability will likely continue to be a push and pull in the ai world. however, we \u2019 re already seeing regulations like gdpr push explainable ai forward. being able to interpret ai remains key to addressing the lack of trust around black box decisions, avoiding vulnerabilities in models, and decreasing the amount of human bias in machine learning. previous postsharenext post we make ai better", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "box decisions, avoiding vulnerabilities in models, and decreasing the amount of human bias in machine learning. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / how - explainable - ai - and - bias -", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##terms of serviceprivacy source : https : / / www. arthur. ai / blog / how - explainable - ai - and - bias - are - interconnected", "metadata": {"source": "https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected", "row": 233, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 234 text : 3 reasons model monitoring is vital for strong ai performance solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##cts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedml model monitoring3 reasons model monitoring is vital for strong ai performanceby : keegan hinesapril 16, 2020model monitoring is key for having continually high - performing artificial intelligence models in place. as we know, ai models make predictions based on historical data. however, as the world", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "is key for having continually high - performing artificial intelligence models in place. as we know, ai models make predictions based on historical data. however, as the world changes, that historical data becomes less and less relevant, which can lead to incorrect predictions and business outcomes. it \u2019 s important to understand the ways in which models fail, so you can become aware the moment it happens, and update your models to reflect the new conditions in production. to help with understanding how model monitoring can be put into use, we \u2019 ve outlined 3 of the top ways data issues can cause ai performance loss below. 1. a changing worldwith a few exceptions", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "be put into use, we \u2019 ve outlined 3 of the top ways data issues can cause ai performance loss below. 1. a changing worldwith a few exceptions, machine learning models do not learn on their own. businesses rely on humans and automated systems to deploy updates. and while the world naturally changes and evolves, your model can fall more and more out of tune with reality. this is data drift, and it can happen in a few different ways. first, a models may be inadvertently used in ways that data scientists did not intend. for instance, suppose your business sells skincare products targeted towards a more mature clientele, and", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "may be inadvertently used in ways that data scientists did not intend. for instance, suppose your business sells skincare products targeted towards a more mature clientele, and you use ai to manage your ad spend. your data is likely built up using demographic information about your original age target. however, if you launch a new product targeted towards teens and begin using those models without updating the training data, your advertising will likely underperform. second, the world isn \u2019 t static - - it evolves, constantly. this means past data points aren \u2019 t always good indicators for future predictions. for example, let \u2019 s say you run an", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- it evolves, constantly. this means past data points aren \u2019 t always good indicators for future predictions. for example, let \u2019 s say you run an app with a built - in messaging feature. you may implement a model to predict and suppress spam in order to improve the user experience. however, over time, spammers will likely figure out what your model looks for and change their behavior to subvert your models. 2. extreme novel eventsgood and bad novel moments happen all the time - we \u2019 re living through one right now with covid19. and these events can have a large impact on businesses. as a", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "moments happen all the time - we \u2019 re living through one right now with covid19. and these events can have a large impact on businesses. as a lighter example, there \u2019 s the event of sudden, and unexpected popularity. take, for example, dalgona coffee. thanks to this viral tiktok coffee trend, searches for instant coffee on google grew by more than 700 % in about a week. retailers online and offline often have models that use historical data to predict how much stock they should carry of any given product, but that data doesn \u2019 t take into account sudden trends - meaning lots of customers found themselves frustrated", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to predict how much stock they should carry of any given product, but that data doesn \u2019 t take into account sudden trends - meaning lots of customers found themselves frustrated at the lack of stock available for purchase and tons of missed revenue for the business. model monitoring, here, could have given these retailers advance notice of the incoming surge in purchases. 3. data ecosystem changesusually, you don \u2019 t just have one model feeding into one outcome. instead, models feed into more models to make much more complex predictions and decisions. this means if you change even something seemingly innocuous like going from nulls to 0s in one model,", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to make much more complex predictions and decisions. this means if you change even something seemingly innocuous like going from nulls to 0s in one model, it can have a big impact in the performance of your whole system. in many cases, changes may not be immediately catastrophic. however, these quieter inaccuracies are often what keep data scientists up at night because they \u2019 re much harder to spot and can cause huge monetary losses. here, model monitoring is vital to catch those subtle ( yet powerful ) changes that would be otherwise overlooked. those small changes and inconsistencies are easy to miss, but having a model monitoring", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "catch those subtle ( yet powerful ) changes that would be otherwise overlooked. those small changes and inconsistencies are easy to miss, but having a model monitoring system in place provides protection from data changes leading to model failures and lost revenue. today, ai is vital for growing and scaling businesses. but we must be cognisant that ai is still quite brittle. understanding the ways in which ai models can lose accuracy enables your business to monitor and address those issues before they cause real problems for your team. these guardrails ensure you will deploy a safe and reliable ai system within your business and get the most out of the models you \u2019", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "problems for your team. these guardrails ensure you will deploy a safe and reliable ai system within your business and get the most out of the models you \u2019 ve spent so much effort to deploy into production. have questions about model monitoring? shoot us an email at info @ arthur. aiprevious postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 202", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##lmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / 3 - reasons - model - monitoring - is - vital - for - strong - ai - performance", "metadata": {"source": "https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance", "row": 234, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 235 text : fairness in machine learning is tricky solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpc", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedai bias & fairnessfairness in machine learning is trickyby : john dickersonapril 8, 2020non - experts and experts alike have trouble even understanding popular definitions of fairness in machine learning \u2014 let alone agreeing on which definitions, if any, should be used in practice. human decision - making processes are", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "even understanding popular definitions of fairness in machine learning \u2014 let alone agreeing on which definitions, if any, should be used in practice. human decision - making processes are known to be biased. look at the promotions process at a typical large company. in the words of tomas chamorro - premuzic, the chief talent scientist at one of the world \u2019 s largest staffing firms, \u201c \u2026 most companies focus on the wrong traits, hiring on confidence rather than competence, charisma rather than humility, and narcissistic tendencies rather than integrity. \u201d this observation isn \u2019 t new, and myriad other examples exist. in australia", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##a rather than humility, and narcissistic tendencies rather than integrity. \u201d this observation isn \u2019 t new, and myriad other examples exist. in australia, if the name on your resume \u201c sounds \u201d middle eastern or chinese, you are less likely to be hired ; in the us, replacing a male - sounding name with a female - sounding one on an otherwise identical cv can result in a lower chance of offer, as well as a lower starting salary. when giving a negative medical evaluation, doctors exhibit similar levels of implicit bias based on race and gender as the general public might \u2014 which is to say, quite a bit. and", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "negative medical evaluation, doctors exhibit similar levels of implicit bias based on race and gender as the general public might \u2014 which is to say, quite a bit. and, while open and explicit redlining for banking and insurance in the us is no longer legal, there is evidence that its impacts are still implicitly felt today. can we just take humans out of the loop and rely on cold, hard data? initially, some proponents of automated decisioning techniques stemming from the data mining and machine learning ( ml ) communities pushed for this. but algorithms take data as input, along with whatever biases come along with how those data were \u2014 or weren", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "machine learning ( ml ) communities pushed for this. but algorithms take data as input, along with whatever biases come along with how those data were \u2014 or weren \u2019 t \u2014 sampled. this includes : which features were stored, which humans made those decisions, and disparities in sample sizes of different subgroups of inputs, among others. worse still, often any resultant discrimination is an emergent, i. e. learned, property of the system ( rather than a hand - coded rule written by an algorithm designer ), making identification and any partial mitigation of the issue more difficult. remember our earlier examples of bias in human decision", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "coded rule written by an algorithm designer ), making identification and any partial mitigation of the issue more difficult. remember our earlier examples of bias in human decision - making systems for promotions, hiring, healthcare, and banking? their counterparts have all been found in the automated versions of those systems as well. so, bias in ml - based decisioning processes is ubiquitous, just like bias in human decision - making processes. can we do anything about it? yes, and we should \u2014 but it \u2019 s important to set our goals realistically, and keep both stakeholders and domain experts heavily and continually involved. one approach taken by the machine learning", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2014 but it \u2019 s important to set our goals realistically, and keep both stakeholders and domain experts heavily and continually involved. one approach taken by the machine learning community is explicitly \u201c defining fairness \u201d \u2014 that is, proposing different metrics of fairness as well as approaches to encoding that into machine learning pipelines. some of these definitions are explicitly or implicitly based on existing legal doctrine. for example, channeling title vii of the civil rights act of 1964, an algorithm is said to result in disparate impact if it adversely affects one group of people of a protected characteristic ( aka \u201c sensitive attribute \u201d ) over another. similarly,", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to result in disparate impact if it adversely affects one group of people of a protected characteristic ( aka \u201c sensitive attribute \u201d ) over another. similarly, that algorithm is said to result in disparate treatment if its decisioning is performed in part based on membership in a group. then, one goal that a fairness in machine learning practitioner might have is to mathematically certify that an algorithm does not suffer from disparate treatment or disparate impact, perhaps given some expected use case or input distribution. toward that end, let \u2019 s dive a bit deeper into disparate treatment \u2014 specifically, what does it mean", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "given some expected use case or input distribution. toward that end, let \u2019 s dive a bit deeper into disparate treatment \u2014 specifically, what does it mean to make a decision based on membership in a group? well, the algorithm could formally discriminate, that is, take as input explicit membership in a group, and then use that in some way to determine its output. this is often illegal to do, so many systems already do not do this. yet, given a rich enough dataset, membership in a protected group is almost surely redundantly encoded, at least to some extent, by other features. indeed, unless the", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "a rich enough dataset, membership in a protected group is almost surely redundantly encoded, at least to some extent, by other features. indeed, unless the target is completely uncorrelated with membership in a protected group, given enough data a well - trained model will completely recapture the protected group membership \u2019 s impact on the target \u2014 without ever having explicit access to that particular ( protected ) feature! a standard example treats \u201c race \u201d as a protected group in a dataset including features such as \u201c zip code. \u201d here, observing \u201c zip code \u201d alone often provides strong signal about \u201c race \u201d \u2014 even without explicit access to the", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "including features such as \u201c zip code. \u201d here, observing \u201c zip code \u201d alone often provides strong signal about \u201c race \u201d \u2014 even without explicit access to the \u201c race \u201d feature. so, do we remove \u201c zip code \u201d from the input as well? maybe not, because it \u2019 s quite likely some other set of features correlate with \u201c zip code, \u201d too, and we \u2019 re back to the drawing board. indeed, and in general, it \u2019 s not immediately clear how to write down a formal set of rules to enforce accepted legal definitions of fairness in decisioning systems. it \u2019 s not the point of this", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "not immediately clear how to write down a formal set of rules to enforce accepted legal definitions of fairness in decisioning systems. it \u2019 s not the point of this article to completely overview the state - of - the - art in definitions of fairness in machine learning ; many definitions have been proposed, \u00b9 and many people have written\u00b2 about them. my point so far is that it \u2019 s tough or impossible to write down agreed - upon legal rules and definitions using formal mathematics \u2014 even for \u201c simple \u201d systems performing binary classification on relatively small, well - defined inputs. for the sake of discussion, though, let \u2019 s say we have decided on", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u201c simple \u201d systems performing binary classification on relatively small, well - defined inputs. for the sake of discussion, though, let \u2019 s say we have decided on a definition of fairness, \u00b3 and we have been able to write it down using formal mathematics, ready to be put into our ml pipeline. now what? given a well - defined definition of fairness implemented in a machine - learning - based system, it is natural to ask what the people impacted by that system ( i ) understand about the system itself and ( ii ) think about the rules under which it is operating. ditto with the operators of the system, as well as other", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "understand about the system itself and ( ii ) think about the rules under which it is operating. ditto with the operators of the system, as well as other stakeholders ( e. g., policymakers, lawyers, domain experts ). and, when different classes of stakeholder have different opinions about what \u201c fairness \u201d means, how should we manage that? let \u2019 s start with a simpler setting : asking one class of stakeholder if they comprehend well - known definitions of fairness. in joint work\u2074 with researchers at maryland and berkeley icsi, we recently did just this : we created a metric to measure comprehension of three common definitions :", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ". in joint work\u2074 with researchers at maryland and berkeley icsi, we recently did just this : we created a metric to measure comprehension of three common definitions : demographic parity, equal opportunity, and equalized odds, and then evaluated it using an online survey with the goal of investigating relationships between demographics, comprehension, and sentiment. in our study, fairness definitions were presented in multiple real - world scenarios ( e. g., in our vignette on hiring, demographic fairness was described as \u201c [ t ] he fraction of applicants who receive job offers that are female should equal the fraction of applicants that are female. similarly, the fraction", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "was described as \u201c [ t ] he fraction of applicants who receive job offers that are female should equal the fraction of applicants that are female. similarly, the fraction of applicants who receive job offers that are male should equal the fraction of applicants that are male \u201d ). then, comprehension and sentiment questions were asked. some takeaways : education is a strong predictor of comprehension, at least for the accessible explanations of fairness used in our study. the negative impacts of ml - based systems are expected to disproportionately impact some segments of society, for example by displacing employment opportunities for those with the least education.", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "systems are expected to disproportionately impact some segments of society, for example by displacing employment opportunities for those with the least education. thus, that already - at - risk group \u2019 s ability to effectively advocate for its members may be adversely impacted by lower comprehension. weaker comprehension correlates with less negative sentiment toward fairness rules. one way to interpret this is that those with the lowest comprehension of fairness concerns in ml systems would also be the least likely to protest against it. one promising direction is to learn stakeholders \u2019 views about fairness via simulation or observation of actions over time. some research has been done in this", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to protest against it. one promising direction is to learn stakeholders \u2019 views about fairness via simulation or observation of actions over time. some research has been done in this space. for example, researchers at eth zurich fit functions to users \u2019 preferences over a finite and pre - determined feature space using pairwise comparisons of simulated profiles. they found that the well - known notion of demographic parity aligned reasonably well with their human subjects \u2019 responses. one outstanding issue in this study, and in most preference and moral value judgment aggregation studies in this space, is the lack of consideration of different classes of stakeholder. for example, how can we combine the", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "most preference and moral value judgment aggregation studies in this space, is the lack of consideration of different classes of stakeholder. for example, how can we combine the input of a layperson with that of a ( domain expert ) doctor in a healthcare setting \u2014 especially when people \u2019 s judgments often disagree? so what do stakeholders of the same type want, in general? perhaps unsurprisingly, leading technology firms such as microsoft and google have taken steps in this direction with respect to the applied production settings that their engineers encounter. for example : researchers at microsoft research surveyed practitioners from 25 ml product teams in 10 major technology firms, and found", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to the applied production settings that their engineers encounter. for example : researchers at microsoft research surveyed practitioners from 25 ml product teams in 10 major technology firms, and found ( broadly ) that the \u201c fair ml \u201d research literature focuses too specifically on methods to assess biases, and would benefit from focusing more broadly on the full machine learning pipeline. a striking example from their paper involved an image labeler in a computer vision application systematically labeling female doctors as \u201c nurses, \u201d \u2075 which would then serve as \u201c gold standard \u201d input to any downstream algorithms. researchers from google recently published a case study of a fielded classification system where adverse actions are taken against examples", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "serve as \u201c gold standard \u201d input to any downstream algorithms. researchers from google recently published a case study of a fielded classification system where adverse actions are taken against examples predicted to be in the positive class ( e. g., \u201c if you are predicted to be a spammer, then i will deactivate your account \u201d ). they found that it is difficult to even measure a direct - from - the - literature definition ( equality of opportunity ), and then give a series of steps they took to build a more applicable tweak to that definition into their models. research suggests that : ( i ) laypeople largely do not", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "of steps they took to build a more applicable tweak to that definition into their models. research suggests that : ( i ) laypeople largely do not understand the accepted definitions of fairness in machine learning ; ( ii ) those who do understand those definitions do not like them ; ( iii ) those who do not understand them could be further marginalized ; and ( iv ) practitioners are not being served well by the current focus of the fairness in ml community. it sounds negative, but there are explicit next steps to take to help mitigate these issues. read on! earlier, i asked if we could \u201c do anything about it \u201d when", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "but there are explicit next steps to take to help mitigate these issues. read on! earlier, i asked if we could \u201c do anything about it \u201d when it comes to the difficulties \u2014 and often, the impossibilities \u2014 of deciding on, and enforcing, fairness in decisioning systems. the answer is, still, that we can \u2014 but responsibly, with input from all appropriate parties, and with an understanding that there is no panacea. below, i give a non - exhaustive list of action items for researchers and practitioners in the \u201c fair ml \u201d space. we need to understand what ( lay", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "below, i give a non - exhaustive list of action items for researchers and practitioners in the \u201c fair ml \u201d space. we need to understand what ( lay ) people perceive to be fair decision making. given an explicit definition of fairness, is it understood, and is it acceptable to a wide audience? if particular subgroups of the general population do not comprehend parts of automated systems that impact them, then they will be more easily disadvantaged. dovetailing with the above, we need to understand what specialists perceive to be fair decision making as well \u2014 and what tools they would need to help them do their jobs. this could mean developing tools", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "need to understand what specialists perceive to be fair decision making as well \u2014 and what tools they would need to help them do their jobs. this could mean developing tools to help audit systems, or to better curate high - quality and well - sampled input datasets, or to permit faster exploratory data analysis ( eda ) to help find holes in the input and output of prototype or deployed systems. we need techniques that, given a definition of fairness or of bias, can measure at enterprise - scale whether or not an ml - based system is adhering to that definition or those definitions \u2014 and, if not, ( i )", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "can measure at enterprise - scale whether or not an ml - based system is adhering to that definition or those definitions \u2014 and, if not, ( i ) describe by how much and ( ii ) alert humans, when appropriate, if the system deviates beyond an acceptable level. additionally, effective ui / ux will be required to allow stakeholders of all walks to comprehend the state - of - the - art in various fielded automated systems. fielded systems ingest ( incomplete ) high - dimensional data and output high - dimensional data, over time. communicating the state of a system vis a vis particular definitions of fairness and bias in a human -", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "- dimensional data and output high - dimensional data, over time. communicating the state of a system vis a vis particular definitions of fairness and bias in a human - understandable way is paramount. quoting directly from the microsoft research study discussed earlier, \u201c [ a ] nother rich area for future research is the development of processes and tools for fairness - focused debugging. \u201d debugging tools with a fairness focus would help practitioners identify, e. g., under - sampled portions of an input dataset, or previously overlooked subgroups being adversely impacted by new decisioning rules. finally, we need to develop shared languages between all", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "portions of an input dataset, or previously overlooked subgroups being adversely impacted by new decisioning rules. finally, we need to develop shared languages between all involved parties, but particularly engineers, laypeople, and policymakers. engineers implement, policymakers make society - wide rules \u2014 and laypeople are impacted by the interaction between the two. all three need to understand the wants, incentives, and limitations of the others through open and continuous communication. throughout, it is important to balance prescriptive and descriptive approaches to understanding, measuring, and implementing fairness in machine - learning - based systems. prescriptive approaches necessarily assume some", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "important to balance prescriptive and descriptive approaches to understanding, measuring, and implementing fairness in machine - learning - based systems. prescriptive approaches necessarily assume some consensus around what \u201c should \u201d occur under a ( societally decided - upon ) definition of fairness, whereas descriptive approaches focus more on uncovering what that consensus might be in the first place. researchers and practitioners interested in fairness in machine learning \u2014 myself included! \u2014 have focused too much on the former, largely due to its amenability to mathematical characterization. yet, the latter is nowhere close to understood, and is an absolutely necessary complement if not precedent to more formal prescriptive", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "amenability to mathematical characterization. yet, the latter is nowhere close to understood, and is an absolutely necessary complement if not precedent to more formal prescriptive approaches. that will, of course, require in - depth discussions with stakeholders of all walks \u2014 laypeople, policymakers, politicians, ethicists, lawyers, and domain experts. yet, these discussions will need to be complemented with accurate and scalable techniques that measure and communicate real - world systems \u2019 adherence to various definitions of bias and fairness in machine learning \u2014 so that they can provide human feedback to further improve automated decision systems performance in practice. thanks to michelle ma", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to various definitions of bias and fairness in machine learning \u2014 so that they can provide human feedback to further improve automated decision systems performance in practice. thanks to michelle mazurek, liz o \u2019 sullivan, and monica watson for comments on earlier versions of this piece. [ 1 ] i \u2019 d urge you to check out the ( free, ever - updating ) fairness and machine learning book by field experts barocas, hardt, & narayanan, a formally published overview by to - be - arthurai researcher verma & rubin, or the proceedings of area - specific conferences such as facct and aies. [ 2 ]", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to - be - arthurai researcher verma & rubin, or the proceedings of area - specific conferences such as facct and aies. [ 2 ] i \u2019 d typically also recommend wikipedia, but at the time of writing, the wikipedia page for fairness ( machine learning ) is a bit of a mess, with fourteen different binary - classification - centric fairness criteria roughly defined amongst a mess of mathematics and essentially nothing else. this one sentence in the third paragraph of the introduction really sums it up, though : \u201c the algorithms used for assuring fairness are still being improved. \u201d still a ways to go! [ 3 ] this is", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "really sums it up, though : \u201c the algorithms used for assuring fairness are still being improved. \u201d still a ways to go! [ 3 ] this is a pretty strong assumption! indeed, it \u2019 s almost always impossible to create a system that ensures three \u201c reasonable \u201d definitions of fairness, even in binary classification : calibration, a form of proportional treatment based on relative group size ; balance for the negative class, which roughly states that people in the ( true ) \u201c zero \u201d class should be scored the same ; and, balance for the positive class, which is its complement for the ( true ) \u201c one \u201d class. [", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "zero \u201d class should be scored the same ; and, balance for the positive class, which is its complement for the ( true ) \u201c one \u201d class. [ 4 ] our study is ongoing. the working paper is available as \u201c measuring non - expert comprehension of machine learning fairness metrics, \u201d with authors debjani saha, candice schumann, duncan mcelfresh, john dickerson, michelle mazurek, and michael tschantz. an initial report on our study, titled \u201c human comprehension of fairness in machine learning, \u201d appeared at the 2020 acm / aaai conference on ai, ethics, & society", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "on our study, titled \u201c human comprehension of fairness in machine learning, \u201d appeared at the 2020 acm / aaai conference on ai, ethics, & society ( aies - 20 ). [ 5 ] here, the practitioners used a \u201c failsoft \u201d solution to mitigate this input bias and combined the \u201c nurse \u201d and \u201c doctor \u201d labels in their input dataset. yet, without explicit monitoring, this systematic labeling error likely would \u2019 ve gone undiscovered. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshield", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / fairness - in - ml", "metadata": {"source": "https://www.arthur.ai/blog/fairness-in-ml", "row": 235, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 236 text : cb insights ai 100 solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvta", "metadata": {"source": "https://www.arthur.ai/blog/cbinsights", "row": 236, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedcompany updatescb insights ai 100by : arthur teammarch 5, 2020today we are honored to announce that we've been included in the cb insights ai 100. we \u2019 re thrilled that cb insights recognizes the value we are bringing to the leading companies in healthcare, finance, technology, and other industries", "metadata": {"source": "https://www.arthur.ai/blog/cbinsights", "row": 236, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "cb insights ai 100. we \u2019 re thrilled that cb insights recognizes the value we are bringing to the leading companies in healthcare, finance, technology, and other industries who trust us with their ai monitoring. cb insights has long been a signal of the best companies to follow in every category, with incredible companies filling in the top 100 list each year. we believe in the amazing potential of ai to transform the world, and are excited to be doing our part to ensure this transformation is done in a way that is safe, reliable, and fair. with so many incredible companies lined up next to us on the top 100 list, the future of ai", "metadata": {"source": "https://www.arthur.ai/blog/cbinsights", "row": 236, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "in a way that is safe, reliable, and fair. with so many incredible companies lined up next to us on the top 100 list, the future of ai looks bright. we're excited to be a part of it. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source", "metadata": {"source": "https://www.arthur.ai/blog/cbinsights", "row": 236, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##oggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / cbinsights", "metadata": {"source": "https://www.arthur.ai/blog/cbinsights", "row": 236, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ": 237 text : team arthur at neurips - 19 : a retrospective solutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / presssolutions evaluationfirewallobservabilityproducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodel", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ducts the most robust way to evaluate llms the first firewall for llms the complete ai performance solution fast, safe, custom ai for businessmodels llmnlpcvtabularresources bloggapdocscompany r & dteamcareersnews / pressrequest a demosign insign inget startedeventsteam arthur at neurips - 19 : a retrospectiveby : john dickersondecember 17, 2019arthur is fresh off the plane returning from neurips, ai \u2019 s largest \u2014 and somewhat infamous \u2014 research conference. while there, arthur announced its seed", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##arthur is fresh off the plane returning from neurips, ai \u2019 s largest \u2014 and somewhat infamous \u2014 research conference. while there, arthur announced its seed round and hosted a 50 - person model monitoring meetup next to the convention center. beyond that, the full neurips was seven packed days of new advances and directional changes in the machine learning community. here \u2019 s what the experience was like for us, written by arthur chief scientist, john dickerson. i \u2019 ll start with a broad takeaway : neurips this year was welcoming. in previous years, the conference was known for being somewhat rowdy, coming to", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "ll start with a broad takeaway : neurips this year was welcoming. in previous years, the conference was known for being somewhat rowdy, coming to a head in 2017 / 18 with controversies that led, in part, to the conference being renamed. with 13, 000 attendees this year, my worry was that things would feel even more chaotic. but, a largely inclusive and constructive tone was set via community - wide norm changes led by leaders in the field, community - building events such as black in ai and queer in ai, and workshops such as the ever - growing women in machine learning. this progress is great to see,", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "building events such as black in ai and queer in ai, and workshops such as the ever - growing women in machine learning. this progress is great to see, as it is increasingly evident that ai algorithms are influenced not just by the raw data fed into them, but also the traits and experiences of those who build them. along those lines, bias, fairness, and explainability were front and center in many talks, posters, and workshops. one of my favorite talks, given by aaron roth from penn, blended ideas from individual and statistical ( aka \u201c group \u201d ) notions of fairness. the former constrains treatment across pairs of inputs (", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "aaron roth from penn, blended ideas from individual and statistical ( aka \u201c group \u201d ) notions of fairness. the former constrains treatment across pairs of inputs ( e. g., \u201c similar individuals are treated similarly \u201d due to dwork et al. ), while the latter partitions inputs into groups ( e. g., based on age, race, or sex ) and enforces equalization of various metrics across those groups. both of these approaches have well - known pros and cons. group - level fairness is relatively easy to define and enforce on general input distributions, but may result in unexpected behavior as one looks", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "known pros and cons. group - level fairness is relatively easy to define and enforce on general input distributions, but may result in unexpected behavior as one looks at sub - groups ( e. g., inputs with a specific race and sex ). individual notions of fairness, unsurprisingly, come with attractive guarantees at the individual input level, but may be hard to define and incorporate into models. aaron and colleagues \u2019 work provides a natural middle ground, where explicit group selection is no longer needed and some level of guarantee ( in expectation ) is given to individual inputs ; check out the paper! explainability was also center stage", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "group selection is no longer needed and some level of guarantee ( in expectation ) is given to individual inputs ; check out the paper! explainability was also center stage across the board, ranging from workshops such as robust ai in financial services : data, fairness, explainability, trustworthiness, and privacy with panel participants affirming the financial services industry \u2019 s need for trustworthy and explainable systems, to industry expo days like fairness and explainability : from ideation to implementation to domain - specific papers and demos focusing on computer vision and natural language processing tasks. understanding why models make particular inferences is important for everything from debugging", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to domain - specific papers and demos focusing on computer vision and natural language processing tasks. understanding why models make particular inferences is important for everything from debugging systems to adhering to regulatory requirements, and it was great to see the research community stepping up to provide new tools that will, hopefully, find their way into industry. this was a big year for nlp and, specifically, all things bert ( bidirectional encoder representations from transformers, from google ) \u2014 and all kinds of visualizations of and attacks on bert and friends. \u00b9over the last couple of years especially, nlp has seen explosive progress, and", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2014 and all kinds of visualizations of and attacks on bert and friends. \u00b9over the last couple of years especially, nlp has seen explosive progress, and that was showcased at this year \u2019 s neurips as well. panel at the neurips - 19 \u201c minding the gap : between fairness and ethics \u201d workshop. probably the biggest nlp paper presented xlnet ( also from google ), which dominates bert on a number of tasks. it \u2019 s exciting to see nlp move forward so quickly, of course, but many \u2014 including big names in the field \u2014 are starting to get grouchy\u00b2 about the field", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "see nlp move forward so quickly, of course, but many \u2014 including big names in the field \u2014 are starting to get grouchy\u00b2 about the field \u2019 s seeming obsession with making already big models even bigger. this twitter thread by yoav goldberg is a nice place to start for under - loved holes in nlp research, ranging from theory to explainability to generalization to incorporating linguistic theory back into deep - learned - based techniques. the generalization capabilities of deep networks are poorly understood \u2014 they famously don \u2019 t align with the traditional statistical view that, after a point, bigger models are worse. rather, they tend to exhibit", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "are poorly understood \u2014 they famously don \u2019 t align with the traditional statistical view that, after a point, bigger models are worse. rather, they tend to exhibit a \u201c double descent \u201d, where as a model grows in size test error decreases, then increases, then decreases again. at least two nice papers came out in this \u201c understanding deep learning generalization \u201d space : the winner of the outstanding new directions paper award from researchers at cmu argues against a particular class of theoretical bounding technique, and another nice paper from princeton and cmu advanced the state of understanding what larger ( namely, infinitely larger ) networks can represent. at the ne", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##ing technique, and another nice paper from princeton and cmu advanced the state of understanding what larger ( namely, infinitely larger ) networks can represent. at the neurips retrospectives workshop on friday, leading researchers asked about their past work, \u201c what should readers of this paper know now, that is not in the original publication? \u201d i love this idea and hope it takes off \u2014 researchers, especially younger researchers ( when many do some of their \u201c most famous \u201d work! ), often write papers from a single perspective. as their research gains prominence, folks from other fields inevitably chime in with references to earlier related work or methods for", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": ", often write papers from a single perspective. as their research gains prominence, folks from other fields inevitably chime in with references to earlier related work or methods for better framing. this feels personal to me : before heading off to do my phd, i submitted my first research paper and was informed a few weeks later that a nearly - identical problem had been solved by researchers in the ussr and at rand in the us in the 1970s, available online in poorly - scanned pdfs \u2014 in both russian and english! it turns out michael littman, a reinforcement learning luminary, had a similar experience. in the 1990s, he published seminal and", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "both russian and english! it turns out michael littman, a reinforcement learning luminary, had a similar experience. in the 1990s, he published seminal and extremely highly - cited work ( pdf ) introducing \u201c markov games \u201d to the cs community, which helped spur thinking in multi - agent reinforcement learning. in his retrospective on that paper, he mentioned wishing he \u2019 d known about \u201c stochastic games \u201d \u2014 effectively the same idea, but introduced in the 1950s (!! ), and with more theoretical rigor. still, mike states he is happy he wrote the paper \u2014 as is the community, because this pushed forward interest", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "! ), and with more theoretical rigor. still, mike states he is happy he wrote the paper \u2014 as is the community, because this pushed forward interest in multi - agent reinforcement learning, a topic of increasing importance today in application areas ranging from simple game playing to energy production and disaster relief operations. ai is such an interdisciplinary field that this type of cross - pollination is more a blessing than a curse ; indeed, we \u2019 d all benefit from looking \u2014 and talking! \u2014 beyond traditional technology fields. also on friday, our co - founder liz spoke on a panel at the minding the gap : between fairness and ethics workshop,", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2014 beyond traditional technology fields. also on friday, our co - founder liz spoke on a panel at the minding the gap : between fairness and ethics workshop, alongside a variety of research scientists and engineers from google focusing on ethics, fairness, and ai. the workshop featured practitioners and, in general, participants from outside the traditional ai / ml communities, which led to in - depth conversations about the gulf between what \u201c standard \u201d ml - based definitions of fairness offer and what practitioners might want or at least find practical to use in their day - to - day jobs. while the ai / ml community ( including myself ) has started to address this", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "want or at least find practical to use in their day - to - day jobs. while the ai / ml community ( including myself ) has started to address this divide, there \u2019 s clearly a ton to be done when it comes to properly incorporating the wants and needs of stakeholders into modern ai systems. i spent saturday at the causalml [ b1 ] aka do the right thing : machine learning and causal inference for improved decision making workshop. invited speakers such as susan athey and susan murphy connected techniques for uncertainty management from the ai / ml world with application areas in business, including labor markets and advertising, and healthcare. i found the poster sessions", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "murphy connected techniques for uncertainty management from the ai / ml world with application areas in business, including labor markets and advertising, and healthcare. i found the poster sessions in this workshop particularly enlightening : causal and counterfactual reasoning are two intertwined topics that are forming the basis for both fair and robust ( e. g., to adversarial manipulations, or to simple noise in data ) automated systems, and i look forward to these concerns and ideas melting more into the greater ml community. finally, what would a large ai / ml conference be without a little drama? bengio and schmidhuber, two", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "more into the greater ml community. finally, what would a large ai / ml conference be without a little drama? bengio and schmidhuber, two deep learning visionaries from different continents, stay at loggerheads about who should give whom credit about what. i won \u2019 t dignify linking to the / r / machinelearning and blind threads discussing this, but suffice it to say that the \u201c who did it first in deep learning \u201d rabbit hole is still getting, ah, deeper. all in all, neurips this year captured the largely positive zeitgeist of the machine learning and ai", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "hole is still getting, ah, deeper. all in all, neurips this year captured the largely positive zeitgeist of the machine learning and ai community. after a period of extreme growth, the conference \u2014 and, by proxy, the community \u2014 feels like it \u2019 s growing, if not grown, up. yet, neurips \u2019 workshops, events, and keynotes captured another, more subtle thread : as the impact of ai continues to spread \u2014 and the community continues to learn and understand how that spread impacts society \u2014 researchers and practitioners alike are desperate for tools and guidance about how to integrate models safely and responsi", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "community continues to learn and understand how that spread impacts society \u2014 researchers and practitioners alike are desperate for tools and guidance about how to integrate models safely and responsibly. there \u2019 s so much left to be done regarding not just the scalability and generalizability of modern ml methods, but also the ability to define and incorporate notions of fairness, bias, trustworthiness, and accountability into models and pipelines in ways that are interpretable at both train / test time as well as in deployment. i \u2019 d love to see more research and development time spent on & $ 58 ; crossing boundaries and discussing exactly what industry practitioners and other", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "well as in deployment. i \u2019 d love to see more research and development time spent on & $ 58 ; crossing boundaries and discussing exactly what industry practitioners and other stakeholders want and need. the human - computer interaction ( hci ) and ai / ml communities are starting to build this knowledge out, but what we need is a feedback loop between stakeholders and researchers formalizing the communication pipelines between both parties. this is the only way we will settle on meaningful definitions of what it means to approach \u201c fairness \u201d in different domainstracking and analyzing the impact of implementing various objectives or constraints ( e. g, promoting combinations of fairness, diversity,", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "to approach \u201c fairness \u201d in different domainstracking and analyzing the impact of implementing various objectives or constraints ( e. g, promoting combinations of fairness, diversity, and economic efficiency ) on truly dynamic systems, that is, systems where the input data distribution drifts over time, metrics change, and so on. simply put, measuring things. a discussion we have constantly at arthur revolves around which metrics we should ( i ) show by default, ( ii ) pre - compute and allow to be toggled on or off, ( iii ) don \u2019 t pre - compute but allow to be computed, and ( iv ) leave to", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "and allow to be toggled on or off, ( iii ) don \u2019 t pre - compute but allow to be computed, and ( iv ) leave to the user to define and pass to our system as a custom metric. we need to help industry understand what it needs, then develop scalable methods to measure that, and then integrate those metrics and measurements in live systems to track model performance. that means not just the so - called efficiency metrics such as accuracy, precision, and f - score, but also measures for fairness, bias, diversity, explainability, and anything else stakeholders might want. that last point is a", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "precision, and f - score, but also measures for fairness, bias, diversity, explainability, and anything else stakeholders might want. that last point is a multidisciplinary one, and \u2014 frankly \u2014 there \u2019 s no way to escape that. as ai practitioners, we aim to build systems for users in myriad fields, and we build them so they ( i ) work and ( ii ) work in a way that users understand. the more progress the field makes toward understanding and then building scalable and general methods, the better. those methods need to consider the competing and sometimes contradictory wants of stakeholders : efficiency, fairness, robust", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "understanding and then building scalable and general methods, the better. those methods need to consider the competing and sometimes contradictory wants of stakeholders : efficiency, fairness, robustness, explainability, and even justice. it \u2019 s not a simple problem, and it \u2019 s not one that can be solved by the ai / ml community alone. i look forward to the work and discussion that will come from future ai / ml conferences \u2014 such as aies and aaai in nyc, arthur \u2019 s home city, in early 2020! [ 1 ] or, in this case, attacks on bert by friends. i \u2019 m a fan of bad jokes", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "\u2019 s home city, in early 2020! [ 1 ] or, in this case, attacks on bert by friends. i \u2019 m a fan of bad jokes, and approve of the team from ai2 and uw building grover, a fake news generator. [ 2 ] there \u2019 s an oscar the grouch joke in here somewhere. previous postsharenext post we make ai better for everyone. sign up for our newsletter to get the latest arthur news! subscribeproductshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyte", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
{"page_content": "##productshieldbenchscopechatllmnlpcvtabularr & dresourcesbloggapdocumentationcompanyteamcareersnewspress inquiriesarthur 2023 \u00a9 all rights reservedterms of serviceprivacy source : https : / / www. arthur. ai / blog / team - arthur - at - neurips - 19 - a - retrospective", "metadata": {"source": "https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective", "row": 237, "content_type": "arthur_blog"}, "type": "Document"}
