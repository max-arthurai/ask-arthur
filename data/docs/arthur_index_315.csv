,text,content_type,source
0,"Arthur
Now Available: Recommender System Support in Arthur ScopeRead MoreSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe AI Delivery EngineLaunch, secure, and optimize AI at scale.Request DemoBuildArthur’s turnkey, plug-and-play solutions allow companies to build on top of their internal knowledge base and make informed, data-driven decisions when integrating the latest generative AI technologies into their operations.Learn MoreProtectDeploy AI confidently and safely by using Arthur to protect your organization from the biggest LLM threats including data leakage, hallucinations, toxic language generation, and prompt injection.Learn MoreOptimizeDrive key business results for your enterprise by using the Arthur platform to optimize model operations and performance at scale across tabular, CV, NLP, and large language models.Learn MoreEvaluateArthur Bench is an open-source evaluation product for comparing LLMs, allowing you to make informed, data-driven decisions when integrating the latest AI technologies into your operations.Learn MoreThe #1 AI Delivery PlatformModelsThe Arthur platform is model- and platform-agnostic, providing the #1 monitoring platform for models ranging from classic tabular and computer vision, to robust LLMs.PlatformsThe Arthur platform scales up and down with complex enterprise needs—it can ingest up to 1MM transactions per second and deliver insights quickly.Learn MoreDeploymentArthur works seamlessly with all leading data science and MLOps tools, including Databricks, Amazon SageMaker, TensorFlow, PyTorch, SingleStore, and Salesforce.GovernanceArthur’s platform offers model risk management capabilities across validation, monitoring, and reporting, helping organizations to avoid adverse consequences from decisions made based on model errors.Learn MoreCollaborationOur platform allows for quick, seamless communication across teams via a centralized performance dashboard, real-time metrics, optimization alerts, and fully customizable permissions across organizations.SecurityHaving achieved SOC 2 Type II compliance, our robust model monitoring solution adheres to best-in-class security and data privacy controls. We are committed to meeting the industry’s most rigorous data security, availability, and confidentiality standards.Learn More“Arthur Bench helped us develop an internal framework to scale and standardize LLM evaluation across features, and to describe performance to the Product team with meaningful and interpretable metrics.”Priyanka OberoiStaff Data Scientist, Axios HQFrom the BlogView MoreAAAI 2024 Recap: Future Visions of Recommendation Ecosystems Read MoreWhat’s Going On With LLM Leaderboards?Read MoreNow Available: Recommender System Support in Arthur ScopeRead MoreFrom the StudioView MoreGround Truth Episode 5: AI Crashes the Science PartyWatchLLMs & Generative AI in Editorial Content Creation & MediaWatchArthur LLM Product Demo & Research on Generative AI ChallengesWatchSee what Arthur can do for you.Get Started
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/
1,"Evaluation
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEvaluationThe solution to help teams evaluate LLM options quickly, easily, and consistently.As the LLM landscape rapidly evolves, companies must continually ensure their LLM choice remains the best fit for the organization’s specific needs. Arthur Bench, our open source evaluation product, helps businesses with:Model selection & validationBudget & privacy optimizationsTranslation of academic benchmarks to real-world performance“Understanding the differences in performance between LLMs can have an incredible amount of nuance. With Bench, we’ve created an open source tool to help teams deeply understand the differences between LLM providers, different prompting and augmentation strategies, and custom training regimes.”Adam WenchelCo-Founder & CEOThe Most Robust Way to Evaluate LLMsBench is our solution to help teams evaluate different LLM options in a quick, easy, and consistent way.Model Selection & ValidationCompare LLM options using a consistent metric to determine the best fit for your application.Budget & Privacy OptimizationNot all applications require the most advanced or expensive LLMs — in some cases, a less expensive AI model can perform just as well.Translating Academic Benchmarks to Real-World PerformanceTest and compare the performance of different models quantitatively with a set of standard metrics to ensure accuracy and consistency.Try BenchRelated ArticlesWhat’s Going On With LLM Leaderboards?Arthur TeamRead MoreIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessArthur TeamRead MoreLLM-Guided Evaluation: Using LLMs to Evaluate LLMsArthur TeamRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/solution/evaluation
2,"Protection
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedComplete protection with the world's first firewall for LLMsDeploying and utilizing LLMs has numerous and well-documented risks which, if not mitigated and monitored, can lead to negative user experiences and significant reputational impact. These risks include:PII or sensitive data leakagePrompt injectionsHallucinationsToxic, offensive, or problematic language generationComplete protection with the world's first firewall for LLMsDeploying and utilizing LLMs has numerous and well-documented risks which, if not mitigated and monitored, can lead to negative user experiences and significant reputational impact. These risks include:PII or sensitive data leakagePrompt injectionsHallucinationsToxic, offensive, or problematic language generation“LLMs are one of the most disruptive technologies since the advent of the Internet. Arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk.”Adam WenchelCo-Founder & CEOThe First Firewall for LLMsShield is our solution to help companies deploy their LLMs confidently and safely.Fits into the LLM architectureSits between the application layer and the deployment layer to validate user prompts and model responses on two endpoints.Works with any LLMWhether you’re using OpenAI or another large language model, Shield will be able to be integrated into the workflow.Provides real-time protectionOur inference deep dive capabilities allow us to detect and intercept any prompts that may potentially be considered harmful or elicit a potentially dangerous output.Try ShieldSee what Arthur can do for you.Get Started
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/solution/firewall
3,"Observability
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedObservabilityMonitor, measure, and improve ML models to deliver better results.Arthur helps enterprise teams optimize model operations and performance at scale. Across LLM, tabular, CV, and NLP models, our platform tracks and improves:Model accuracy & data driftExplainability & transparencyFairness & bias detection“Thanks to Arthur, we know that our preventative care models are fair, and that we can catch any potential issues before they impact our members… and the Arthur platform allows us to detect and fix data drift before it becomes a real problem.”Heather Carroll CoxChief Analytics Officer, HumanaThe Complete AI Performance SolutionUse Arthur Observability across LLM, tabular, CV, and NLP models to monitor, measure, and improve model metrics and achieve business results.Accuracy & data driftTrack model performance to detect and react to data drift, improving model accuracy for better business outcomes.Explainability & transparencyBuild trust, ensure compliance, and drive more actionable ML outcomes with Arthur’s explainability features.Fairness & bias detectionGet actionable insights into how your model treats different population groups, set fairness thresholds that make sense for your business, and get notified instantly if there are any problems.Try ObservabilityRelated ArticlesDetecting Unexpected Drift in Time Series FeaturesAkash KhannaRead MoreModel Schemas Within the MLOps EcosystemSarah OstermeierRead MoreKeep the Lights On: Making Deployed AI/ML Better for EveryoneJohn DickersonRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/solution/observability
4,"Arthur Bench
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe Most Robust Way to Evaluate LLMsBench is our solution to help teams evaluate the different LLM options out there in a quick, easy and consistent way.Try Bench“LLMs are one of the most disruptive technologies since the advent of the Internet. Arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk.”Adam WenchelCo-Founder & CEOModel Selection & ValidationArthur Bench helps companies compare the different LLM options available using consistent metrics so they can determine the best fit for their application in a rapidly evolving AI landscape.Budget & Privacy OptimizationNot all applications require the most advanced or expensive LLMs — in some cases, a less expensive AI model can perform tasks equally as well. Additionally, bringing models in-house can offer greater controls around data privacy. Translating Academic Benchmarks to Real-World PerformanceBench helps companies test and compare the performance of different models quantitatively with a set of standard metrics to ensure accuracy and consistency. Additionally, companies can add and configure customized benchmarks, enabling them to focus on what matters most to their specific business and customers.Try BenchArthur Bench is the key to fast, data-driven LLM evaluationFull Suite of Scoring MetricsFrom summarization quality to hallucinations, Bench comes complete with a full suite of scoring metrics, ready to leverage. Additionally, you can create and add your own scoring metrics.Intuitive User InterfaceLeverage the Arthur user interface to quickly and easily conduct and compare your test runs and visualize the different performance of the LLMs.Local and Cloud-based VersionsGain access via our GitHub repo and run it locally or sign up for our cloud-based SaaS offering. We offer both versions for greatest flexibility.Completely Open SourceThe best part is that Bench is completely open source, so new metrics and other valuable features will continue to be added as the project and community grows.Visit Our GitHub RepoThe Generative Assessment ProjectA research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open source models.Learn MoreThe Generative Assessment ProgramA research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open source models.Learn MoreRelated ArticlesWhat’s Going On With LLM Leaderboards?February 19, 2024Read MoreIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessDecember 5, 2023Read MoreLLM-Guided Evaluation: Using LLMs to Evaluate LLMsSeptember 29, 2023Read More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/product/bench
5,"Arthur Shield
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe First Firewall for LLMsShield is our solution to help companies deploy their LLMs confidently and safely.Learn MoreFits into the LLM architectureSits between the application layer and the deployment layer to validate user prompts and model responses on two endpoints.Works with any LLMWhether you’re using OpenAI or another large language model, Shield will be able to be integrated into the workflow.Provides real-time protectionOur inference deep dive capabilities allow us to detect and intercept any prompts that may potentially be considered harmful or elicit a potentially dangerous output.Learn MoreArthur Shield is the key to deploying LLMs quickly and safelySensitive Data LeakageProtect your user’s data as well as your company’s proprietary data from being unintentionally leaked.ToxicityBlock LLM responses that are not value-aligned with your organization.Prompt InjectionsIdentify and block attempts to override the intended behavior of an LLM by malicious users.HallucinationsDetect likely incorrect or unsubstantiated responses from an LLM before they can cause harm to the end user.Related ArticlesWhat’s Going On With LLM Leaderboards?Arthur TeamRead MoreIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessArthur TeamRead MoreThe Real-World Harms of LLMs, Part 2: When LLMs Do Work as ExpectedSarah OstermeierRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/product/shield
6,"Scope
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe Complete AI Performance SolutionWith Scope, enterprise teams can optimize ML operations and performance, delivering better results across LLM, tabular, CV, and NLP models.Learn MoreMonitors all model typesArthur Scope is the leading performance solution for all model types, including NLP, CV, tabular, and LLM.Highly scalable with complex enterprise needsThe Arthur platform’s highly scalable microservices architecture makes it the leading platform for enterprises that want to run high-performing ML models at scale.Learn MoreArthur Scope is the key to accelerating model operations to drive business resultsAccuracy & data driftTrack model performance to detect and react to data drift, improving model accuracy for better business outcomes.Fairness & bias detectionGet actionable insights into how your model treats different population groups, set fairness thresholds that make sense for your business, and get notified instantly if there are any problems.Explainability & transparencyBuild trust, ensure compliance, and drive more actionable ML outcomes with Arthur’s explainability features.Real-time alertsMaintain peace of mind knowing you will proactively receive alerts on any metric after poor model performance is detected.Related ArticlesWhat’s Going On With LLM Leaderboards?Arthur TeamRead MoreIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessArthur TeamRead MoreThe Real-World Harms of LLMs, Part 2: When LLMs Do Work as ExpectedSarah OstermeierRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/product/scope
7,"Arthur Chat
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedFast, Safe, Custom AI for BusinessAs a completely turnkey AI chat platform built on top of your enterprise documents and data, Arthur Chat is the fastest way to unlock the value of your LLM.Try ChatCustom, Completely Turnkey Chat ExperienceLLM applications are hard to build. They require resources, knowledge, and time for your team to ramp up on new concepts. With Arthur Chat, you can focus more on delivering value, rather than delivering code.Powered by Your DataThe Arthur Chat platform is built on top of your internal knowledge base, composed of many documents and data—both structured and unstructured—so you can provide the best and most accurate responses to your users. Just bring your own data and we’ll handle the rest.Protected by Arthur ShieldArthur Shield, the world’s first firewall for LLMs, is embedded within the Arthur Chat platform, enabling real time checks and protection against PII/sensitive data leakage, toxic language generation, hallucinations, and prompt injection.Try ChatArthur Shield is the key to deploying LLMs quickly and safelySensitive Data LeakageProtect your user’s data as well as your company’s proprietary data from being unintentionally leaked.ToxicityBlock LLM responses that are not value-aligned with your organization.Prompt InjectionsIdentify and block attempts to override the intended behavior of an LLM by malicious users.HallucinationsDetect likely incorrect or unsubstantiated responses from an LLM before they can cause harm to the end user.Related ArticlesWhat’s Going On With LLM Leaderboards?Arthur TeamRead MoreIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessArthur TeamRead MoreThe Real-World Harms of LLMs, Part 2: When LLMs Do Work as ExpectedSarah OstermeierRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/product/chat
8,"LLM
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe Control Plane for LLMsEverything you need to quickly and safely deploy LLMs into mission-critical applications.Arthur is the all-in-one solution for deploying and running LLMs, trusted by the most important companies in the world with mission-critical applications. From evaluation and validation to firewall protection and monitoring, we’ve developed a state-of-the-art LLM product suite that makes generative AI simple, useful, and safe.FirewallEvaluationObservabilityChatCompanies across industries are rapidly integrating large language models into their operations, but they don’t have a way to ensure deployment that’s both fast and safe.Arthur Shield, the world’s first firewall for LLMs, protects organizations against the most serious risks and safety issues with LLMs in production.Mitigate risks like:PII or sensitive data leakageHallucinationsToxic, offensive, or problematic language generationPrompt injectionsLearn MoreAs the LLM landscape rapidly evolves, it’s crucial for companies to keep abreast of advancements and continually ensure their LLM choice remains the best fit for the organization’s specific needs.With Arthur Bench, our open source evaluation product, companies can make informed, data-driven decisions by comparing different LLM options.Bench helps businesses with:Model selection & validationBudget & privacy optimizationTranslation of academic benchmarks to real-world performanceLearn MoreArthur helps enterprise teams optimize model operations and performance at scale. Our platform tracks and improves key metrics for not only your LLMs in production, but for tabular, CV, and NLP models as well.With Arthur Scope, you can:Detect model and data issues immediatelySurface actionable insights to improve performanceOptimize model portfolio managementReduce risk with comprehensive ML governanceLearn MoreLLM applications are hard to build—they require resources, knowledge, and time for your team to ramp up on new concepts. Arthur Chat is a highly configurable, plug-and-play, LLM-powered chat experience that allows you to focus more on delivering value, rather than delivering code.Chat provides organizations with:A completely turnkey chat experience, ready to deploy
in under an hourThe ability to customize and build on top of your internal knowledge baseProtection from Arthur Shield, the world’s first firewall for LLMsLearn MoreFirewallEvaluationObservabilityChatCompanies across industries are rapidly integrating large language models into their operations, but they don’t have a way to ensure deployment that’s both fast and safe.Arthur Shield, the world’s first firewall for LLMs, protects organizations against the most serious risks and safety issues with LLMs in production.Mitigate risks like:PII or sensitive data leakageHallucinationsToxic, offensive, or problematic language generationPrompt injectionsLearn MoreAs the LLM landscape rapidly evolves, it’s crucial for companies to keep abreast of advancements and continually ensure their LLM choice remains the best fit for the organization’s specific needs.With Arthur Bench, our open source evaluation product, companies can make informed, data-driven decisions by comparing different LLM options.Bench helps businesses with:Model selection & validationBudget & privacy optimizationTranslation of academic benchmarks to real-world performanceLearn MoreArthur helps enterprise teams optimize model operations and performance at scale. Our platform tracks and improves key metrics for not only your LLMs in production, but for tabular, CV, and NLP models as well.With Arthur Scope, you can:Detect model and data issues immediatelySurface actionable insights to improve performanceOptimize model portfolio managementReduce risk with comprehensive ML governanceLearn MoreLLM applications are hard to build—they require resources, knowledge, and time for your team to ramp up on new concepts. Arthur Chat is a highly configurable, plug-and-play, LLM-powered chat experience that allows you to focus more on delivering value, rather than delivering code.Chat provides organizations with:A completely turnkey chat experience, ready to deploy in under an hourThe ability to customize and build on top of your internal knowledge baseProtection from Arthur Shield, the world’s first firewall for LLMsLearn More“Thanks to Arthur, we know that our preventative care models are fair, and that we can catch any potential issues before they impact our members… and the Arthur platform allows us to detect and fix data drift before it becomes a real problem.”Heather Carroll CoxChief Analytics Officer, HumanaLLM SolutionsArthur is the all-in-one solution for deploying and running LLMs, trusted by the most important companies in the world with mission-critical applications. From evaluation and validation to firewall protection and monitoring, we’ve developed a state-of-the-art LLM product suite that makes generative AI simple, useful, and safe.FirewallEvaluationObservabilityCompanies across industries are rapidly integrating large language models into their operations, but they don’t have a way to ensure deployment that’s both fast and safe.Arthur Shield, the world’s first firewall for LLMs, protects organizations against the most serious risks and safety issues with LLMs in production.Mitigate risks like:PII or sensitive data leakageHallucinationsToxic, offensive, or problematic language generationPrompt injectionsLearn MoreAs the LLM landscape rapidly evolves, it’s crucial for companies to keep abreast of advancements and continually ensure their LLM choice remains the best fit for the organization’s specific needs.With Arthur Bench, our open source evaluation product, companies can make informed, data-driven decisions by comparing different LLM options.Bench helps businesses with:Model selection & validationBudget & privacy optimizationTranslation of academic benchmarks to real-world performanceLearn MoreArthur helps enterprise teams optimize model operations and performance at scale. Our platform tracks and improves key metrics for not only your LLMs in production, but for tabular, CV, and NLP models as well.With Arthur Scope, you can:Take action to improve performanceIncrease speed & efficiencyDemocratize ML performanceReduce risk through ML governanceLearn MoreFirewallEvaluationObservabilityCompanies across industries are rapidly integrating large language models into their operations, but they don’t have a way to ensure deployment that’s both fast and safe.Arthur Shield, the world’s first firewall for LLMs, protects organizations against the most serious risks and safety issues with LLMs in production.Mitigate risks like:PII or sensitive data leakageHallucinationsToxic, offensive, or problematic language generationPrompt injectionsLearn MoreAs the LLM landscape rapidly evolves, it’s crucial for companies to keep abreast of advancements and continually ensure their LLM choice remains the best fit for the organization’s specific needs.With Arthur Bench, our open source evaluation product, companies can make informed, data-driven decisions by comparing different LLM options.Bench helps businesses with:Model selection & validationBudget & privacy optimizationTranslation of academic benchmarks to real-world performanceLearn MoreArthur helps enterprise teams optimize model operations and performance at scale. Our platform tracks and improves key metrics for not only your LLMs in production, but for tabular, CV, and NLP models as well.With Arthur Scope, you can:Detect model and data issues immediatelySurface actionable insights to improve performanceOptimize model portfolio managementReduce risk with comprehensive ML governanceLearn MoreSee what Arthur can do for you.Get StartedSee what Arthur can do for you.Get Started
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/model-type/llm
9,"NLP
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedUnderstand your NLP models like never before.Try the first-ever complete NLP monitoring & explainability solution today.Request DemoEnsure Consistent PipelineEnsure consistency in information extraction pipelines and monitor for data drift.Explore Key InsightsEasily filter, search, and explore key insights in your NLP models such as anomalous inferences or specified attributes about each document.Use Explainability TechniquesUse explainability techniques to identify the most important features in determining the predictions of your NLP models.Explainability and Monitoring for Your NLP ModelsRequest DemoCompare the similarity of new input documents to the documents used to train your NLP models.Detect biases in your NLP models by uncovering differences in accuracy and other performance metrics across different subgroups. Identify the specific words within a document that contributed the most to a given prediction.Download Our NLP WhitepaperFrom simple chatbots to document classifiers to generative models like GPT-4, natural language processing models are seemingly everywhere these days. NLP models are powerful tools for processing unstructured text data—but with great power comes great responsibility. If you’re not monitoring your NLP models just as you would your tabular models, you can overlook many sticky issues that could quickly become billion-dollar problems.Arthur’s “Increase ML Model Visibility with NLP Monitoring” whitepaper is everything you need to know about model monitoring for natural language processing whitepaper covers what any organization deploying NLP models into production should be doing to ensure that those models continue to perform as expected.Learn more about how model monitoring can help you improve your NLP model performance with the help of Arthur.Download WhitepaperLearn more about monitoring NLP models with ArthurData Drift Detection Part II: Unstructured Data in NLP and CVKarthik Rao and Rowan CheungRead MoreArthur releases the first computer vision model monitoring solution for enterpriseArthur TeamRead MoreIntroducing Monitoring for Computer Vision ModelsArthur TeamRead MoreSee what Arthur can do for you.Get Started
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/model-type/nlp
10,"CV Models
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedSee your CV models more clearly.Try the first-ever computer vision monitoring solution today.Request DemoIdentify Anomalies & Data DriftAutomatic out-of-distribution detection lets you identify where your model is likely making mistakes.Improve Your Models and Explore Your DatasetsExplore the results of your vision models (classification and object detection) with an interactive interface that makes it easy to identify issues.Understand Your Models BetterVisualize important image regions that are impactful for model predictions.Explainability and Monitoring for Your CV ModelsMonitor CV model pipelines for data anomalies using built-in out-of-distribution detection and track the accuracy of bounding box models.Detect biases in your CV models by evaluating image classification outputs using an interactive interface and locating where your models misclassify and perpetuate biases.Visualize which regions of an image are impactful for an image classification model’s decision or how your object detection models are performing on pipeline images.Download Our CV WhitepaperAs computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes.The adoption of computer vision into industry applications promises enormous potential upside; however, computer vision models, like any ML model, must be carefully monitored. A promising model that has gone off the rails can quickly become a dangerous liability.In this whitepaper, we lay out several aspects of computer vision models that are important for users to understand and demonstrate how Arthur’s product offers simple solutions to these pressing problems.Download WhitepaperRelated ArticlesData Drift Detection Part II: Unstructured Data in NLP and CVKarthik Rao and Rowan CheungRead MoreArthur releases the first computer vision model monitoring solution for enterpriseArthur TeamRead MoreIntroducing Monitoring for Computer Vision ModelsArthur TeamRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/model-type/cv
11,"Tabular
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedMonitor, measure, and improve your tabular models.Achieve better results across accuracy, explainability, and fairness.Request DemoIdentify Anomalies & Data DriftAutomatic out-of-distribution detection lets you identify where your model is likely making mistakes.Use Explainability TechniquesUse explainability techniques to identify the most important features in determining the predictions of your models.Quickly Mitigate BiasUse Arthur’s proprietary techniques, or customize with your own metrics, to reduce business risk and prevent discrimination.Learn about monitoring your tabular models with ArthurDetecting Unexpected Drift in Time Series FeaturesAkash KhannaRead MoreModel Schemas Within the MLOps EcosystemSarah OstermeierRead MoreKeep the Lights On: Making Deployed AI/ML Better for EveryoneJohn DickersonRead More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_site,https://www.arthur.ai/model-type/tabular
12,"Welcome to Arthur Scope!
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Welcome to Arthur Scope!As your team's data science operations center, Arthur helps enterprise teams monitor, measure and optimize AI performance at scale.Suggest EditsLooking to operationalize your machine learning systems in production? Arthur's observability platform helps enterprise teams monitor, measure, and improve machine learning at scale.
What Happens When AI Meets the Real World?
As the AI performance company, Arthur helps data scientists, product owners, and business leaders accelerate model operations at scale. Our platform monitors, measures, and improves machine learning models for better results across three core areas: accuracy, explainability, and fairness.
Video Overview
Got 4 minutes? Check out a video overview of our product:
Get Started Today
Jump into the Quickstart Guide or start learning about the Arthur Platform.Updated 3 months ago What’s NextQuickstartTable of Contents
What Happens When AI Meets the Real World?
Video Overview
Get Started Today",arthur_scope_docs,https://docs.arthur.ai/docs
13,"Binary Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Binary ClassificationBinary Classification for Text Models in ArthurSuggest EditsBinary classification models predict a binary outcome (ie. one of two potential classes). In Arthur, these models fall into the category of classification and are represented by the Multiclass model type.
Some common examples of Text binary classification are:
Is this email spam or not?
Was this review written by a human or a robot?
Frequently, these models output not only a yes/no answer but also a probability for each (i.e. prob_yes and prob_no). These probabilities are then categorized into yes/no based on a threshold. In these cases, during onboarding teams will supply their classification threshold and continuously track the class probabilities (i.e. prob_yes, prob_no).
Formatted Data in Arthur
Text binary classification models require three things to be specified in their schema: the text input, predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (text input)Probability of Prediction AProbability of Prediction BGround TruthNon-Input Attribute (numeric or categorical)Ubi amor, ibi dolor.95.05AMaleLupus in fabula.86.14BFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their binary classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 3 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below are common examples of the required runnable predict function (that outputs two values, the probability of each potential class).
prediction to ground truth mappingExample Prediction FunctionExample Prediction Function with Transformations## Option 1:
Single Prediction Column, Single Ground Truth Column
# Map PredictedValue Column to its corresponding GroundTruth value.
# This tells Arthur that the `pred_proba_credit_default` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map_1 = {'pred_proba_credit_default' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_1,
)
## Option 2:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
pred_to_ground_truth_map_2 = {'pred_0' : 0,
'pred_1' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_2,
positive_predicted_attr = 'pred_1'
)
## Option 3:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
pred_to_ground_truth_map_3 = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# Building the Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=pred_to_ground_truth_map_3,
positive_predicted_attr = 'pred_1'
)
# example_entrypoint.py
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(x)
# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Available Metrics
When onboarding Text classification models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) when teams onboard a binary classification model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case text) drift is calculated with multivariate drift to accommodate the multivariate nature / relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the only attribute required to onboard a model is the text attribute. So, it is only possible to monitor non-input attributes for fairness in text models.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXX (Non-Input Attributes)Updated 3 months ago What’s NextLearn more about the model onboarding process or jump right into an NLP onboarding quickstartModel OnboardingNLP OnboardingTable of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/text-binary-classification-1
14,"Binary Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Binary ClassificationBinary Classification for Image Models in Arthur ScopeSuggest EditsBinary classification models predict a binary outcome (i.e., one of two potential classes). In Arthur Scope, these models fall into the classification category and are represented by the Multiclass model type.
Some common examples of Image binary classification are:
Does this CT scan contain a tumor?
Is a roof in a satellite image showing signs of damage?
Frequently, these models output both a yes/no answer and a probability for each (i.e., prob_yes and prob_no). These probabilities are then categorized into yes/no based on a threshold. In these cases, during onboarding, teams will supply their classification threshold and continuously track the class probabilities (i.e., prob_yes, prob_no).
Formatted Data in Arthur
Image binary classification models require three things to be specified in their schema: the image input, the predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (image input)Probability of Prediction AProbability of Prediction BGround TruthNon-Input Attribute (numeric or categorical)image_1.jpg.95.05AMaleimage_2.jpg.86.14BFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their binary classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 3 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below are common examples of the required runnable predict function (that outputs two values, the probability of each potential class).
prediction to ground truth mappingExample Prediction FunctionExample Prediction Function with Transformations## Option 1:
Single Prediction Column, Single Ground Truth Column
# Map PredictedValue Column to its corresponding GroundTruth value.
# This tells Arthur that the `pred_proba_credit_default` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map_1 = {'pred_proba_credit_default' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_1,
)
## Option 2:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
pred_to_ground_truth_map_2 = {'pred_0' : 0,
'pred_1' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_2,
positive_predicted_attr = 'pred_1'
)
## Option 3:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
pred_to_ground_truth_map_3 = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# Building the Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=pred_to_ground_truth_map_3,
positive_predicted_attr = 'pred_1'
)
# example_entrypoint.py
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(x)
# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Available Metrics
When onboarding tabular classification models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
When teams onboard a binary classification model, the following metrics are automatically available in the UI (out-of-the-box). Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the only attribute required to onboard a model is the text attribute. So, monitoring non-input attributes for fairness in image models is only possible.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXX (Non-Input Attributes)Updated 3 months ago What’s NextModel OnboardingTable of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/image-binary-classification-3
15,"Spark ML
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Spark MLSuggest EditsThis guide provides an example of integrating with the ArthurAI platform to monitor a SparkML model. We'll use an example dataset to train a SparkML model from scratch, but you could also use an existing Spark Pipeline.
Pythonfrom pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
import pyspark.sql.functions as f
from pyspark.ml.classification import LogisticRegression
import pandas as pd
import numpy as np
from arthurai import ArthurAI
from arthurai.client.apiv3 import InputType, OutputType, Stage
Train and Save SparkML Model
First, we'll instantiate a Spark session and load in a sample dataset. In this example, we'll use a dataset derived from the famous Boston Housing dataset to build a simple model.
Pythonspark = SparkSession.builder.appName('app').getOrCreate()
data = spark.read.csv('./data/boston_housing.csv', header=True, inferSchema=True)
train, test = data.randomSplit([0.7, 0.3])
We'll use a LASSO classification model to try to predict the is_expensive column from all the others. This column encodes whether or not a property value was above or below the local average.
As preprocessing, we'll use the VectorAssembler class to pull together the input columns into a single numeric feature vector.
Pythonfeature_columns = data.columns[:-1] # here we omit the final column
assembler = VectorAssembler(inputCols=feature_columns,outputCol=""features"")
lasso_classifier = LogisticRegression(featuresCol=""features"", labelCol=""is_expensive"", maxIter=10, regParam=0.3, elasticNetParam=1.0)
Using a Pipeline, we'll combine our preprocessing steps and our ML model, and we'll fit to the training data and save. If you have an existing Spark Pipeline, you can load from disk.
Pythonpipeline = Pipeline(stages=[assembler, lasso_classifier])
fitted_pipeline = pipeline.fit(train)
fitted_pipeline.write().overwrite().save('./data/models/boston_housing_spark_model_pipeline')
Onboard to Arthur
Pythonarthur = ArthurAI(url='https://app.arthur.ai', login=""<YOUR_USERNAME_OR_EMAIL>"", password=""<YOUR_PASSWORD>"")
To onboard our model with Arthur, we'll register the schema of the data coming into and out of the model. For simplicity, you can use a Pandas Dataframe for this step. We will take a sample of the SparkDF to the driver, and use this to register the model to Arthur.
Pythonsample_df = train.take(5000).toPandas()
sample_Y = sample_df.loc[['is_expensive']]
sample_X = sample_df.drop('is_expensive', axis=1)
Python# instantiate basic model
arthur_model = arthur.model({
""partner_model_id"": ""Boston Housing"",
""input_type"": InputType.Tabular,
""output_type"": OutputType.Multiclass,
""is_batch"": True})
# use pandas DataFrames to register data schema
arthur_model.from_dataframe(sample_X, Stage.ModelPipelineInput)
arthur_model.add_binary_classifier_output_attributes(
positive_predicted_attr='expensive',
pred_to_ground_truth_map={
'prediction_expensive': 'ground_truth_expensive',
'prediction_cheap': 'ground_truth_cheap'
},
threshold=0.75
)
The from_dataframe() method will inspect your dataset and infer the input schema, datatypes, and sample statistics. You can review the model structure and see if any fixes are needed.
Pythonarthur_model.review()
Python# chas and rad were inferred as categorical, lets change those to be continuous
arthur_model.get_attribute('chas', Stage.ModelPipelineInput).set(categorical=False)
arthur_model.get_attribute('rad', Stage.ModelPipelineInput).set(categorical=False)
arthur_model.review()
Monitoring for bias
For any attributes that you want to monitor for bias, you set the monitor_for_bias boolean. In fact, these don't have to be model inputs, they can also be of stage NonInputData.
Pythonsensitive_attributes = [""Gender"", ""Race"", ""Income_Bracket""]
for attribute_name in sensitive_attributes:
arthur_model.get_attribute(attribute_name, Stage.ModelPipelineInput).monitor_for_bias = True
Save
Now you're ready to save your model and finish onboarding.
Pythonarthur_model.save()
Set reference data
You can set a baseline dataset in order to speed up the calculation of data drift and inference anomaly scoring.
This reference set is typically the training set the model was fitted to, or a subsample. You can use either a pandas
DataFrame or a directory of parquet files. The reference data can include model input features, ground truth features
or model predictions on training sets. However, it is recommended that only model input features are provided.
Pythonarthur_model.set_reference_data(directory_path=""./data/august_training_data/"")
(sparkml_explainability)=
Enabling Explainability for SparkML
To enable explainability, you'll supply a python file that implements a predict() function for a single observation
(a numpy array). This predict function can contain anything you need, including loading a serialized model,
preprocessing/transformations, and making a final prediction. The returned result should be a numpy array. You'll also
supply a requirements file for all the dependencies for running an inference through your model.
For more details around enabling explainability, see the {doc}/user-guide/walkthroughs/explainability guide.
Below we provide a Spark specific example.
The first step is to save your SparkML model and pipeline so it can be imported for use in the predict() function
Pythonfitted_pipeline.write().overwrite().save('./data/models/boston_housing_spark_model_pipeline')
Next is to create your predict() function.
Python# entrypoint.py
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
# To start the spark session on the model server specify the master url as local.
# By default this will run spark using 1 thread, to increase threads you can specify
# local[x] where x is the number of threads. When allocating more compute and memory to the spark
# session be sure to increase the amount allocated to the model server when calling ArthurModel.enable_explainability()
# in the sdk (by default 1 cpu and 1gb of memory is allocated to the model server).
spark = SparkSession.builder.master('local').appName('app').getOrCreate()
loaded_pipeline = PipelineModel.load(""./data/models/boston_housing_spark_model_pipeline"")
def predict(input_data):
col_names = ['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','b','lstat']
input_df = pd.DataFrame(input_data, columns=col_names)
spark_df = spark.createDataFrame(input_df)
predictions = loaded_pipeline.transform(spark_df)
return np.array([float(x.prediction) for x in predictions.select('prediction').collect()])
You are then ready to {ref}enable explainability <enabling_explainability>
Pythonarthur_model.enable_explainability(
df=sample_df,
project_directory='.',
user_predict_function_import_path='entrypoint',
requirements_file='requirements.txt')
Send Batch of Inferences
Once your model has been onboarded, it is ready to receive inferences and model telemetry.
There are some standard inputs needed to identify inferences and batches.
First, each inference needs a unique identifier so that it can later be joined with ground truth. Include a column named partner_inference_id and ensure these IDs are unique across batches. For example, if you run predictions across your customer base on a daily-batch cadence, then a unique identfier could be composed of your customer_id plus the date.
Second, each inference needs to be associated with a batch_id, but this id will be shared among one or more inferences.
Finally, each inference needs an inference_timestamp and these don't have to be unique.
Additionally, the predictions/scores from your model should match the column names in the registered schema. If we take a look above at arthur_model.review() we'll recall that columns we created correspond to the clasiffier's output probabilities over the classes (""prediction_cheap"" and ""prediction_expensive"") and the corresponding ground truth over the possible classes in one-hot form (""ground_truth_cheap"" and ""ground_truth_expensive"").
We will process a batch of datapoints through the Pipeline and save the inputs (and predictions) to parquet. We will do the same for the ground truths.
Pythonloaded_pipeline = PipelineModel.load(""./data/models/boston_housing_spark_model_pipeline"")
inferencesDF = loaded_pipeline.transform(test).withColumnRenamed(""probability"", ""prediction_expensive"")
uuidUdf= udf(lambda : str(uuid.uuid4()), StringType())
inferencesDF = inferencesDF.withColumn('partner_inference_id', uuidUdf())
# add required columns
inferencesDF[""inference_timestamp""] = datetime.utcnow()
inferencesDF[""batch_id""] = ""inferences_batch_001""
inference_df[""partner_inference_id""] = ...
# write inferences
inferencesDF.write.parquet(""./data/inference_files/inferences.parquet"")
# write ground truths
ground_truth_DF = test.select([""ground_truth_cheap"", ""ground_truth_expensive""])
ground_truth_DF[""partner_inference_id""] = ...
ground_truth_DF[""ground_truth_timestamp""] = datetime.utcnow()
ground_truth_DF[""batch_id""] = ""gt_batch_001""
ground_truth_batch.write.parquet(""./data/ground_truth_files/ground_truth.parquet"")
With our model's inputs and outputs save as parquet, we upload a batch by pointing to the directory containing one or more parquet files. The directory will be traversed and all parquet files will be joined into the corresponding batch.
Note, the model inputs and predictions will be uploaded separately from the ground truth.
Pythonarthur_model.send_bulk_inferences(directory_path='./data/inference_files/')
You can separately upload ground truths for each inference. Every row in the ground truth file(s) should have an external_id column that matches any IDs you create for the inferences.
Pythonarthur_model.send_bulk_ground_truths(directory_path='./data/ground_truth_files/')
Updated 3 months ago Table of Contents
Train and Save SparkML Model
Onboard to Arthur
Set reference data
Enabling Explainability for SparkML
Send Batch of Inferences",arthur_scope_docs,https://docs.arthur.ai/docs/spark-ml
16,"Appendix
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by AppendixSuggest EditsRunning the Velero CLI
Velero provides a Command-Line Interface (CLI) for taking backups and performing restores. The CLI can be installed locally, or it can be invoked by kubectl exec on the Velero Backup Controller pod.
Local Installation
Refer to the Velero Documentation for installing Velero on your platform.
Velero uses your KUBECONFIG file to connect to the cluster.
$ velero --help
Velero is a tool for managing disaster recovery, specifically for Kubernetes
cluster resources. It provides a simple, configurable, and operationally robust
way to back up your application state and associated data.
If you're familiar with kubectl, Velero supports a similar model, allowing you to
execute commands such as 'velero get backup' and 'velero create schedule'. The same
operations can also be performed as 'velero backup get' and 'velero schedule create'.
Usage:
velero [command]
Available Commands:
backup
Work with backups
backup-location
Work with backup storage locations
bug
Report a Velero bug
client
Velero client related commands
completion
Generate completion script
create
Create velero resources
debug
Generate debug bundle
delete
Delete velero resources
describe
Describe velero resources
get
Get velero resources
help
Help about any command
install
Install Velero
plugin
Work with plugins
restic
Work with restic
restore
Work with restores
schedule
Work with schedules
snapshot-location Work with snapshot locations
uninstall
Uninstall Velero
version
Print the velero version and associated image
Flags:
--add_dir_header
If true, adds the file directory to the header
--alsologtostderr
log to standard error as well as files
--colorized optionalBool
Show colored output in TTY. Overrides 'colorized' value from $HOME/.config/velero/config.json if present. Enabled by default
--features stringArray
Comma-separated list of features to enable for this Velero process. Combines with values from $HOME/.config/velero/config.json if present
-h, --help
help for velero
--kubeconfig string
Path to the kubeconfig file to use to talk to the Kubernetes apiserver. If unset, try the environment variable KUBECONFIG, as well as in-cluster configuration
--kubecontext string
The context to use to talk to the Kubernetes apiserver. If unset defaults to whatever your current-context is (kubectl config current-context)
--log_backtrace_at traceLocation
when logging hits line file:N, emit a stack trace (default :0)
--log_dir string
If non-empty, write log files in this directory
--log_file string
If non-empty, use this log file
--log_file_max_size uint
Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
--logtostderr
log to standard error instead of files (default true)
-n, --namespace string
The namespace in which Velero should operate (default ""velero"")
--skip_headers
If true, avoid header prefixes in the log messages
--skip_log_headers
If true, avoid headers when opening log files
--stderrthreshold severity
logs at or above this threshold go to stderr (default 2)
-v, --v Level
number for the log level verbosity
--vmodule moduleSpec
comma-separated list of pattern=N settings for file-filtered logging
Use ""velero [command] --help"" for more information about a command.
Executing on the Velero Backup Controller Pod
If it is not possible to install the Velero CLI on the local workstation, you can still run Velero commands directly on the Velero pod as follows:
Shell$ velero_namespace=""Put your Velero namespace here""
$ kubectl exec velero-699dc869d4-r24bh -n $velero_namespace -c velero -- /velero help
Velero is a tool for managing disaster recovery, specifically for Kubernetes
cluster resources. It provides a simple, configurable, and operationally robust
way to back up your application state and associated data.
<<<output-truncated-for-brevity>>>
Backups using Velero
Creating a Backup
To take a backup of Arthur, you would invoke the CLI as follows.
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ velero_namespace=""Put your Velero namespace here""
$ velero backup create <some-unique-name> \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--storage-location=docs-demo-backup-location-velero
Listing all Backups
You can list all backups using the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero backup get -n $velero_namespace
Describing a Backup
You can get an overview of the backup using the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero backup describe <insert-backup-name> -n $velero_namespace
Debugging a Backup
For debugging a backup, you can access the backup's logs using the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero backup logs <insert-backup-name> -n $velero_namespace  head
Restores using Velero
Similar to Backup, Restore happens using the Velero CLI. A restore takes a Backup object and then executes the restore procedure.
Attempting a Restore
You can execute a restore with the following Velero CLI command:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero restore create \
--from-backup <insert-backup-name> \
--namespace $velero_namespace \
--restore-volumes=true
Listing all Restore attempts
Just like with the Backup, Velero will create a Restore Velero Resource, which you can inspect with the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero restore get -n $velero_namespace
Describing a Restore attempt
You can get an overview of the restore attempt using the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero restore describe <insert-restore-name> -n $velero_namespace
Debugging a Restore attempt
For debugging a restore attempt, you can access the logs using the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero restore logs <insert-restore-name> -n $velero_namespace  head
Running Backups on a Schedule
There are two ways the Arthur platform can be backed up on a schedule:
Scripting the entire backup process {ref}(see example) <scripted_solution>, and executing it on a fixed schedule from a job runner (Jenkins, Gitlab-CI etc.)
Leveraging native schedulers to back up the individual components of the platform:
Clickhouse is backed up at midnight (by default) using Kubernetes CronJobs out-of-the-box.
Use Velero Schedules to create Velero Backups:
Messaging infrastructure
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
$ storage_location=""Put your storage location here""
$ arthur_namespace=""Put your Arthur namespace here""
$ velero_namespace=""Put your Velero namespace here""
$ velero schedule create messaging-infra-backup-nightly \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='app in (cp-zookeeper,cp-kafka)' \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,services,endpoints,configmaps,poddisruptionbudgets
--storage-location=$storage_location \
--schedule ""0 0 * * *"" \
--ttl 720h0m0s
Enrichments (infrastructure and workflows)
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
$ storage_location=""Put your storage location here""
$ arthur_namespace=""Put your Arthur namespace here""
$ velero_namespace=""Put your Velero namespace here""
$ velero schedule create enrichments-workflows-backup-nightly \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--include-resources=workflows \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--schedule ""0 0 * * *"" \
--ttl 720h0m0s
$ velero schedule create qa-enrichments-infra-backup-nightly \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='component in (kafka-mover-init-connector, model_server)' \
--include-resources=deployments,services \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--schedule ""0 0 * * *"" \
--ttl 720h0m0s
RDS databases can be automatically backed up on a schedule, not at a specific point in time but within a 30-minute window. And during this window, the database is snapshotted at a random time. Due to this limitation from AWS, ensure there are no operations (like model CRUD, etc.) on the Arthur platform during the backup window.
shell
$ aws rds modify-db-instance \
--db-instance-identifier RDS_DB_NAME \
--backup-retention-period 14 \
--preferred-backup-window 23:45-00:15 \
--profile AWS_PROFILE_NAME \
--region AWS_REGION \
--apply-immediately
Sample Backup Script (manual)
The following script can be used to run all the backup steps together:
Shell#!/bin/bash
set -euo pipefail
IFS=$'\n\t'
# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location=""Put your storage location here""
arthur_namespace=""Put your Arthur namespace here""
velero_namespace=""Put your Velero namespace here""
backup_date=$(date +%Y-%m-%d-%H-%M-%S);
name=arthur-backup-$backup_date
echo ""Creating a new backup with name $name""
echo ""Taking a backup of CH data""
kubectl create job $name-clickhouse-backup \
--namespace=$arthur_namespace \
--from=cronjob/clickhouse-backup-cronjob
ch_backup_jobname=$(kubectl get jobs -o name -n ""$arthur_namespace""  grep ""$name-clickhouse-backup"")
kubectl wait $ch_backup_jobname \
--namespace=$arthur_namespace \
--for=condition=complete \
--timeout=30m
echo ""Taking a backup of the enrichments infrastructure""
velero backup create $name-enrichments \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='component in (kafka-mover-init-connector, model_server)' \
--include-resources=deployments,services \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
echo ""Taking a backup of workflows""
velero backup create $name-workflows \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--include-resources=workflows \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
echo ""Taking a backup of Kafka/Kafka-ZK StatefulSets, their EBS Volumes, and related components""
velero backup create $name-messaging \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='app in (cp-zookeeper,cp-kafka)' \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,services,endpoints,configmaps,poddisruptionbudgets \
--storage-location=$storage_location \
--wait
echo ""Taking a backup of the RDS database""
aws rds create-db-cluster-snapshot \
--db-cluster-snapshot-identifier $name-snapshot \
--db-cluster-identifier RDS_DB_NAME \
--profile AWS_PROFILE_NAME \
--region AWS_REGION
Updated 3 months ago Table of Contents
Running the Velero CLI
Local Installation
Executing on the Velero Backup Controller Pod
Backups using Velero
Creating a Backup
Listing all Backups
Describing a Backup
Debugging a Backup
Restores using Velero
Attempting a Restore
Listing all Restore attempts
Describing a Restore attempt
Debugging a Restore attempt
Running Backups on a Schedule
Sample Backup Script (manual)",arthur_scope_docs,https://docs.arthur.ai/docs/appendix
17,"Installation Overview
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Installation OverviewSuggest EditsArthur Scope can be installed to existing Kubernetes clusters (K8s) or virtual machines (VM). For both Kubernetes and virtual machine deployments, Arthur supports two types of installation methods, online and airgapped. The online method allows fast and continuous differential upgrades. The airgapped install supports environments where the cluster has no outbound internet connectivity.Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/on-prem-deployment
18,"Installing Kubernetes
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Installing KubernetesSuggest EditsThis section covers the steps required for installing Arthur on a Kubernetes cluster. There are separate steps required for online and airgapped installations. An additional set of instructions are also available for installing Arthur that’s scoped within a K8s namespace.Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/installing-kubernetes
19,"Backing Up the Arthur Platform
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Backing Up the Arthur PlatformSuggest EditsOnce all the Pre-Requisites have been met, the various Arthur platform components can be backed up. The process to manually backup individual components is detailed below, which may also be {ref}scripted<scripted_solution>.
Backing up Clickhouse Data
By default, the Arthur Platform ships with a Kubernetes CronJob, which backs up Clickhouse daily at midnight.
To manually back up ClickHouse data, you can run the following commands:
Shellarthur_namespace=""Put your Arthur namespace here""
$ kubectl get cronjobs -n $arthur_namespace  grep -i clickhouse
NAME
SCHEDULE
SUSPEND
ACTIVE
LAST SCHEDULE
AGE
clickhouse-backup-cronjob
0 0 * * *
False
0
14h
2d18h
$ kubectl create job clickhouse-backup --from=cronjob/clickhouse-backup-cronjob -n $arthur_namespace
job.batch/clickhouse-backup created
$ kubectl get jobs -n $arthur_namespace
NAME
COMPLETIONS
DURATION
AGE
clickhouse-backup-cronjob-27735840
1/1
8m35s
14m
Backing Up Enrichments
The Arthur Platform uses Velero to take a backup of the Enrichments Infrastructure and the Enrichments workflows.
The Enrichments infrastructure and Enrichment Workflows are orchestrated as separate backups and will require running 2 separate commands.
Backing Up Enrichments Infrastructure
To manually back up the Enrichments infrastructure, run the following commands:
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location=""Put your storage location here""
arthur_namespace=""Put your Arthur namespace here""
velero_namespace=""Put your Velero namespace here""
$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
arthur_namespace=<insert-arthur-namespace-here>
velero_namespace=<insert-velero-namespace-here>
$ velero backup create $name-enrichments \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='component in (kafka-mover-init-connector, model_server)' \
--include-resources=deployments,services \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
Backing Up Enrichments Workflows
To manually back up the Enrichments Workflows, run the following commands:
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location=""Put your storage location here""
arthur_namespace=""Put your Arthur namespace here""
velero_namespace=""Put your Velero namespace here""
$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
velero backup create $name-workflows \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--include-resources=workflows \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,secrets,configmaps \
--storage-location=$storage_location \
--wait
Backing Up Messaging Infrastructure
The Arthur Platform uses Velero to take a backup of the Kafka (and ZooKeeper) Deployment State and EBS Volumes.
To manually back up Kafka, run the following commands:
Shell# You need to configure this by getting the name of the backup storage location
# eg: `velero backup-location get` or `kubectl get backupstoragelocation -n <velero-namespace>`
storage_location=""Put your storage location here""
arthur_namespace=""Put your Arthur namespace here""
velero_namespace=""Put your Velero namespace here""
$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
$ velero backup create $name-messaging \
--namespace=$velero_namespace \
--include-namespaces=$arthur_namespace \
--selector='app in (cp-zookeeper,cp-kafka)' \
--exclude-resources=clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io,controllerrevisions.apps,endpointslices.discovery.k8s.io,customresourcedefinitions.apiextensions.k8s.io,services,endpoints,configmaps,poddisruptionbudgets \
--storage-location=$storage_location \
--wait
Backing Up RDS Postgres
RDS database backups are called Snapshots. To manually create a snapshot of an RDS database, execute the below script:
Shell$ backup_date=$(DATE +%Y-%m-%d-%H-%M-%S);
$ name=arthur-backup-$backup_date
$ aws rds create-db-cluster-snapshot \
--db-cluster-snapshot-identifier $name-snapshot \
--db-cluster-identifier RDS_DB_NAME \
--profile AWS_PROFILE_NAME \
--region AWS_REGION
📘RDS CompatibilityThe command is only compatible for a multi-region RDS database cluster. If you are using a single-region RDS database, the command to use is aws rds create-db-snapshot.
For more information, please refer to the AWS Documentation:
Multi-region RDS cluster and AWS CLI command
Single-region RDS instance and AWS CLI command
Updated 3 months ago Table of Contents
Backing up Clickhouse Data
Backing Up Enrichments
Backing Up Enrichments Infrastructure
Backing Up Enrichments Workflows
Backing Up Messaging Infrastructure
Backing Up RDS Postgres",arthur_scope_docs,https://docs.arthur.ai/docs/backing-up-the-arthur-platform
20,"Token Sequence (LLM)
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Token Sequence (LLM)Suggest EditsGenerative text model outputs are computer-generated text that mimics human language patterns and structures based on patterns learned from a large dataset. In Arthur, these models are listed under the Token Sequence model type.
Some common examples of Generative Text models are:
Headline summarization
Question and Answering (chatbots)
Formatted Data in Arthur
Depending on how you built your generative model and what you are looking to track, there are different types of data that you can track in the platform.
Attribute (User Text Input)Output (Text Output)Output Likelihood (Token Likelihood) (Optional)Non Input Attribute (numeric or categorical)Dulce est desipere in locoActa, non verba[{""Acta"": 0.7, "","": 0.3, ""non"": 0.8, ""verba"": 0.34}]Political TopicSi vis amari amaCastigat ridendo mores[{""Cast"": 0.56, ""igat"": 0.4, ""ridendo"": 0.24, ""mores"": 0.67}]Entertainment Topic
Predict Function and Mapping
Teams need to specify the relationship between the prediction and ground truth columns to help set up their Arthur model's environment to calculate default performance metrics. Here is an example of what that might look like:
Registering Predict / GT mapping## Registering columns
arthur_model.build_token_sequence_model(
input_column=""user_input"",
output_text_column=""output_text""
## optional for model types with token likelihood
output_likelihood_column=""token_likelihoods""
)
Available Metrics
When onboarding Token Sequence models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) when teams onboard a Token Sequence model. Find out more about how to use these metrics in the
Performance Metrics section. For metric definitions, check out the Glossary.
MetricMetric TypeAverage Token LikelihoodPerformanceLikelihood StabilityPerformanceAverage Sequence LengthPerformanceInference CountIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Some things of note: For unstructured data types (like text and image), feature drift is calculated for non-input attributes. Additionally, generative text models create text input and output that can be tracked with multivariate drift.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftMultivariate Drift for Prompts (Text Input)Multivariate DriftMultivariate Drift for Predictions (Text Output)Multivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/generative-llm
21,"Airgap Kubernetes Cluster (K8s) Install
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Airgap Kubernetes Cluster (K8s) InstallSuggest EditsEnsure your K8s cluster is ready for Arthur platform installation by following Installing Arthur Pre-requisites guide.
Preparing Container Registries
Prepare your private container image registry for Arthur artifacts by creating the following list of repositories:
Admin Console:
arthurai/dex
arthurai/kotsadm
arthurai/kotsadm-migrations
arthurai/local-volume-fileserver
arthurai/local-volume-provider
arthurai/minio
arthurai/postgres
Application:
arthurai/alert-service
arthurai/alpine
arthurai/api-service
arthurai/argocli
arthurai/argoexec
arthurai/aws-cli
arthurai/beta-client
arthurai/busybox
arthurai/clickhouse-operator
arthurai/clickhouse-server
arthurai/client
arthurai/cp-kafka
arthurai/cp-kafka-connect
arthurai/cp-schema-registry
arthurai/cp-zookeeper
arthurai/custom-hpa
arthurai/dataset-service
arthurai/ingestion-service
arthurai/kafka-connect-monitor
arthurai/kafka-exporter
arthurai/kafka-prometheus-jmx-exporter
arthurai/kubectl
arthurai/mc
arthurai/metric-service
arthurai/metrics-exporter
arthurai/minio
arthurai/model-server
arthurai/model-server-controller
arthurai/postgresql
arthurai/python-jobs
arthurai/python-spark-jobs
arthurai/pytorch-jobs
arthurai/query-service
arthurai/redis
arthurai/scala-spark-jobs
arthurai/schema-service
arthurai/workflow-controller
arthurai/zookeeper-exporter
As an example, here's how you can create a new arthurai/alert-service repository on AWS ECR.
Shellexport AWS_REGION=<your_region>
aws ecr create-repository --repository-name=arthurai/alert-service
Download Installation Files
Go to the download portal using the URL and the password provided by Arthur.
Select the ""Bring my own cluster"" option
Click the “Download license” button to download your license in the YAML file.
Download the ""KOTS Airgap Bundle"" and the ""arthur Airgap Bundle"".
Setup for Installation
Make sure you're in the correct kubectl environment context before running the installer.
Shellkubectl config current-context
Install the KOTS kubectl extension on your local machine:
Shellcurl https://kots.io/install  bash
If the Linux workstation you're running kubectl from is also in the airgap environment; download the ""KOTS CLI"" from the download portal and install it like below:
Shelltar zxf kots_linux_amd64.tar.gz
# move it to a location that's on your path
sudo mv kots /usr/local/bin/kubectl-kots
📘The ""KOTS CLI"" and ""KOTS Airgap Bundle"" must be installed at the same time and therefore will be on the same version.
If your workstation is a Mac, you can download the latest Kots CLI Darwin binary version from https://kots.io/.
Start Installation
Push the Admin Console images to your private registry:
Shellkubectl kots admin-console push-images ./kotsadm.tar.gz [Your private registry host]/arthurai \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
As an option, you can also pre-upload the application images to your private registry before running the installer:
Shellkubectl kots admin-console push-images ./arthur-x.x.x.airgap [Your private registry host]/arthurai \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
Install the Admin Console (see here for {doc}Namespace-Scoped Installs <k8s_install_namespace_scoped>):
Shellkubectl kots install arthur \
--no-port-forward \
--namespace arthur \
--shared-password [Provide an Admin Console password] \
--kotsadm-namespace arthurai \
--kotsadm-registry [Your private container image repository] \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
Create a port forwarding tunnel to Admin Console. Go to http://localhost:8800 to access the Admin Console:
Shellkubectl kots admin-console --namespace arthur
Follow the instructions on the Admin Console to complete your installation by providing the private registry details and arthur-x.x.x.airgap bundle.
📘The upload process can take couple of hours so ensure your laptop does not go to sleep.You may follow the instructions Airgap Kubernetes Cluster (K8s) Install with CLI to install the Admin Console and Arthur app programmatically using the CLI only
Configure Arthur.
Verify Installation
Monitor the Admin Console dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with thekubectl CLI:
Shellkubectl get deployment,statefulset,pod -n arthur
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
Customize Installation
Configure graphs on Admin Console by clicking on the Configure Prometheus Address button and providing your Prometheus endpoint (e.g.,http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090).Updated 3 months ago Table of Contents
Preparing Container Registries
Download Installation Files
Setup for Installation
Start Installation
Verify Installation
Customize Installation",arthur_scope_docs,https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install
22,"Text
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by TextSuggest EditsText input models are a type of machine learning model that operates on text data, such as natural language text found in documents, emails, social media posts, and other forms of written communication. These models are designed to learn the patterns and relationships between words and phrases in order to perform tasks such as sentiment analysis, language translation, and text classification. Text input models can be built using a variety of techniques, including neural networks, decision trees, and support vector machines.
Embeddings in Arthur
Text models in Arthur take in Text input, i.e. raw text of the documents or social media posts teams are using to predict. While there are a few enrichments (namely Anomaly Detection) that use model embeddings, Arthur computes these embeddings internally.
Currently, Arthur does not take in embeddings or vector inputs.
Tokenization in Arthur
Text inputs are tokenized in Arthur for both anomaly detection and explainability. To create these tokens, Arthur has a few different text delimiters.
Text Delimiters
Here are the different delimiters available in Arthur.
NameDefined in ArthurDescriptionCOMMA"",""Splits on a single comma.COMMA_PLUS"",+""Splits on one or more commas.NOT_WORD""\W+""Splits on any character that is not a word.PIPE""""Splits on a single pipe.PIPE_PLUS""+Splits on one or more pipes.WHITESPACE""\s+""Splits on whitespace.Updated 3 months ago Table of Contents
Embeddings in Arthur
Tokenization in Arthur",arthur_scope_docs,https://docs.arthur.ai/docs/text
23,"Creating a Connection to Arthur
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…JUMP TOIntroductionCreating a Connection to ArthurArthur API ReferenceArthur SDK ClientArthur SDK DocumentationtionTest Explainability LocallyPowered by JUMP TOIntroductionCreating a Connection to ArthurArthur API ReferenceArthur SDK ClientArthur SDK DocumentationtionTest Explainability LocallyPowered by Creating a Connection to ArthurThis page will help you get started with Arthur API.There are a few ways to create a connection to Arthur.
Creating an API Key
To access the Arthur platform outside of Arthur, you specify your access rights. One way to do that is through an API key. To get an API key from Arthur, users need to go into the Settings of their Organizational dashboard. From there, they need to click on the API Keys Tab and click the Create API Key button.
📘API Key == Header in API ExamplesWhen using the API example structure in this documentation, you should place your API key in the header section of the call you are trying to build.
Connecting to a Specific Arthur Model within the SDK
After connecting to your Arthur instance, whether you are using the API or Arthur Python SDK, it is common for teams to want to connect to a specific Arthur model. There are two ways to identify a specific model within Arthur, shown below:
Partner Model ID: The Partner Model ID is an ID given to the model by the customer team/user that onboarded the model. This can be found in the UI under the Details section of your model.
Arthur Model ID: The Arthur Model ID is a unique ID given to the model by the Arthur platform. This can be found in your Arthur instance in the URL of your model.
Connecting to the Python SDK
The Arthur Python SDK, discussed in more detail here, is one of the most common ways teams interact with the Arthur platform in a notebook environment.
Connect to Platform
When connecting to the Arthur platform with the SDK, there are two
API KeyUsername Passwordimport arthurai as ArthurAI
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"",
password = ""<YOUR_PASSWORD>"")
import arthurai as ArthurAI
arthur = ArthurAI(url=""YOUR_ORGANIZAIONS_ARTHUR_URL"",
login=""username"", password = ""password"")
Specifying Organization ID
If you log in with your Username and Password and are a member of multiple organizations within Arthur, you must specify which organization you want to connect to within the notebook. This can be done by adding the organization_id to your connection.
Pythonimport arthurai as ArthurAI
connection = ArthurAI(url=""YOUR_ORGANIZAIONS_ARTHUR_URL"",
login=""username"", password = ""password"", organization_id = ""org_id"")
You should contact your system administrator if you are unsure what your organization ID is. You can also see a list of all possible organization IDs you have access to in a notebook by running this Python script:
Pythonconnection = ArthurAI(url=""YOUR_ORGANIZAIONS_ARTHUR_URL"",
login=""username"", password = ""password"", verify_ssl=False)
import requests
headers = {
'Content-Type': 'application/json',
'Authorization': connection.client.session.headers['Authorization']
}
session = requests.Session()
response = session.get(""https://<YOUR_ORGANIZATIONS_URL>/api/v3/organizations?page_size=1000000"", headers = headers,verify=False)
print(response.json())
Connect to Specific Arthur Model
Either of these can be used to connect to Arthur with this code:
Python## Using Partner Model ID
arthur_model = connection.get_model(place_partner_id, id_type=""partner_model_id"")
## Using Arthur Model ID
arthur_model = connection.get_model(place_model_id, id_type=""id"")
Connecting to the API
To connect to the API, users have the same permissions as above.",arthur_scope_docs,https://docs.arthur.ai/reference
24,"Kubernetes Cluster (K8s) Install with Namespace Scope Privileges
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Kubernetes Cluster (K8s) Install with Namespace Scope PrivilegesSuggest EditsIf you would like to install the Arthur platform with namespace scoped privileges, there are certain components that will fail since they will need cluster-level access. These cluster-level components are CRDs (Custom Resource Definitions) which are required for the proper functioning and operation of the Arthur platform. However, these CRDs only need to be installed once with cluster-admin privileges, and elevated access is not required for normal usage of the platform.
📘Both the Admin Console and Arthur application can be installed at the cluster-level or namespace-scope independent of each other.
CRDs leveraged by Arthur Platform
The Arthur platform makes use of the following two CRDs:
Argo Workflows : Kubernetes-native Open-source workflow manager
ClickHouse Operator : Column-oriented OLAP datastore
Installing Admin Console within a Namespace
By default, the Admin Console is installed at the Cluster level, available in all namespaces. If you would like to install the Admin Console only within a specific namespace, you can use the following flag to the kots command:
Shellkubectl kots install arthur \
--use-minimal-rbac
--skip-rbac-check
Since the Admin Console will not have access to the Cluster, certain Preflight checks will fail. The Cluster Admin is responsible for ensuring sufficient resources are provisioned with the correct version of K8s.
Installing Cluster-level CRDs for Arthur from Nexus
Since the Arthur platform requires CRDs for normal operation, these will need to be installed by the Cluster Admin before installing Arthur itself, in no particular order. The instructions below show how to download the CRD charts from our publicly hosted repository.
Argo Workflows:
Shellhelm repo add arthurai-released https://repository.arthur.ai/repository/charts --username <nexus-username> --password <nexus-password>
helm install argo-workflows-crd arthurai-released/argo-workflows-crd --version 0.19.1-arthur-1
ClickHouse Operator:
Shellhelm repo add arthurai-released https://repository.arthur.ai/repository/charts --username <nexus-username> --password <nexus-password>
helm install clickhouse-operator-crd arthurai-released/clickhouse-operator-crd --version 0.19.2-arthur-1
🚧Please reach out to our Sales Team if you do not have credentials to our Nexus repository.
Installing Cluster-level CRDs for Arthur from Airgap Bundle
If you are in an air-gapped environment with no access to the public internet, the CRD charts are also available in the Airgap bundle provided to you. The instructions below show how to extract the charts from the Airgap bundle.
Shelltar -xvf arthur-<version>.airgap
cd arthur-<version>
tar -xvf app.tar.gz
cd app
Argo Workflows:
Shellhelm install argo-workflows-crd argo-workflows-crd-0.14.0-arthur-2.tgz
ClickHouse Operator:
Shellhelm install clickhouse-operator-crd arthurai-released/clickhouse-operator-crd-0.18.4-arthur-2.tgz
You can verify the CRDs have been installed successfully by executing the following command:
Shellkubectl get crd  grep -iE 'argoclickhouse'
Now that we have the prereqs installed with elevated access, we can now switch over to namespace-scoped access to complete the installation either using the Admin Console using the Admin Console or using the CLI.Updated 3 months ago Table of Contents
CRDs leveraged by Arthur Platform
Installing Admin Console within a Namespace
Installing Cluster-level CRDs for Arthur from Nexus
Installing Cluster-level CRDs for Arthur from Airgap Bundle",arthur_scope_docs,https://docs.arthur.ai/docs/kubernetes-cluster-k8s-install-with-namespace-scope-privileges
25,"Externalizing the Relational Database
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Externalizing the Relational DatabaseSuggest EditsIf desired, you can bring your own Postgres instance to use as your Arthur's relational database. Follow the steps on this page to prepare your Postgres instance.
First, deploy your Postgres instance in your desired environment with appropriate ingress firewall configuration.
Create databases for the Arthur platform.
CREATE DATABASE arthurai
CREATE DATABASE alert_service;
CREATE DATABASE dataset_service;
CREATE DATABASE metric_service;
-- for stand alone instance
CREATE USER arthurai WITH PASSWORD 'SuperSecret';
-- for RDS instance
CREATE ROLE arthurai WITH PASSWORD 'SuperSecret' LOGIN;
REVOKE ALL PRIVILEGES ON DATABASE postgres FROM arthurai;
GRANT ALL PRIVILEGES ON DATABASE arthurai TO arthurai;
GRANT ALL PRIVILEGES ON DATABASE alert_service TO arthurai;
GRANT ALL PRIVILEGES ON DATABASE dataset_service TO arthurai;
GRANT ALL PRIVILEGES ON DATABASE metric_service TO arthurai;
If you have been using the embedded database and you wish to switch to using an external Postgres, backup the embedded
database and restore it to the new external Postgres with pg_dump and pg_restore.
Connecting to the database using SSL/TLS
If your postgres instance supports SSL/TLS connections, and you want to connect to your external database
with an encrypted connection, you simply need to set Database SSL Mode in the initial configuration. By default, this
is set to disable. However, you can enable an encrypted connection using the value require.
{note}An externally managed Postgres instance is strongly recommended for production-grade installs.
Updated 3 months ago Table of Contents
Connecting to the database using SSL/TLS",arthur_scope_docs,https://docs.arthur.ai/docs/externalizing-the-relational-database
26,"Regression
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by RegressionImage Regression Models within ArthurSuggest EditsRegression models predict a numeric outcome. In Arthur, these models are listed under the regression model type.
Some common examples of Image regression are:
Predict user age based on a photograph
Predict house price from the image of the home
Formatted Data in Arthur
Image regression models require two columns: image input and numeric output. When onboarding a reference dataset (and setting a model schema), you need to specify a target column for each inference's ground truth. Many teams also choose to onboard metadata for the model (i.e., any information you want to track about your inferences) as non-input attributes.
Attribute (Image Input)Prediction (numeric)Ground Truth (numeric)Non-Input Attribute (numeric or categorical)image_1.jpg45.3462.42High School Educationimage_2.jpg55.153.2Graduate Degree
Predict Function and Mapping
These are some examples of common values teams need to onboard for their regression models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics.
Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Single Column Ground Truth
output_mapping = {
'prediction_column':'gt_column'}
# Build Arthur Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
## Example prediction function for binary classification
def predict(x):
return model.predict(x)
Available Metrics
When onboarding regression models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a regression model. Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeRoot Mean Squared ErrorPerformanceMean Absolute ErrorPerformanceR SquaredPerformanceInference CountIngestionAverage PredictionIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/image-regression-2
27,"Generative Text
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Generative TextSuggest EditsThis page discusses the basics of setting up a generative text model and onboarding it to Arthur Scope to monitor generative performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"")
Preparing Data for Arthur
Arthur Scope does not need your model object itself to monitor performance - only predictions are required
All you need to monitor your model with Arthur is to upload the predictions your model makes. Here's how to format
predictions for common generative text model schemas.
Use the Arthur data type TOKENS for tokenized input and output texts. Arthur expects a list of strings as below for
tokenized data.
Python[
{
""input_text"": ""this is the raw input to my model"",
""input_tokens"": [""this"", ""is"", ""the"", ""raw"", ""input"", ""to"", ""my"", ""model""],
""output_text"": ""this is model generated text"",
""output_tokens"": [""this"", ""is"", ""model"", ""generated"", ""text""]
}
]
Use the Arthur data type TOKEN_LIKELIHOODS for generated outputs of tokens and their likelihoods. Arthur expects this data type to be formatted as an array of maps from token strings to float likelihoods. Each array index should correspond to one token in the generated sequence. If supplying both TOKENS and TOKEN_LIKELIHOODS for predicted values, the two arrays must be equal in length.
Python[
{
""input_text"": ""this is the raw input to my model"",
""input_tokens"": [""this"", ""is"", ""the"", ""raw"", ""input"", ""to"", ""my"", ""model""],
""output_text"": ""this is model generated text"",
""output_tokens"": [""this"", ""is"", ""model"", ""generated"", ""text""],
""output_probs"": [
{""this"": 0.4, ""the"": 0.5, ""a"": 0.1},
{""is"": 0.8, ""could"": 0.1, ""may"": 0.1},
{""model"": 0.33, ""human"": 0.33, ""robot"": 0.33},
{""generated"": 0.9, ""written"": 0.03, ""dreamt"": 0.07},
{""text"": 0.7, ""rant"": 0.2, ""story"": 0.1}
]
}
]
Arthur supports maps of up to 5 token - float key pairs.
The Arthur SDK provides helper functions for mapping OpenAI response objects or log tensor arrays to Arthur format.
See the SDK reference for more guidance on usage.
Registering a Generative Text Model
Each generative text model is created with a name and with output_type = OutputType.TokenSequence. We also need to specify an input type, which in this case will be InputType.NLP for a text to text model. Here, we register a token sequence model with NLP input specifying a text_delimiter of NOT_WORD:
Pythonarthur_nlp_model = arthur.model(name=""NLPQuickstart"",
input_type=InputType.NLP,
model_type=OutputType.TokenSequence,
text_delimiter=TextDelimiter.NOT_WORD)
Arthur uses the text delimiter to tokenize model input texts and generated texts and track derived insights like sequence length. You can also register your own pre-tokenized values with Arthur for more complex tokenizers. If the registered model uses a custom tokenizer, this is the recommended process outlined in the below section on building a generative text model.
Below, we show different ways of building a generative text model that depends on which attributes you want to monitor for your model.
Building a Generative Text Model
To build a generative text model in the Arthur SDK, use the build_token_sequence_model method on the Arthur Model.
Here we add one attribute for the input text and one attribute for the model output or generated text.
Both of these attributes will have the UNSTRUCTURED_TEXT value type in the ArthurModel after calling this method - this means that this data is saved as a string in each inference.
You should build your model this way if you will only monitor its input and output text and not monitor any of its token processing or likelihood scores.
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text')
Registering Pre-tokenized Text
Optionally, token sequence models also support adding token information. In the below example, the tokenized input text is specified in theinput_token_column and the final tokens selected for the generated output are specified in theoutput_token_column.
This method builds a model with four attributes to monitor for your generative text model.
While the text attributes will still have the UNSTRUCTURED_TEXT value type, the token attributes will have the TOKENS value type means that these attributes are represented as a list of tokens for each inference.
You should build your model this way if you are going to monitor the inferences in their tokenized form as well as in their text form - this may help distinguish performance behaviors due to the base model from performance behaviors due to the tokenization.
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text',
input_token_column='input_tokens',
output_token_column='output_tokens')
Registering Tokens With Likelihoods
You can attach likelihoods to the generated tokens by specifying the output_likelihood_column:
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text',
input_token_column='input_tokens',
output_token_column='output_tokens',
output_likelihood_column='output_probs')
It is not required to specify both a output_token_column and an output_likelihood_column-
if only the output_likelihood_column is specified, greedy decoding will be assumed.
Registering a Ground Truth Sequence
Lastly, adding a ground truth sequence to the model is optional. Ground truth has the same tokenization support as model input and output texts.
Pythonarthur_nlp_model.build_token_sequence_model(input_column='input_text',
output_text_column='generated_text',
ground_truth_text_column='ground_truth_text')
Adding Inference Metadata
We now have a model schema with input, predicted value, and ground truth data defined. Additionally, we can add non-input data attributes to track other information associated with each inference but not necessarily part of the model pipeline.
For generative text models, tracking production signals as performance feedback is often of interest. Here,
we add one continuous attribute and one boolean attribute to measure the success of our model for a use case.
Pythonarthur_nlp_model.add_attribute(name='edit_duration', value_type=ValueType.Float, stage=Stage.NonInputData)
arthur_nlp_model.add_attribute(name='accepted_by_user', value_type=ValueType.Boolean, stage=Stage.NonInputData)
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save()you can call arthur_model.review() the model schema to check that it is correct.
For a TokenSequence model with NLP input, the model schema should look similar to this:
Python
name
stage
value_type
categorical
is_unique
0
text_attr
PIPELINE_INPUT
UNSTRUCTURED_TEXT
False
True
1
pred_value
PREDICTED_VALUE
UNSTRUCTURED_TEXT
False
False
...
2
pred_tokens
PREDICTED_VALUE
TOKEN_LIKELIHOODS
False
False
3
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct use thearthur_model.review(), you are finished registering your model and its attributes, ready to complete onboarding your model.
To finish onboarding your TokenSequence model, the following steps apply, which is the same for NLP models as it is for models of any InputType and OutputType:
finishing_onboarding.md
Sending Inferences
Since we've already formatted the data, we can use the send_inferences method of the SDK to upload the inferences to Arthur. This functionality is also available directly through the API.
Pythonarthur_nlp_model.send_inferences([
{
""input_text"": ""this is the raw input to my model"",
""input_tokens"": [""this"", ""is"", ""the"", ""raw"", ""input"", ""to"", ""my"", ""model""],
""output_text"": ""this is model generated text"",
""output_tokens"": [""this"", ""is"", ""model"", ""generated"", ""text""],
""output_probs"": [
{""this"": 0.4, ""the"": 0.5, ""a"": 0.1},
{""is"": 0.8, ""could"": 0.1, ""may"": 0.1},
{""model"": 0.33, ""human"": 0.33, ""robot"": 0.33},
{""generated"": 0.9, ""written"": 0.03, ""dreamt"": 0.07},
{""text"": 0.7, ""rant"": 0.2, ""story"": 0.1}
]
}
])
Arthur supports maps of up to 5 token - float key pairs.
The Arthur SDK provides a helper function to map tensor arrays into an Arthur format.
See the SDK reference for more guidance on usage
Enrichments
For an overview of configuring enrichments for NLP models, see the {doc}/user-guide/walkthroughs/enrichments guide.
Explainability is not currently supported for TokenSequence models, but anomaly detection will be enabled by default.Updated 3 months ago Table of Contents
Getting Started
Preparing Data for Arthur
Registering a Generative Text Model
Building a Generative Text Model
Adding Inference Metadata
Reviewing the Model Schema
Finishing Onboarding
Sending Inferences
Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/generative-text
28,"CV Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by CV OnboardingSuggest EditsThis page shows the basics of setting up computer vision (CV) models and onboarding Arthur Scope to monitor vision-specific performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"")
Registering a CV Model
Each computer vision model is created with input_type = InputType.Image and with specified width and height
dimensions for the processed images. Here, we register a classification model on 1024x1024 images:
Pythonarthur_cv_model = arthur.model(name=""ImageQuickstart"",
input_type=InputType.Image,
model_type=OutputType.Multiclass,
pixel_height=1024,
pixel_width=1024)
You can send images to the Arthur platform with any dimensions, and we'll keep the original you send as wellas a resized copy in the model dimensions. If you enable explainability for your model, the resized versions will be
passed to it to generate explanations.
The different OutputType values currently supported for computer vision models are classification, regression, and object detection.
Formatting Data
Computer vision models require the same structure as Tabular and NLP models. However, the attribute value for Image attributes should be a valid path to the image file for that inference.
Here is an example of a valid reference_data data frame to build an ArthurModel with:
Python
image_attr
pred_value
ground_truth
non_input_1
0
'img_path/img_0.png'
0.1
0
0.2
1
'img_path/img_1.png'
0.05
0
-0.3
2
'img_path/img_2.png'
0.02
1
0.7
...
3
'img_path/img_3.png'
0.8
1
1.2
4
'img_path/img_4.png'
0.4
0
-0.5
...
Non-Input Attributes
Any non-pixel features to be tracked in images for performance comparison or bias detection should be added as
non-input attributes. For example, metadata about people's identities captured in images for a CV model should be included as non-input attributes.
Reviewing the Model Schema
Before you call arthur_model.save()you can call arthur_model.review() the model schema to check that your data is parsed correctly.
For an image model, the model schema should look like this:
Python
name
stage
value_type
categorical
is_unique
0
image_attr
PIPELINE_INPUT
IMAGE
False
True
1
pred_value
PREDICTED_VALUE
FLOAT
False
False
...
2
ground_truth
GROUND_TRUTH
INTEGER
True
False
3
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Object Detection
Formatting Bounding Boxes
If using an Object Detection model, bounding boxes should be formatted as lists in the form:
[class_id, confidence, top_left_x, top_left_y, width, height]
The first two components of the bounding box list represent the classification being made within the bounding box. Theclass_id represents the ID of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction (0.0 for completely unconfident and 1.0 for completely confident).
The next four components of the bounding box list represent the location of the bounding box within the image: the
top_left_x and top_left_y represent the X and Y pixel coordinates of the top-left corner of the bounding box. These pixel coordinates are calculated from the origin, which is in the top left corner of the image. This means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. The width represents the number of pixels the bounding box covers from left to right and the height represents the number of pixels the bounding box covers from top to bottom.
So using the following model schema as an example:
Python	name
stage
value_type
0	image_attr
PIPELINE_INPUT
IMAGE
1	label
GROUND_TRUTH
BOUNDING_BOX
2	objects_detected
PREDICTED_VALUE
BOUNDING_BOX
a valid dataset would look like
Python#
image_attr
objects_detected
ground_truth
non_input_1
0,
'img_path/img_0.png',
[[0, 0.98, 12, 20, 50, 25],
[0, 1, 14, 22, 48, 29],
0.2
[1, 0.47, 92, 140, 80, 36]]
1,
'img_path/img_1.png',
[[1, 0.22, 4, 5, 14, 32]]
[1, 1, 25, 43, 49, 25]
-0.3
#...
#
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using thearthur_model.review(), you are finished locally configuring your model and its attributes - so you are ready to complete onboarding your model.
To finish onboarding your CV model, the following steps apply, which is the same for CV models as it is for models
of any InputType and OutputType:
finishing_onboarding.md
Enrichments
For an overview of configuring enrichments for image models, see the Enabling Enrichments section.
For a step-by-step walkthrough of setting up the explainability Enrichment for image models, see the Assets Required For Explainability section.Updated 3 months ago Table of Contents
Getting Started
Registering a CV Model
Formatting Data
Non-Input Attributes
Reviewing the Model Schema
Object Detection
Formatting Bounding Boxes
Finishing Onboarding
Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/cv-onboarding
29,"Organizations and Users
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Organizations and UsersSuggest EditsBy default, a new organization, ""My Organization"" is created by the installer for convenience. You can also create new organizations using the API with the superadmin user.
Log in with superadmin credentials
The first thing you will need is a superadmin authorization token. To obtain this, you will need to make a POST request to your organization's /login endpoint with the password set in the Admin Console Config page.
TextJSONPOST /login
{
""login"": ""superadmin"",
""password"": ""<superadmin-password>""
}
The response will look like this:
JSON{
""id"": ""ed1dcb56-352a-4130-8f52-1fd1225196b1"",
""first_name"": ""Super"",
""last_name"": ""Admin"",
""email"": ""[email protected]"",
""username"": ""superadmin"",
""roles"": null,
""active"": true,
""show_intro_sequence"": true,
""help_mode_enabled"": false,
""created_at"": ""2021-08-09T19:57:44.92047Z""
}
The response will also include a set-cookie HTTP header with an authorization token.` Copy the authorization token value and use it in subsequent requests as your auth token.
set-cookie: Authorization=<authorization-token>; Path=/; Expires=Mon, 30 Aug 2021 16:51:07 GMT; Secure;
cURL example
Bashcurl --location --request POST 'https://<your-domain>/api/v3/login' --header 'Content-Type: application/json' --data-raw '{ ""login"": ""superadmin"", ""password"": ""<superadmin-password>"" }' -v
Create a New Organization
To create a new organization, you will need to make a POST request to /organizations with the body specifying the name. Ensure you are using a super admin authentication token to make this request.
TextJSONPOST /organizations
{
""name"": ""my-new-organization""
}
The response will look like this:
JSON{
""id"": ""38faff8b-4edf-44c5-b103-aeca4ea71110"",
""name"": ""my-new-organization"",
""plan"": ""enterprise"",
""created_at"": ""2021-08-18T19:51:22.291504554Z""
}
Remember to save the id; you will need this to add users to your organization.
cURL Example
Bashcurl --location --request POST '<your-domain>/api/v3/organizations' --header 'Content-Type: application/json' --header 'Authorization: <your-superadmin-access-control-token>' --data-raw '{ ""name"": ""my-new-organization"" }' -v
Create The First User in an Organization
To create a new user in the new organization, you will need to make a POST request to
/users?organization_id=<your_organization_id> using a super admin authentication token. You can set the role of the new user to Administrator,
Model Owner, or User. Refer to the Platform Access Control for the description of the roles.
TextJSONPOST /users?organization_id=<your_organization_id>
{
""username"": ""newuser"",
""email"": ""[email protected]"",
""password"": ""G00dP@$$w0rd!"",
""first_name"": ""New"",
""last_name"": ""User"",
""roles"": [
""Administrator""
],
""alert_notifications_enabled"": true
}
The response will look like this.
JSON{
""id"": ""b6554927-9ac4-4531-bf76-fe640b8223b7"",
""first_name"": ""New"",
""last_name"": ""User"",
""email"": ""[email protected]"",
""username"": ""newuser"",
""roles"": null,
""active"": true,
""show_intro_sequence"": true,
""help_mode_enabled"": true,
""created_at"": ""2021-08-18T20:20:18.535137592Z""
}
You can now log in to the dashboard as this user.
cURL Example
This action can be performed as either the super administrator or an organization administrator.
If you'd like to use an organization administrator, repeat the Login API call performed earlier with the credentials for that user and save the returned authorization token.
Bashcurl --location --request POST 'https://<your-domain>/api/v3/users?organization_id=<your-organization-id>' --header 'Content-Type: application/json' --header 'Authorization: <your-superadmin-token>' --data-raw '{ ""username"": ""<username>"", ""email"": ""<email-address>"", ""password"": ""<password>"", ""first_name"": ""<first-name>"", ""last_name"": ""<last-name>"", ""roles"": [ ""Administrator"" ], ""alert_notifications_enabled"": true }'
Adding Additional Users
Although you can continue to create users through the API, it is generally easier to create an Administrator user and then invite additional users from the UI. To add additional users this way, login to Arthur AI with an Administrator user on a web browser and follow these steps:
In the top right corner, you will see a series of icons. Click on the Organization icon that looks like a tree with three nodes.
You will see a dropdown menu. Click on Manage Members
Under the heading, Invite Members, you can type in the email address of the person you wish to invite. That person will receive email instructions for creating a user in the organization.
Once the new user follows the emailed instructions, they can log in with their newly created username and password. You will then be able to view that new user on this Manage Members page.
As an Administrator, you can continue to use this page to manage users and roles.
Adding Existing Users To Existing Organizations via API:
To add an existing user to an existing organization, create a PATCH request to /organizations/<org_id>/users. Supplying in the body a JSON object defining the role (Administrator, Model Owner, or User) you want to add the user with. Any attributes other than roles that are supplied in the body will affect the user across all organizations that the user is a part of.
TextJSONPATCH /organizations/<org_id>/users
[
{
""user_id"": ""b6554927-9ac4-4531-bf76-fe640b8223b7"",
""role"": ""Model Owner""
} ,
{
""user_id"": ""b6554927-9ac4-4531-bf76-fe640b8223b7"",
""role"": ""Model Owner""
}
]
The response will look like this.
JSON{
""updated_user_count"": 10
}
Updated 3 months ago Table of Contents
Log in with superadmin credentials
cURL example
Create a New Organization
cURL Example
Create The First User in an Organization
cURL Example
Adding Additional Users
Adding Existing Users To Existing Organizations via API:",arthur_scope_docs,https://docs.arthur.ai/docs/organizations-and-users
30,,arthur_scope_docs,https://staging.docs.arthur.ai/docs/pages-in-the-arthur-platform
31,"Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ExplainabilityUnderstand why your model is making decisionsSuggest EditsExplainability is one of the core pillars of the Arthur Scope platform. Teams utilize explainability to build trust with valuable insights into how their ML models make decisions. They use it to explore patterns, investigate problems, and ensure compliance with explanations for their model outputs. Finally, they can use it to debug by understanding not only why predictions are being made but also evaluating how changes to model inputs change predictive outputs.
Levels of Explainability in Arthur UI
Explainability can be evaluated at multiple levels within the Arthur platform. These levels include:
Global Explainability (Tabular Models)
Global explainability in machine learning models refers to the ability to understand and explain a model's overall behavior and decision-making process across its entire dataset or population. Teams often use global explainability as a gut check to ensure that the features they consider the most important are the ones driving decisions.
Teams can also drive value from global explanations through interactions with other metrics within Arthur Scope. In the Arthur UI, for example, global explainability can be visualized in conjunction with a data drift of choice at the bottom of the Overview page.
In this figure, we can see that Pay_0 is the most important feature.
Using explainability in conjunction with other metrics can help teams understand and debug models and drive actions such as model retraining or new feature/architecture exploration.
📘Global Explanations are an aggregate of local explanationsWithin Arthur, global explanations are created by aggregating the absolute value of local explanations. The absolute value is taken because, as you will see in local explainability, features can positively or negatively impact whether or not a prediction is made. By taking the average absolute value of all local explanations, global explanations are a measure of feature impact (not necessarily positive or negative).
Local Explainability
Local explainability in machine learning refers to the ability to explain the reasoning behind a model's prediction for a particular instance or input. Teams can access local explainability for inferences within an Arthur Models Inference Tab.
Select Positive Predicted Attribute: Teams need to select a positive predicted attribute when running local explanations because it focuses the explanation on a specific prediction of interest. I.E., we are generating explanations for what causes that prediction to occur.
For example, in binary classification, this helps us read the explanation to understand whether the feature is driving the prediction for that class in a positive way or a negative way by driving the prediction for the other class instead. This is the same for multiclass models; however, unlike binary, for negative importance scores, we cannot attribute this negative importance to the other class. This is because there are more than two classes. It could be driving predictions to any other predicted class instead.
Tabular Inferences
We can visualize local importance scores for each model input feature for tabular inferences.
Here we can see
What If Capabilities
""What If"" local explainability functionality refers to the ability to interactively explore and understand the impact of changing input features on a model's prediction for a particular instance or input. To utilize What-Ifs for a specific inference, a user needs to toggle on the ""What-If"" functionality in the Arthur UI for that inference. Then they can change any feature and evaluate how it changes the model's predictions or relative importance scores.
This example shows how different inputs for this inference affected the local importance scores for the inference and the predicted probabilities.
Text Inferences
For text models, teams can visualize the words that drive predictions. Note: Explainability is only available for classification and regression text models. We do not currently have explainability functionality for generative text models. Instead, teams can look into Token Likelihood for generative models.
In this example, we can see that the model mispredicted consult_history_and_phy instead of urology. We can use explainability to examine the top tokens that drove this misprediction between the two classes.
Is Bag of Words: Bag of words is a text representation technique in natural language processing that converts a document into a collection of its words. When ""Is Bag Of Words"" is turned on, explainability scores are calculated per word - not considering where duplicate words are placed in the text. However, when it is turned off, each duplicate word is treated as a unique token. So, placement in the text is taken into consideration.
Tokenization: Features for text input models are often called tokens. As described further in the Text input section, Arthur default to create word-based tokens based on whitespace. If whitespace does not make sense in your use case, make sure to set up a different tokenization when enabling explainability.
Image Inferences
For image classification/regression models, teams can visualize the segments of the image that drive different predictions.
Querying More Levels of Explainability with the Python SDK
Beyond the explainability functionality provided within the UI, many teams choose to pull custom reports or charts with our Arthur Query Service. Common examples can be found here:
Querying Explainability
Available Post-Hoc Explainers in Arthur
Arthur supports open-source LIME and SHAP for explainability within the platform.
ExplainerAvailable For Tabular ModelsAvailable For Text ModelsAvailable For Image ModelsLIMEXXXSHAPX
Please reference our Enabling Enrichments section in the Model Onboarding section for recommendations regarding which explainer to use.
Available Arthur Schemas
Explainability is available for all model types except Object Detection and Generative Text.Updated 3 months ago What’s NextLearn more about enabling explainability in general in the enabling enrichments section of Model Onboarding. However, if you are having specific troubles check out our pages on troubleshooting and debugging explainability enablement.Enabling EnrichmentsTroubleshooting ExplainabilityTable of Contents
Levels of Explainability in Arthur UI
Global Explainability (Tabular Models)
Local Explainability
Querying More Levels of Explainability with the Python SDK
Available Post-Hoc Explainers in Arthur
Available Arthur Schemas",arthur_scope_docs,https://docs.arthur.ai/docs/explainability
32,"Arthur Permissions by Standard Roles
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur Permissions by Standard RolesSuggest Edits
Model ReaderAlert ManagerUserModel WriterFree Tier UserModel OwnerOrg ManagerAdministratorOrg CreatorSuper AdminuserreadXXXXXXmodelreadXXXXXXtagreadXXXXXXalert_rulereadXXXXXXalertreadXXXXXXalertresolveXXXXXXalert_notification_configreadXXXXXXalert_notification_configwriteXXXXXXalert_notification_configdeleteXXXXXXinsightreadXXXXXXinsightresolveXXXXXXalert_summary_configreadXXXXXXalert_summary_configwriteXXXXXXalert_summary_configdeleteXXXXXXalert_summary_subscriberreadXXXXXXalert_summary_subscriberwriteXXXXXXalert_summary_subscriberdeleteXXXXXXreference_datareadXXXXXraw_datareadXXXXXqueryexecuteXXXXXmetric_queryreadXXXXXXenrichment_configreadXXXXXXorganization_usagereadXXXXXmodel_pinned_columns_globalreadXXXXXXmodelwriteXXXXXmodeldeleteXXXXXtagwriteXXXXXtagdeleteXXXXXalert_rulewriteXXXXXalert_ruledeleteXXXXXmetric_querywriteXXXXXmetric_querydeleteXXXXXreference_datawriteXXXXraw_datawriteXXXXenrichment_configwriteXXXXorganization_metricsreadXXXXservice_accountreadXXXXservice_accountwriteXXXXservice_accountdeleteXXXXground_truthwriteXXXXinvite_userwriteXXXXuserwriteXXXuserdeleteXXXauthentication_configreadXXXauthentication_configwriteXXXalertnotifyXXalert_summarynotifyXXinvite_userwriteXXmodel_pinned_columns_globalwriteXXmodel_pinned_columns_globaldeleteXXorganization_globalreadXXorganization_globalwriteXXorganizationreadXXorganizationdeleteXXcustom_rolesreadXXcustom_roleswriteXXcustom_rolesdeleteXXsystem_configwriteXmodel_statuswriteXenrichment_statuswriteXUpdated about 2 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/arthur-permissions-by-standard-roles
33,"Configuring for High Availability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Configuring for High AvailabilitySuggest EditsIntroduction
The Arthur Platform is built to run in a High Availability configuration, ensuring that the application can function
in the event of a Data Center outage, hardware outages, or other similar infrastructure issues.
In order to take advantage of this, there are a few requirements in how your infrastructure is setup:
Installing across 3 Availability Zones
Specifying the correct Instance Types
Configuring the cluster for Auto-scaling
Notes about this document
{note}Note that this document is written using AWS terminology, as this is one of the environments/infrastructure that Arthur uses
internally for our environments. However, these setup steps should work across various cloud providers using similar features.
{note}Note that this document is written with the pre-requisite that you are installing Arthur in a High Availability configuration.
At the minimum, this means that there should be 3 instances across which Arthur is deployed.
Installing across 3 Availability Zones
In order to ensure continuous operation during an Availability Zone (AZ) outage, Arthur must be installed on a cluster
that has 3 Availability Zones. This ensures that in the event of one AZ outage that the rest of the components can still
operate.
To do this in AWS, create 3 separate Auto-Scaling Groups (ASGs) - one for each AZ. You can configure which AZ an ASG provisions
instances into when you create the ASG.
When Arthur deploys, the stateful services (eg: databases, messaging queues, etc.) will be balanced across the 3 AZs automatically using
kubernetes pod anti-affinity rules (pods will not schedule onto nodes where there already exists another pod that is of the same component).
Specifying the correct Instance Types
Generally speaking, the best way to ensure you have deployed the correct Instance Types is to monitor resource utilization across the cluster
to determine when your services are hitting resource limits.
When initially configuring a cluster for Arthur, we recommend 3 nodes, where each node has at least 16 vCPUs and 64G of RAM (eg: an m5a.4xlarge instance type).
This is a good starting point for a general-purpose cluster that will grow with your production usage.
Configuring the cluster for Auto-scaling
Arthur's stateless components horizontally auto-scale, but in order to take the maximum advantage of this, you will need to configure and install
an additional component that performs node autoscaling (eg: adding more instances).
AWS specifies how to setup cluster autoscaling in their documentation.
Generally speaking, it involves setting up an IAM role and granting permissions to autoscale the cluster, and then installing a third-party component
to perform the autoscaling (eg: cluster-autoscalerUpdated 3 months ago Table of Contents
Introduction
Notes about this document
Installing across 3 Availability Zones
Specifying the correct Instance Types
Configuring the cluster for Auto-scaling",arthur_scope_docs,https://docs.arthur.ai/docs/configuring-for-high-availability
34,"Enrichments
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by EnrichmentsEnrich your monitoring process with state-of-the-art techniquesSuggest EditsEnrichments are additional services that the Arthur platform provides for state-of-the-art proactive model monitoring.
Enrichments in Arthur
Anomaly Detection: monitor and alert on incoming changes to your data distribution (compared to the reference dataset) based on complex interactions between features
Hotspots: automatically illuminate segments of underperformance within incoming inferences
Explainability: understand why your model is making decisions, by computing the importance of individual features from your data on your model's outcomes
Bias Mitigation: methods for model post-processing that improve the fairness of outcomes without re-deploying your model
Once activated, these enrichments are automatically computed on Arthur's backend, with results viewable in the online UI dashboard and queryable from Arthur's API.
Available Enrichments By Different Model Types
Due to the specialized nature of enrichments, they are only available for certain model types.
Model TypeAnomaly DetectionBias MitigationExplainabilityHot SpotsTabular ClassificationXXXXTabular RegressionXXText ClassificationXXText RegressionXXText Sequence Generation (LLM)X (on inputs)CV ClassificationXXCV RegressionXXCV Object DetectionX
Viewing Enabled Enrichments in the UI
You are also able to view the enrichments enabled for your specific model within the Arthur UI by clicking on the details sections of the model's overview page.
Enrichment Workflows
As enrichments are add-ons meant to enrich standard model monitoring, they run on their own workflows within Arthur.Updated 3 months ago Table of Contents
Enrichments in Arthur
Available Enrichments By Different Model Types
Viewing Enabled Enrichments in the UI
Enrichment Workflows",arthur_scope_docs,https://docs.arthur.ai/docs/what-are-enrichments
35,"Data Drift Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Data Drift MetricsTrack the stability of your model by comparing real-world data to a reference datasetSuggest EditsData drift is one of the top causes of model performance decay over time. Data drift measures how much the input data stream to the model changes over time. Tracking data drift over time can help teams identify when models are no longer performing as expected and take proactive steps to maintain or improve their performance. Teams use data drift to illuminate and debug issues like:
Upstream data issues, such as a third-party data provider changing their tagging of missing data fromnull to -1
Data quality issues, such as a faulty sensor tagging a feature 10x lower
Natural demographic shifts, such as an interesting segment of new users from a younger age group using your platform
Sudden changes in relationships, such as the covid-19 pandemic, immediately shift relationships between features and predictions
Selecting Samples For Comparison
Data drift metrics are essentially metrics built to compare two samples of the same data distribution. It calculates how much that distribution drifts from one sample to another. This section talks about choosing those two samples of comparison.
Using a Reference Dataset
The reference dataset is a representative sample of the input features your model ingests. It is used to compute baseline model analytics. By capturing the data distribution you expect your model to receive, Arthur can detect, surface, and diagnose data drift before it impacts results.
More information about reference datasets can be found in the documentation
Examples of how reference data is structured for different model types can be found in the Model Input / Output Types section.
Best practices for selecting a reference dataset can be found in theCreating An Arthur Model Object section
Comparing Points in Time
While the Arthur UI is designed to compare production data against a reference dataset, Arthur can compare any two distribution samples. One of the most popular ways teams use this is by comparing two samples of production data to one another (i.e., how has data from one week, one month, or one quarter ago drifted compared to now). An example of this query can be seen in Querying Data Drift resource.
Type of Data Drift Metrics
We have established that data drift metrics are a comparison of distributions. Now, we can look at the distributions that ML teams often compare.
Feature Drift
Feature drift (also known as covariate drift) refers to the changes in the distribution of input variables to a machine learning model.
Metrics in Arthur for Feature Drift
Arthur allows teams to decide between the most common metrics for feature drift for easy comparison within the UI.
Metrics AvailablePSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest
Prediction Drift
Prediction Drift tracks the discrepancy of your ML model outputs over time.
📘Prediction drift as a proxy for concept driftTeams may be familiar with the different types of distributional drifts that can occur within ML systems, the two most popular being covariate and concept drift. Covariate, known in Arthur as feature drift , refers to the distributions of features going into the model changing. Concept drift refers to a changing relationship between the inputs (features) and outputs (predictions) of the model. While concept drift is best tracked with model accuracy metrics, in situtations where there is not ground truth prediction drift is a common proxy for tracking concept drift. See more in this blog post: https://neptune.ai/blog/concept-drift-best-practices
Metrics in Arthur for Prediction Drift
Since prediction drift is another univariate drift technique, it has all the same options as feature drift for available univariate drift metrics.
Metrics AvailablePSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest
Multivariate Drift (Anomaly Detection)
The previous statistical drift metrics listed for both feature and prediction drift are univariate metrics of data drift. This means they only track one attribute at a time, which is incredibly useful for diagnosing specific issues within a feature. However, sometimes teams want to explore the changing relationships between features. This is the purpose of multivariate drift.
Metrics in Arthur for Multivariate Drift
Currently, in the Arthur platform, Arthur provides one multivariate drift metric based on the average of our model-based anomaly score technique.
Metrics AvailableMultivariate Drift
Using Drift to Drive Action
In practice, data drift is best used as a technique to instigate action within teams. To drive this action, teams have to use different features within Arthur.
Investigating Features in Tabular Models
While data drift is commonly used as a proxy for performance for models that do not receive ground truth soon after the time of prediction (if ever), it is also.
This gif shows an example of how data drift can be used in conjunction with feature importance to track down the root cause of model underperformance.
In the Arthur UI, there are two charts below distributional drift to enable quick evaluation:
Feature Importance x Drift: Using drift in conjunction with feature importance allows teams to understand how impactful drifted features are in modeling predictions
Attribute Distribution: Evaluate the numerical or categorical distributions of the attribute that have drifted to understand the cause of univariate drift.
Anomaly Detection + Multivariate Drift
While univariate drift can be tracked for NonInputData attributes, the most common data distributional drift tracked for Text and Image models is multivariate drift (or anomaly detection). For a more detailed look at how teams use our anomaly detection enrichment to drive value, please refer to the Anomaly Detection page.Updated about 2 months ago Table of Contents
Selecting Samples For Comparison
Using a Reference Dataset
Comparing Points in Time
Type of Data Drift Metrics
Feature Drift
Prediction Drift
Multivariate Drift (Anomaly Detection)
Using Drift to Drive Action
Investigating Features in Tabular Models
Anomaly Detection + Multivariate Drift",arthur_scope_docs,https://docs.arthur.ai/docs/drift-and-anomaly
36,"Installing Arthur Pre-requisites
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Installing Arthur Pre-requisitesSuggest EditsThis is a guide to help you prepare your existing Kubernetes cluster for installing the Arthur platform.
The examples use Helm 3.
Make sure you're in the correct kubectl environment context before running the installer.
Install Ingress
Nginx
Nginx is an enterprise-grade cloud-agnostic open source ingress controller that can be used to access the Arthur application.
Shellhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm upgrade --install -n ingress-system \
--create-namespace \
ingress-nginx \
ingress-nginx/ingress-nginx
[Optional] To monitor nginx using Prometheus and add an AWS managed SSL certificate, create a values.yaml file with following contents -
Shellcontroller:
metrics:
enabled: true
serviceMonitor:
enabled: true
additionalLabels:
release: ""kube-prometheus-stack""
service:
annotations:
service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: ""60""
service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: ""true""
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <ACM certificate ARN>
service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https
service.beta.kubernetes.io/aws-load-balancer-type: <nlb> # optional annotation that creates a Network Load Balancer. Defaults to elb (Classic Load Lalancer)
service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01
service.beta.kubernetes.io/aws-load-balancer-internal: true # optional annotation that creates a non-internet-facing loadbalancer. Defaults to false
targetPorts:
http: ""tohttps""
allowSnippetAnnotations: ""true""
config:
http-snippet: 
server {
listen 2443;
return 308 https://$host$request_uri;
}
use-forwarded-headers: ""true""
ingressClassResource:
name: nginx
enabled: true
default: false
controllerValue: ""k8s.io/internal-ingress-nginx"" # default: k8s.io/ingress-nginx
containerPort:
http: 8080
tohttps: 2443
https: 80
Upgrade or install the helm chart with the values.yaml you created.
Shellhelm upgrade --install -n ingress-system \
--create-namespace \
ingress-nginx \
ingress-nginx/ingress-nginx \
-f values.yaml
If you need to install Nginx in the same namespace as Arthur (not recommended) and want to use our network-policy to restrict ingress to the Arthur application, use the below command to add labels to the pods and services.
The network-policy allows traffic between pods and services that have these labels.
Shellhelm upgrade --install -n arthur --set controller.podLabels.network-app=arthurai,controller.service.labels.network-app=arthurai,defaultBackend.podLabels.network-app=arthurai,.service.labels.network-app=arthurai \
ingress-nginx \
ingress-nginx/ingress-nginx
Look up the hostname for the Ingress and configure it in your DNS (e.g. arthur.mydomain.com).
Shellkubectl get svc -n ingress-system ingress-nginx-controller -ojsonpath='{.status.loadBalancer.ingress[*].hostname}'
Install Prometheus
Installing the Chart
Shellhelm repo add \
prometheus-community \
https://prometheus-community.github.io/helm-charts
helm repo update
Shellhelm upgrade --install -n monitoring \
--create-namespace \
kube-prometheus-stack \
prometheus-community/kube-prometheus-stack \
-f /path/to/values.yaml # see below for contents of this file
helm upgrade --install -n monitoring \
--create-namespace \
prometheus-adapter \
prometheus-community/prometheus-adapter
Note: The values.yaml is not incremental. Helm uses a single values.yaml file, so all configuration must be present in the same values.yaml file. If you are doing this step by step, you must re-apply the prior changes to values.yaml file.
Setting up retention for Grafana and Prometheus
By default, Prometheus and Grafana will use local pod storage to store metrics/dashboards. These metrics/dashboards will be lost if the pod restarts for any reason. To avoid this and keep the metrics/dashboards for a longer period of time, add the following to your values.yaml to use a persistent volume store:
prometheus:
prometheusSpec:
serviceMonitorSelectorNilUsesHelmValues: false
retention: 30d # metrics rolled over every 30 days
retentionSize: 49GiB # size of metrics retained before they are rolled over
storageSpec:
volumeClaimTemplate:
spec:
storageClassName: gp2
accessModes: [""ReadWriteOnce""]
resources:
requests:
storage: 50Gi # size of disk for metrics
grafana:
persistence:
type: pvc
enabled: true
storageClassName: gp2
accessModes:
- ReadWriteOnce
size: 1Gi # size of disk for dashboards
Run the following command to apply the updated configurations (replace the path to the values.yaml file):
Shellhelm upgrade --install -n monitoring \
kube-prometheus-stack \
prometheus-community/kube-prometheus-stack \
-f /path/to/values.yaml
Setting up ingress for Prometheus, AlertManager and Grafana
Grafana and Prometheus are useful to have exposed on an ingress route so that cluster administrators can access real-time telemetry and observe the behavior of the Arthur Platform.
Please note that Grafana comes with a default username and password which should be changed immediately once installed. We also highly recommend installing Prometheus and Grafana within a VPC where the domains will not be exposed to the public internet.
The steps to enable ingress are:
Copy the values.yaml file below with the ingress configuration
Make the following edits to the yaml file that describe your environment:
The ingressClassName
If you installed using the nginx chart above, this should be nginx.
If you are using a custom nginx ingressClass, this will be the name of that ingress class
If you unsure what your ingressClass is called, run kubectl get ingressclass
The URL hostnames that you want to expose for these services
Note - these URL hostnames will need to be published DNS entries
Run the following command to deploy (replace the path to the values.yaml file)
helm upgrade --install -n monitoring \
kube-prometheus-stack \
prometheus-community/kube-prometheus-stack \
-f /path/to/values.yaml
To confirm this is working, navigate to the URL hostname defined in the values.yaml and you should be taken
to the front page for either Grafana or Prometheus.
Change the default password for Grafana.
Here is the values.yaml file that configures ingress for prometheus/grafana:
prometheus:
ingress:
enabled: true
ingressClassName: nginx
# Confirm this is correct or replace me
hosts:
- prometheus.mydomain.com
# Replace me
alertmanager:
ingress:
enabled: true
ingressClassName: nginx
hosts:
- alertmanager.mydomain.com
# Replace me
grafana:
ingress:
enabled: true
ingressClassName: nginx
# Confirm this is correct or replace me
hosts:
- grafana.mydomain.com
# Replace me
Verifying the install
Verify that Prometheus CRDs are installed:
Shellkubectl api-resources  grep monitoring
Verify that Prometheus is up and running:
Shellkubectl --namespace monitoring get pods -l ""release=kube-prometheus-stack""
If everything is installed correctly, the following command should not return ""ServiceUnavailable"":
Shellkubectl get --raw /apis/custom.metrics.k8s.io/v1beta1
Monitoring and Alerting for the Arthur Platform
When you are ready to setup monitoring and Alerting, please reach out to your Arthur support representative and we can share additional details on this.
Prometheus alerts can be configured to trigger when certain rules that are setup to track Prometheus metrics violate for a period of time, which is customizable. For more information see the Prometheus alerting documentation These rules can then be configured to send via a notification channel (eg: email, Slack, etc) so that someone can be notified. This is the job of the Alert Manager.
Install Metrics Server
Example:
Shellhelm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
helm upgrade --install -n monitoring \
--create-namespace \
metrics-server \
bitnami/metrics-server \
--set apiService.create=true \
--set --extraArgs.kubelet-preferred-address-types=InternalIP
Verify that you can retrieve metric snapshots.
Shellkubectl top node
Configure the cluster-autoscaler
In a production environment, it is vital to ensure that there are enough resources (memory and cpu) available for pods to get scheduled on the Kubernetes cluster. Please follow the instructions for your cloud provider to install the cluster-autoscaler on your cluster.
Verify that the cluster-autoscaler is successfully installed.
Shellkubectl get deployments -n kube-system  grep -i cluster-autoscaler
Cloud Provider-specific Configuration
If installing on an existing Amazon AWS EKS, follow the additional steps Deploying on Amazon AWS EKSUpdated 14 days ago Table of Contents
Install Ingress
Nginx
Install Prometheus
Installing the Chart
Setting up retention for Grafana and Prometheus
Setting up ingress for Prometheus, AlertManager and Grafana
Verifying the install
Monitoring and Alerting for the Arthur Platform
Install Metrics Server
Configure the cluster-autoscaler
Cloud Provider-specific Configuration",arthur_scope_docs,https://docs.arthur.ai/docs/kubernetes-preparation
37,"Multiclass Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Multiclass ClassificationSuggest EditsMulticlass classification models predict one class from more than two potential classes. In Arthur, these models fall into the category of classification and are represented by the Multiclass model type.
Some common examples of Text multiclass classification are:
Is the sentiment of this tweet neutral, positive, or negative?
What category does this document fall into?
Similar to binary classification, these models frequently output not only the predicted class but also a probability for each class predicted. The highest probability class is then the predicted output. In these cases, a threshold does not need to be provided to Arthur and it will automatically track the highest probability class as the predicted output.
Formatted Data in Arthur
Text multiclass classification models require three things to be specified in their schema: text input, predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (Text Input)Probability of Prediction AProbability of Prediction BProbability of Prediction CGround TruthNon-Input Attribute (numeric or categorical)Alea iacta est.90.05.05AMaleCastigat ridendo mores..46.14.40BFemalePlenus venter non studet libenter..16.17.71CFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their multi-class classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 2 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Option 1:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
output_mapping_1 = {
'pred_class_one_column':'one',
'pred_class_two_column':'two',
'pred_class_three_column':'three'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=output_mapping_1
)
## Option 2:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
output_mapping_2 = {
'pred_class_one_column':'gt_class_one_column',
'pred_class_two_column':'gt_class_two_column',
'pred_class_three_column':'gt_class_three_column'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping_2
)
## Example prediction function for binary classification
def predict(x):
return model.predict_proba(x)
Available Metrics
When onboarding tabular classification models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a multiclass classification model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the only attribute required to onboard a model is the image attribute. So, it is only possible to monitor non-input attributes for fairness in image models.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/text-multi-class-classification-1
38,"SageMaker
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by SageMakerUsing SageMaker Data CaptureSuggest EditsModels deployed with AWS SageMaker can be configured to automatically push their real-time inferences to Arthur Scope by utilizing SageMaker Data Capture. This guide walks through setting up that integration and utilizing a Lambda function to send Data Capture to log files to be ingested by the Arthur platform.
Prerequisites
The model for which inferences are being ingested has already been onboarded onto Arthur.
The SageMaker model schema matches that of its Arthur model counterpart.
SageMaker Configuration
AWS SageMaker offers two features that enable this Arthur integration: Real-time endpoints & Data Capture. Endpoints are APIs that expose a trained model. Users can use the API to retrieve predictions from the hosted model in the endpoint. Data Capture is a feature that logs the inputs and outputs of each prediction from the hosted model endpoints.
To enable Data Capture in a way that accurately logs all input and output data needed for the Arthur integration, a configuration must be passed in when deploying an endpoint (see below).
Configuring Data Capture through the SageMaker SDK
An extended description of the following configuration can be found in the ""SageMaker Python SDK"" tab of the SageMaker Data Capture documentation.
Pythonfrom sagemaker.model import Model
from sagemaker.model_monitor import DataCaptureConfig
s3_capture_upload_path = f""s3://{bucket-name}/{model-specific-path}/datacapture""
model = Model( ... )
data_capture_config = DataCaptureConfig(
enable_capture=True,
sampling_percentage=100,
destination_s3_uri=s3_capture_upload_path,
capture_options=['REQUEST','RESPONSE'],
)
model.deploy(
data_capture_config=data_capture_config,
...
)
This integration requires that DataCaptureConfig be set such that:
capture_options includes both REQUEST and RESPONSE to record model inputs and outputs for each inference
sampling_percentage is set to 100to comprehensively ingest all new inferences
enable_capture is set to True
Configuring Data Capture through the SageMaker API
Users can also call the CreateEndpoint API to create a real-time endpoint via the API. To ensure that this endpoint is deployed with Data Capture enabled, it must receive an EndpointConfigName that matches an EndpointConfig created using the CreateEndpointConfig API with the following specifications:
{
...,
""DataCaptureConfig"": {
""CaptureContentTypeHeader"": {
""CsvContentTypes"": [ ""string"" ],
""JsonContentTypes"": [ ""string"" ]
},
""CaptureOptions"": [
{
""CaptureMode"": ""Input""
},
{
""CaptureMode"": ""Output""
}
],
""DestinationS3Uri"": ""string"",
""EnableCapture"": true,
""InitialSamplingPercentage"": 100,
""KmsKeyId"": ""string""
},
""EndpointConfigName"": ""string"",
...
}
This integration requires that DataCaptureConfig be set such that:
CaptureContentTypeHeader be specified to an Arthur-supported content type (see section below). If no CsvContentTypes or JsonContentTypes are specified, SageMaker will by default base64 encode when capturing the data. This content type is currently not supported by the Arthur platform.
CaptureOptions be set to both the Input and Output Capture Modes.
EnableCapture be set to true.
InitialSamplingPercentage be set to 100.
Supported Data Formats
AWS SageMaker algorithms can accept and produce numerous MIME types for the HTTP payloads used in retrieving predictions from endpoint-hosted models. The MIME type utilized in an endpoint invocation also corresponds to the format of the Data Captured inference.
The Arthur platform supports the following MIME types/data formats for those types:
MIME Type: text/csv
37,Self-emp-not-inc,227253,Preschool,1,Married-civ-spouse,Sales,Husband,White,Male,0,0,30,Mexico\n24,Private,211129,Bachelors,13,Never-married,Exec-managerial,Other-relative,White,Female,0,0,60,United-States\n
Each inference is represented as an ordered row of comma-separate values, where each value represents a feature in the inference
These features must be specified in the same order as their counterparts in the corresponding Arthur Model
If multiple inferences are included in a single call to invoke_endpoint, each inference is separated by \n
MIME Type: application/json
Arthur currently supports two unique JSON formats, described with examples below.
Option 1: Column-Ordered List of Feature-Values
JSON{
""instances"": [
{
""features"": [1.5, 16, ""testStringA"", false]
},
{
""features"": [2.0, 12, ""testStringB"", true]
}
]
}
Each inference is represented as a new object inside a JSON array
The upper-level key mapping to this inference array is named one of the following: instances, predictions
Each object within this JSON array is a key mapping to an ordered array of features
The second level key mapping to this feature array is named one of the following: features, probabilities
Option 2: Feature-Name Keys to Values Map
JSON{
""predictions"": [
{
""closest_cluster"": 5,
""distance_to_cluster"": 36.5
},
{
""closest_cluster"": 2,
""distance_to_cluster"": 90.3
}
]
}
Each inference is represented as an object inside a JSON array
The upper-level key mapping to this inference array is named one of the following: instances, predictions
Each object within this JSON array has keys representing feature names mapping to their corresponding feature values.
The names of these features cannot be any one of the following: instances, predictions, features, probabilities
Specifying Partner Inference ID on Arthur-Ingested Data Capture Inferences
The Arthur platform enforces that each uploaded inference has a Partner Inference ID, which is a unique identifier used as the matching mechanism for joining ground truth data. Arthur's SageMaker integration populates the Arthur Inference ID from two possible sources in SageMaker. The default is to use SageMaker's EventID, which is a random ID auto-generated by SageMaker for each request. SageMaker's EventID is captured in the eventMetadata/eventId field of the data capture output files. As another option, SageMaker allows Invoke-Endpoint API callees to specify an InferenceId (or inference-id) to a call when using the API, SDK function, or CLI to invoke an endpoint. When InferenceId is specified, SageMaker appends an eventMetadata/inferenceId field to the Data Capture event. Both approaches generate a single eventId or inferenceId for each call to Invoke-Endpoint. If an InferenceId is specified, Arthur will use it as the Arthur Partner Inference ID. Otherwise, it will default to the SageMaker EventId.
One tricky part about SageMaker's Invoke-Endpoint API it allows requesting multiple inferences in a single Invoke-Endpoint API call. In this case, the SageMaker EventId or callee-specified InferenceId would be shared by all inferences in the call and would not be unique. When this occurs, the Arthur integration will append an index number to either the EventId or InferenceId based on the inference order in the call to Invoke-Endpoint.
When ingesting Data Capture inferences from SageMaker, the following table describes the partner inference ID any given inference is assigned on the Arthur platform.
SageMaker Invoke Call
Without Inference ID provided in Invoke Endpoint
With Inference ID provided in Invoke Endpoint
Single Inference in Invoke Endpoint
EventId
InferenceId
Multiple Inferences in Invoke Endpoint
EventId_{index_within_invoke_endpoint_call}
InferenceId_{index_within_invoke_endpoint_call}
InferenceId and EventId refer to the Inference ID and Event ID, respectively, provided when calling Invoke Endpoint either through the API or boto3 SDK.
index_within_invoke_endpoint_call refers to the index of the specific inference within a group of multiple/mini-batch inferences sent through the Invoke-Endpoint call.
For example, for an Invoke-Endpoint call including CSV data 1,2,3,4\n5,6,7,8\n and Inference ID abcdefg-12345, the inference containing the features 1,2,3,4 would have a partner inference ID of abcdefg-12345_0 on the Arthur platform and the inference containing the features 5,6,7,8 would have a partner inference ID of abcdefg-12345_1.
(s3_batch_ingestion)=
AWS Lambda Setup
This section provides an example of a single-Lambda-per-Arthur-model setup. The following code is meant to serve as an example and can be implemented in a variety of ways that fit your organization's tech stack.
This build sets up an S3 object creation Lambda trigger to run the function whenever SageMaker writes a file to the bucket. This sample code will then pull the file and upload it to Arthur. In its current form, the code assumes that all S3 object notifications will be for the single model for the configured ARTHUR_MODEL_ID. See the following sections for the configurations required to use the lambda.
Lambda Function Configurations
To create the Lambda function, go to the AWS Lambda Console, click ""Create function"" and select ""Use a Blueprint"", then search for and select s3-get-object-python.
Then ensure you select or create an Execution Role with access to your Data Capture upload bucket(s).
Finally, ensure the following configurations are set and create the function:
Timeout: 15 min 0 sec (must be set after Lambda function creation in the ""Configuration"" / ""General configuration"" tab)
Environment Variables
ARTHUR_HOST: The host URL for your Arthur deployment (or https://app.arthur.ai/ for SaaS customers)
ARTHUR_MODEL_ID: The ID assigned to the Arthur model to accept the new inferences
ARTHUR_ACCESS_TOKEN: An access token for the Arthur platform
This can be replaced by the retrieval of this token at Lambda runtime
The token must, at the very least, provide raw_data write access
Lambda Function Trigger
Source: S3
Bucket: Your Data Capture configured S3 bucket
Event: All object create events
Prefix: Path to your model's specific Data Capture output directory
Suffix: .jsonl
AWS S3 Trigger Overlap Error
AWS prevents multiple S3 triggers from applying to the same subset(s) of files. Therefore, be careful in specifying your Prefix / Suffix and the files they indicate. For example, in the following setup of S3 triggers, AWS would raise errors because of overlap with .jsonl files in /modelA/datacapture (triggers A + C) as well as overlap with .tar.gz files in /modelA (triggers B + C):
Trigger A: (Prefix: s3://bucket/modelA/datacapture) (Suffix: .jsonl)
Trigger B: (Prefix: s3://bucket/modelA) (Suffix: .tar.gz)
Trigger C: (Prefix: s3://bucket/modelA) (Suffix: Unspecified)
In the above cases, AWS will still successfully create the Lambda but will then raise the following error at the top of their UI:
Your Lambda function ""lambda-function-name"" was successfully created, but an error occurred when creating the trigger: Configuration is ambiguously defined. Cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type.
Lambda Code
Pythonimport urllib.parse
import boto3
import os
import requests
s3 = boto3.client('s3')
ARTHUR_MODEL_ID = os.environ[""ARTHUR_MODEL_ID""]
# 12345678-1234-1234-1234-1234567890ab
ARTHUR_HOST = os.environ[""ARTHUR_HOST""]
# https://app.arthur.ai/
if ARTHUR_HOST[-1] != '/':
ARTHUR_HOST += '/'
# Ensure trailing slash exists
# TODO BY USER
# FILL IN CODE TO RETRIEVE AN ARTHUR API KEY
ARTHUR_ACCESS_TOKEN = os.environ[""ARTHUR_ACCESS_TOKEN""]
ARTHUR_ENDPOINT = f""api/v3/models/{ARTHUR_MODEL_ID}/inferences/integrations/sagemaker_data_capture""
def lambda_handler(event, context):
# Get the object from the event
bucket = event['Records'][0]['s3']['bucket']['name']
key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
try:
s3_object = s3.get_object(Bucket=bucket, Key=key)
datacapture_body = s3_object.get('Body')
request_url = urllib.parse.urljoin(ARTHUR_HOST, ARTHUR_ENDPOINT)
print(f""Request: POST {request_url}"")
response = requests.post(
request_url,
files={'inference_data': ('smdatacapture.jsonl', datacapture_body, s3_object['ContentType'])},
headers={'Authorization': ARTHUR_ACCESS_TOKEN}
)
print(f""Response: {response.content}"")
except Exception as e:
print(e)
print('Error getting object {} from bucket {}. '
'Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
raise e
Summary
With your SageMaker endpoint deployed (with Data Capture configured) and a Lambda function ready for S3 updates, you can send requests to your SageMaker endpoint to generate predictions. The predictions will be logged as files in S3 by Data Capture, and the lambda function will upload the inferences to Arthur, where you can see them in the dashboard.Updated 3 months ago Table of Contents
Prerequisites
SageMaker Configuration
Configuring Data Capture through the SageMaker SDK
Configuring Data Capture through the SageMaker API
Supported Data Formats
Specifying Partner Inference ID on Arthur-Ingested Data Capture Inferences
AWS Lambda Setup
Lambda Function Configurations
Lambda Function Trigger
Lambda Code
Summary",arthur_scope_docs,https://docs.arthur.ai/docs/sagemaker-data-capture
39,"What does ongoing maintenance look like?
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by What does ongoing maintenance look like?Suggest EditsUpdated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/what-does-ongoing-maintenance-look-like
40,"Composing Advanced Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Composing Advanced FunctionsSuggest EditsTo see functions you can use with this syntax, checkout the Aggregation Functions and the Transformation Functions guides.
Any function with a parameter with the type signature [string or nested] is able to accept the following as a value:
a string constant that represents a property of the model
an object of the form:
JSON{
""alias_ref"": ""string"",
""nested_function"": {""..."":
""...""}
}
Only one of alias_ref or nested_function may be present at a time. See the following explanations of each.
Alias References
The alias_ref field allows specifying another selected column's alias to use as input to the function. This example uses an alias_ref to pull in another column to the add function:
JSON{
""select"": [
{
""function"": ""abs"",
""alias"": ""absLoan"",
""parameters"": {
""property"": ""loan""
}
},
{
""function"": ""add"",
""alias"": ""plus2"",
""parameters"": {
""left"": 2,
""right"": {
""alias_ref"": ""absLoan""
}
}
}
]
}
This request returns:
JSON{
""query_result"": [
{
""absLoan"": 55.45,
""plus2"": 57.45
}
]
}
Nested Functions
The nested_function field allows specifying another function definition to use as input. Here's an example of how to calculate absolute error for a regression model. In this example, we pass the nested subtract function as input to the abs function via the nested_function object for the property parameter of abs:
JSON{
""select"": [
{
""function"": ""abs"",
""alias"": ""abs_error"",
""parameters"": {
""property"": {
""nested_function"": {
""function"": ""subtract"",
""alias"": ""error"",
""parameters"": {
""left"": ""Predicted_FICO_Score"",
""right"": {
""alias_ref"": ""ground_truth""
}
}
}
}
}
},
{
""property"": ""Predicted_FICO_Score""
},
{
""property"": ""Consumer_Credit_Score"",
""alias"": ""ground_truth""
}
]
}
This query returns:
JSON{
""query_result"": [
{
""Consumer_Credit_Score"": 660,
""Predicted_FICO_Score"": 688.10004,
""abs_error"": 28.100040000000035
},
{
""Consumer_Credit_Score"": 663,
""Predicted_FICO_Score"": 681,
""abs_error"": 18
},
""...""
]
}
📘If you use the same function multiple times in a query, you need to give each one a distinct ""alias"". Otherwise, the names will conflict.Updated 3 months ago Table of Contents
Alias References
Nested Functions",arthur_scope_docs,https://docs.arthur.ai/docs/composing-functions
41,"Hot Spots
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Hot SpotsAutomatically illuminate underperforming segments in your dataSuggest EditsWhen a system has high-dimensional data, finding the right data input regions, such as troubleshooting, becomes a difficult problem. Hotspots automate identifying regions associated with poor ML performance to significantly reduce the time and error of finding such regions. Arthur Scope utilizes a proprietary tree-based algorithm to search out areas of underperformance and explain them through human-understandable language. Find out more about the algorithm here:
Hot Spots in Arthur Scope
Hot Spots are under the Insights tab in your Arthur Model dashboard.
Time Intervals: To highlight when underperformance occurred, Hot Spots are calculated for specific segments of time. For batch models, this is every batch. For streaming models, this is every 7 days.
Performance Threshold: In the UI, Hot Spots we currently only have accuracy Hot Spots available to detect performance under 40%.
Subgroups: The segments of data that have been identified as underperforming in each Hot Spot
Clicking on a Hot Spot
Subgroup Performance: The accuracy rate for this subgroup of data
Inference Count: How many inferences were included in this subgroup
Subgroup Rules: The rules that define the inferences included in this subgroup. Rules are set up to incorporate any metadata sent to Arthur, not just model features (i.e., can include any non-input attributes provided to Arthur). To ensure actionable Hot Spots, we allow a maximum of 7 rules.
View Inferences Button: Click on this button to go to the inferences deep dive page automatically filtered to match the rules within this subgroup for continued exploration
Status Button: Change the status from New to Acknowledged to alert other team members that you have evaluated this hot spot.
Available Arthur Schemas
Currently only available for Tabular binary and multiclass classification models within Arthur.
Understanding the Algorithm
To learn more about the algorithm used for Hot Spots. Please refer to the Arthur Algorithms documentation section.Updated 3 months ago What’s NextLearn more about enabling enrichments for your model in the Model Onboarding section. Otherwise, click on Explainability to learn about another type of enrichment.Enabling EnrichmentsExplainabilityTable of Contents
Hot Spots in Arthur Scope
Clicking on a Hot Spot
Available Arthur Schemas
Understanding the Algorithm",arthur_scope_docs,https://docs.arthur.ai/docs/hot-spots
42,"Sending Inferences
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Sending InferencesSuggest EditsNow that you have registered your model successfully, you can connect your production pipeline to Arthur.
Creating Arthur Connection
To be able to send inference data to the platform, you will need to create a connection to not only your Arthur platform but also the model the inferences are being tracked for. Information about creating your API key and connecting to the Arthur platform/model objects can be found in the UI Guide.
Formatting Inference Data
The first thing you need to do to send inferences to Arthur is to format the data into a structure Arthur will understand. This will follow a similar structure to the formatting to onboard your reference dataset. However, there are some differences in added attributes to point out.
The following attributes are formatted the exact same as your reference dataset and are required for all inferences sent.
Model Attributes: All features the model uses to create predictions
Model Predictions: Model predictions for each row of data
The next two parameters are required for inference datasets. However, these are not explicitly required for teams onboarding inferences with the Arthur Python SDK.
Inference Timestamp: Typically refers to the time of model prediction
If the inference timestamp is not specified, the SDK will auto-populate this field with the time the inferences were logged into the Arthur platform
Partner Inference ID: A way to match specific inferences in Arthur against your other systems and update your inferences with ground truth labels as they become available in the future. The most appropriate choice for a partner inference ID depends on your specific circumstances, but common strategies include using existing IDs and joining metadata with non-unique IDs.
If you already have existing IDs that are unique to each inference and easily attached to future ground truth labels, you can simply use those (casting to strings if needed).
Another common approach is constructing a partner inference ID from multiple pieces of metadata. For example, if your model makes predictions about your customers at least once daily, you might construct your partner inference IDs as {customer_id}-{date}. This would be easy to reconstruct when sending ground truth labels much later: simply look up the labels for all the customers passed to the model on a given day and append that date to their ID.
If you don’t supply partner inference IDs, the SDK will generate them for you and return them to your send_inferences() call. These can be kept for future reference or discarded if you’ve already sent ground truth values or don’t plan to in the future.
🚧Match Partner Inference ID with Internal Distinct IDsMany ML models in production do not receive ground truth at the time of prediction. The technique that Arthur uses to onboard ground truth later utilizes your partner inference ID. Teams that want to take advantage of metrics, unique Arthur enrichments, and popular reporting workflows that require ground truth will have to have a linkable connection between their data base and ours (through Partner Inference ID).
Finally, the remaining information can be onboarded to Arthur but does not need to be:
Non-Input Attributes: Specifying values for non-input attributes is not required at the time of inference, i.e., you can send inferences with Null non-input attributes
❗️Sent Inference Data is ImmutableYou may not update non-input attribute data (or any inference data for that matter) after sending it to the Arthur platform. The only value that can be updated in Arthur is ground truth, which we will see in the next section.
Send Inferences to Arthur
Inferences are commonly sent to Arthur using our Python SDK but can also be sent with our API or 3rd party integrations.
Python SDK (Quick Integration) : The most common way to log inferences is with the Python SDK. This can be done by adding our send_inferences()to your model's prediction function. Teams must connect to Arthur within their prediction script, run predictions, and send results to Arthur. However, this option would have you add latency to the speed with which your model generates inferences. For more efficient approaches, see options 2 and 3.
Python SDK (Streaming Uploads): If you write your model's inputs and outputs to a data stream, you can add a listener to that stream to log those inferences with Arthur. For example, if you have a Kafka topic, you might add a new arthur consumer group to listen to new events and pass them to the send_inferences() method. If your inputs and predictions live in different topics or you want to add non-input data from another topic, you might use Kafka Streams to join the various topics before sending them to Arthur.
Python SDK (Inference Upload Jobs): Another option is to read data from the rest and send it into the Arthur platform. Depending on their architecture, some teams choose a job or event-driven approach. They often have jobs that look up inferences since the last run, run a script that formats and writes the data into parquet files, and then use the Python SDK function send_bulk_inferences() to send the parquet files to the Arthur platform.
JSON Payload Function: For model deployments that do not have a Python script to run, teams often choose to send inferences to our API through JSON payload.
3rd Party Integration: Arthur has several integrations with third-party services, frameworks, and platforms. Check out our Integrations page to explore more potential integrations.
Python SDKJSON PayloadInference Upload Jobs####################################################
# New code to fetch the ArthurModel
# connect to Arthur
import os
from arthurai import ArthurAI
arthur = ArthurAI(
url=""https://app.arthur.ai"",
access_key=os.environ[""ARTHUR_API_KEY""])
# retrieve the arthur model
arthur_model = arthur.get_model(os.environ[""ARTHUR_PARTNER_MODEL_ID""], id_type='partner_model_id')
####################################################
# your original model prediction function
# which can be on its own as a python script
# or wrapped by an API like a Flask app
def predict():
# get data to apply model to
inference_data = ...
# generate inferences
# in this example, the predictions are classification probabilities
predictions = model.predict_proba(...)
####################################################
#### NEW PART OF YOUR MODEL'S PREDICTION SCRIPT
# SEND NEW INFERENCES TO ARTHUR
arthur_model.send_inferences(
inference_data,
predictions=predictions)
####################################################
return predictions
Updated 3 months ago Table of Contents
Creating Arthur Connection
Formatting Inference Data
Send Inferences to Arthur",arthur_scope_docs,https://docs.arthur.ai/docs/sending-inferences-1
43,"Platform Readiness for Existing Cluster Installs
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Platform Readiness for Existing Cluster InstallsSuggest EditsThe Arthur platform can be installed on an on-prem or cloud-based pre-provisioned Kubernetes cluster, so all the data and controls adhere to existing corporate practices.
Arthur Cluster Installs FAQs
SysAdmin
What kind of privileges/hardware does the SysAdmin installing the Arthur platform need?
The SysAdmin will need a workstation with the following requirements:
running Linux or MacOS. The KOTS CLI installer does not support Windows.
sudo/root access. To install the KOTS CLI plugin for kubectl.
connection to the Kubernetes cluster, using kubectl, and privileges to deploy K8s Objects at either Cluster or Namespace scope (at least).
(recommended) access to the Internet. For downloading the installer, plugins and fetching updates.
How can I download the artifacts required for installing the Arthur platform?
All artifacts required for installing the Arthur platform are available on a customer-specific password-protected portal, which your sales team can give you access to. It is
recommended that the portal is accessible from within your corporate network, since the artifacts are around mutiple GBs in size.
Does my kubernetes cluster need access to the internet?
The Arthur platform can be installed without Internet access, once all the required files are downloaded and available locally. However, we recommend access to the Internet from the Kubernetes cluster for an efficient install and upgrade experience. Please inform your sales team about any network restrictions and optionally, if its possible to {ref}whitelist specific URLs <requirements_for_online_installation>.
Cloud Providers
Which Kubernetes distributions that Arthur supports out-of-the-box?
Arthur is architected to run on any distribution of Kubernetes, however certain commercial distributions are untested. The Arthur application is validated/tested on:
Amazon AWS EKS
Which cloud providers has Arthur been tested on?
The Arthur platform has been tested on the following cloud providers:
Amazon AWS
What container runtimes does Arthur support?
Containers in the Arthur Platform run on the following container runtimes:
docker (slated to be deprecated in Kubernetes 1.24)
containerd
Kubernetes Server
What version(s) of Kubernetes Server does Arthur support?
Please refer to the requirements documentation.
Can the Arthur platform be scoped to a dedicated namespace?
The Arthur platform can be deployed and scoped to a specific namespace, though there are some cluster-level CustomResourceDefinitions that need to be pre-installed. See details here.
What are the minimum resource requirements for operating the Arthur Platform?
Optimal performance of the Arthur platform is ensured on a 6 node cluster (though test clusters can be provisioned with 3 nodes) with each node having 16 CPUs, 32GB Memory (RAM) and 1000 GB Storage with at least 3000 IOPS. However, please reach out to your sales team for a tailored configuration custom to your projected workloads.
Is there a default StorageClass defined on the Kubernetes cluster?
The Kubernetes cluster must have a default StorageClass defined before starting the Arthur platform installation. If a default StorageClass does not exist, adding the storageclass.kubernetes.io/is-default-class: ""true"" annotation to a StorageClass should remedy this requirement.
What Ingress Controller are you planning to use to access the Arthur platform? Is it already installed?
The Arthur platform needs to expose a couple of services so the application is accessible outside the cluster. All compatible Kubernetes Ingress controllers should work, though {ref}Nginx Ingress Controller <k8s_install_prep_install_ingress> installed in a separate namespace is recommended.
Are there any SecurityContext requirements on the Kubernetes cluster?
The Arthur platform is architected to leverage as few permissions as deemed necessary for optimal functioning. No container is run as root. All processes are owned by non-system users. Please reach out to your sales team if you have specific SecurityContext requirements.
Does Arthur support running on SELinux environments?
The Arthur platform requires SELinux to be running in permissive mode, if enabled.
Are there any Network Policies configured on the Kubernetes cluster?
Pods of the Arthur platform will need to communicate with each other. By default, pods can communicate with each other. Please reach out to your sales team if you have custom Network Policies configured on the Kubernetes cluster.
How many IP addresses should be available for the Arthur Platform?
The Arthur platform is architected to be scalable, using resources on-demand. Given the dynamic nature of the infrastructure involved, we recommend at least 128 IP address CIDR blocks attached to the relevant subnets. However, this number can increase as more models are onboarded to the platform.
Are there any namespace-level constraints enforced on the Kubernetes cluster?
Please let your sales team know if there are any constraints configured at the namespace-level on the Kubernetes cluster, as this will help prepare for a smooth installation experience.
Are there any cluster-level constraints enforced on the Kubernetes cluster?
Please let your sales team know if there are any specific cluster-level contraints configured on the Kubernetes cluster, as this will help prepare for a smooth installation experience.
Does the Kubernetes cluster have access to a private/public container registry?
The Kubernetes cluster on which the Arthur platform will be installed must have connectivity to a container registry. The SysAdmin performing the installation must also have Read/Write access to the same container registry.
Does the Kubernetes cluster have access to a private/public Pypi/Conda registry?
The Kubernetes cluster on which the Arthur platform will be installed must have connectivity to a PyPI/Conda registry, which ensures optimum utilization of the features of the platform.
Other Considerations
Does your enterprise have a software procurement process?
Please keep your sales team informed of any software procurement process that maybe in place before installing new software, and potential turnaround times for such processes.
Do you want to deploy Arthur on infrastructure that isn't mentioned above (eg: Cloud Providers, Kubernetes Distributions, etc.)?
If so, please inform your sales team as soon as possible so we can setup an architecture review between your platform team and Arthur's platform team.
Can any of the Arthur platform components be externalized, so its not managed by Arthur?
The platform supports the use of AWS S3 as well as most S3 compatible systems as the Object/Blob store. The embedded metadata database can be replaced by a recent version of Postgres.
A managed service for S3 and/or Postgres is recommended for production-grade installs.
Can the Arthur platform be deployed on a Kubernetes cluster that is shared with other applications?
The Arthur platform has been architected to be highly scalable and reliable. Based on usage (number of models) and load (data ingestion), pods are scaled in short periods of time to ensure efficient operation. As such, if other applications will be installed on the same Kubernetes platform, talk to your sales team about provisioning dedicated nodegroups for the cluster.
Does the Arthur platform support different organizations/business units using the same application?
Yes. See our guide on User and Org Management.
Updated 3 months ago Table of Contents
Arthur Cluster Installs FAQs
SysAdmin
Cloud Providers
Kubernetes Server
Other Considerations",arthur_scope_docs,https://docs.arthur.ai/docs/platform-readiness-for-existing-cluster-installs
44,"Multiclass Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Multiclass ClassificationSuggest EditsMulticlass classification models predict one class from more than two potential classes. In Arthur, these models fall into the classification category and are represented by the Multiclass model type.
Some common examples of Image multiclass classification are:
What breed of dog is in this photo?
What part of the car is damaged in this photo?
Similar to binary classification, these models frequently output the predicted class and a probability for each class predicted. The highest probability class is then the predicted output. In these cases, a threshold does not need to be provided to Arthur, and it will automatically track the highest probability class as the predicted output.
Formatted Data in Arthur
Image multiclass classification models require three things to be specified in their schema: the image input, the predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e., any information you want to track about your inferences) as non-input attributes.
Attribute (Image Input)Probability of Prediction AProbability of Prediction BProbability of Prediction CGround TruthNon-Input Attribute (numeric or categorical)image_1.jpg.90.05.05AMaleimage_2.jpg.46.14.40BFemaleimage_3.jpg.16.17.71CFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their multiclass classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 2 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Option 1:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
output_mapping_1 = {
'pred_class_one_column':'one',
'pred_class_two_column':'two',
'pred_class_three_column':'three'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=output_mapping_1
)
## Option 2:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
output_mapping_2 = {
'pred_class_one_column':'gt_class_one_column',
'pred_class_two_column':'gt_class_two_column',
'pred_class_three_column':'gt_class_three_column'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping_2
)
## Example prediction function for binary classification
def predict(x):
return model.predict_proba(x)
Available Metrics
When onboarding multiclass classification models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a multiclass classification model. Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias. For text models, however, the image attribute is the only attribute required to onboard a model. So, monitoring non-input attributes for fairness in image models is only possible.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/image-multi-class-classification-2
45,"Custom RBAC
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Custom RBACSuggest EditsManaging RBAC and Organizations for SSO users
For customers using SSO (on-prem only), Arthur can set up a fully customizable RBAC.
Please follow the below:
When setting up your identity provider via the YAML configuration, supply a global
role name and set of permissions under globalRoleDefs that your identity provider will authenticate users with.
This configuration will create the global role in the Arthur authorization system when it is applied. See the
{ref}Creating Global Roles for Managing Organizations and RBAC Policies Guide <creating_global_roles_in_arthur_config> for
more information.
That global role can then create custom role mappings for each organization:
During organization creation, including the role configuration JSON (see below, for example) in the request body when calling the organizations endpoint.
After an organization is created, create or add custom_roles by sending the role configuration JSON (see below, for example) in the request body when calling authorization custom roles
endpoint.
Users logging in through your IdP must now have a valid known role in their token when accessing the Arthur
Platform. Arthur will use this role to authenticate that the user is a member of the organization and
determine their permissions.
Managing Roles and Permissions
Understanding Permissions
A permission is a combination of a resource and an action. For example raw_data read, users write, models delete.
For a full list of available permissions. please see Arthur Permissions by Standard Roles.
For a directory of permissions by API endpoint, please see Arthur Permissions by Endpoint.
Create Custom Roles
The Create Organization Custom Roles endpoint is available for customers using SSO to operate on custom roles for each organization. A few notes:
This endpoint only operates on permission scopes within each organization. Permissions with global scope (such as creating a new organization) cannot be granted via this endpoint, those permissions must be assigned to a role with global privileges via the Arthur IdP configuration YAML. See
{ref}Creating Global Roles for Managing Organizations and RBAC Policies Guide <creating_global_roles_in_arthur_config> for more
information.
Roles can have a list of permissions to allow and/or a list of other roles to inherit permissions from.
Role names cannot conflict with Arthur Permissions by Standard Roles
Supplied permissions must be valid, known as Arthur permissions.
Roles can inherit the permissions of other roles that are either Arthur Permissions by Standard Roles or roles also defined in the same organization. Unknown inherited role names will be rejected.
Get Custom Roles
To retrieve a list of roles defined for an organization, use: Get Organization Custom Roles. To filter on specific roles, pass a comma-separated list of role names in a roles query parameter. For example: /authorization/custom_roles?roles=role1,role2. If you wish to return all roles, simply leave out the query parameter or pass ""*"" as role.
Delete Custom Roles
To delete a role or multiple roles from an organization, use Delete Organization Custom Roles. Specify which roles to delete in the JSON request body. For example, to delete a single role:
JSON{
""roles"": [
""role3""
]
}
To delete all roles pass ""*.""
📘If you do not specify an organization_id, this will delete all custom roles you have created
JSON{
""roles"": [
""*""
]
}
Example Role Configuration JSON
Below is an example JSON request body that creates three roles. role1 has 3 permissions defined, role2 gets
additional permission and then inherits the 3 permissions from role1, and role3 inherits the permissions from Arthur's
default ""Model Owner"" role. For more details on the expected schema for each endpoint, see API Documentation.
JSON{
""roles"": [
{
""role_name"": ""role1"",
""permissions"": [
{
""resource"": ""metric_data"",
""action"": ""read""
},
{
""resource"": ""metric_data"",
""action"": ""write""
},
{
""resource"": ""tag"",
""action"": ""read""
}
]
},
{
""role_name"": ""role2"",
""permissions"": [
{
""resource"": ""user_self"",
""action"": ""read""
}
],
""inherited_role_names"": [
""role1""
]
},
{
""role_name"": ""role3"",
""inherited_role_names"": [
""Model Owner""
]
}
]
}
Updated 3 months ago Table of Contents
Managing RBAC and Organizations for SSO users
Managing Roles and Permissions
Understanding Permissions
Create Custom Roles
Get Custom Roles
Delete Custom Roles
Example Role Configuration JSON",arthur_scope_docs,https://docs.arthur.ai/docs/custom-rbac
46,"Access Control
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Access ControlSuggest EditsArthur supports a variety of mechanisms for authentication, who a user is, and authorization, what a user can do (RBAC). By default, Arthur will use a built-in authentication and authorization system. In on-prem installations, cluster administrators can optionally configure an external Identity Provider (IdP) to control user authentication and authorization. See the sections below for an overview of Arthur Standard Access Control and Arthur SSO Access Control.Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/single-sign-on
47,"Arthur Permissions by Endpoint
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur Permissions by EndpointSuggest EditsFor more details on the expected schema for each endpoint, see Authorization API Documentation. If an endpoint is not listed here, there is no authorization restriction on that endpoint, and all authenticated users should be able to access it.
For more details on the expected schema for each endpoint, see authorization. If an endpoint is not listed here, there is no authorization restriction on it, and all authenticated users should be able to access it.
Org, User, and Authorization Related Permissions
PermissionEndpointRESTResource in SystemActionGet all organizations/organizationsGETorganization_globalreadCreate an organization/organizationsPOSTorganization_globalwriteGet an organization/organizations/{organization_id}GETorganizationreadDelete an organization/organizations/{organization_id}DELETEorganizationdeleteAdd users to an organization/organizations/{organization_id}/usersPATCHuserwriteCreate user/usersPOSTuserwriteGet all users/usersGETuserreadGet current user/users/meGETN/AN/AUpdate current user/users/mePATCHN/AN/AGet user/users/{user_id}GETuserreadUpdate user/users/{user_id}PATCHuserwriteDelete user/users/{user_id}DELETEuserdeleteInvite a new user/users/invite_usersPOSTinvite_userwriteGet Current Authentication Info/users/me/auth_infoGETN/AN/AGet Permssions for a role/authorization/permissionsGETN/AN/ACheck Authorization for a permission and role/authorization/authorizePOSTN/AN/ACreate Custom roles for RBAC/authorization/custom_rolesPOSTcustom_roleswriteDelete Custom Roles for RBAC/authorization/custom_rolesDELETEcustom_rolesdeleteGet configured custom roles for RBAC/authorization/custom_rolesGETcustom_rolesreadGet current organization of session/organizations/currentGETN/AN/ASet current organization of session/organizations/currentPUTN/AN/AGet usage/usage/{rollup}GETorganization_metricsread
Model and Data Related Permissions
PermissionEndpointRESTResource in SystemActionGet models/modelsGETmodelreadCreate a model/modelsPOSTmodelwriteGet a model's health score (most recent)/models/healthGETmodelreadGet model/models/{model_id}GETmodelreadUpdate model/models/{model_id}PUTmodelwriteDelete model/models/{model_id}DELETEmodeldeleteGet model bias groups/models/{model_id}/bias_groupsGETmodelreadGet attributes/models/{model_id}/attributesGETmodelreadUpdate attributes/models/{model_id}/attributesPUTmodelwriteEdit model attributes/models/{model_id}/attributesPATCHmodelwriteDelete model attributes/models/{model_id}/attributesDELETEmodeldeleteGet an attribute/models/{model_id}/attributes/{attribute_id}GETmodelreadUpdate an attribute/models/{model_id}/attributes/{attribute_id}PUTmodelwriteDelete an attribute/models/{model_id}/attributes/{attribute_id}DELETEmodeldeleteGet tags/tagsGETtagreadUpdate a tag/tags/{tag_name}PUTtagwriteDelete a tag/tags/{tag_name}DELETEtagdeleteGet model groups/model_groupsGETmodelreadGet a model group/model_groups/{model_group_id}GETmodelreadUpdate a model group/model_groups/{model_group_id}PATCHmodelwriteDelete a model group/model_groups/{model_group_id}DELETEmodeldeleteGet a model group's versions/model_groups/{model_group_id}/versionsGETmodelreadGet latest version for a model group/model_groups/{model_group_id}/versions/latestGETmodelreadRetrieve the prediction and explanation for an inference/models/{model_id}/what_ifPOSTraw_datareadGenerate on-demand explanation for an inference/models/{model_id}/inferences/{partner_inference_id}/explanationGETraw_datareadSave inferences/models/{model_id}/inferencesPOSTraw_datawriteSave inferences from file/models/{model_id}/inferences/filePOSTraw_datawriteUpdate inferences/models/{model_id}/inferencesPATCHraw_datawriteUpdate ground truth/models/{model_id}/ground_truthPATCHground_truthwriteGet image inference/models/{model_id}/inferences/images/{image_id}GETraw_datareadGet batch information for batch of a model/models/{model_id}/batches/{batch_id}GETraw_datareadCloses a batch/models/{model_id}/batches/{batch_id}PATCHraw_datawriteGet inference/models/{model_id}/inferences/query/{partner_inference_id}GETraw_datareadGet all datasets for a model/models/{model_id}/datasetsGETraw_datareadGet reference data information for model/models/{model_id}/reference_dataGETreference_datareadCloses a reference dataset/models/{model_id}/reference_dataPATCHreference_datawriteUploads a parquet or json file containing reference set data/models/{model_id}/reference_dataPOSTreference_datawriteExecute query/models/{model_id}/inferences/queryPOSTquery (✱ see footnote)executeExecute query and retrun data drift values/models/{model_id}/inferences/query/data_driftPOSTqueryexecuteExecute query and return psi buckets/models/{model_id}/inferences/query/data_drift_psi_bucket_calculation_tablePOSTqueryexecuteExecute query and return scatterplot distributions/models/{model_id}/inferences/query/distributionPOSTqueryexecuteGet bias mitigation curves/models/{model_id}/bias_mitigation_curves/attributes/{attribute_id}GETraw_datareadGet bias mitigation curves/models/{model_id}/enrichments/bias_mitigation/curvesGETraw_datareadFind Hotspots/models/{model_id}/enrichments/hotspots/findGETraw_datareadGet metric queries/models/{model_id}/metricsGETmetric_queryreadCreate a metric query/models/{model_id}/metricsPOSTmetric_querywriteGet a metric query/models/{model_id}/metrics/{metric_id}GETmetric_queryreadUpdate a metric query/models/{model_id}/metrics/{metric_id}PUTmetric_querywriteDelete a metric query/models/{model_id}/metrics/{metric_id}DELETEmetric_querydeleteGet all enrichment configs/models/{model_id}/enrichmentsGETenrichment_configreadUpdate an enrichment config/models/{model_id}/enrichmentsPATCHenrichment_configwriteGet explainability config/models/{model_id}/enrichments/explainabilityGETenrichment_configreadUpdate explainability config/models/{model_id}/enrichments/explainabilityPATCHenrichment_configwriteGet anomaly detection config/models/{model_id}/enrichments/anomaly_detectionGETenrichment_configreadUpdate anomaly detection config/models/{model_id}/enrichments/anomaly_detectionPATCHenrichment_configwriteGet bias mitigation config/models/{model_id}/enrichments/bias_mitigationGETenrichment_configreadUpdate bias mitigation config/models/{model_id}/enrichments/bias_mitigationPATCHenrichment_configwriteGet hotspots config/models/{model_id}/enrichments/hotspotsGETenrichment_configreadUpdate hotspots config/models/{model_id}/enrichments/hotspotsPATCHenrichment_configwriteGet Pinned Columns/models/{model_id}/pinned_columnsGETpinned_columnsreadSet Pinned Columns/models/{model_id}/pinned_columnsPUTpinned_columnswriteAdd/Remove Pinned Columns/models/{model_id}/pinned_columnsPATCHpinned_columnswriteDelete Pinned Columns/models/{model_id}/pinned_columnsDELETEpinned_columnsdelete
✱ query requires both queryexecute permissions as well as EITHER raw_dataread OR reference_dataread permissions depending on what the posted query is.
Alert Related Permissions
PermissionEndpointRESTResource in SystemActionCreate an alert rule/models/{model_id}/alert_rulesPOSTalert_rulewriteGet alert rules/models/{model_id}/alert_rulesGETalert_rulereadDelete an alert rule/models/{model_id}/alert_rules/{alert_rule_id}DELETEalert_ruledeleteEdit an alert rule/models/{model_id}/alert_rules/{alert_rule_id}PATCHalert_rulewriteBulk resolve alert for an alert rule/models/{model_id}/alert_rules/{alert_rule_id}/bulk_alertsPATCHalertresolveGet alerts/alertsGETalertreadGet alert counts by model/alerts/model_countsGETalertreadUpdate alert status/alerts/{alert_id}PATCHalertresolveSend manual alert notification/alerts/{alert_id}/notificationsPOSTalertnotifyGet alert notification configurations/alert_notification_configurationsGETalert_notification_configreadCreate an alert notification configuration/alert_notification_configurationsPOSTalert_notification_configwriteGet an alert notification configuration/alert_notification_configurations/{configuration_id}GETalert_notification_configreadEdit an alert notification configuration/alert_notification_configurations/{configuration_id}PATCHalert_notification_configwriteDelete an alert notification configuration/alert_notification_configurations/{configuration_id}DELETEalert_notification_configdeleteGet insights/models/{model_id}/insightsGETinsightreadUpdate the status of insights/models/{model_id}/insightsPATCHinsightresolveGet an insight/models/{model_id}/insights/{insight_id}GETinsightwriteUpdate the status of an insight/models/{model_id}/insights/{insight_id}PATCHinsightresolveGet all alert summary configurations/alert_summary_configurationsGETalert_summary_configreadCreate an alert summary configuration/alert_summary_configurationsPOSTalert_summary_configwriteGet an alert summary configuration/alert_summary_configurations/{configuration_id}GETalert_summary_configreadDelete an alert summary configuration/alert_summary_configurations/{configuration_id}DELETEalert_summary_configdeleteUpdate an alert summary configuration/alert_summary_configurations/{configuration_id}PATCHalert_summary_configwriteGet all alert summary subscribers/alert_summary_configurations/{configuration_id}/subscribersGETalert_summary_subscriberreadCreate an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribersPOSTalert_summary_subscriberwriteGet an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribers/{subscriber_id}GETalert_summary_subscriberreadUpdate an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribers/{subscriber_id}PATCHalert_summary_subscriberwriteDelete an alert summary subscriber/alert_summary_configurations/{configuration_id}/subscribers/{subscriber_id}DELETEalert_summary_subscriberdeleteSend manual alert summary notifications to subscribers/alert_summary_configurations/{configuration_id}/notificationsPOSTalert_summarynotifyUpdated about 2 months ago Table of Contents
Org, User, and Authorization Related Permissions
Model and Data Related Permissions
Alert Related Permissions",arthur_scope_docs,https://docs.arthur.ai/docs/arthur-permissions-by-endpoint
48,"NLP Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by NLP OnboardingSuggest EditsThis page walks through the basics of setting up a natural language processing (NLP) model and onboarding it to
Arthur Scope to monitor language-specific performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur Scope.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"")
Registering an NLP Model
Each NLP model is created with a name and with input_type = InputType.NLP. Here, we register a classification model on text specifying a text_delimiter of NOT_WORD:
Pythonarthur_nlp_model = arthur.model(name=""NLPQuickstart"",
input_type=InputType.NLP,
model_type=OutputType.Multiclass,
text_delimiter=TextDelimiter.NOT_WORD)
The different OutputType values currently supported for NLP models are classification, multi-labeling, and regression.
Text Delimiter
NLP models optionally allow specifying a text_delimiter, which specifies how a raw document is split into tokens.
If a text delimiter is not provided, a default text_delimiter will be TextDelimiter.NOT_WORD. This delimiter will ignore punctuation and tokenize text based only on the words present. However, suppose punctuation and non-word text needs to be considered by your model. In that case, you should consider using other options for a delimiter to ensure those other pieces of text are processed by your NLP model.
For a full list of available text delimiters with examples, see the
TextDelimiter constant documentation in our SDK reference.
Additionally, Arthur supports sending the pre-tokenized text. For steps on registering tokens with Arthur, see our generative text walkthrough.
Formatting Reference/Inference Data
Column names can contain only alphanumeric and underscore characters. The rest of the string values can have
additional characters as raw text.
Python
text_attr
pred_value
ground_truth
non_input_1
0
'Here-is some text'
0.1
0
0.2
1
'saying a whole lot'
0.05
0
-0.3
2
'of important things!'
0.02
1
0.7
3
'With all kinds of chars?!'
0.2
0
0.1
...
4
'But attribute/column names'
0.6
1
-0.6
5
'can only use underscore.'
0.9
1
-0.9
...
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save(), you can call arthur_model.review() the model schema to check that your data is parsed correctly.
For an NLP model, the model schema should look like this:
Python
name
stage
value_type
categorical
is_unique
0
text_attr
PIPELINE_INPUT
UNSTRUCTURED_TEXT
False
True
1
pred_value
PREDICTED_VALUE
FLOAT
False
False
...
2
ground_truth
GROUND_TRUTH
INTEGER
True
False
3
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using arthur_model.review(), you are finished registering your model and its attributes - so you are ready to complete onboarding your model.
To finish onboarding your NLP model, the following steps apply, which is the same for NLP models as it is for models
of any InputType and OutputType:
finishing_onboarding.md
Enrichments
For an overview of configuring enrichments for NLP models, see the {doc}/user-guide/walkthroughs/enrichments guide.
For a step-by-step walkthrough of setting up the explainability Enrichment for NLP models, see
{ref}nlp_explainability.Updated 3 months ago Table of Contents
Getting Started
Registering an NLP Model
Text Delimiter
Formatting Reference/Inference Data
Reviewing the Model Schema
Finishing Onboarding
Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/nlp-onboarding
49,"Aggregation Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Aggregation FunctionsSuggest EditsFor an explanation of nested functions, see the Composing Functions guide
Mathematical Functions
Average
Take the average of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""avg"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<avg_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""avg"",
""alias"": ""avgAge"",
""parameters"": {
""property"": ""age""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""avgAge"": 55.45
}
]
}
back to top
Average of Absolute Values
Take the average of absolute values of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""avgAbs"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<avg_abs_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""avgAbs"",
""alias"": ""avgafloat"",
""parameters"": {
""property"": ""afloat""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""avgafloat"": 55.45
}
]
}
back to top
Max
Take the max of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""max"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<max_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""max"",
""alias"": ""maxAge"",
""parameters"": {
""property"": ""age""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""maxAge"": 55.45
}
]
}
back to top
Min
Take the min of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""min"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<min_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""min"",
""alias"": ""minAge"",
""parameters"": {
""property"": ""age""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""minAge"": 17
}
]
}
back to top
Sum
Take the sum of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""sum"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<sum_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""sum"",
""alias"": ""sumAge"",
""parameters"": {
""property"": ""age""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""sumAge"": 1745
}
]
}
back to top
Variance
Take the variance of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""variance"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<variance> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""variance"",
""alias"": ""varAge"",
""parameters"": {
""property"": ""age""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""varAge"": 10.234
}
]
}
back to top
Standard Deviation
Take the standard deviation of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""stdDev"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<stdDev> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""stdDev"",
""alias"": ""stdDevAge"",
""parameters"": {
""property"": ""age""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""stdDevAge"": 3.199
}
]
}
back to top
Count
Count
Count a property.
Query Request:
JSON{
""select"": [
{
""function"": ""count"",
""alias"": ""<alias_name> [optional string]""
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<count> [int]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""count""
}
],
""filter"": [
{
""property"": ""income"",
""comparator"": ""lt"",
""value"": 90000
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""count"": 5432
}
]
}
back to top
Count If
Conditionally count a property.
Query Request:
JSON{
""select"": [
{
""function"": ""countIf"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<string or number to compare with property>""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<count> [int]""
}
]
}
See Endpoint Overview for a list of valid comparators.
Sample Request:
JSON{
""select"": [
{
""function"": ""countIf"",
""alias"": ""michigan_count"",
""parameters"": {
""property"": ""state"",
""comparator"": ""eq"",
""value"": ""michigan""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""michigan_count"": 5432
}
]
}
back to top
Categories Count
Count the number of categories of a property. To be used for discrete features.
Query Request:
JSON{
""select"": [
{
""function"": ""categoriesCount"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<categories_count> [int]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""categoriesCount"",
""alias"": ""categoriesCountZipcode"",
""parameters"": {
""property"": ""zipcode""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""categoriesCountZipcode"": 732
}
]
}
back to top
Rate
Calculates the rate of a condition on a column.
Query Request:
JSON{
""select"": [
{
""function"": ""rate"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<string or number to compare with property>""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<rate_value> [float]""
}
]
}
See Endpoint Overview for a list of valid comparators.
Sample Request: Calculate the positive predictive rate, with predictions classified as positive when pos_class above .5 (standard definition of positive predictive rate).
JSON{
""select"": [
{
""function"": ""rate"",
""alias"": ""pos_rate"",
""parameters"": {
""property"": ""pos_class"",
""comparator"": ""gt"",
""value"": 0.5
}
}
]
}
Response:
JSON{
""query_result"": [
{
""pos_rate"": ""0.1""
}
]
}
back to top
Distributions
Distribution
Return the distribution of a column with a specified number of bins.
You may specify one of either num_bins or bin_thresholds.
For num_bins,, if there is not enough data, fewer than the specified number of bins will be returned.
For bin_thresholds, there will be a bucket below your lowest bin and a bucket above your highest bin, i.e. n+1 buckets when supplied n thresholds.
Query Request with num_bins:
JSON{
""select"": [
{
""function"": ""distribution"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""num_bins"": ""<number_of_bins> [int]""
}
}
]
}
Query Request with bin_thresholds:
JSON{
""select"": [
{
""function"": ""distribution"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""bin_thresholds"": ""<list_of_floats> [List[int]]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": [
{
""lower"": ""<bin_lower_bound> [float]"",
""upper"": ""<bin_upper_bound> [float]"",
""count"": ""<bin_count> [float]""
}
]
}
]
}
Lower bounds are inclusive while upper bounds are exclusive, so a bucket response like this:
JSON{
""lower"": 30,
""upper"": 50,
""count"": ""<bin_count> [float]""
}
would include values such as 30, 40, or 49.999, but not 50.
Sample Request with num_bins:
JSON{
""select"": [
{
""function"": ""distribution"",
""parameters"": {
""property"": ""FICO_predicted"",
""num_bins"": 50
}
}
]
}
Sample Response with num_bins:
JSON{
""query_result"": [
{
""distribution"": [
{
""lower"": 500,
""upper"": 600,
""count"": 1000
},
{
""lower"": 600,
""upper"": 700,
""count"": 5000
},
{
""lower"": 700,
""upper"": 800,
""count"": 2000
},
{
""lower"": 800,
""upper"": 850,
""count"": 550
}
]
}
]
}
Sample Request with bin_thresholds:
JSON{
""select"": [
{
""function"": ""distribution"",
""parameters"": {
""property"": ""FICO_predicted"",
""bin_thresholds"": [600, 700]
}
}
]
}
Sample Response with bin_thresholds:
JSON{
""query_result"": [
{
""distribution"": [
{
""upper"": 600,
""count"": 1000
},
{
""lower"": 600,
""upper"": 700,
""count"": 5000
},
{
""lower"": 700,
""count"": 2550
}
]
}
]
}
back to top
Quantile
Get the quantile of a column at the specified point. The level parameter between 0 and 1 specifies the cut point, with 0.5 representing the median, 0.9 the 90th percentile, and so on.
Query Request:
JSON{
""select"": [
{
""function"": ""quantile"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""level"": ""<quantile level> [float (0.0, 1.0)]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<quantile> [constant]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""quantile"",
""alias"": ""medianAge"",
""parameters"": {
""property"": ""age"",
""level"": ""0.5""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""medianAge"": 47
}
]
}
Decile
Get the decile of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""deciles"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": {
""max"": ""<max_value> [number]"",
""min"": ""<min_value> [number]"",
""q1"": ""<first decile> [number]"",
""q2"": ""<second decile> [number]"",
""q3"": ""<third decile> [number]"",
""q4"": ""<fourth decile> [number]"",
""q5"": ""<fifth decile> [number]"",
""q6"": ""<sixth decile> [number]"",
""q7"": ""<seventh decile> [number]"",
""q8"": ""<eighth decile> [number]"",
""q9"": ""<ninth decile> [number]"",
}
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""deciles"",
""alias"": ""likelihoodDeciles"",
""parameters"": {
""property"": ""likelihood""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""likelihoodDeciles"": {
""max"": 0.9427181028155817,
""min"": 0.04451819608908539,
""q1"": 0.5422967935750383,
""q2"": 0.6379949435686291,
""q3"": 0.7381396364893235,
""q4"": 0.802484647469614,
""q5"": 0.8282093987794898,
""q6"": 0.8474456853909271,
""q7"": 0.8662622658358209,
""q8"": 0.884452391809424,
""q9"": 0.901106156817971
}
}
]
}
back to top
Arg Values
Arg Max
Take the value of a property at which a different property as at its maximum.
Query Request:
JSON{
""select"": [
{
""function"": ""argMax"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""argument"": ""<attribute_name> [string or nested]"",
""value"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<arg_max_value> [constant]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""argMax"",
""alias"": ""mostExpensiveZipCode"",
""parameters"": {
""argument"": ""zipCode"",
""value"": ""homePrice""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""mostExpensiveZipCode"": 94027
}
]
}
back to top
Arg Min
Take the value of a property at which a different property as at its minimum.
Query Request:
JSON{
""select"": [
{
""function"": ""argMin"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""argument"": ""<attribute_name> [string or nested]"",
""value"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<arg_min_value> [constant]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""argMin"",
""alias"": ""leastExpensiveZipCode"",
""parameters"": {
""argument"": ""zipCode"",
""value"": ""homePrice""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""leastExpensiveZipCode"": 46953
}
]
}
back to top
Any If
Returns the selected property for any row which matches the provided condition. A common use case for this function is querying data with unique identifiers (as described in this guide Grouped Inference Queries).
Query Request:
JSON{
""select"": [
{
""function"": ""anyIf"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<string or number to compare with property>"",
""result"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<result> [constant]""
}
]
}
See Endpoint Overview for a list of valid comparators.
Sample Request:
JSON{
""select"": [
{
""function"": ""anyIf"",
""alias"": ""some_michigan_price"",
""parameters"": {
""property"": ""state"",
""comparator"": ""eq"",
""value"": ""michigan"",
""result"": ""homePrice""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""some_michigan_price"": 459312
}
]
}
back to top
Regional Feature Importance
Returns the regional importance score for a particular attribute. This is the average of the absolute value of the explainability value for all the inferences. This is only available if Explainability has been enabled for the model.
Query Request:
JSON{
""select"": [
{
""function"": ""regionalFeatureImportance"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""attribute_name"": ""<pipeline_input_attribute_name> [string]"",
""predicted_attribute_name"": ""<predicted_attribute_name> [string]"",
""explanation_algorithm"": ""[limeshap]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<feature_importance_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""regionalFeatureImportance"",
""parameters"": {
""attribute_name"": ""AGE"",
""predicted_attribute_name"": ""prediction_0"",
""explanation_algorithm"": ""lime""
}
}
],
""filter"": [
{
""property"": ""inference_timestamp"",
""comparator"": ""gte"",
""value"": ""2020-12-01T10:00:00Z""
},
{
""property"": ""inference_timestamp"",
""comparator"": ""lt"",
""value"": ""2020-12-22T11:00:00Z""
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""AGE"": 0.001406118694451489
}
]
}
back to top
Regional Feature Importances
Returns the regional importance scores for all of the pipeline input attributes. This is the average of the absolute value of the explainability value for all the inferences for each pipeline input attribute. This is only available if Explainability has been enabled for the model.
Query Request:
JSON{
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""predicted_attribute_name"": ""<predicted_attribute_name> [string]"",
""explanation_algorithm"": ""[limeshap]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<attribute_name>"": ""<feature_importance_value> [float]"",
""<attribute_name>"": ""<feature_importance_value> [float]"",
""<attribute_name>"": ""<feature_importance_value> [float]"",
""<attribute_name>"": ""<feature_importance_value> [float]"",
""<attribute_name>"": ""<feature_importance_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""regionalFeatureImportances"",
""parameters"": {
""predicted_attribute_name"": ""prediction_0"",
""explanation_algorithm"": ""lime""
}
}
],
""filter"": [
{
""property"": ""inference_timestamp"",
""comparator"": ""gte"",
""value"": ""2020-12-01T10:00:00Z""
},
{
""property"": ""inference_timestamp"",
""comparator"": ""lt"",
""value"": ""2020-12-22T11:00:00Z""
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""explainer_attribute"": ""PAY_0"",
""regionalFeatureImportance"": 0.055036517803945396
},
{
""explainer_attribute"": ""PAY_2"",
""regionalFeatureImportance"": 0.026880464089676884
},
{
""explainer_attribute"": ""PAY_3"",
""regionalFeatureImportance"": 0.024027941129616155
},
{
""explainer_attribute"": ""LIMIT_BAL"",
""regionalFeatureImportance"": 0.022367882999425544
},
{
""explainer_attribute"": ""PAY_AMT2"",
""regionalFeatureImportance"": 0.019145911247181836
},
{
""explainer_attribute"": ""PAY_AMT1"",
""regionalFeatureImportance"": 0.019052984358794038
},
{
""explainer_attribute"": ""PAY_AMT3"",
""regionalFeatureImportance"": 0.012942233755875516
},
{
""explainer_attribute"": ""PAY_5"",
""regionalFeatureImportance"": 0.011911442095349226
},
{
""explainer_attribute"": ""PAY_4"",
""regionalFeatureImportance"": 0.010464962507962139
},
{
""explainer_attribute"": ""PAY_6"",
""regionalFeatureImportance"": 0.00891260261770653
},
{
""explainer_attribute"": ""BILL_AMT4"",
""regionalFeatureImportance"": 0.007211523900878019
},
{
""explainer_attribute"": ""BILL_AMT5"",
""regionalFeatureImportance"": 0.006279087267628024
},
{
""explainer_attribute"": ""BILL_AMT1"",
""regionalFeatureImportance"": 0.006221344024007549
},
{
""explainer_attribute"": ""PAY_AMT4"",
""regionalFeatureImportance"": 0.005310133724715099
},
{
""explainer_attribute"": ""PAY_AMT6"",
""regionalFeatureImportance"": 0.004135643379284112
},
{
""explainer_attribute"": ""MARRIAGE"",
""regionalFeatureImportance"": 0.004089899824740581
},
{
""explainer_attribute"": ""EDUCATION"",
""regionalFeatureImportance"": 0.003931984513777395
},
{
""explainer_attribute"": ""PAY_AMT5"",
""regionalFeatureImportance"": 0.0033734464617669853
},
{
""explainer_attribute"": ""SEX"",
""regionalFeatureImportance"": 0.0029222783744727687
},
{
""explainer_attribute"": ""BILL_AMT6"",
""regionalFeatureImportance"": 0.002707692309829875
},
{
""explainer_attribute"": ""BILL_AMT2"",
""regionalFeatureImportance"": 0.001955133839877692
},
{
""explainer_attribute"": ""BILL_AMT3"",
""regionalFeatureImportance"": 0.001779632159224476
},
{
""explainer_attribute"": ""AGE"",
""regionalFeatureImportance"": 0.001406118694451494
}
]
}
rankedItemMaxK
Returns the maximum position of a specified ranked list item across all ranked list data for a model. Maximum position in this case refers to the largest index the item sits at in a ranked list array, not the highest ranking. If the specified item is not in any array, the function will return None. For use with ranked list type attributes only. The property name should have the _item_id suffix appended to it.
Query Request:
JSON{
""select"": [
{
""function"": ""rankedItemMaxK"",
""parameters"": {
""property"": <ranked_list_attr_name>_item_id,
""item_filter"": <item_identifier>
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""rankedItemMaxK"": <maxK>
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""rankedItemMaxK"",
""parameters"": {
""property"": ""recommendations_item_id"",
""item_filter"": ""item1""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""rankedItemMaxK"": 4
}
]
}
back to topUpdated about 2 months ago Table of Contents
Mathematical Functions
Average
Average of Absolute Values
Max
Min
Sum
Variance
Standard Deviation
Count
Count
Count If
Categories Count
Rate
Distributions
Distribution
Quantile
Decile
Arg Values
Arg Max
Arg Min
Any If
Regional Feature Importance
Regional Feature Importances
rankedItemMaxK",arthur_scope_docs,https://docs.arthur.ai/docs/aggregation-functions
50,"Arthur Algorithms
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur AlgorithmsSuggest EditsAnomaly Detection
For an explanation of how to enable Anomaly Detection with Arthur, please see the Anomaly Detection
Anomaly scores are computed by training a model on the reference set you provide to Arthur and using that model to assign an Anomaly Score to each inference you send to Arthur. Scores of 0.5 are given to ""typical"" examples from your reference set, while higher scores are given to more anomalous inferences, and lower scores are given to instances that the model judges as similar to the reference data with high confidence.
How Anomaly Detection Works
We calculate Anomaly Scores with an Isolation Forest algorithm. This algorithm works by building what is essentially a density model of the data by iteratively isolating data points from one another. Because anomalies tend to be farther away from other points and occur less frequently, they are easier to isolate from other points, so we can use a data point's ""ease of isolation"" to describe its anomaly. The method is based on the paper linked here.
The Isolation Forest ""method takes advantage of two [quantitative properties of anomalies]:
i) they are the minority consisting of fewer instances and
ii) they have attribute-values that are very different from those of normal instances.
In other words, anomalies are 'few and different', which make them more susceptible to isolation than normal points.""
The idea is to build a binary tree that randomly segments the data until each instance can be uniquely selected (or a maximum height is reached). Anomalous instances will take fewer steps to become isolated on the tree because of the abovementioned properties.
In the example above, we can see that the in-distribution x_i takes many more steps to reach than the out-of-distribution x_0
Of course, using a single randomly generated tree would be noisy. So we train multiple trees to construct an Isolation Forest of multiple trees and use the average path length, noting that average path lengths converge:
The Isolation Forest algorithm is highly efficient compared to other density estimation techniques because the individual trees can be built from data samples without losing performance.
When you add a reference set to your model in Arthur, we fit an Isolation Forest model to that data to compute an anomaly score for the inferences your model receives.
Generating Anomaly Scores
The path length, or the number of steps taken to reach the partition that a data point belongs to, varies between 0 and n-1 where n is the number of data points in the training set. Following our intuition above, the shorter the path length, the more anomalous a point is. To measure anomaly, an anomaly score between 0 and 1 is generated by normalizing the path length by the average path length and applying an inverse logarithmic scale.
In particular, the anomaly scores algorithm shown below is the average path length to data pointx from a collection of isolation trees and c(n) is the average path length given the number of data points in the dataset n.
Every inference you send to Arthur will be evaluated against the trained Isolation Forest model and given an anomaly score.
This can be seen in the anomaly score contour of 64 normally distributed points:
At Arthur, we also visualize the distribution of anomaly scores among all the inferences your model has retrieved since training. When you select an inference in our UI you'll see where it falls on this distribution:
Interpreting Anomaly Scores
The resulting anomaly scores can be interpreted in the following way:
if instances return s very close to 1, then they are definitely anomalies
if instances have s smaller than 0.5, then they are quite safe to be regarded as normal instances
if all the instances return s approximately at 0.5, then the entire sample does not really have any distinct anomaly
Bias Mitigation
If you are interested in mitigation capabilities, we're happy to discuss your needs and what approaches would work best for you. Within the Arthur product, we offer postprocessing methods and encourage the exploration of alternate (pre- or in-processing) methods if your data science team has the bandwidth to do so.
We currently have one postprocessing method for the product: the Threshold Mitigator.
Threshold Mitigator
This algorithm is an extension of the Threshold Optimizer implemented in the fairlearn library, which in turn is based on Moritz Hardt's 2016 paper introducing the method.
The intuition behind this algorithm is based on the idea that if the underlying black box is biased, then different groups have different predicted probability distributions — let's call the originally predicted probability, for example, x the score s_x. Then, for a fixed threshold t, P(s_x > t  x in A) > P(sx > t  x in B), where _B indicates membership in the disadvantaged group. For example, the default threshold is t = 0.5, where we predict Y^ = 1 if s > 0.5.
However, we might be able to mitigate bias in the predictions by choosing group-specific thresholds for the decision rule.
So, this algorithm generally proceeds as follows:
Try every possible threshold t from 0 to 1, where for any x, we predict Y^ = 1 if and only if s_x > t.
Calculate each threshold's group-specific positivity, true positive, and false positive rates.
Let's say we're trying to satisfy demographic parity. We want the group-wise positivity rates to be equal —
P(Y^ = 1  A) = P(Y^
= 1  B). Then, for any given positivity rate, the threshold t_a that achieves this positivity rate might differ from the threshold t_b that achieves this positivity rate.
This also hints at why we need the notion of tradeoff curves rather than a single static threshold: if we want to achieve a positivity rate of 0.3, we'll need different t_a, t_b than if we wanted to achieve a positivity rate of 0.4.
Then, once we've generated the curves, we pick a specific point on the curve, such as a positivity rate of 0.3. When making predictions on the future
and using the corresponding t_a, t_b to make predictions
What's Compatible?
Fairness constraints: We can generate curves satisfying each of the following constraints (fairness metrics):
Demographic Parity (equal positivity rate)
Equal Opportunity (equal true positive rate)
Equalized Odds (equal TPR and FPR)
Sensitive attributes: This algorithm can handle sensitive attributes that take on any number of discrete values (i.e., not limited to binary sensitive attributes). We can specify buckets for the continuous values for continuous sensitive attributes, then treat them like categorical attributes.
The tradeoff curve
Generating the Curve
Each group has its own curve.
To generate a single curve, we generate all possible thresholds (i.e. [0, 0.001, 0.002, 0.003, ..., 0.999, 1.00]) — default 1000 total thresholds; as described above, we calculate many common fairness metrics for each of those thresholds for the result if we were to use that threshold to determine predictions.
Visualizing the Curve
The tradeoff curve will look different depending on the fairness constraint we're trying to satisfy.
For equal opportunity and demographic parity, where a single quantity must be equalized across the two groups, the tradeoff curve plots the quantity to equalize on the x-axis and accuracy on the y-axis.
Understanding the Curve Artifact
The curve artifact can be pulled down according to our docs here. This mitigation approach is somewhat constrained because we will generate a separate set of curves for each sensitive attribute to mitigate on; for each constraint to follow. Then, each set of curves entails a single curve for each sensitive attribute value.
Conceptually, the curves are organized something like the one below; in the API, however, you'll be getting a single < list of (x, y) coordinate pairs that define the curve > at a time, with additional information about the attribute being mitigated, the constraint being targeted, and the feature value the specific list of pairs is for.
**mitigating on gender -- demographic parity**
gender == male
< list of (x, y) coordinate pairs that define the curve >
gender == female
< list of (x, y) coordinate pairs that define the curve >
gender == nb
< list of (x, y) coordinate pairs that define the curve >
**mitigating on gender -- equalized odds**
gender == male
< list of (x, y) coordinate pairs that define the curve >
gender == female
< list of (x, y) coordinate pairs that define the curve >
gender == nb
< list of (x, y) coordinate pairs that define the curve >
**mitigating on age -- demographic parity**
age < 35
< list of (x, y) coordinate pairs that define the curve >
age >= 35
< list of (x, y) coordinate pairs that define the curve >
Summary of Curves by Constraint
Equal opportunity (equal TPR): TPR vs. accuracy; the accuracy-maximizing solution is the point along the x-axis with the highest accuracy (y-axis) for both groups.
Demographic parity (equal selection rates): selection rate vs. accuracy; the accuracy-maximizing solution is the point along the x-axis with the highest accuracy (y-axis) for both groups.
Equalized odds (equal TPR and FPR): FPR vs. TPR (canonical ROC curve); the accuracy-maximizing solution is the point on the curves that are closest to the top left corner of the graph (i.e., low FPR and high TPR).
Choosing a Set of Thresholds (Point on Curve)
As mentioned above, a single ""point"" on the solution curve for a given constraint and sensitive attribute corresponds to several thresholds mapping feature value to a threshold. But how do we choose which point on the curve to use?
The default behavior — and, in fact, the default in fairlearn — is to automatically choose the accuracy-maximizing thresholds subject to a particular fairness constraint. This might work in most cases; however, accuracy is not necessarily the only benchmark an end user will care about. One client, for example, needs to satisfy a particular positivity rate and is fine with sacrificing accuracy to do so. What our feature introduces that goes beyond what's available in fairlearn is the ability to try different thresholds; see what hypothetical predictions would be; and see what hypothetical results/metrics (e.g., positivity rate, TPR, etc.) would look like if
a certain set of thresholds was applied. The ultimate choice of thresholds is up to data scientists' needs, etc.
Hotspots
For an explanation of how to enable Hotspots with Arthur, please see the Hot Spots
We might have an ML model deployed in production and some monitoring in place. We might notice that performance degrades from classic performance metrics or drift monitoring combined with explainability techniques. We’ve identified that our model is failing; the next step is to identify why our model is failing.
This process would involve slicing and dicing our input data that caused model degradation. We want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks or retraining our models on those regions.
This basically boils down to the time-consuming task of finding needles in a haystack. What if we could reverse engineer the process and surface all of the needles, i.e., input regions associated with poor performance, directly to the user?
We can with Hotspots!
How Hotspots Works
The outputs of a model are encoded as a special classification task to partition data to separate out the poor-performing data points from the correct predictions/classifications on a per-batch basis for batch models or a per 7-day window for streaming models.
Hotspot enrichments are used to surface input regions where the model is currently underperforming for inferences. Hotspots are extracted from a custom Arthur tree model, where nodes are associated with particular input regions and have associated performance metrics, e.g., a node with 70% accuracy with data points where variable X is less than 1000. Nodes are candidates for hotspots. Depending on user-specified thresholds, e.g., a threshold of 71% accuracy, the tree is traversed until all nodes with less than 71%, such as our node with 70% accuracy, have been identified and returned to the user as hotspots, not including the hotspot nodes' children, which would be either (1) purer than the hotspot node and therefore in further violation of the, e.g., 71% threshold or (2) pure nodes with correct inferences, which are not of interest to the user for remediation purposes.
See our blog post here for a more in-depth overview.Updated 5 months ago Table of Contents
Anomaly Detection
How Anomaly Detection Works
Generating Anomaly Scores
Interpreting Anomaly Scores
Bias Mitigation
Threshold Mitigator
Hotspots
How Hotspots Works",arthur_scope_docs,https://docs.arthur.ai/docs/arthur-algorithms
51,"Managing Alerts
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Managing AlertsSuggest EditsAny team that has suffered from alert fatigue knows that alerts are only as good as your ability to manage and take action on them. Knowing this, there are a few ways in which teams can work and interact with alerts within Arthur, all focused on the ability for teams to derive value and track their alerts.
Recent Organization Alert Overview
Upon opening the UI, teams begin to interact with alerts at an organizational level. On the control panel home page, teams can see the most recent critical alerts triggered within their organization and a high-level description.
From this page, teams can quickly assess which model's to dive deeper into from a recent alerting perspective.
Alert Management Per Model
There is an Alert tab within each Arthur Model dashboard, as well, that provides a management dashboard for all alerts triggered for that particular model.
Here teams can:
View Recent History of Alerts: Visualize the recent patterns of alerting over the past two weeks. Understand when new critical alerts or warnings appeared and in what quantity.
Create Alert Rules: Described in the Alerting overview tab. This allows teams to create some of the most common alert rules that we see in an easy, predefined structure.
Filter and Sort Triggered Alerts: Sort alerts by metrics and severity to immediately find the alerts of interest.
Operationalize Alert Management: Teams can tag alerts into different categories through the JIRA board-like structure. While teams can determine the best way to operationalize these categories for these teams, we typically see:
CategoryUse Case DescriptionNewNew alerts coming into the model that require investigation.AcknowledgedAlerts being investigated or looked into by a team member, reducing the repetition of investigation or work within teams.ResolvedAlerts that have been investigated and resolved, either through the team triggering retraining, solving an upstream data issue, or even flagging the alert as a false positive and potentially changing the threshold/alert rules
Managing Alert Rules
Within each model's alerting dashboard, teams can view and manage all model alert rules by clicking the Manage Alert Rules button. Understand all alert rules created for your model (enabled or disabled), their severity, and the date they were last updated in one central place.
Investigating a Specific Alert - (Root Cause Analysis)
After clicking on an alert, users are taken to an overview page for the alert. This page describes more about this alert, including a historical list of when it was triggered. It also provides three common analysis charts for the alert to allow teams to begin their root cause analysis into the cause.
These charts include:
The inference count over time: See how many inferences have been affected in creating this alert
Alerted metric over time: See how the specific metric of interest
Alerted metric over time (for the attributes that experienced the most drift): Immediately evaluate if recent data distributional drifts may have been a root cause of the alert
Beyond some initial drilled-down charts for exploration, this alert details tile provides insights
View in Overview: Jump to the Model Overview Tab in the Arthur UI to investigate further. This tab will have global filtered automatically applied to the window of time and/or data segments the alert was triggered on.
Send to Relevant Team-members or stakeholders: You can easily share the alert with others by hitting the send button on the top corner. This alert can be shared by email to anyone who has enabled email alert notifications for this model. Otherwise, users can select Copy Link to copy a link to this alert details tile.Updated 3 months ago Table of Contents
Recent Organization Alert Overview
Alert Management Per Model
Managing Alert Rules
Investigating a Specific Alert - (Root Cause Analysis)",arthur_scope_docs,https://docs.arthur.ai/docs/alerting-dashboard
52,"Default Access Control
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Default Access ControlSuggest EditsIn both SaaS and On-prem installations, Arthur ships with a built-in access control system that can be used to manage users, permissions, and access to organizations. This system has different capabilities than the SSO-based paradigm. If your installation is using SSO, please see the Platform Access Control.
Authentication
Users authenticate to Arthur using a username and password set when their account is created and can be changed later in the UI. Users can also use the Login API endpoint to retrieve a token with Arthur APIs.
Applications and automated systems can authenticate with Arthur using API keys, which can be created in the Arthur UI from the organization menu in the upper right corner, then clicking on Manage API Keys.
🚧Note on using Session KeysIt is not recommended to use API-keys for non-automated use cases as they are not tied to user
identities and can obscure who is performing actions. As a best practice, use API keys minimally only in the systems that need automated access, and be sure to create a rotation practice to ensure safe keeping.
Authorization (RBAC)
The Arthur standard access control system uses role-based access control (RBAC) with a set of pre-defined roles. The available roles for users are User, Model Owner, Administrator, and SuperAdmin. If enrolled in multiple organizations, the user can have a different role in each organization. For a full list of permissions for these 4 standard roles, please reference Arthur Permissions by Standard Roles.
User: Has read-only access to the models and data within the organization.
Model Owner: Can onboard new models in the enrolled organization as well as send data, including reference data,
inferences, and ground truth.
Administrator: Organization-level administrator that has access to manage users and models within the organization.
Super Admin: Has full access to all data, models, and actions on the platform. Can create new organizations and manage users. Only available on-prem.
📘Custom RolesIf your installation uses SSO, you can take advantage of creating custom roles to fine-tune user
access to Arthur resources. See the documentation on Custom RBAC for more information.
Adding Users to an Organization in the UI
To complete this section, you must have the ""Administrator"" role in your organization.
Click on the organization menu in the upper right corner and then ""Manage Members."" From this screen, you can enter the emails of additional users to add to the organization, manage the roles of existing users, and remove users from the organization.
🚧In order for email-based user invites to work, your installation must have an email integration set up. If not, you can use the Arthur APIto create user accounts directly in your organization.
Adding Users to an Organization in the API
Arthur also supports managing users via automated workflows using the REST API. To create a user in your organization, you must have Administrator privileges or access to the super admin user for your Arthur on-prem installation. The following APIs are helpful for managing users:
Create User
Update User
Send User Invites
Switching Between Organizations
If a user is invited to multiple organizations, they can switch between them in the UI. Users can click on the organization menu in the upper right corner and choose one of the other available organizations from that menu to switch to it. If no other organizations appear, that user cannot access any other organizations.Updated 3 months ago Table of Contents
Authentication
Authorization (RBAC)
Adding Users to an Organization in the UI
Adding Users to an Organization in the API
Switching Between Organizations",arthur_scope_docs,https://docs.arthur.ai/docs/default-access-control
53,"Welcome to Arthur Scope!
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Welcome to Arthur Scope!As your team's data science operations center, Arthur helps enterprise teams monitor, measure and optimize AI performance at scale.Suggest EditsLooking to operationalize your machine learning systems in production? Arthur's observability platform helps enterprise teams monitor, measure, and improve machine learning at scale.
What Happens When AI Meets the Real World?
As the AI performance company, Arthur helps data scientists, product owners, and business leaders accelerate model operations at scale. Our platform monitors, measures, and improves machine learning models for better results across three core areas: accuracy, explainability, and fairness.
Video Overview
Got 4 minutes? Check out a video overview of our product:
Get Started Today
Jump into the Quickstart Guide or start learning about the Arthur Platform.Updated 3 months ago What’s NextQuickstartTable of Contents
What Happens When AI Meets the Real World?
Video Overview
Get Started Today",arthur_scope_docs,https://docs.arthur.ai/docs/getting-started
54,"API Documentation - Arthur Documentation
API Documentation  Arthur Documentation
Link
Overview
Getting Started
Arthur Model
Arthur Attribute
Reference Set
Model Inferences
Concepts and
Terminology
User
Guide
Algorithmic Bias
Explainability
Enrichments
Batch Ingestion from S3
SparkML Integration Guide
Authentication and Deployment
SDK
Install Guide
SDK Docs
API
API
Query Guide
Endpoint Overview
Aggregation Functions
Model Evaluation Functions
Transformation Functions
Explainability
Data Drift
Alert Rules Guide
Examples on
Github
ReDoc",arthur_scope_docs,https://legacy.docs.arthur.ai/api-documentation/v3-api-docs.html
55,"Pages in the Arthur Scope Platform
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Pages in the Arthur Scope PlatformSuggest EditsOrganization Wide
In Arthur Scope, an organization is the highest level of grouped control within Arthur utilized to organize and manage access to resources that exist on the platform. Users can be added to multiple organizations and given roles that provide them with Read and/or Write access to some subset of that organization's resources, as defined by their user role.
Home Page
Each organization's home page works as a control panel for all the models in production.
Organization-Wide Model Dashboard
The organization-wide model dashboard provides a single pane of glass view into all your organization's models in production.
Model Onboarding Status
Model onboarding status provides a look into your model's creation phase. This is the place to check after onboarding to ensure your model is ready to receive inferences.
Pending: If your model status is Pending, model creation will begin soon. Please check back for updates or contact support if you don’t see any updates within a few minutes.
Creating: If your model status is Creating, the model creation process is in-progress. Please check back for updates or contact support if you don’t see any updates within 10 minutes.
Ready: If your model status is Ready, the model creation process has been completed successfully! Your model is now ready to ingest data.
Creation Failed: If your model status is Creation Failed, the model creation process was unsuccessful, and the necessary infrastructure was not provisioned fully. While in this state, your model cannot infrastructure ingest any data. You may try re-saving the model or contact support if the problem persists.
Health Scores
Health scores provide a quick insight into the model's overall performance from three important aspects.
Accuracy: Classification: F1 Score. Regression: 1 - Normalized MAE.
Ingestion: Variance of normalized time periods between ingestion events. The variance of normalized volume differences between ingestion events.
Drift: 1 - average anomaly score
These scores are classified into three color-coded sections: green, yellow, and red.
Settings
Users can access the settings for their organization by clicking on their initial button in the lower left corner.
Pages available within Settings:
Account: Change account setting information, such as your user password
API Keys: Create and copy new API Keys from
Members: A look at all members and roles in the Arthur organization
Usage: Evaluate monthly Arthur platform usage
Notifications: Organization-wide alert configuration, learn more at Managing Alerts
Changing Organizations: Users can have access to multiple organizations. To see what organizations they can access and/or switch between, they can click on the switch button in the Settings tab.
Arthur Scope - Model Dashboards
Overview Tab
The overview tab provides a look into your model's performance over time, in particular Performance Metrics, Fairness Metrics, and Data Drift Metrics.
Inference Deep Dive
The inference deep dive page provides a look into the individual inferences.
Certain administrator-level permissions will have the ability to save and edit column presets by pinning or unpinning columns on this page. These pinning selections happen at a per-model level. For those with non-adminstrator permissions, users will see the columns pinned by the admin by default and are able to pin columns to their view only.
Alert Dashboard
A control panel for all things Alerting for your Arthur model, including creating, managing, and triaging alerts in one dashboard.
Insights
Insights is a page dedicated to the information stored from enrichments within Arthur. Currently, insights only contain insights about
Hot Spots.
Updated about 2 months ago Table of Contents
Organization Wide
Home Page
Organization-Wide Model Dashboard
Settings
Arthur Scope - Model Dashboards
Overview Tab
Inference Deep Dive
Alert Dashboard
Insights",arthur_scope_docs,https://docs.arthur.ai/docs/pages-in-the-arthur-platform
56,"Token Likelihood
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Token LikelihoodSuggest EditsThere are currently no standard post-hoc explainability techniques for generative text (or token sequence) models. However, teams looking to better understand their models' outputs can turn to Token Likelihood for insights.
📘Token Likelihood Availability by LLM TypeTeams may choose from a number of LLM providers (OpenAI, Anthropic, Cohere, etc) to build their model. All of these model types can be monitored with Arthur. However, not every LLM provider allows the option for outputting token likelihood. To track token likelihood in Arthur, teams must provide token likelihood outputs from an LLM that outputs them.
Understanding Token Likelihood
The token likelihood is a number between 0 and 1 that quantifies the model’s level of surprise that this token was the next predicted token of the sentence. If a token has a low likelihood (close to 0), it means that the model is more unsure about selecting this token. While a likelihood close to 1 indicates that the model is very confident in predicting this token.
For example, if I was writing the sentence: I need to pack my backpack and ____
We can see from this example how token likelihood works. Textbooks is seen as a much more likely next word for the sentence than other terms like umbrella.
In LLM models, each token predicted has a likelihood.
What it looks like in Arthur
In Arthur Scope, Token Likelihood can be found, per inference provided for, in the Inferences Tab or the UI for text token sequence types.
The color of tokens ranges from green to red, with bright green being the most likely and bright red being the least for tokens in each inference.
Tracking Likelihood Metrics as Performance
We've seen how looking at individual inferences Token Likelihood can provide insight into a single prediction, but what if we wanted to use likelihood to assess trends?
Average Token Likelihood
The average token likelihood is created for each inference by taking the average likelihood score of predicted tokens in that inference. The average of these scores is then calculated for inferences within whatever time interval specified, i.e., daily, weekly, snapshot, etc.
The average token likelihood is a way to evaluate how confident your model is in its overall predictions. By tracking this over time, teams can better track this confidence.
Likelihood Stability
Likelihood stability looks at how stable your likelihood is between tokens for each inference.
Updated 3 months ago Table of Contents
Understanding Token Likelihood
What it looks like in Arthur
Tracking Likelihood Metrics as Performance
Average Token Likelihood
Likelihood Stability",arthur_scope_docs,https://docs.arthur.ai/docs/token-likelihood
57,"Registering Model Attributes Manually
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Registering Model Attributes ManuallySuggest EditsThis guide is useful for teams that want to manually register all their model attributes or register a model without a reference dataset.
Setting Up To Individually Onboard Model Attributes
Setting up your model to add each attribute individually is similar to the typical starting point for Creating Arthur Model Objects. Teams must define their Arthur Model Object:
Pythonmodel = arthur.model(partner_model_id=f""CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}"",
display_name=""Credit Risk Batch"",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass,
is_batch=True)
More information on defining this and all the steps can be found on the Creating Arthur Model Objects page here.
Set up Arthur Input Attributes
Teams looking to manually onboard models can start by manually adding all PipelineInput and NonInput attributes to their Arthur Model.
🚧Sending Null Values for AttributesUnless otherwise specified below in different types, Null values are allowed for different input types. Null and NaN values are allowed for onboarding through the Arthur SDK. On the other hand, Null values are allowed when onboarding with the API, but Null values are not.
Numerical Attribute
Numerical attributes are input attributes meant to track continuous numerical values. They can be manually added to a model in Arthur using the add_attribute function.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name=""Num_Attr_Name"",
value_type=ValueType.Float,
stage=Stage.ModelPipelineInput
)
📘Inferring Numerical Attributes as CategoricalWhen Arthur is inferring the model schema, Float and Integer columns are assumed to be categorical if there are fewer than 20 unique values and if Float values are all whole numbers. String and boolean columns are always assumed to be categorical for Tabular models.
Python## Ensure that numerical attributes are valued as numerical and not categorical
arthur_model.get_attribute(""Num_Attr_Name"", stage=Stage.ModelPipelineInput).categorical = False
Teams may also choose to specify the exact value type. The options are ""Integer"" or ""Float"".
Pythonarthur_model.get_attribute(""Num_Attr_Name"", stage=Stage.ModelPipelineInput).value_type = 'INTEGER'
Categorical Attribute
Categorical Attributes are attributes that represent a finite group of values (or categories). They can be manually added to a model in Arthur using the add_attribute function.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name=""Cat_Attr_Name"",
value_type=ValueType.String,
stage=Stage.ModelPipelineInput
)
👍Ensure All Possible Production Attributes Are SpecifiedAttributes that are set to categorical must have at least one column. In edge cases where this is not possible, the category list can be set using a single ""dummy"" category (e.g., [""n/a""]). While new categories will be taken in by the platform, they will not be utilized in drift calculations or segmented visualization in the UI. So, it is important to ensure that all potential categories are listed before onboarding.
Setting Possible Categories
Based on the callout above, teams may manually specify the potential categories.
Python## Set Categories to N/A
arthur_model.get_attribute(""Cat_Attr_Name"", stage=Stage.ModelPipelineInput).categories = [""n/a""]
## Set Categories to a List
arthur_model.get_attribute(""Cat_Attr_Name"", stage=Stage.ModelPipelineInput).categories = [""n/a"", ""bachelors"",""masters"",""highschool""]]]
Setting Attribute Labels
When teams have set up numerical encoding for their categorical variables, providing the mapping back to human understanding for the Arthur platform may be useful. This will make it easier for end users to utilize the UI to understand categorical attributes better.
Python# labels the value 0 for the attribute 'education_level'
# to have the label 'elementary', etc.
arthur_model.set_attribute_labels(
'education_level',
{0 : 'elementary', 1 : 'middle', 2 : 'high', 3 : 'university'}
)
Timestamp Attribute
Timestamp Attributes are model features that represent a date/time. These are frequently found in time series models.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a timestamp input attribute directly to the model
arthur_model.add_attribute(
name=""Timestamp_Attribute_Name"",
value_type=ValueType.Timestamp,
stage=Stage.ModelPipelineInput
)
It is important to note that the DateTime object being put into Arthur, must be in a DateTime format and include a timezone. A common example of how to set up these transformations can be seen below:
Python## Example of Converting to Pandas DateTime
## This function will need to change depending on how your time strings are formatted
def get_timestamps(x):
new_time = x.split('.')[0]
return datetime.strptime(new_time, '%Y-%m-%d %H:%M:%S')
df['timestamps'] = df['timestamps'].apply(lambda x: get_timestamps(x))
## Ensure Appropriate tzinfo Timezone Added
df['timestamp'] = df['timestamp'].apply(lambda x: x.replace(tzinfo=pytz.UTC))
❗️Null and NaN Values Are Not AllowedFor Timestamp attributes, Null and NaN values are not allowed within Arthur.
Time Series Attribute
Time Series Attributes are model features that represent a value over time.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a time series input attribute directly to the model
arthur_model.add_attribute(
name=""Time_Series_Attribute_Name"",
value_type=ValueType.TimeSeries,
stage=Stage.ModelPipelineInput
)
It is important to note that the TimeSeries object being put into Arthur must be formatted as a list of dicts with ""timestamp"" and ""value"" keys. The timestamps must be formatted according to the restrictions on Timestamp attributes. The values must be floats.
❗️There must be data for every timestamp on a regular time intervalEach time series attribute should be considered to have some regular time interval (eg. 1 day, 1 week, etc.) at which the value it is recording is polled. The value thus must be recorded on consistent time intervals, and if data is not recorded on a given timestamp in that consistent interval, a data point with a Null value must still be recorded.
Text (NLP) Attribute
Unstructured Text Attributes refer to input text for NLP models. They can be manually added to a model in Arthur using the add_attribute function. The ArthurAttribute type of UnstructuredText is designed to be used for NLP models only.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name=""NLP_Input_Attribute_Name"",
value_type=ValueType.UnstructuredText,
stage=Stage.ModelPipelineInput
)
Generative Text Models
Teams wanting to monitor generative text models should refer to the Generative Text Model Onboarding Guide. This provides a step-by-step walkthrough in manually onboarding those models.
Image Attribute
Image Attributes refer to the images input to computer vision models. They can be manually added to a model in Arthur using the add_image_attribute function.
Pythonmodel.add_image_attribute(""ImageColumnName"")
The ImageColumnName string contains the column's name in your future reference or inference data frames containing the path to each image.
Unique Identifier
A Unique Identifier Attribute within Arthur is created to specify unique values within the platform. These String-type categorical attributes within Arthur specify unique values for every category.
Pythonfrom arthurai.common.constants import Stage, ValueType
# adds a float input attribute directly to the model
arthur_model.add_attribute(
name=""Cat_Unique_Attr_Name"",
value_type=ValueType.String,
stage=Stage.ModelPipelineInput
)
## Specify this category as unique
arthur_model.get_attribute(""Cat_Unique_Attr_Name"", stage=Stage.ModelPipelineInput).unique = True
## Specify no categories
arthur_model.get_attribute(""Cat_Unique_Attr_Name"", stage=Stage.ModelPipelineInput).categories = []
🚧Do Not Onboard Your Partner Inference ID with Reference DataWhile the Partner Inference ID (your internal teams inference identifier) is the most common unique identifier within Arthur, this is something that you can specify (or Arthur will create) when you send inferences onto the platform. This is not specified when building out a reference dataset.
Set up Arthur Predicted/Ground Truth Attributes
After sending all input attribute information, teams can specify their model's predicted and ground truth attributes. This will depend on the model task type they are trying to onboard.
❗️Null Values Are Not Allowed for Predicted or Ground Truth AttributesNull values are not supported for predicted or ground truth attributes.
Classification
To add output attributes for classification tasks, teams must first specify what type of classification model they want to onboard. They can choose between:
Binary Classification with columns for each predicted attribute and ground truth value
Multi-Class Classification with columns for each predicted attribute and ground truth value
Either Binary or Multi-Class Classification with columns for each predicted attribute but only a single column for ground truth
The type of classification you choose should be based on the schema you expect for onboarding reference or inference data later.
Binary Classification
If you expect your inference schema to consist of two predictions and two ground truth columns for your binary classification task, then you should utilize the add_binary_classifier_output_attributes function. In this function, you need to provide:
Prediction to Ground Truth Mapping: Mapping of each predicted column to its corresponding ground truth column
Positive Predicted Attribute: The positive class is the class that is related to your objective function. For example, if you want to classify whether the objects are present in a given scenario. So all the data samples where objects are predicted present will be considered positively predicted.
Python# map PredictedValue attributes to their corresponding GroundTruth attributes
PRED_TO_GROUND_TRUTH_MAP = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# add the ground truth and predicted attributes to the model
# specifying that the `pred_1` attribute is the
# positive predicted attribute, which means it corresponds to the
# probability that the binary target attribute is 1
arthur_model.add_binary_classifier_output_attributes(positive_predicted_attr='pred_1',
pred_to_ground_truth_map=PRED_TO_GROUND_TRUTH_MAP)
Multi-Class Classification
If you expect your inference schema to consist of multiple predictions and their corresponding multiple ground truth columns for your classification task, then you should utilize the add_multiclass_classifier_output_attributes function. In this function, you need to provide:
Prediction to Ground Truth Mapping: Mapping of each predicted column to its corresponding ground truth column
Positive Predicted Attribute: The positive class is the class that is related to your objective function. For example, if you want to classify whether the objects are present in a given scenario. So all the data samples where objects are predicted present will be considered positively predicted.
Python# map PredictedValue attributes to their corresponding GroundTruth attributes
PRED_TO_GROUND_TRUTH_MAP = {
""dog"": ""dog_gt"",
""cat"": ""cat_gt"",
""horse"": ""horse_gt""
}
# add the ground truth and predicted attributes to the model
arthur_model.add_multiclass_classifier_output_attributes(
pred_to_ground_truth_map = PRED_TO_GROUND_TRUTH_MAP
)
Single Column Classification
Single-column classification is very similar to previous techniques; however, there is only a single ground truth column in this technique.
Prediction to Ground Truth Mapping: Mapping of each predicted column to its corresponding ground truth value
Positive Predicted Attribute: The positive class is the class that is related to your objective function. For example, if you want to classify whether the objects are present in a given scenario. So all the data samples where objects are predicted present will be considered positively predicted.
Ground_Truth_Column: You must specify the single-column ground truth
Python# Map PredictedValue attribute to its corresponding GroundTruth attribute value.
# This tells Arthur that the `pred_survived` column represents
# the probability that the ground truth column has the value 1
PRED_TO_GROUND_TRUTH_MAP = {
""pred_value"": 1
}
# Add the ground truth and predicted attributes to the model,
# specifying which attribute represents ground truth and
# which attribute represents the predicted value.
arthur_model.add_classifier_output_attributes_gtclass(
positive_predicted_attr = 'pred_value',
pred_to_ground_truth_class_map = PRED_TO_GROUND_TRUTH_MAP,
ground_truth_column = 'gt_column'
)
Regression
To manually specify your regression output, teams need to specify a prediction to ground truth mapping with the following:
Predicted Value: The column that contains your numerical predicted output
Ground Truth Value: The column that contains the ground truth
Pythonfrom arthurai.common.constants import ValueType
# map PredictedValue attributes to their corresponding GroundTruth attributes
PRED_TO_GROUND_TRUTH_MAP = {
""pred_value"": ""gt_value"",
}
# add the ground truth and predicted attributes to the model
arthur_model.add_regression_output_attributes(
pred_to_ground_truth_map = PRED_TO_GROUND_TRUTH_MAP,
value_type = ValueType.Float
)
Object Detection
To manually specify your object detection models, teams need to specify the following:
Predicted Attribute Name: This is the column name that will store your predicted bounding boxes
Ground Truth Attribute Name: This is the name of the column with the true labeled bounding boxes
Class Labels: All potential object labels for that your model is detecting
predicted_attribute_name = ""objects_detected""
ground_truth_attribute_name = ""label""
class_labels = ['cat', 'dog', 'person']
arthur_model.add_object_detection_output_attributes(
predicted_attribute_name,
ground_truth_attribute_name,
class_labels)
Generative Text (LLM)
Teams wanting to monitor generative text models should refer to the Generative Text Model Onboarding Guide. This provides a step-by-step walkthrough in manually onboarding those models.
Setting Reference Data Later
For teams that have chosen to manually onboard all of their model attributes to ensure that they were inferred correctly but still want to include a reference dataset for drift calculations, they can! After manually creating the model schema above, this can be done by setting the reference dataset.
Python# reference dataframe of model inputs
reference_set = pd.DataFrame(....)
# produce model predictions on reference set
# in this example, the predictions are classification probabilities
preds = model.predict_proba(reference_set)
# assign the column corresponding to the positive class
# as the `pred` attribute in the reference data
reference_set[""pred""] = preds[:, 1]
# set ground truth labels
reference_set[""gt""] = ...
# configure the ArthurModel to use this dataframe as reference data
arthur_model.set_reference_data(data=reference_set)
Updated 2 months ago Table of Contents
Setting Up To Individually Onboard Model Attributes
Set up Arthur Input Attributes
Numerical Attribute
Categorical Attribute
Timestamp Attribute
Time Series Attribute
Text (NLP) Attribute
Image Attribute
Unique Identifier
Set up Arthur Predicted/Ground Truth Attributes
Classification
Regression
Object Detection
Generative Text (LLM)
Setting Reference Data Later",arthur_scope_docs,https://docs.arthur.ai/docs/registering-model-in-sdk-piece-by-piece
58,"Enabling Enrichments
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Enabling EnrichmentsSuggest EditsAs discussed in detail in the Enrichments section above, many teams require more than standard inference logging monitoring. For this reason, Arthur provides Enrichments.
Functions within Enrichment Enablement
There are three main functionalities for toggling enrichments on and off within Arthur. These are:
Getting the current configuration of the enrichment: understand whether or not the enrichment is enabled / disabled. This can be validated in a notebook environment (as shown in the below examples) or within the Arthur UI for the model under details.
Enable the enrichment: functionality to turn the enrichment on. This is not currently available in the UI and must be done with the Python SDK or an API call.
Disable the enrichment: functionality to turn the enrichment off. This is not currently available in the UI and must be done with the Python SDK or an API call.
Anomaly Detection
Anomaly detection allows users to go beyond univariate drift analysis and look at complex interactions between features that may cause drift. To learn more in detail about anomaly detection, please refer to its doc's section here:
Enable Anomaly Detection
Anomaly detection is the only enrichment automatically enabled within Arthur, as long as your model has a reference dataset attached to it.
Python## Using the Arthur Python SDK
# view current configuration
arthur_model.get_enrichment(Enrichment.AnomalyDetection)
# enable
arthur_model.update_enrichment(Enrichment.AnomalyDetection, True, {})
# disable
arthur_model.update_enrichment(Enrichment.AnomalyDetection, False, {})
Explainability
One of the most commonly enabled enrichments is explainability. Explainability allows teams to build trust with valuable insights into how models make decisions. It allows teams to understand why predictions are being made and evaluate how changes to model input change predictions.
You can learn more about how to use our explainability capabilities here, but
Model Asset Requirements
Arthur can automatically calculate explanations (feature importances) for every prediction your model makes. To make this possible, we package up your model in a way that allows us to call it'spredict function, which allows us to calculate explanations. We require a few things from your end:
A Python script that wraps your model's predict function
For Image models, a second function, load_image is also required.
A directory containing the above file, along with any serialized model files, and other supporting code
A requirements.txt with the dependencies to support the above
More detailed explanations about the model assets required here.
PythonEntrypoint FileExample Requirements.txt## enabling explainability
arthur_model.enable_explainability(
df=X_train.head(50),
project_directory=""/path/to/model_folder/"",
requirements_file=""requirements.txt"",
user_predict_function_import_path=""model_entrypoint"",
ignore_dirs=[""folder_to_ignore""] # optionally exclude directories within the project folder from being bundled with predict function
)
## Common examples per model type can be found in the model input/output pages
# example_entrypoint.py
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(x)
boto3>=1.0
numpy>=1.0
pandas>=1.0
scikit-learn>=0.23.0
Find more information about troubleshooting explainability here.
Hot Spots
Hot spots help teams automatically surface rule-based segments of your data for underperformance. Learn more about how in the hot spots section of the documentation here.
Enable Hot Spots
Since Hot Spots rely only on inference data, no additional configuration is needed to enable them within the platform.
Python# view current configuration
arthur_model.get_enrichment(Enrichment.Hotspots)
# enable
arthur_model.update_enrichment(Enrichment.Hotspots, True, {})
# disable
arthur_model.update_enrichment(Enrichment.Hotspots, False, {})
Bias Mitigation
As a reminder about a few of bias mitigation's ""only's"" that affect enrichment enablement.
Bias Mitigation is only available for binary classification models
It can only be enabled if at least one model attribute is marked as monitor_for_bias=True
So by default, any binary classifier that you want to enable bias mitigation for will automatically train a mitigation model for all attributes marked as monitor_for_bias=True.
Enable Bias Mitigation
Default Bias Mitigation (all marked attributes)Bias Mitigation for a Specific Attribute# view current configuration
arthur_model.get_enrichment(Enrichment.BiasMitigation)
# enable
arthur_model.update_enrichment(Enrichment.BiasMitigation, True, {})
# or
arthur_model.enable_bias_mitigation()
Updated 3 months ago What’s NextSending InferencesAssets Required For ExplainabilityTroubleshooting ExplainabilityTable of Contents
Functions within Enrichment Enablement
Anomaly Detection
Enable Anomaly Detection
Explainability
Model Asset Requirements
Hot Spots
Enable Hot Spots
Bias Mitigation
Enable Bias Mitigation",arthur_scope_docs,https://docs.arthur.ai/docs/enabling-enrichments
59,"Data Preparation for Arthur
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Data Preparation for ArthurWhat type of data do I need to prepare for the Arthur platform ?Suggest EditsAt a high level, teams often want to know what they need to set up to onboard Arthur before starting the onboarding process. As shown below, the assets teams need to prepare to use Arthur are directly related to how they want to use Arthur for their models.
We can see that the only assets required to monitor inferences actively are your model structure and inference data. However, teams that wish to use more aspects of the Arthur platform than seeing inputs and outputs need to onboard/send additional assets to Arthur.
Model Structure
A model structure, otherwise known as an Arthur model schema, defines a wireframe for what Arthur should expect as inputs and outputs to your model. By recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built for your model in Arthur.
Example model input and output
To define this wireframe, teams must provide either example model input and output data (as in a reference dataset) or define all model inputs (features) and outputs (predictions or ground truth) as well as their expected values. These values must be a type allowed by Arthur.
At a high level, these are the available Arthur Inputs:
Available Input Value TypesAllowed Data TypesAdditional Information RequiredNumericalInteger, FloatCategoricalInteger, String, Boolean, FloatSpecified Available CategoriesTimestampDateTimeMust be time zone aware. Cannot be Null or NaNText (NLP)StringGenerative Sequence (LLM)StringA tokens (and optional token likelihood) column is also requiredImage.jpg, .png, .gifUnique IdentifierStringTime SeriesList of Dictionaries with ""timestamp"" and ""value"" keysTimestamps must be timezone aware DateTimes and values must be floats.
These are the available Arthur Outputs:
Arthur Output TypeAllowed Data TypesNotesClassificationInteger, FloatCannot be Null or NaNRegressionInteger, FloatCannot be Null or NaNObject DetectionLiteral Array of Bounding Box ValuesCannot be Null or NaNGenerative Sequence Unstructured TextStringCannot be Null or NaNGenerative Sequence Token LikelihoodArray of Float ValuesThis is optional. Cannot be Null or NaNRanked ListList of Dictionaries with ""item_id"", ""label"", and ""score"" keys""item_id"" and ""label"" values must be strings, ""score"" value must be a float. ""label"" and ""score"" keys are optional.
For more specific examples of troubleshooting onboarding for specific model input and output types, please refer to Registering Model Attributes Manually.
Map of Data Relationship
As well as information about how our model predictions relate to our ground truth attributes. This is used to help prepare the Arthur platform to calculate performance metrics correctly.
Teams can register this structure about their models manually ( Registering Model Attributes Manually). However, the most common way teams define their model structure in Arthur is by having it automatically inferred by onboarding their Reference dataset with the Python SDK.
Reference Dataset
A reference dataset is a representative sample of input features for your model. In other words, it is a sample of what is typical or expected for your model. Typically, teams onboard their model's training or validation data for reference.
In the Arthur platform, your reference dataset provides the baseline for data drift and anomaly metrics. Therefore, for those techniques to appear, a reference dataset must be set for your Arthur model object.
The only required stage to be included in the reference dataset is ModelPipelineInput. But we also recommend including data from the PredictedValue, GroundTruth, and NonInputData stages so that Arthur can also measure drift in those attributes over time.
Note: As mentioned in the data drift metric section, univariate data drift metrics can be calculated in the Python SDK by comparing inferences to one another without a reference dataset.
For more information on best practices/guidelines for selecting your reference dataset, please refer to the Creating Arthur Model Object page.
Model Assets
Model assets are only required for teams that wish to enable explainability for their model. Explainability, as described better in the Explainability section, can be used to understand the model's decisions better. These assets are further described in (add explainability section), but they include:
Requirements.txt File: a file containing all
the requirements needed to run your predict function
Model: the compressed runnable model used to generate predictions (typically .pkl format)
Entrypoint Python File: this file contains your Python predict function that will be used to generate predictions to produce explanations
Inputs: List
Outputs: List
Pythonimport …
model = load.model(‘pkl_model’)
## Python Predict Function
## takes in and returns a list
def predict(x):
return model.predict_proba(x)
Inference Data
For teams that want to use Arthur for the primary purpose of monitoring, it is essential to set up active monitoring of the data and predictions your model is running on in production. In Arthur, these rows of data are called inferences. Data associated with each inference might include (1) input data, (2) model predictions, and/or (3) corresponding ground truth.
Teams with the most success monitoring work to automate or create a consistent process for sending inferences to the platform.
Inference Ground Truth
While ground truth can be sent simultaneously as inference data, many ML models do not receive ground truth labels during prediction. In these instances, teams can set up a process to match ground truth in the platform with outside labels.
Note on sending large data to Arthur
While Arthur supports ingesting reference data, inferences, and ground truth labels, there are some constraints on ingestion that are worth being aware of. Clients are responsible for ensuring that data sent to Arthur respects these limits except for where called out below:
Arthur's ingress proxy has a 4Gb limit on the size of any request payload that's made to Arthur. Requests which exceed this 4Gb limit will error out indicating that the body was too large.
Arthur's ingestion-service (responsible for ingesting data) has a fixed amount of memory available. By default, this is 2Gb, but is configurable through the administrative interface or deployment scripts. Requests exceeding this limit may cause ingestion-service to run out of memory and crash.
SDK users that are ingesting Images using send_bulk_inferences, send_bulk_ground_truths or set_reference_data will have their files chunked so that they respect these limits.
Updated 2 months ago Table of Contents
Model Structure
Example model input and output
Map of Data Relationship
Reference Dataset
Model Assets
Inference Data
Inference Ground Truth
Note on sending large data to Arthur",arthur_scope_docs,https://docs.arthur.ai/docs/preparing-for-onboarding
60,"PagerDuty
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by PagerDutySuggest EditsWith the Arthur + PagerDuty integration, you can notify on-call teams in PagerDuty of alerts Arthur triggers. To set up this integration, follow these steps:
Step 1: Set up your email integration in PagerDuty
There are three ways to configure an email integration in PagerDuty. Please follow one of the three options in the PagerDuty Email Integration Guide to retrieve your integration email address.
Step 2: Configure your integration in Arthur
To configure the PagerDuty integration for a model in Arthur, you can send a POST request to the/alert_notification_configurations.
model_id - UUID of the model this alert notification configuration belongs to.
type - Type of notification to send. In this case, ""PagerDuty"".
destination - The integration email address obtained in Step 1.
enabled - Whether or not the notification configuration is enabled. It defaults to true.
Example Query Request:
JSON{
""model_id"" : ""<model_id> [string]"",
""type"" : ""[PagerDuty]"",
""destination"" : ""<[email protected]> [string]"",
}
For more information on configuring alert notifications, please see the notification section of the Alerting page.
Step 3: Start monitoring!
Your integration is now ready to use! When an alert is triggered in Arthur for this model, an incident will be created in your PagerDuty.Updated 3 months ago Table of Contents
Step 1: Set up your email integration in PagerDuty
Step 2: Configure your integration in Arthur
Step 3: Start monitoring!",arthur_scope_docs,https://docs.arthur.ai/docs/pagerduty
61,"Transformation Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Transformation FunctionsSuggest EditsFor transformation functions, it will be helpful to include theproperty in the request to help associate the transformation function values, for example:
JSON{
""select"": [
{
""property"": ""<attribute_name> [string]""
},
{
""function"": ""roundTimestamp"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string]"",
""time_interval"": ""[secondminutehourdaymonthyear]""
}
}
]
}
We omit property for brevity in the following examples.
For an explanation of nested functions, see the guide Composing Functions.
Round Timestamp
Rounds a timestamp property to the provided time interval. This function requires one property
which must be an attribute of type datetime and one parameter, time_interval.
Query Request:
JSON{
""select"": [
{
""function"": ""roundTimestamp"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""time_interval"": ""[secondminutehourdaymonthyear]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<rounded_timestamp> [string]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""roundTimestamp"",
""parameters"": {
""property"": ""inference_timestamp"",
""time_interval"": ""day""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""roundTimestamp"": ""2020-08-10T00:00:00.000Z""
},
{
""roundTimestamp"": ""2020-08-09T00:00:00.000Z""
},
{
""roundTimestamp"": ""2020-08-08T00:00:00.000Z""
}
]
}
back to top
Label By Max Column
Given a list of columns, returns a string column with the name of the column containing the max value for the row. For example, this function can be used to extract the max probability from a vector of probability properties.
Query Request:
JSON{
""select"": [
{
""function"": ""labelByMaxColumn"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""properties"": [
""<property name> [string]""
]
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<one of the properties from the given list> [string]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""labelByMaxColumn"",
""alias"": ""classPrediction"",
""parameters"": {
""properties"": [
""class_1"",
""class_2"",
""class_3""
]
}
}
]
}
Sample Response:
JSON{
""classPrediction"": [
{
""classPrediction"": ""class_1""
},
{
""classPrediction"": ""class_1""
},
{
""classPrediction"": ""class_2""
}
]
}
back to top
If/Then/Else
Given a column and a condition, returns the ""then"" value if the condition is true on that column, otherwise returns the ""else"" value.
Query Request:
JSON{
""select"": [
{
""function"": ""if"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<property name> [string or nested]"",
""comparator"": ""<gtltgtelteeqne>"",
""value"": ""<any value to compare to>"",
""then"": ""<any value to return when true>"",
""else"": ""<any value to return when false>""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<then or else value based on the conditional>""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""if"",
""alias"": ""predicted_class"",
""parameters"": {
""property"": ""positive_probability"",
""comparator"": ""gte"",
""value"": 0.6,
""then"": ""positive"",
""else"": ""netivate""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""predicted_class"": ""positive""
},
{
""predicted_class"": ""negative""
},
{
""predicted_class"": ""positive""
}
]
}
back to top
Bin Continuous
This function bins a continuous value based on supplied thresholds. The bins will be formed as:
[< threshold_1, threshold_1 <= x < threshold_2, ... , threshold_(n-1) <= x < threshold_(n), threshold_(n) < x].
The response bins will be labeled with an integer id corresponding to the ordered bin, starting at 1.
If n thresholds are given, n+1 bins will be returned.
Query Request:
JSON{
""select"": [
{
""function"": ""binContinuous"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<property name> [string or nested]"",
""bin_thresholds"": [
""<threshold_1> [number]"",
""<threshold_2> [number]"",
""<threshold_3> [number]""
]
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<bin_id> [int]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""property"": ""age""
},
{
""function"": ""binContinuous"",
""alias"": ""ageBin"",
""parameters"": {
""property"": ""age"",
""bin_thresholds"": [
18,
65,
95
]
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""age"": 10,
""ageBin"": 1
},
{
""age"": 20,
""ageBin"": 2
},
{
""age"": 70,
""ageBin"": 3
}
]
}
back to top
Bins To Quantiles
Returns an array of values representing the quantiles based on the number of bins passed to the function. For example if you supply ""num_bins"": ""10"", then this query will return the value in your data at the 10%, 20%, ... , 90%, 100% quantiles.
Query Request:
JSON{
""select"": [
{
""function"": ""binsToQuantiles"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]"",
""num_bins"": ""<num_bins> [int]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": [
""float""
]
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""binsToQuantiles"",
""alias"": ""quantiles"",
""parameters"": {
""property"": ""age"",
""num_bins"": 10
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": [
19,
28,
37,
46,
55,
64,
73,
82,
91
]
}
]
}
back to top
Date Diff
Returns the difference of two timestamps in units. Valid units are:
second, minute, hour, day, week, month, quarter, and year.
Query Request:
JSON{
""select"": [
{
""function"": ""dateDiff"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""unit"": ""[secondminutehourdayweekmonthquarteryear]"",
""start_date"": ""<attribute_name> [string or nested]"",
""end_date"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""difference [int]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""dateDiff"",
""alias"": ""date_diff"",
""parameters"": {
""unit"": ""second"",
""start_date"": ""inference_timestamp"",
""end_date"": ""prev_timestamp""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""date_diff"": 100
}
]
}
back to top
Neighbor
Returns the value of the column offset rows next to this row in the ordering.
default is the value that is returned when the offset goes out of bounds on the row set.
It is recommended to use this function in a subquery with an order_by clause to get consistent ordering.
Query Request:
JSON{
""select"": [
{
""function"": ""neighbor"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""offset"": ""<offset> [int]"",
""property"": ""<attribute_name> [string or nested]"",
""default"": ""<default_value> [any]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""neighbor_value [any]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""neighbor"",
""alias"": ""prev_timestamp"",
""parameters"": {
""property"": ""inference_timestamp"",
""offset"": -1,
""default"": null
}
}
],
""order_by"": [
{
""property"": ""inference_timestamp"",
""direction"": ""desc""
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""prev_timestamp"": ""2021-06-15T00:00:00.000Z""
}
]
}
back to top
Arithmetic
add, subtract, multiply, and divide are valid arithmetic functions.
Each takes two columns as input and returns the result of the arithmetic expression.
Query Request:
JSON{
""select"": [
{
""function"": ""[addsubtractmultiplydivide]"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""left"": ""<attribute_name> [string or nested]"",
""right"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""expression_result [number]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""multiply"",
""alias"": ""double_home_value"",
""parameters"": {
""left"": ""Home_Value"",
""right"": 2
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""double_home_value"": 20000
}
]
}
Sample nested request to compute (Home_Value + Car_Value) * 2
JSON{
""select"": [
{
""function"": ""multiply"",
""alias"": ""double_loans"",
""parameters"": {
""left"": {
""nested_function"": {
""function"": ""add"",
""alias"": ""total_loan"",
""parameters"": {
""left"": ""Home_Value"",
""right"": ""Car_Value""
}
}
},
""right"": 2
}
}
]
}
Sample Nested Response:
JSON{
""query_result"": [
{
""double_loans"": 20000
}
]
}
back to top
Absolute Value
Take the absolute value of a property.
Query Request:
JSON{
""select"": [
{
""function"": ""abs"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""property"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<abs_value> [float]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""abs"",
""alias"": ""abs_delta"",
""parameters"": {
""property"": ""delta""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""abs_delta"": 55.45
}
]
}
back to top
Logical Functions
equals, and, and or are valid logical functions. Each takes two columns as input and returns the result of the logical expression. These follow the same API as Arithmetic Functions
Query Request:
JSON{
""select"": [
{
""function"": ""[equalsandor]"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""left"": ""<attribute_name> [string or nested]"",
""right"": ""<attribute_name> [string or nested]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""expression_result [0 or 1]""
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""equals"",
""alias"": ""has_phd"",
""parameters"": {
""left"": ""education"",
""right"": 4
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""has_phd"": 1
}
]
}
Sample nested request to compute has_phd or has_masters
JSON{
""select"": [
{
""function"": ""or"",
""alias"": ""has_higher_education"",
""parameters"": {
""left"": {
""nested_function"": {
""function"": ""equals"",
""alias"": ""has_phd"",
""parameters"": {
""left"": ""education"",
""right"": 4
}
}
},
""right"": {
""nested_function"": {
""function"": ""equals"",
""alias"": ""has_masters"",
""parameters"": {
""left"": ""education"",
""right"": 3
}
}
}
}
}
]
}
Sample Nested Response:
JSON{
""query_result"": [
{
""has_higher_education"": 1
}
]
}
back to topUpdated 3 months ago Table of Contents
Round Timestamp
Label By Max Column
If/Then/Else
Bin Continuous
Bins To Quantiles
Date Diff
Neighbor
Arithmetic
Absolute Value
Logical Functions",arthur_scope_docs,https://docs.arthur.ai/docs/transformation-functions
62,"Time Series
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Time SeriesSuggest EditsTime Series input models are a type of machine learning model that operates on time series data, or data which measures a certain value over time, such as credit card balance over time. These models can perform tasks such as predictions or recommendations based on past patterns.
Formatted Data in Arthur
Time Series input models require the following data formatting:
JSON[
{
""timestamp"": ""2023-10-05T00:00:00Z"",
""value"": 1
},
{
""timestamp"": ""2023-10-06T00:00:00Z"",
""value"": 4
}
]
Arthur requires that all times will be present in a given series according to a regular interval (eg. one value each day).
There is an upper bound of 500 timestamps in a single time series inference.
Arthur supports sending time series data in JSON files or DataFrames.Updated about 2 months ago Table of Contents
Formatted Data in Arthur",arthur_scope_docs,https://docs.arthur.ai/docs/time-series
63,,arthur_scope_docs,https://staging.docs.arthur.ai/docs/quickstart
64,"Config Template
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Config TemplateSuggest EditsThe Configuration template for Arthur version 3.4.0 is below:
YAMLapiVersion: kots.io/v1beta1
kind: ConfigValues
metadata:
creationTimestamp: null
name: arthur
spec:
values:
IAM_permission_type:
default: access_keys
advanced_cache_options:
default: ""0""
advanced_messaging_connect_cpu_limits:
default: ""2""
advanced_messaging_connect_cpu_limits_not_validate: {}
advanced_messaging_connect_cpu_requests:
default: ""1""
advanced_messaging_connect_cpu_requests_not_validate: {}
advanced_messaging_connect_heap_options:
default: -Xms1g -Xmx3g
advanced_messaging_connect_memory_limits:
default: 4Gi
advanced_messaging_connect_memory_limits_not_validate: {}
advanced_messaging_connect_memory_requests:
default: 2Gi
advanced_messaging_connect_memory_requests_not_validate: {}
advanced_olap_options:
default: ""0""
advanced_other:
default: ""0""
value: ""1""
alert_service_update_rule_metrics:
default: ""0""
api_token_ttl:
default: ""24""
arthur_user_id:
default: ""1000""
audit_log_event_bridge_bus_name: {}
audit_log_event_bridge_bus_region: {}
audit_log_event_bridge_detail_type:
default: events.arthur.ai
audit_log_event_bridge_source:
default: arthur-audit-log
audit_log_sink_destination:
default: none
batch_workflow_parallelism:
default: ""120""
beta_ui:
default: ""0""
beta_ui_alternate_site:
default: ""0""
beta_ui_hostname: {}
bootstrap_job_backoff_limit:
default: ""100""
bootstrap_job_ttl:
default: ""86400""
cache_cpu_limits: {}
cache_cpu_limits_not_validate: {}
cache_cpu_requests: {}
cache_cpu_requests_not_validate: {}
cache_memory_limits: {}
cache_memory_limits_not_validate: {}
cache_memory_requests: {}
cache_memory_requests_not_validate: {}
cache_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
cache_replicas:
default: ""0""
cicd_credentials:
default: ""0""
cluster_nodes:
# Only Relevant for ""fixed"" cluster sizes.
Enter the number of nodes in the cluster. This number cannot be decreased from the current value unless it's greater than `6`.
default: ""1""
value: ""3""
config_job_and_workflow_retention:
default: ""0""
database_admin_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
database_hostname:
# Leave the default configuration to use the embedded database. If you would like to use an external Postgres instance, provide the hostname here and follow this guide: https://docs.arthur.ai/platform-management/installation/externalize_postgres.html.
default: database-primary
database_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
database_port:
value: ""5432""
database_ssl_mode:
# This option allows you to enable SSL communication between services and the postgres database.
See https://www.postgresql.org/docs/10/libpq-ssl.html for full descriptions of each option.
By default, the postgres database has ssl disabled.
default: disable
database_username:
default: arthurai
default_messaging_partition_count:
default: ""3""
value: ""1""
disable_ssl_redirect_on_ingress:
default: ""0""
email_selection:
default: none
enable_audit_log:
default: ""0""
enable_olap_backup:
default: '""0""'
enable_olap_backup_user:
default: ""0""
enable_password_rotation_cache:
default: ""0""
enable_password_rotation_olap:
default: ""0""
existing_database_primary_pvc: {}
existing_or_vm:
default: existing_cluster
fixed_or_autoscale:
# The `fixed` mode is recommended for clusters with a fixed number of nodes. The `autoscale` mode is used for clusters that can autoscale and automatically expand their node count.
value: fixed
full_name_override:
default: arthurai
global_identity_provider:
default: none
global_model_limit_Count:
default: ""500""
global_model_limits:
default: ""0""
global_workflow_parallelism:
default: ""150""
http_proxy: {} # Relevant if you are using Explainability and your organization is behind a proxy server.
If PIP and/or Conda need to route through the proxy server to pull down public packages this will set the environment variable HTTP_PROXY to the supplied value. Ex. http://sysproxy.my-company.com:port
http_proxy_user: {}
https_proxy: {}
https_proxy_user: {}
ingestion_service_cpu_limits: {}
ingestion_service_cpu_limits_not_validate: {}
ingestion_service_cpu_requests: {}
ingestion_service_cpu_requests_not_validate: {}
ingestion_service_memory_limits: {}
ingestion_service_memory_limits_not_validate: {}
ingestion_service_memory_requests: {}
ingestion_service_memory_requests_not_validate: {}
ingress_ambassador_enabled:
default: ""false""
ingress_class:
default: nginx
ingress_hostname:
value: arthur.mydomain.ai
ingress_namespace_label_key:
value: name
ingress_namespace_label_value:
value: ingress-system
ingress_nginx_additional_hostname:
value: """"
irsa_annotations: {}
irsa_annotations_user:
default: 
eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/my-role
k8_storageclass:
# Provide Kubernetes StorageClass profile. Use 'gp2' for Amazon EKS, 'default' if you're using embedded Kubernetes provided by the installer
value: default
kafka_ecosystem_common_replication_calc:
default: ""1""
max_arthur_replicas:
default: ""1""
max_messaging_partition_count:
default: ""3""
max_model_server_replicas:
default: ""2""
messaging_cpu_limit:
default: ""1""
messaging_heap:
default: -Xmx2G -Xms1G
messaging_memory_limit_and_request:
default: 2560Mi
messaging_rack_aware_enabled:
default: ""0""
messaging_rack_label:
default: topology.kubernetes.io/zone
messaging_replicas:
default: ""3""
messaging_sa_create:
default: ""0""
messaging_sa_fullnameoverride: {}
messaging_zookeeper_timeout:
default: ""20000""
meta_replicas:
default: ""0""
metric_service_update_default_metrics:
default: ""0""
min_arthur_replicas:
default: ""1""
model_servers_always_on:
# For use with what-if and on-demand explainability. See https://docs.arthur.ai/user-guide/explainability.html
If set to ""true"", then on-demand and what-if explanations are available, but uses additional cluster resources, 1 CPU and 1 GB memory per model with explainability enabled. If set to ""false"", on-demand and what-if explanations are unavailable, but less cluster usage when there is no data being sent. Regardless of the setting here, streaming explainability will be available if enabled. This only effects what-if and on-demand explanations.
default: ""true""
network_policy_enabled:
default: ""0""
no_proxy: {} # Relevant if you are using Explainability and your organization is behind a proxy server.
If PIP and/or Conda need to route through the proxy server to pull down public packages this will set the environment variable NO_PROXY to the supplied value. Ex. localhost,127.0.0.1,.my-company.com
no_proxy_user: {}
number_of_olap_backups_to_keep:
default: ""7""
oidc_identity_provider_config_yaml: {}
oidc_identity_provider_config_yaml_user: {}
olap_backup_s3_bucket:
default: arthurai
olap_backup_s3_bucket_region:
default: us-east-1
olap_backup_s3_endpoint:
default: s3.us-east-1.amazonaws.com
olap_backup_s3_path:
default: olap_backups
olap_backup_service_account:
default: arthurai-arthurai
olap_cpu_limits: {}
olap_cpu_limits_not_validate: {}
olap_cpu_requests:
default: 1000m
olap_cpu_requests_not_validate: {}
olap_database_operator_password:
# The OLAP database is installed along with a Kubernetes Operator to manage it.
This operator needs credentials to access the database.
We recommend overwriting the default password below.
default: 5ugYLDJ2uLhRdEgz5t
value: ch/0gntnboTNbQpxmzx4GuPCRnjqSNwTpOT6FwgQ9q4iY7CHiQLeFQ3snnZgxYnFt4gSyInce3KhYiMR7eebBtGbe5sIuY/aBPAySrSjExfO+1VYPBp176bP+zQ=
olap_database_user_password:
# Password used internally in our application to query the olap database, currently only supports alpha-numeric characters.
default: eQ3iBo8UGh5zqJKQWuEEySrR
value: ch/0gntnboTNbQppnGJgGvCjSmPlS/l8orO+UggQ/rstcryCj2r/GRXR8UNr+u3plPIj+uLMdXGGFiRtko6pTsClBoQkoeLXqDVr1jeqsThCZI/bTfovlA==
olap_memory_limits: {}
olap_memory_limits_not_validate: {}
olap_memory_requests:
default: 1Gi
olap_memory_requests_not_validate: {}
olap_node_label_key: {}
olap_node_label_value: {}
olap_replicas:
default: ""1""
olap_zookeeper_cpu_limits: {}
olap_zookeeper_cpu_limits_not_validate: {}
olap_zookeeper_cpu_requests:
default: 500m
olap_zookeeper_cpu_requests_not_validate: {}
olap_zookeeper_heap_options:
default: -Xms4G -Xmx4G
olap_zookeeper_memory_limits: {}
olap_zookeeper_memory_limits_not_validate: {}
olap_zookeeper_memory_requests:
default: 1Gi
olap_zookeeper_memory_requests_not_validate: {}
password_rotation_cron_schedule:
default: 0 0 1 */6 *
pending_batch_workflows_limit:
default: ""100""
prometheus_host:
# Leave the default configuration if you're using the embedded K8s. Provide your Prometheus hostname if you're running your own K8s.
default: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local
prometheus_labels:
# If your prometheus installation requires labels to identify ServiceMonitors and PrometheusRules, add them here. They should be in yaml format just as you would specify inside the ""metadata.labels"" block. Do not indent.
default: 
prometheus: monitor
app: prometheus
prometheus_namespace:
default: monitoring
prometheus_port:
# Leave the default configuration if you're using the embedded K8s. Provide your Prometheus hostname if you're running your own K8s.
default: ""9090""
pypi_registry_conda: {} # This is set as a channel in the '.condarc' file. Do not include 'https://' prefix (e.g. repository.arthur.ai/repository/conda-proxy/main).
pypi_registry_conda_user: {}
pypi_registry_index: {} # This maps to the 'index key' in the 'pip.conf' file. Do not include 'https://' prefix (e.g repository.arthur.ai/repository/pypi-virtual/pypi).
pypi_registry_index_url: {} # This maps to the 'index-url' key in the 'pip.conf' file. Do not include 'https://' prefix (e.g. repository.arthur.ai/repository/pypi-virtual/simple).
pypi_registry_index_url_user: {}
pypi_registry_index_user: {}
pypi_registry_password:
default: bO4Mxhdaevso/029YtUgz98Wk7qPcxEpa1P/uVqG4cy4UY1B3+YN5Q==
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
pypi_registry_password_user:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
pypi_registry_username: {}
pypi_registry_username_user: {}
raw_anaconda_config: {}
raw_anaconda_config_user: {}
raw_pypi_config: {}
raw_pypi_config_user: {}
rbac_privileges:
# Change to ""cluster_scope"" to install CRDs too
default: namespace_scope
run_as_root:
default: ""0""
value: ""0""
s3_access_key_id:
default: access_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_access_key_id_user:
default: access_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_bucket:
default: arthurai
s3_bucket_user:
default: arthurai
s3_region:
default: us-east-1
s3_region_user:
default: us-east-1
s3_secret_access_key:
default: secret_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_secret_access_key_user:
default: secret_key
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
s3_url:
default: http://minio:9000
s3_url_user:
default: http://minio:9000
saml_identity_provider_config_yaml: {}
saml_identity_provider_config_yaml_user: {}
secondary_token_validation_key:
value: Aj3ziCI/YcnTT3QR3WAtMNDNEzzqTa8W9iJCoHjNFMteiO6lrcnUKw==
ses_region: {}
ses_role: {}
show_advanced_arthur_microservice_options:
default: ""0""
show_advanced_messaging:
default: ""0""
value: ""1""
show_hidden_variables:
default: ""0""
value: ""0""
show_token_signing_and_validation_options:
default: ""0""
signing_cert: {}
signing_cert_user: {}
signing_private_key: {}
signing_private_key_user: {}
single_or_ha:
# The `single` configuration is a minimal deployment suitable for non-production environments. For production deployment, select `ha`.
value: single
smtp_from: {} # Provide the email address to send alerts from (e.g. [email protected])
smtp_host: {} # Provide the address of the SMTP server (e.g. smtp.arthur.ai)
smtp_password:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
smtp_port: {}
smtp_user: {}
superadmin_email:
default: [email protected]
superadmin_firstname:
default: Super
superadmin_lastname:
default: Admin
superadmin_password:
default: SuperSecret
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
superadmin_username:
value: superadmin
token_signing_primary_key:
value: YSDFzjg5I83KMBJ+wHQmU/ejDQ7tTthIpaDcCRM+iqcDTofiul7DZzTblFkb0e2U0+UJ74TuIx28oGnxPM+pkmKlc1yx2uvj
use_external_blob_storage:
# Select ""Yes"" if and only if you are supplying your own S3 compatible storage, otherwise select ""No"" to use the embedded blob storage.
default: ""no""
use_external_postgres:
default: ""no""
use_raw_python_repository_configs:
# The PyPi registry section is only relevant when using the explainability enrichment (https://docs.arthur.ai/user-guide/enrichments.html#explainability).
# Provide your private PyPi registry if you have an airgapped enrivonment or your model requirements file includes packages only hosted in a private repository.
# Leaving this section blank will cause the public PyPi to be used. If the public PyPi is inaccessible from the cluster, the explainability feature will not work.
default: ""no""
use_smtp:
default: ""0""
workflow_ttl_seconds:
default: ""3600""
workflow_ttl_seconds_after_success:
default: ""60""
status: {}
Do note that these parameters are sorted alphabetically. Unfortunately, this is how the 'packager' software we use for our installer outputs the list of parameters. In reality, these parameters should be grouped based on their purpose.
Most of these parameters can be commented, unless you are advised by Arthur Support to configure them. It's also important to point out that all 'default' values will be ignored by the installer.
For reference, this is the same configuration template, grouped by topic, and with only the most necessary parameters uncommented:
YAMLapiVersion: kots.io/v1beta1
kind: ConfigValues
metadata:
creationTimestamp: null
name: arthur
spec:
values:
###############################################################################
# Install privileges.
# Change to ""namespace_scope"" for restricted permissions.
###############################################################################
# Values: namespace_scope, cluster_scope
rbac_privileges:
value: cluster_scope
###############################################################################
# Ingress
###############################################################################
ingress_hostname:
value: arthur.mlops.company.com
ingress_nginx_additional_hostname:
value: ""a1788baaec5c4473aa4ec3bf4ef81bb5.XXXXXXXXXX.us-east-1.elb.amazonaws.com""
ingress_class:
# Values: ""nginx"", ""ambassador""
value: nginx
ingress_ambassador_enabled:
value: ""false""
ingress_namespace_label_key:
value: name
ingress_namespace_label_value:
value: ingress-system
disable_ssl_redirect_on_ingress:
value: ""0""
# Ingress for new UI
beta_ui:
value: ""1""
beta_ui_alternate_site:
value: ""1""
beta_ui_hostname:
value: ""arthur.mlops.company.com""
###############################################################################
# Installation Type
###############################################################################
# The `single` configuration is a minimal deployment suitable for non-production environments.
# For production deployment, select `ha`.
single_or_ha:
value: ha
# The `fixed` mode is recommended for clusters with a fixed number of nodes. The `autoscale` mode is used for
# clusters that can autoscale and automatically expand their node count.
# 'autoscale' will assume a minimum of 6 nodes, do not set for 'autoscale' for clusters with < 6 nodes
fixed_or_autoscale:
value: fixed
# Only Relevant for ""fixed"" cluster sizes.
Enter the number of nodes in the cluster. This number
# cannot be decreased from the current value unless it's greater than `6`.
cluster_nodes:
value: ""3""
# Provide Kubernetes StorageClass profile. Use 'gp2' or 'gp3' for Amazon EKS, 'default' if you're using
# embedded Kubernetes provided by the installer
k8_storageclass:
value: gp3
# Network Policy
network_policy_enabled:
value: ""0""
###############################################################################
# Security and Authentication
###############################################################################
run_as_root:
value: ""0""
arthur_user_id:
value: ""1000""
# Single Sigh On
# Values: ""none"", ""oidc"", ""saml""
global_identity_provider:
value: none
oidc_identity_provider_config_yaml: {}
oidc_identity_provider_config_yaml_user: {}
saml_identity_provider_config_yaml: {}
saml_identity_provider_config_yaml_user: {}
# IAM Integration - Values: ""access_keys"", ""IRSA"", ""IAM Node Roles""
IAM_permission_type:
value: ""IRSA""
irsa_annotations:
value: 
eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/arthur-eks-role
irsa_annotations_user:
value: 
eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/arthur-eks-role
# SSL / Token Signature
show_token_signing_and_validation_options:
value: ""0""
signing_cert: {}
signing_cert_user: {}
signing_private_key: {}
signing_private_key_user: {}
# token_signing_primary_key:
#
value: YSDFzjg5I83KMBJ+wHQmU/ejDQ7tTthIpaDcCRM+iqcDTofiul7DZzTblFkb0e2U0+UJ74TuIx28oGnxPM+pkmKlc1yx2uvj
# secondary_token_validation_key:
#
value: Aj3ziCI/YcnTT3QR3WAtMNDNEzzqTa8W9iJCoHjNFMteiO6lrcnUKw==
# api_token_ttl:
#
value: ""24""
###############################################################################
# S3 Integration
###############################################################################
# Select ""Yes"" if and only if you are supplying your own S3 compatible storage,
# otherwise select ""No"" to use the embedded blob storage.
use_external_blob_storage:
value: ""yes""
s3_access_key_id: {}
s3_access_key_id_user: {}
s3_secret_access_key: {}
s3_secret_access_key_user: {}
s3_url: {}
s3_url_user: {}
s3_bucket:
value: arthur-s3-eks
s3_bucket_user:
value: arthur-s3-eks
s3_region:
value: us-east-1
s3_region_user:
value: us-east-1
###############################################################################
# Superadmin Configuration
###############################################################################
superadmin_email:
value: [email protected]
superadmin_firstname:
value: Super
superadmin_lastname:
value: Admin
superadmin_password:
value: Password1234
superadmin_username:
value: superadmin
###############################################################################
# SMTP Configuration
###############################################################################
# Values: ""ses"", ""smtp"", ""none""
email_selection:
value: none
use_smtp:
value: ""0""
# Provide the email address to send alerts from (e.g. [email protected])
smtp_from: {}
# Provide the address of the SMTP server (e.g. smtp.arthur.ai)
smtp_host: {}
smtp_password:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
smtp_port: {}
smtp_user: {}
# SES Configuration
ses_region: {}
ses_role: {}
###############################################################################
# Prometheus Integration
###############################################################################
# Leave the default configuration if you're using the embedded K8s.
# Provide your Prometheus hostname if you're running your own K8s.
prometheus_host:
value: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local
# If your prometheus installation requires labels to identify ServiceMonitors and PrometheusRules,
# add them here. They should be in yaml format just as you would specify inside the ""metadata.labels"" block.
# Do not indent.
prometheus_labels:
value: 
prometheus: monitor
app: prometheus
prometheus_namespace:
value: monitoring
# Leave the default configuration if you're using the embedded K8s.
# Provide your Prometheus hostname if you're running your own K8s.
prometheus_port:
value: ""9090""
###############################################################################
# Private Python Registry
###############################################################################
use_raw_python_repository_configs:
# The PyPi registry section is only relevant when using the explainability enrichment
# (https://docs.arthur.ai/user-guide/enrichments.html#explainability).
# Provide your private PyPi registry if you have an airgapped enrivonment or your model requirements file
# includes packages only hosted in a private repository.
# Leaving this section blank will cause the public PyPi to be used. If the public PyPi is inaccessible from the
# cluster, the explainability feature will not work.
value: ""no""
# This is set as a channel in the '.condarc' file. Do not include 'https://' prefix
# (e.g. repository.arthur.ai/repository/conda-proxy/main).
pypi_registry_conda: {}
pypi_registry_conda_user: {}
# This maps to the 'index key' in the 'pip.conf' file. Do not include 'https://' prefix
# (e.g repository.arthur.ai/repository/pypi-virtual/pypi).
pypi_registry_index: {}
# This maps to the 'index-url' key in the 'pip.conf' file. Do not include 'https://' prefix
# (e.g. repository.arthur.ai/repository/pypi-virtual/simple).
pypi_registry_index_url: {}
pypi_registry_index_url_user: {}
pypi_registry_index_user: {}
pypi_registry_password:
value: password1234
pypi_registry_password_user:
value: password1234
pypi_registry_username: {}
pypi_registry_username_user: {}
raw_anaconda_config: {}
raw_anaconda_config_user: {}
raw_pypi_config: {}
raw_pypi_config_user: {}
# Relevant if you are using Explainability and your organization is behind a proxy server.
# If PIP and/or Conda need to route through the proxy server to pull down public packages this will set
# the environment variable NO_PROXY to the supplied value. Ex. localhost,127.0.0.1,.my-company.com
no_proxy: {}
no_proxy_user: {}
# Relevant if you are using Explainability and your organization is behind a proxy server.
# If PIP and/or Conda need to route through the proxy server to pull down public packages this will
# set the environment variable HTTP_PROXY to the supplied value. Ex. http://sysproxy.my-company.com:port
http_proxy: {}
http_proxy_user: {}
https_proxy: {}
https_proxy_user: {}
###############################################################################
# Postgres Integration
###############################################################################
use_external_postgres:
value: ""yes""
database_admin_password:
value: password_for_RDS_admin_user
database_hostname:
# Leave the default configuration to use the embedded database. If you would like to use an external
# Postgres instance, provide the hostname here and follow this guide:
# https://docs.arthur.ai/platform-management/installation/externalize_postgres.html.
value: arthur-db.cluster-xptowtzabcd.us-east-1.rds.amazonaws.com
database_username:
value: arthurai
database_password:
value: password_for_RDS_arthurai_user
database_port:
value: ""5432""
database_ssl_mode:
# This option allows you to enable SSL communication between services and the postgres database.
# See https://www.postgresql.org/docs/10/libpq-ssl.html for full descriptions of each option.
# By default, the postgres database has ssl disabled.
value: disable
existing_database_primary_pvc: {}
#
meta_replicas:
#
value: ""0""
###############################################################################
# OLAP DB Settings
###############################################################################
# OLAP Password
olap_database_operator_password:
# The OLAP database is installed along with a Kubernetes Operator to manage it.
# This operator needs credentials to access the database.
We recommend overwriting the default password below.
value: gntnboTNbQpxmzx4GuPCRnjqSNwTpOT6FwgQ9q4iY7CHiQLeFQ3snnZgxYnFt4gSyInce3KhYiMR7eebBtGbe5sIuY
olap_database_user_password:
# Password used internally in our application to query the olap database,
# currently only supports alpha-numeric characters.
value: gntnboTNbQpxmzx4GuPCRnjqSNwTpOT6FwgQ9q4iY7CHiQLeFQ3snnZgxYnFt4gSyInce3KhYiMR7eebBtGbe5sIuY
# enable_password_rotation_olap:
#
value: ""0""
# password_rotation_cron_schedule:
#
value: 0 0 1 */6 *
# # OLAP Backup
# enable_olap_backup:
#
value: '""0""'
# enable_olap_backup_user:
#
value: ""0""
# number_of_olap_backups_to_keep:
#
value: ""7""
# olap_backup_s3_bucket:
#
value: denisd-s3-eks
# olap_backup_s3_bucket_region:
#
value: us-east-1
# olap_backup_s3_endpoint:
#
value: s3.us-east-1.amazonaws.com
# olap_backup_s3_path:
#
value: olap_backups
# olap_backup_service_account:
#
value: arthurai-arthurai
# # OLAP Optimization
# advanced_olap_options:
#
value: ""0""
# olap_cpu_limits: {}
# olap_cpu_limits_not_validate: {}
# olap_cpu_requests:
#
value: 1000m
# olap_cpu_requests_not_validate: {}
# olap_memory_limits: {}
# olap_memory_limits_not_validate: {}
# olap_memory_requests:
#
value: 1Gi
# olap_memory_requests_not_validate: {}
# olap_node_label_key: {}
# olap_node_label_value: {}
# olap_replicas:
#
value: ""1""
# olap_zookeeper_cpu_limits: {}
# olap_zookeeper_cpu_limits_not_validate: {}
# olap_zookeeper_cpu_requests:
#
value: 500m
# olap_zookeeper_cpu_requests_not_validate: {}
# olap_zookeeper_heap_options:
#
value: -Xms4G -Xmx4G
# olap_zookeeper_memory_limits: {}
# olap_zookeeper_memory_limits_not_validate: {}
# olap_zookeeper_memory_requests:
#
value: 1Gi
# olap_zookeeper_memory_requests_not_validate: {}
###############################################################################
# Cache Settings
###############################################################################
cache_password:
value: VwC3tnE9cpzObSxIhTx9U/34Ky+mA6p8veb9bCk+iqcAEaOarGzGEFf7ozoGxO3m05QY5YTuIx3ezMI694TUX0gj7RHSyHoK
# cache_replicas:
#
value: ""0""
# advanced_cache_options:
#
value: ""0""
# enable_password_rotation_cache:
#
value: ""0""
# # Cache Optimization
# cache_cpu_limits: {}
# cache_cpu_limits_not_validate: {}
# cache_cpu_requests: {}
# cache_cpu_requests_not_validate: {}
# cache_memory_limits: {}
# cache_memory_limits_not_validate: {}
# cache_memory_requests: {}
# cache_memory_requests_not_validate: {}
###############################################################################
# Kafka Optimization Settings
###############################################################################
# default_messaging_partition_count:
#
value: ""1""
# max_messaging_partition_count:
#
value: ""3""
# messaging_replicas:
#
value: ""3""
# kafka_ecosystem_common_replication_calc:
#
value: ""1""
# messaging_cpu_limit:
#
value: ""1""
# advanced_messaging_connect_cpu_limits:
#
value: ""2""
# advanced_messaging_connect_cpu_limits_not_validate: {}
# advanced_messaging_connect_cpu_requests:
#
value: ""1""
# advanced_messaging_connect_cpu_requests_not_validate: {}
# messaging_heap:
#
value: -Xmx2G -Xms1G
# messaging_memory_limit_and_request:
#
value: 2560Mi
# advanced_messaging_connect_heap_options:
#
value: -Xms1g -Xmx3g
# advanced_messaging_connect_memory_limits:
#
value: 4Gi
# advanced_messaging_connect_memory_limits_not_validate: {}
# advanced_messaging_connect_memory_requests:
#
value: 2Gi
# advanced_messaging_connect_memory_requests_not_validate: {}
# messaging_rack_aware_enabled:
#
value: ""0""
# messaging_rack_label:
#
value: topology.kubernetes.io/zone
# messaging_sa_create:
#
value: ""0""
# messaging_sa_fullnameoverride: {}
# messaging_zookeeper_timeout:
#
value: ""20000""
###############################################################################
# Audit Log Settings
###############################################################################
# enable_audit_log:
#
value: ""0""
# audit_log_event_bridge_bus_name: {}
# audit_log_event_bridge_bus_region: {}
# audit_log_event_bridge_detail_type:
#
value: events.arthur.ai
# audit_log_event_bridge_source:
#
value: arthur-audit-log
# audit_log_sink_destination:
#
value: none
###############################################################################
# Admin Console Settings
###############################################################################
# advanced_other:
#
value: ""1""
# show_advanced_arthur_microservice_options:
#
value: ""0""
# show_advanced_messaging:
#
value: ""1""
# show_hidden_variables:
#
value: ""0""
# config_job_and_workflow_retention:
#
value: ""0""
###############################################################################
# Backend Performance Optimization Settings
###############################################################################
# Limits
# global_model_limit_Count:
#
value: ""500""
# global_model_limits:
#
value: ""0""
# global_workflow_parallelism:
#
value: ""150""
# # Job Limits
# batch_workflow_parallelism:
#
value: ""120""
# bootstrap_job_backoff_limit:
#
value: ""100""
# bootstrap_job_ttl:
#
value: ""86400""
# pending_batch_workflows_limit:
#
value: ""100""
# workflow_ttl_seconds:
#
value: ""3600""
# workflow_ttl_seconds_after_success:
#
value: ""60""
# # Ingestion Service Optimizations
# ingestion_service_cpu_limits: {}
# ingestion_service_cpu_limits_not_validate: {}
# ingestion_service_cpu_requests: {}
# ingestion_service_cpu_requests_not_validate: {}
# ingestion_service_memory_limits: {}
# ingestion_service_memory_limits_not_validate: {}
# ingestion_service_memory_requests: {}
# ingestion_service_memory_requests_not_validate: {}
# # Explainability Optimizations
# model_servers_always_on:
#
# For use with what-if and on-demand explainability. See https://docs.arthur.ai/user-guide/explainability.html
#
# If set to ""true"", then on-demand and what-if explanations are available, but uses additional cluster
#
# resources, 1 CPU and 1 GB memory per model with explainability enabled. If set to ""false"", on-demand
#
# and what-if explanations are unavailable, but less cluster usage when there is no data being sent.
#
# Regardless of the setting here, streaming explainability will be available if enabled.
#
# This only effects what-if and on-demand explanations.
#
value: ""true""
# max_model_server_replicas:
#
value: ""2""
# metric_service_update_default_metrics:
#
value: ""0""
# alert_service_update_rule_metrics:
#
value: ""0""
###############################################################################
# Internal Arthur Use
###############################################################################
# cicd_credentials:
#
value: ""0""
# existing_or_vm:
#
value: existing_cluster
# full_name_override:
#
value: arthurai
# min_arthur_replicas:
#
value: ""1""
# max_arthur_replicas:
#
value: ""1""
status: {}
PS: This template assumes integration with S3 and RDS database, using IRSA (IAM Roles for Service Accounts) configured.
Most of these settings can be modified in the Admin Console UI. The Config screen will include fields for all the settings in the template:
Some of the settings in this template can cause an installation to fail, if they are not correctly set. Some of the critical parameters are:
YAML
k8_storageclass:
value: gp3
This parameter defines the storage class that will be used to create the Persistent Volumes for the cluster. Having the wrong storage class defined here will cause the installer to fail to provision storage, which will compromise the installation. Make sure to have the correct value set before running the installer.
YAML
ingress_hostname:
value: arthur.mlops.company.com
ingress_nginx_additional_hostname:
value: ""a1788baaec5c4473aa4ec3bf4ef81bb5.XXXXXXXXXX.us-east-1.elb.amazonaws.com""
These parameters will configure which ingress addresses Arthur will accept. By definition, Arthur will reject requests sent to addresses that are not in this list, even if they are correct (for instance, accessing Arthur through https://localhost does not work). If these hostname parameters do not match existing load balancer addresses, Arthur will be inaccessible, even once the instance is successfully installed. This can be reconfigured after installation, though it is recommended to ensure the proper value at installation time.Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/config-template
65,"Restoring the Arthur Platform
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Restoring the Arthur PlatformSuggest EditsThis document details restoring various Arthur platform components from existing backups.
Restore RDS Postgres
Use the AWS RDS documentation to restore the database from an existing RDS Snapshot.
Please ensure that you correctly match the following configuration of the DB from which the snapshot was taken:
The connection port
The VPC and Security Group IDs
DB Subnet Group
DB Instance Type
Any other configuration which might be overridden
This operation might take a while, and the DB must show as Available before proceeding to install the platform.
Install the Arthur Platform
🚧Ensure Database is ReadyOnly proceed to installing the Arthur platform AFTER the restored database shows as ""Available"" in the RDS Console.
Install the Arthur platform either using the Airgap Kubernetes Cluster (K8s) Install` or Online Kubernetes Cluster (K8s) Install. Although most configurations for the Arthur platform should remain the same, the following two configurations might need to be updated:
The ""Meta Database"" section of the Admin Console should point to the newly restored DB instance.
🚧Ensure restore cluster is pointing to the right locationIt is very critical to update the configuration to point to the newly restored DB Instance. Failure to complete this step WILL CAUSE DATA CORRUPTION.
Update the ingress URL in the ""Network"" section of the Admin Console.
Wait for the platform to come back online before proceeding to the next steps. All Deployments and StatefulSets should be completely stood up (eg: all Pods should be ready and ""Running"") and all Jobs should be ""Completed"".
$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl get pods -n $arthur_namespace
NAME
READY
STATUS
RESTARTS
AGE
argo-workflows-server-75fc4d9d55-wfsqc
1/1
Running
0
12h
argo-workflows-workflow-controller-7b95b66b94-66hrs
1/1
Running
0
119m
arthurai-alert-service-858784dd7f-4kgq2
1/1
Running
0
4h58m
arthurai-api-service-7fc58f4958-trcvg
1/1
Running
0
4h58m
arthurai-custom-hpa-646bb978dd-t9b68
1/1
Running
0
12h
arthurai-dataset-service-86c8dd54cc-bwwtr
1/1
Running
0
4h58m
arthurai-frontend-78cc85fbc5-ffx79
1/1
Running
0
12h
arthurai-frontend-beta-5cb8756f68-8hljq
1/1
Running
0
12h
arthurai-frontend-classic-5ff79bd579-rhqv8
1/1
Running
0
12h
arthurai-ingestion-service-5f7896bf5c-jxwvk
1/1
Running
0
12h
arthurai-ingestion-service-5f7896bf5c-vzdn2
1/1
Running
0
4h58m
arthurai-kafka-connect-monitor-54cfcc8f7d-dcgr6
1/1
Running
2
12h
arthurai-metric-service-78f85cb548-s65dj
1/1
Running
0
12h
arthurai-query-service-64d7c9f846-h2ms9
1/1
Running
0
2d12h
arthurai-schema-service-69b8c484bd-thhkr
1/1
Running
0
4h58m
cache-master-0
1/1
Running
0
12h
cache-slave-0
1/1
Running
0
17h
cache-slave-1
1/1
Running
0
119m
chi-olap-installation-arthur-0-0-0
2/2
Running
0
119m
chi-olap-installation-arthur-0-1-0
2/2
Running
0
12h
chi-olap-installation-arthur-0-2-0
2/2
Running
0
17h
database-primary-0
1/1
Running
0
12h
database-read-0
1/1
Running
0
17h
database-read-1
1/1
Running
0
119m
kafka-exporter-744dbd8476-wwztw
1/1
Running
0
45h
kotsadm-5db494c84f-b9vtq
1/1
Running
0
119m
kotsadm-minio-0
1/1
Running
0
17h
kotsadm-rqlite-0
1/1
Running
0
12h
messaging-0
2/2
Running
0
17h
messaging-1
2/2
Running
2
11h
messaging-2
2/2
Running
0
119m
messaging-connect-5db8c6fbcf-jk7st
2/2
Running
0
45h
messaging-connect-5db8c6fbcf-jstq7
2/2
Running
0
119m
messaging-cp-zookeeper-0
3/3
Running
0
17h
messaging-cp-zookeeper-1
3/3
Running
0
2d11h
messaging-cp-zookeeper-2
3/3
Running
0
119m
messaging-schema-registry-7c646d8c7-mxshj
2/2
Running
0
2d12h
messaging-schema-registry-7c646d8c7-q77bh
2/2
Running
0
119m
messaging-schema-registry-7c646d8c7-z5s4v
2/2
Running
0
45h
olap-installation-zookeeper-0
3/3
Running
0
2d12h
olap-installation-zookeeper-1
3/3
Running
0
17h
olap-installation-zookeeper-2
3/3
Running
0
119m
olap-operator-7999d4fdb8-kkprt
2/2
Running
0
119m
$ kubectl get jobs -n $arthur_namespace
NAME
COMPLETIONS
DURATION
AGE
arthurai-additional-images-bootstrap-xmmes
0/1
16s
16s
arthurai-api-key-bootstrap-cfrhy
0/1
16s
16s
arthurai-database-migration-hhlpc
0/1
16s
16s
arthurai-default-entities-bootstrap-lddho
0/1
15s
16s
arthurai-meter-events-connector-deploy-xvxgz
0/1
15s
15s
arthurai-model-health-score-connector-deploy-qhvg1
0/1
15s
15s
arthurai-query-service-migration-17ehe
0/1
15s
15s
clickhouse-v23-migration-job-xynkq
0/1
15s
15s
messaging-config-031e7f1c
0/1
15s
15s
Restore ClickHouse Data
The Arthur Platform ships with a Kubernetes CronJob that executes a ClickHouse restore that is scheduled never to run.
To restore ClickHouse data, execute the following commands:
Get the name of the clickhouse-backup that coincides with the kafka/enrichments/workflow backups that you are restoring
Using the clickhouse pod itself -
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl exec chi-olap-installation-arthur-0-0-0 -n $arthur_namespace -c backup -- clickhouse-backup list
<<<output-truncated-for-brevity>>>
2023/05/12 15:12:16.199434
info SELECT * FROM system.macros logger=clickhouse
chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-11-00-00-07
10.33MiB
11/05/2023 00:00:14
remote
tar, regular
chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-11-00-00-07
10.33MiB
11/05/2023 00:00:15
remote
tar, regular
chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-11-00-00-07
10.33MiB
11/05/2023 00:00:15
remote
tar, regular
chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-12-00-00-06
10.33MiB
12/05/2023 00:00:14
remote
tar, regular
chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-12-00-00-06
10.33MiB
12/05/2023 00:00:14
remote
tar, regular
chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-12-00-00-06
10.33MiB
12/05/2023 00:00:15
remote
tar, regular
2023/05/12 15:12:18.317324
info clickhouse connection closed logger=clickhouse
Using AWS S3 CLI -
Shell$ aws s3 ls s3://<s3-bucket-name>/<backup-path>/ --profile AWS_PROFILE
PRE chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-11-00-00-07/
PRE chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2023-05-12-00-00-06/
PRE chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-11-00-00-07/
PRE chi-olap-installation-arthur-0-1-arthur-clickhouse-backup-2023-05-12-00-00-06/
PRE chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-11-00-00-07/
PRE chi-olap-installation-arthur-0-2-arthur-clickhouse-backup-2023-05-12-00-00-06/
Extract the ARTHUR_BACKUP_NAME from the backups. The backups are named in the CLICKHOUSE_NODE_NAME-ARTHUR_BACKUP_NAME format. For example, chi-olap-installation-arthur-0-0-arthur-clickhouse-backup-2022-05-12-00-00-06 can be parsed into:
Clickhouse node name: chi-olap-installation-arthur-0-0
Arthur's backup name: arthur-clickhouse-backup-2022-05-12-00-00-06
Create the restore job, and configure it to use the Arthur backup name from above
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl create job --from=cronjob/clickhouse-restore-cronjob -n $arthur_namespace -o yaml clickhouse-restore --dry-run=client --save-config > clickhouse-restore.yaml
$ backup_name=""arthur-clickhouse-backup-2022-05-12-00-00-06"" # value extracted in above step
$ sed -i -e ""s/insert-backup-name-here/$backup_name/"" clickhouse-restore.yaml
$ cat clickhouse-restore.yaml  grep -C2 ""name: BACKUP_NAME"" # verify the replacement is correct
$ kubectl apply -f clickhouse-restore.yaml -n $arthur_namespace
job.batch/clickhouse-restore created
Restore Messaging Infrastructure
The Arthur Platform restores Kafka Deployment State, PersistentVolumes, and PersistentVolumeClaims using Velero.
To restore the messaging infrastructure (Kafka and ZooKeeper), run the following commands:
Delete the StatefulSets related to messaging infrastructure that was created while installing the platform
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl get sts -n $arthur_namespace  grep -i messaging # there should only be two STSs returned
$ kubectl delete sts messaging -n $arthur_namespace
$ kubectl delete sts messaging-cp-zookeeper -n $arthur_namespace
Delete the PersistentVolumeClaims related to messaging infrastructure that was created while installing the platform
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl get pvc -n $arthur_namespace  grep -i messaging # the number of PVCs returned depends on your configuration
$ kubectl get pvc -n $arthur_namespace  grep -i messaging  awk '{print $1}'  xargs kubectl delete pvc -n $arthur_namespace
Confirm the PersistentVolumes have automatically been deleted (due to a 'delete' retention policy)
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl get pv -n $arthur_namespace  grep -i messaging  wc -l # should return 0
If the PersistentVolumes still do not get deleted automatically after a few minutes, delete them manually
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl get pv -n $arthur_namespace  grep -i messaging # the number of PVs returned depends on your configuration
$ kubectl get pv -n $arthur_namespace  grep -i messaging  awk '{print $1}'  xargs kubectl delete pv -n $arthur_namespace
🚧Make sure restore steps are completeDo not proceed until the above deletion commands have fully completed. Check with the kubectl get <resource> commands.
Get the relevant Velero Backup by using the Velero CLI:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero backup get -n $velero_namespace  grep messaging
NAME
STATUS
ERRORS
WARNINGS
CREATED
EXPIRES
STORAGE LOCATION
SELECTOR
arthur-backup-2023-05-11t15.22.37-04.00-messaging
Completed
0
0
2023-05-11 15:22:48 -0400 EDT
27d
docs-demo-storage-location
app in (cp-kafka,cp-zookeeper)
$ velero restore create \
--from-backup ""arthur-backup-2023-05-11t15.22.37-04.00-messaging"" \
--namespace $velero_namespace
Velero will update the Pod Specs, point to the PVs using the EBS Volume Snapshots, and restore the kubernetes resources associated with Kafka.
Wait for the messaging infrastructure and Arthur platform to become ""Ready""
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl kots get apps -n $arthur_namespace
SLUG
STATUS
VERSION
arthur
ready
3.4.0
Restore Enrichments
The Arthur Platform uses Velero to restore the Enrichments infrastructure and workflows, which will require running 2 separate commands.
Restore Enrichments Infrastructure
To restore the enrichments infrastructure, run the following commands:
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero backup get -n $velero_namespace  grep enrichments
NAME
STATUS
ERRORS
WARNINGS
CREATED
EXPIRES
STORAGE LOCATION
SELECTOR
arthur-backup-2023-05-11t15.25.24-04.00-enrichments
Completed
0
0
2023-05-11 15:25:33 -0400 EDT
27d
default
component in (kafka-mover-init-connector,model_server)
$ velero restore create \
--from-backup ""arthur-backup-2023-05-11t15.25.24-04.00-enrichments"" \
--namespace $velero_namespace
Restore Enrichments Workflows
Restoring the workflows is a 2-step process:
Restore the workflows from the Velero backup
Shell$ velero_namespace=""Put your Velero namespace here""
$ velero backup get -n $velero_namespace  grep workflows
NAME
STATUS
ERRORS
WARNINGS
CREATED
EXPIRES
STORAGE LOCATION
SELECTOR
arthur-backup-2022-09-23t11.23.25-04.00-workflows
Completed
0
0
2022-09-23 11:24:35 -0400 EDT
27d
default
<none>
$ velero restore create \
--from-backup ""arthur-backup-2022-09-23t11.23.25-04.00-workflows"" \
--namespace $velero_namespace
Restore Batch Workflows which are recoverable using an Arthur Admin Endpoint
In one terminal window, port-forward to the dataset service:
Shell$ arthur_namespace=""Put your Arthur namespace here""
$ kubectl port-forward -n $arthur_namespace svc/arthurai-dataset-service 7899:80
In another terminal window, run the following commands:
Shell$ curl -k -XPOST http://localhost:7899/api/v1/workflows/batch/recover
{""message"":""success""}
Smoke Tests and Validation
The restore process is now complete. All data should be restored and consistent from when the backup was taken. Any data sent during or after the backup will need to be re-sent. Perform any validation/smoke tests to ensure that the platform is operating.Updated 3 months ago Table of Contents
Restore RDS Postgres
Install the Arthur Platform
Restore ClickHouse Data
Restore Messaging Infrastructure
Restore Enrichments
Restore Enrichments Infrastructure
Restore Enrichments Workflows
Smoke Tests and Validation",arthur_scope_docs,https://docs.arthur.ai/docs/restoring-the-arthur-platform
66,"Ranked List Outputs Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Ranked List Outputs OnboardingSuggest EditsThis page walks through the basics of setting up a recommender system model (ranked list output) and onboarding it to Arthur Scope to monitor performance. The inputs to a recommender system model could be time series inputs or more traditional tabular inputs.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur Scope.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"")
Registering a Recommender System Model
Each recommender system model is created with a name and with output_type = OutputType.RankedList. Here, we register a recommender model:
Pythonarthur_model = arthur.model(name=""RecSysQuickstart"",
input_type=InputType.Tabular,
model_type=OutputType.RankedList)
Formatting Reference/Inference Data
Column names can contain only alphanumeric and underscore characters.
Ranked list data can be uploaded to Arthur either in a DataFrame or a JSON file. Typically, a JSON file is a more natural formatting for ranked list data. For a recommender system model recommending a loan policy, the reference data might look like this:
JSON{
""reference_data"": {
""id"": ""6euQxGJai11qr0gENGgvgh"",
""account_id"": ""8klQSGJil78qr4gLJKklsy"",
""recommendations"": [
{
""label"": ""Loan Policy 3"",
""item_id"": ""64NWp2MbJXd7oHg7XmXCIa"",
""score"": 90
},
{
""label"": ""Loan Policy 1"",
""item_id"": ""0CoZWIVqaHHGArYRTJD1V5"",
""score"": 83
},
],
""gt"": [
""0CoZWIVqaHHGArYRTJD1V5"", // ids of relevant recommendations
""72EAUBslQ047R3j9dxMCf4"",
]
},
... // more inferences here
}
Data Requirements
The list of ranked list items should be sorted in rank order, such that the highest ranked item is first.
Each ranked list output model in Arthur can have max 1000 total unique recommended items in its reference dataset.
Each ranked list output model can have max 100 recommendations per inference/ground truth.
If the label or score metadata field in a ranked list item is specified for one inference, it must be specified for all of them.
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save(), you can call arthur_model.review() on the model schema to check that your data was parsed correctly in your call to arthur_model.build().
For a recommender system model, the model schema should look like this:
Python		 name
stage
value_type
categorical
is_unique
0
ranked_list_pred_attr PREDICTED_VALUE
RANKED_LIST
False
False
1
ground_truth
GROUND_TRUTH
ARRAY(STRING)
False
False
...
2
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using arthur_model.review(), you are finished registering your model and its attributes - so you are ready to complete onboarding your model.
See this guide for further details on how to save your model, send inferences, and get performance results from Arthur. These steps are the same for recommender system models as for models of any InputType and OutputType .Updated about 2 months ago Table of Contents
Getting Started
Registering a Recommender System Model
Formatting Reference/Inference Data
Data Requirements
Reviewing the Model Schema
Finishing Onboarding",arthur_scope_docs,https://docs.arthur.ai/docs/ranked-list-outputs-onboarding
67,"Exporting Platform Configurations
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Exporting Platform ConfigurationsSuggest EditsAfter a successful installation of the Arthur platform, you can download the configuration that was used as a .yaml file by following the below steps:
Navigate to the Admin Console and login.
Click on the ""View Files"" tab.
Go to upstream → userdata and click on config.yaml file.
Copy the contents of that file and save it for future use (or check-in to source control).
Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/exporting-platform-configurations
68,"Anomaly Detection
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Anomaly DetectionMonitor and alert on incoming changes to your data distribution based on complex interactions between featuresSuggest EditsGo beyond single distribution analysis and look at complex interactions between features that may cause drift (multivariate). Both in low, but especially in high dimensional spaces where there is often data sparsity, anomaly detection can help capture changes in complex interactions between features.
Anomaly Detection in Practice
Anomaly Detection as Multivariate Drift
Model-based monitoring techniques allow you to look at inferences as a whole (not split by individual distributions) to better understand how many anomalous or weird inputs your model is receiving. Special use cases such as NLP and Computer Vision have different feature relationships than traditional tabular methods, which require model-based anomaly detection to capture their drift.
Search Out Anomalous Inferences
Easily investigate anomalous inferences by filtering by individual anomaly scores in the inference tab. While also available for Tabular or Text inputs, we can see this example in Arthur below.
In the example gif provided above, we are looking into a model trained to detect whether or not a satellite image is from France or Russia. This model was only trained on satellite images of amusement parks. Here, we can see the most anomalous inferences are of stadiums (a context that the model was not trained to understand).
Teams frequently use this feature to find potential data pipeline issues, select inputs for re-training, and better understand the environment their model is running within.
Inference Anomaly Score Distribution Chart
For each inference, teams can also visualize the anomaly score distribution chart. This chart represents all of the anomaly scores in your Reference Dataset as a point of comparison. Inferences are ranked on a scale of 0-1, where 0 represents no anomaly, and 1 represents an anomalous input.
Raw Anomaly Score (For Text and Image Input Types: As you can see in the visual above, there is the ability to visualize Raw Anomaly Score in the distribution chart. This is only available for text and image model types. The raw anomaly scores are the calculated loss of the model used to predict anomaly. Due to the way anomalies are scored against the reference dataset, we recommend using the raw anomaly score to track and sort anomalies in instances such as image quality assurance where all images are the same.
Understanding the Algorithm
To learn more about the algorithms used for anomaly detection. Please refer to the Arthur Algorithms documentation section.
Available Arthur Schemas
Anomaly Detection can be enabled for models with any Input or Output Type other than Time Series input type models. Only a reference data set is required - this can be a set of the model's train or test data. Once a reference set is uploaded, anomaly scores are calculated automatically.Updated 2 months ago What’s NextLearn more about enabling enrichments for your model in the Model Onboarding section. Otherwise, click on Hot Spots to learn about another type of enrichment.Enabling EnrichmentsHot SpotsTable of Contents
Anomaly Detection in Practice
Anomaly Detection as Multivariate Drift
Search Out Anomalous Inferences
Inference Anomaly Score Distribution Chart
Understanding the Algorithm
Available Arthur Schemas",arthur_scope_docs,https://docs.arthur.ai/docs/anomaly-detection
69,"Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by MetricsSuggest EditsMetrics are functions for measuring model performance. They might compare predicted values to ground truth, measure distributional shift, evaluate model fairness, surface explainability trends, track feature distributions, inference volumes, or anything else you can imagine. Metrics are a foundational part of evaluating and exploring your models. Arthur’s powerful Metrics API gives you the defaults you need to hit the ground running and the flexibility to define model performance however it best suits your business.
Arthur’s metrics are defined as template queries in our Query API
format. These template queries are evaluated with your specified parameters, filters, and rollup. For example, when you’re viewing a Feature Drift chart in the UI, behind the scenes, the Arthur dashboard is:
evaluating the Feature Drift metric for your model
specifying your selected Drift Metric parameter (e.g., “PSI”) and your specified Drift Attribute parameter(s) (e.g. “Age” and “FICO Score” input attributes)
specifying your selected timeframe as a filter over the “inference_timestamp” field
specifying your selected rollup of “day,” “hour”, etc., to determine the granularity of the graph (or “batch_id” for batch models)
Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/metrics-1
70,"Arthur SDK Reference documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
Arthur SDK Reference
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Arthur Docs
SDK Home
Permissions by Function
arthuraiToggle child pages in navigation
arthurai.clientToggle child pages in navigation
arthurai.client.apiv3
arthurai.client.authToggle child pages in navigation
arthurai.client.auth.AuthRefresher
arthurai.client.clientToggle child pages in navigation
arthurai.client.client.new_requests_client
arthurai.client.client.ArthurAI
arthurai.client.helperToggle child pages in navigation
arthurai.client.helper.construct_url
arthurai.client.helper.get_arthur_internal_user_org
arthurai.client.helper.get_auth_info
arthurai.client.helper.get_current_org
arthurai.client.helper.user_login
arthurai.client.httpToggle child pages in navigation
arthurai.client.http.arthurToggle child pages in navigation
arthurai.client.http.arthur.ArthurHTTPClient
arthurai.client.http.baseToggle child pages in navigation
arthurai.client.http.base.AbstractHTTPClient
arthurai.client.http.requestsToggle child pages in navigation
arthurai.client.http.requests.HTTPClient
arthurai.client.validationToggle child pages in navigation
arthurai.client.validation.validate_multistatus_response_and_get_failures
arthurai.client.validation.validate_response_status
arthurai.commonToggle child pages in navigation
arthurai.common.constantsToggle child pages in navigation
arthurai.common.constants.AccuracyMetric
arthurai.common.constants.Enrichment
arthurai.common.constants.EnrichmentStatus
arthurai.common.constants.ImageContentType
arthurai.common.constants.ImageResponseType
arthurai.common.constants.InferenceType
arthurai.common.constants.InputType
arthurai.common.constants.ListableStrEnum
arthurai.common.constants.ModelStatus
arthurai.common.constants.OutputType
arthurai.common.constants.Role
arthurai.common.constants.Stage
arthurai.common.constants.TextDelimiter
arthurai.common.constants.TimestampInferenceType
arthurai.common.constants.ValueType
arthurai.common.exceptionsToggle child pages in navigation
arthurai.common.exceptions.arthur_excepted
arthurai.common.logToggle child pages in navigation
arthurai.common.log.disable_debug_logging
arthurai.common.log.enable_debug_logging
arthurai.common.log.initialize_logging
arthurai.common.log.InfoFilter
arthurai.coreToggle child pages in navigation
arthurai.core.alertsToggle child pages in navigation
arthurai.core.alerts.validate_parameters_for_alert
arthurai.core.alerts.Alert
arthurai.core.alerts.AlertRule
arthurai.core.alerts.AlertRuleBound
arthurai.core.alerts.AlertRuleSeverity
arthurai.core.alerts.AlertStatus
arthurai.core.alerts.Metric
arthurai.core.alerts.MetricType
arthurai.core.attributesToggle child pages in navigation
arthurai.core.attributes.get_attribute_order_stage
arthurai.core.attributes.ArthurAttribute
arthurai.core.attributes.AttributeBin
arthurai.core.attributes.AttributeCategory
arthurai.core.auth_infoToggle child pages in navigation
arthurai.core.auth_info.AuthInfo
arthurai.core.baseToggle child pages in navigation
arthurai.core.base.ArthurBaseJsonDataclass
arthurai.core.biasToggle child pages in navigation
arthurai.core.bias.bias_metricsToggle child pages in navigation
arthurai.core.bias.bias_metrics.BiasMetrics
arthurai.core.bias.bias_wrapperToggle child pages in navigation
arthurai.core.bias.bias_wrapper.ArthurBiasWrapper
arthurai.core.bias.threshold_mitigationToggle child pages in navigation
arthurai.core.bias.threshold_mitigation.Curves
arthurai.core.bias.threshold_mitigation.ThresholdMitigation
arthurai.core.data_serviceToggle child pages in navigation
arthurai.core.data_service.DatasetService
arthurai.core.data_service.ImageZipper
arthurai.core.dataset_validation_utilsToggle child pages in navigation
arthurai.core.dataset_validation_utils.ensure_obj_matches_attr_value_type
arthurai.core.dataset_validation_utils.get_first_elem_if_valid_list
arthurai.core.dataset_validation_utils.obj_value_type_mismatch_err
arthurai.core.dataset_validation_utils.valid_rec_obj
arthurai.core.dataset_validation_utils.validate_attr_names
arthurai.core.dataset_validation_utils.validate_series_data_type
arthurai.core.dataset_validation_utils.validate_token_likelihoods_type
arthurai.core.decoratorsToggle child pages in navigation
arthurai.core.decorators.log_prediction
arthurai.core.enrichment_status_waiterToggle child pages in navigation
arthurai.core.enrichment_status_waiter.await_enrichments_ready
arthurai.core.enrichment_status_waiter.EnrichmentStatusWaiter
arthurai.core.enrichment_status_waiter.StatusForEnrichment
arthurai.core.inferencesToggle child pages in navigation
arthurai.core.inferences.add_inference_metadata_to_dataframe
arthurai.core.inferences.add_predictions_or_ground_truth
arthurai.core.inferences.nest_inference_and_ground_truth_data
arthurai.core.inferences.nest_reference_data
arthurai.core.inferences.parse_stage_attributes
arthurai.core.model_status_waiterToggle child pages in navigation
arthurai.core.model_status_waiter.ModelStatusWaiter
arthurai.core.model_utilsToggle child pages in navigation
arthurai.core.model_utils.check_attr_is_bias
arthurai.core.model_utils.check_has_bias_attrs
arthurai.core.model_utils.get_positive_predicted_class
arthurai.core.model_utils.tensors_to_arthur_inference
arthurai.core.modelsToggle child pages in navigation
arthurai.core.models.ArthurModel
arthurai.core.models.ArthurModelGroup
arthurai.core.models.ExplainabilityParameters
arthurai.core.status_waiterToggle child pages in navigation
arthurai.core.status_waiter.StatusWaiter
arthurai.core.utilToggle child pages in navigation
arthurai.core.util.can_cast
arthurai.core.util.dataframe_like_to_list_of_dicts
arthurai.core.util.intersection_is_non_empty
arthurai.core.util.is_bool_like
arthurai.core.util.is_date_like
arthurai.core.util.is_float_like
arthurai.core.util.is_int_like
arthurai.core.util.is_list_like
arthurai.core.util.is_str_like
arthurai.core.util.is_valid_datetime_obj
arthurai.core.util.retrieve_json_files
arthurai.core.util.retrieve_parquet_files
arthurai.core.util.series_to_df
arthurai.core.util.standardize_pd_obj
arthurai.core.util.update_column_in_list_of_dicts
arthurai.core.util.NumpyEncoder
arthurai.core.vizToggle child pages in navigation
arthurai.core.viz.style
arthurai.core.viz.utilsToggle child pages in navigation
arthurai.core.viz.utils.get_pred_and_gt_attrs
arthurai.core.viz.utils.savgol_filter
arthurai.core.viz.visualizerToggle child pages in navigation
arthurai.core.viz.visualizer.DataVisualizer
arthurai.datasetsToggle child pages in navigation
arthurai.datasets.arthur_exampleToggle child pages in navigation
arthurai.datasets.arthur_example.ArthurExample
arthurai.datasets.arthur_example.ArthurExampleSchema
arthurai.datasets.downloadToggle child pages in navigation
arthurai.datasets.download.download_from_s3
arthurai.datasets.download.get_file_keys_in_s3_folder
arthurai.datasets.download.load_downloaded_file
arthurai.datasets.download.ArthurDatasetSource
arthurai.datasets.download.ArthurExampleDownloader
arthurai.explainabilityToggle child pages in navigation
arthurai.explainability.arthur_explainerToggle child pages in navigation
arthurai.explainability.arthur_explainer.ArthurExplainer
arthurai.explainability.arthur_explainer.EmptyLimeExplanation
arthurai.explainability.explanation_packagerToggle child pages in navigation
arthurai.explainability.explanation_packager.ExplanationPackager
arthurai.explainability.validationToggle child pages in navigation
arthurai.explainability.validation.validate_predicted_attribute_order_matches_dataframe
arthurai.explainability.validation.validate_predicted_attribute_order_matches_remote
arthurai.utilToggle child pages in navigation
arthurai.util.format_time_series_attr_timestamps
arthurai.util.format_timestamp
arthurai.util.format_timestamps
arthurai.util.generate_timestamps
arthurai.util.is_valid_datetime_string
arthurai.util.normal_random_ints_fixed_sum
arthurai.util.value_type_to_python_type
arthurai.version
Back to top
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Arthur SDK Python API Reference#
This page contains the complete Python API reference for Arthur’s SDK. For guides and concepts, see
our main docs.
Essentials#
There are a few touch points that are most commonly needed in the SDK. These are:
The ArthurAI client to create a connection to Arthur
Its ArthurAI.model() method to create a new (empty) model
Its ArthurAI.get_model() to fetch an existing model already registered with Arthur
The ArthurModel class (returned by both of the above methods) to interact with a particular model
Its ArthurModel.build() method to construct a new model from a DataFrame
Its ArthurModel.save() method to register a model with the Arthur platform
Contents#
Arthur Docs
SDK Home
Permissions by Function
arthurai
arthurai.client
arthurai.common
arthurai.core
arthurai.datasets
arthurai.explainability
arthurai.util
arthurai.version
Indices#
Index
Module Index
Next
Permissions by Function
Copyright © 2022, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Arthur SDK Python API Reference
Essentials
Contents
Indices",arthur_scope_docs,https://sdk.docs.arthur.ai
71,"Model Input / Output Types
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Model Input / Output TypesUnderstanding Model Schemas within Arthur ScopeSuggest EditsRole of a Model Schema in Arthur Scope
Arthur's model schema records important properties for your model's attributes, including their value type and stage. These data types define the data that enters and exits your model.
The InputType of a model specifies whether data enter your model as a tabular data frame, image, or raw text.
The OutputType of a model specifies the modeling task: whether your model predicts values for a regression task, probabilities for a classification task, bounding boxes for a computer vision object-detection task, or token sequences (and their probabilities for generative tasks).
Supported Schemas in Arthur Scope
Arthur Scope supports several input and output types.
The input types supported are:
The output types supported are:
Updated 2 months ago What’s NextQuick look at model monitoring in Arthur by typeTabularTextImageTable of Contents
Role of a Model Schema in Arthur Scope
Supported Schemas in Arthur Scope",arthur_scope_docs,https://docs.arthur.ai/docs/model-input-output-types
72,"Regression
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by RegressionSuggest EditsRegression models predict a numeric outcome. In Arthur, these models are listed under the regression model type.
Some common examples of Text regression are:
What is the predicted review score for written restaurant reviews?
Predict house price from description text
Formatted Data in Arthur
Text regression models require two columns: text input and numeric output. When onboarding a reference dataset (and setting a model schema), you need to specify a target column for each inference's ground truth. Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (Text Input)Prediction (numeric)Ground Truth (numeric)Non-Input Attribute (numeric or categorical)Dulce est desipere in loco45.3462.42High School EducationSi vis amari ama55.153.2Graduate Degree
Predict Function and Mapping
These are some examples of common values teams need to onboard for their regression models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics.
Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Single Column Ground Truth
output_mapping = {
'prediction_column':'gt_column'}
# Build Arthur Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
## Example prediction function for binary classification
def predict(x):
return model.predict(x)
Available Metrics
When onboarding regression models, there are several default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a regression model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeRoot Mean Squared ErrorPerformanceMean Absolute ErrorPerformanceR SquaredPerformanceInference CountIngestionAverage PredictionIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case text) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/text-regression-1
73,"Sending Ground Truth
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Sending Ground TruthSuggest EditsOne of the greatest differences between evaluating models in experimentation vs. monitoring in production is the delayed nature of responses.
A model that predicts an action may have ground truth available immediately after prediction. One common example is models, common in the realm of advertising, used to predict whether or not a user will click on the advertisement.
A bank using a model that predicts whether or not a customer will default on their loan in the first 6 months will not know whether or not they were correct until 6 months have passed or the customer defaults on their loan.
In rarer cases, though more often in models of unstructured data types (like text or image), ground truth may never be collected.
Due to these varying timelines of receiving ground truth data, many teams use Data Drift Metrics as proxies for performance when ground truth is delayed. With these techniques, however, it is still best practice to format and send in your ground truth data when available.
Formatting Ground Truth Data
After receiving ground truth data, matching ground truth labels with the correct inferences within Arthur is essential. To ensure this, Arthur requires two values for every inference value you would like to send ground truth for:
Ground Truth Label: True label for that inference row
Partner Inference ID: A unique inference identifier meant to connect inferences with how teams keep track of inferences internally
Sending Ground Truth Data with Python SDK
With the need to wait for ground truth, there tend to be three main workflows for teams updating their Arthur Model to receive ground truth :
At the time of prediction: Some ML models run in systems where ground truth is provided nearly instantaneously after prediction. In these instances, it is best to include ground truth as an additional column to there.send_inferences() workflow.
At the time of labeling: Similar to attaching send_inferences within the Python script where your model makes inferences, teams with a receiving ground truth workflow may choose to attach update_inference_ground_truths() to their Python script
In bulk: Other teams may wait to onboard ground truth labels until a certain number of labels or time has passed. Teams that choose to send labels in bulk from a data frame may either use.update_inference_ground_truths() or send_bulk_ground_truths() for updating ground truth for more than 100k inferences at a time
Python SDK####################################################
# we can collect a set of folder names each corresponding to a batch run, containing one or
#
more Parquet or Json files with the input attributes columns, non-input attribute columns, and
#
prediction attribute columns as well as a ""partner_inference_id"" column with our unique
#
identifiers and an ""inference_timestamp"" column
inference_batch_dirs = ...
# then suppose we have a directory with one or more parquet or json files containing matching
#
""partner_inference_id""s and our ground truth attribute columns as well as a
#
""ground_truth_timestamp"" column
ground_truth_dir = ...
# send the inferences to Arthur
for batch_dir in inference_batch_dirs:
batch_id = batch_dir.split(""/"")[-1]
# use the directory name as the Batch ID
arthur_model.send_bulk_inferences(
directory_path=batch_dir,
batch_id=batch_id)
# send the ground truths to Arthur
arthur_model.send_bulk_ground_truths(directory_path=ground_truth_dir)
Updating Ground Truth with the API
Ground truth can also be updated using the Arthur API using either:
The Update Inference Ground Truths endpoint
The Send Inference File endpoint and specifying the Ground Truth file
Updated 2 months ago Table of Contents
Formatting Ground Truth Data
Sending Ground Truth Data with Python SDK
Updating Ground Truth with the API",arthur_scope_docs,https://docs.arthur.ai/docs/sending-ground-truth
74,"Upgrading
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by UpgradingSuggest EditsArthur's platform installer leverages an open-source solution called Kubernetes Off-The-Shelf (KOTS).
Please refer to the below links for upgrading the platform components.
Upgrading the platform application
Upgrading the Admin Console
Upgrading the embedded Kubernetes cluster for VM install
Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/upgrading
75,"Virtual Machine Installation
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Virtual Machine InstallationSuggest EditsThis section covers the steps required for installing Arthur on a virtual machine (VM). We have included separate steps required for online and air-gapped installations.
The VM installs are not recommended for production-grade deployments. They are great for development and testing purposes.
Online Virtual Machine (VM) Install
Go to the download portal using the URL and the password provided by Arthur.
Click the ""Download license"" button to download your license in YAML file.
SSH into your virtual machine (VM) and run the command below to install the Admin Console:
Shellcurl -sSL https://k8s.kurl.sh/arthur  sudo bash
Log in to the Admin Console at <yourhost>:8800 using the provided password in the install output.
Follow the instruction to set up your secure connection with TLS certificate.
Upload your license file.
Provide your configurations
Review the preflight checks to ensure that your machine meets the minimum requirements before proceeding with the installation.
Monitor the dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with kubectl CLI:
Shell# Reload your shell if you haven't
bash -l
kubectl get deployment,statefulset,pod -n <yournamespace>
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
When using kubectl, you might run into a permission issue loading the kubernetes/admin.conf file. Please remediate it by running the command below.
Shellsudo chmod +r /etc/kubernetes/admin.conf
Airgap Virtual Machine (VM) Install
Go to the download portal using the URL and the password provided by Arthur.
Select the ""Embedded cluster"" option
Click the “Download license” button to download your license in the YAML file.
Download the ""Latest kURL embedded install"" and the ""Latest Arthur Airgap bundle"".
Preparing the embedded cluster
Arthur leverages Kubernetes as the base. This step installs the base Kubernetes cluster and Arthur's Admin Console
on your VM with a single CLI command.
First, upload the kURL embedded install bundle on your VM instance.
Example:
Shellscp -i mykey.pem ~/Downloads/arthur.tar.gz ubuntu@hostname:arthur.tar.gz
Unpack the bundle and install the embedded Kubernetes cluster on your VM instance.
Shelltar xvf arthur.tar.gz
cat install.sh  sudo bash -s airgap
Save the output from the install, including the Kotsadm Admin Console URL and the password.
You now have a K8s cluster, kubectl CLI, and the Admin Console installed on your VM.
Deploying the application to the embedded cluster
Load the Admin Console UI on port 8800 from your browser using the Kotsadm URL and the password you recorded earlier.
Follow the instructions on the Admin Console to complete your installation by providing the arthur-x.x.x.airgap bundle and necessary configurations.
Monitor the Admin Console dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with kubectl CLI on the VM:
Shell# Reload your shell if you haven't
bash -l
kubectl get deployment,statefulset,pod
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
Note: You may also follow the instructions {doc}here <vm_install_airgapped_cli> to install the Admin Console and Arthur app programmatically using the CLI only.
Airgap Virtual Machine (VM) Install with CLI
If you prefer to install programmatically using CLI only, follow the steps below.
Upload the license file and the arthur-x.x.x.airgap bundle on your VM instance.
Example:
Shellscp -i mykey.pem ~/Downloads/Test\ Customer.yaml ubuntu@hostname:license.yaml
scp -i mykey.pem ~/Downloads/arthur-x.x.x.airgap ubuntu@hostname:arthur-x.x.x.airgap
Create a config.yaml file on the VM instance using {doc}the configuration template </platform-management/reference/config_template>.
Run this install command from your VM's SSH session:
Shell
kubectl kots install arthur \
--airgap-bundle ./arthur-x.x.x.airgap \
--license-file ./license.yaml \
--config-values ./config.yaml \
--namespace arthur \
--shared-password [The Kotsadm password you saved earlier]
Updated 3 months ago Table of Contents
Online Virtual Machine (VM) Install
Airgap Virtual Machine (VM) Install
Preparing the embedded cluster
Deploying the application to the embedded cluster
Airgap Virtual Machine (VM) Install with CLI",arthur_scope_docs,https://docs.arthur.ai/docs/virtual-machine-installation
76,"Creating Arthur Model Object
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Creating Arthur Model ObjectSuggest EditsNow that we are ready, we can start onboarding a model to Arthur. This page is a walkthrough of creating an ArthurModel object to monitor your ML model.
TL;DR
An ArthurModel object sends and retrieves data important to your deployed ML system. The ArthurModel object is separate from the trained underlying model and makes predictions; it is a wrapper for the underlying model to access Arthur platform functionality.
The general steps for creating an ArthurModelObject are as followed:
Creating a production-ready ML model
Prepare your model schema/ reference dataset for Arthur:
Define build out and save your Arthur model object
A quick overview of the code needed can be seen below.
Pythonarthur_model = arthur.model(partner_model_id=f""CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}"",
display_name=""Credit Risk Batch"",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass,
is_batch=True)
prediction_to_ground_truth_map = {
""prediction_1"": 1
}
## Building out with Reference DataFrame
arthur_model.build(ref_df,
ground_truth_column=""gt"",
pred_to_ground_truth_map=prediction_to_ground_truth_map
non_input_columns=['age','sex','race','education'])
arthur_model.save()
Create a Production Ready Model
The first step to onboarding a model to Arthur is to create a model ready for or already deployed to production. Since Arthur is model and platform agnostic, it does not matter how the model is built or where it is deployed.
📘Pre-Production MonitoringSome teams use Arthur’s techniques or platform to evaluate models pre-production. This is definitely an option. However, the same sentiment remains that you need to have a finished model to onboard to Arthur and evaluate.
Creating Arthur Connection
To be able to send inference data to the platform, you will need to create a connection to not only your Arthur platform but also the model the inferences are being tracked for. Information about creating your API key and connecting to the Arthur platform/model objects can be found in the UI/Platform Guide.
Prepare Model Schema / Reference Dataset for Arthur
Teams are required to define the model's structure or schema. A model structure, otherwise known as an Arthur model schema, defines a wireframe for what Arthur should expect as inputs and outputs to your model. By recording essential properties for your model's attributes, including their value type and stage, this structure ensures that the proper default environment and metrics are built for your model in Arthur.
When you are onboarding a model, Arthur categorizes each attribute into a different Stage,
depending on the role of the attribute in the model pipeline:
Attribute StageDescriptionModelPipelineInputAll the features the model uses to create a prediction. For tabular, these are the features that go into the model. These are the text or image inputs for text or NLP.PredictedValueOutput values (or predictions) that your model producesGroundTruthValues you can provide to compare the model's outputs against performance metrics (commonly referred to as target or label)NonInputDataAny attributes that are not predictive features within the model but are additional metadata you would like to track. (i.e., protected attributes like age, race, or sex, or business-specific data like unique customer id)
Attributes are analogous to the different columns that comprise your model's data. Each attribute has a value type: these can be standard types like int and str, or datatypes for complex models like raw text and images.
Available Input Value TypesAllowed Data TypesAdditional Information RequiredNumericalInteger, FloatCategoricalInteger, String, Boolean, FloatSpecified Available CategoriesTimestampDateTimeMust be time zone awareText (NLP)StringGenerative Sequence (LLM)StringA tokens (and optional token likelihood) column is also requiredImage.jpg, .png, .gifUnique IdentifierStringTime SeriesList of Dictionaries with ""timestamp"" and ""value"" keysTimestamps must be timezone aware DateTimes and values must be floats.
As you log data over time with Arthur, the model schema is used to type-check ingested data. This prevents analytics from being skewed by scenarios like int values suddenly replacing float values causing silent bugs.
Arthur also records attribute properties in the model schema, like the range of possible values an attribute has in your data. These properties are used to understand your data’s high-level structure, not to strictly enforce that future attributes have these same properties.
Some key things to keep in mind when onboarding models schemas are:
Verify Categories Listed: Verifying the available categories for categorical attributes is important. You want to ensure that all expected categories appear or are added to each attribute's list of possible categories. This is because these categories will be used to calculate drift metrics and appear for advanced segmentation within the UI. Other category values may be sent to the platform but will not be part of this functionality.
Specifying Attribute Bounds: Teams can set specified attribute bounds for numerical attributes, such as minimum and maximum values. These are only used to make setting alert thresholds (i.e., data-bound alerts on inferences passing an acceptable maximum value) easier.
High Cardinality Models (over 1000 unique attributes) : Arthur does not allow for high-cardinality models (i.e., models with many unique columns) with more than 1000 unique attributes.
High Cardinality Variables (over 1000 unique categories) : Arthur does not allow for high-cardinality variables (i.e., variables with many unique classes) with more than 1000 unique categories. This is more common for Non-Input metadata attributes than Model Input attributes. In these cases, we recommend manually overwriting
High Cardinality Variables (over 50 unique categories) : Attributes do not need over 1000 unique categories to be considered high cardinality. An attribute with over 50 unique categories can be onboarded o Arthur, but there is some feature loss for this attribute. This attribute will not be used when calculating enrichments; this includes explainability, anomaly detection, and hot spots.
Monitoring Specific Attributes for Bias: For attributes you know you want to track using Fairness Metrics, teams must designate those inferences for bias detection tracking. This is more explained later in the document here.
Typically, teams choose to infer this schema automatically from a reference dataset. However, a more in-depth look at how teams may choose to define schemas manually can be found here.
Selecting a Reference Dataset
A reference dataset is a representative sample of input features for your model. In other words, it is a sample of what is typical or expected for your model. Typically, teams onboard their model's training or validation data for reference.
The reference dataset is a representative sample of the input features your model ingests. It is used to compute baseline model analytics. By capturing the data distribution you expect your model to receive, Arthur can detect, surface, and diagnose data drift before it impacts results. Examples of how reference data is structured for different model types can be found in the Model Input / Output Types section.
Selecting a Reference Dataset: We typically recommend using your training dataset as the reference. This is because there is no better dataset representation of the patterns your model has been built to learn than the actual dataset it learned on. However, teams do often choose to use other datasets as a reference. In particular when:
The training dataset is oversampled / not representative of real-world data patterns: This commonly occurs when your positive predicted attribute is rare (i.e., tumor detection or credit card fraud).
The training dataset is incredibly large: Reference datasets larger than 1GB can cause platform performance issues unless gradually onboarded in chunks. We recommend using a sample of your training or validation dataset for a larger dataset (typically, about 50,000 to 100,000 rows are sufficient to calculate drift metrics). It is important to ensure this is still representative of the entire dataset. In particular, we recommend including samples with extreme values included in the sample. For help onboarding in chunks or configuring your sample, please contact Arthur Support).
In these instances, we recommend using your validation dataset, or another evaluated dataset with representative patterns, as the reference.
Define Arthur Model Object
After cleaning the data, we need to start defining the Arthur model. To do this, we will need to provide both structural and user-defined information about the model:
Pythonarthur_model = arthur.model(partner_model_id=f""CreditRisk_Batch_QS-{datetime.now().strftime('%Y%m%d%H%M%S')}"",
display_name=""Credit Risk Batch"",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass,
is_batch=True)
User-Defined Information
Model Display Name
This name will appear for your model within the Arthur UI. Making this descriptive and intuitive is important, as it will be how most users search for and find your model. We recommend making this title description about that desired task your model is completing (i.e., Credit Card Fraud Classification). This is because you can create and track multiple model versions within the same display name.
Model Partner ID
Within Arthur, the Model Partner ID must be unique and cannot be duplicated across other models within your organization.
Often, teams may have an internal ID or identifying hash for their models. We do not recommend making this ID your model display name. As one, hashes are often hard to identify and search for without expert knowledge. And two, tracking multiple versions for a specific model use case is incredibly difficult. Instead, teams are encouraged to onboard that specific model's identifier as the Model Partner ID.
📘Using Timestamps Within Partner IDsTo avoid any confusion later, it is best practice to construct a partner_model_id with a timestamp embedded in the string, so that each time the model is onboarded, a new partner_model_id is generated.
Structural Information
This is information about the model's structure, i.e., how it runs. While Arthur is model agnostic, so it doesn't care how your model was built, we require you to specify some information so we can correctly set up your model's environment based on its specifications.
Data Type
Also known as the model input type, this is the type of data that the model will use to make predictions. The four possible data types are tabular, image, text, and time series.
Task Type
Also known as the model output type, this is the output your model will generate. The four possible model output types are multiclass (all classification), regression, object detection, and ranked list. This information is needed as the platform has to set up the correct default metric types within your model environment.
Batch v. Streaming
Next, teams should specify how they plan to send data to Arthur. We have two choices:
Batch: Lower frequency used (and monitored). Typical high inference load at a time
Streaming: High frequency of use (and monitoring). Typical lower inference load at a time
So, if you have users sending data frequently (e.g., daily), we recommend streaming models. Conversely, if you have users sending data infrequently (e.g., monthly), we'd recommend batching models for them. However, if you want guarantees about when metrics are calculated or alerts fire and cannot wait for the scheduled jobs to run, we recommend using batch models.
Another call-out is that batch models generate alerts by batch (i.e., a set threshold has been passed on average within a batch). Streaming models generate alerts by a designated lookback window (i.e., the average over the past day, week, etc.). More information on alerting can be found here.
Indicating a batch model means supplying an additional batch_id to group your inferences. Arthur will default to measuring performance for each batch rather than by the inference timestamps.
Prediction Mapping
Finally, when teams build out their Arthur model, they must specify a prediction mapping function.
This ensures that Arthur correctly identifies predictive and ground truth classes when setting up the model's environment and calculating metrics. This mapping needs to identify which columns are prediction and ground truth but also map the labels within the columns to one another.
Examples of each model type's prediction mapping can be found in its Model Input / Output Types section.
Register Arthur Model Object
The simplest method of registering your attributes is to use the ArthurModel.build() function parses a Pandas DataFrame of your reference dataset containing inputs, metadata, predictions, and ground truth labels. In addition, a pred_to_ground_truth_map is required, which tells Arthur which of your attributes represent your model’s predicted values and how those predicted attributes correspond to your model’s ground truth attributes.
Python## General Format Example
## pred_mapping = {""model_inputs"" :""model_outputs""}
# Map our prediction attribute to the ground truth value
# This tells Arthur that in the data you send to the platform,
# the `predicted_probability` column represents
# the probability that the ground-truth column has the value 1
prediction_to_ground_truth_map = {
""prediction_1"": 1
}
## Building out with Reference DataFrame
arthur_model.build(ref_df,
ground_truth_column=""gt"",
pred_to_ground_truth_map=prediction_to_ground_truth_map)
Examples of each model type's mapping can be found within their specific Model Input / Output Types descriptions.
Non-Input Attributes (Optional)
Many teams want to track and monitor performance around metadata related to their model but are not necessarily model inputs or outputs. These features can be added as non-input attributes in the ArthurModel and must be specified in this build function.
Python# Specifying additional non input attributes when building a model.
# This tells Arthur to monitor ['age','sex','race','education']
# in the reference and inference data you send to the platform
arthur_model.build(
reference_df,
ground_truth_column='ground_truth_label',
pred_to_ground_truth_map=pred_to_ground_truth_map,
non_input_columns=['age','sex','race','education']
)
Verify Model and Onboard to Arthur
As mentioned above, your model schema can't be updated after you've saved your Arthur model object. For this reason, many teams choose to review their model schema before sending it to the platform. This can be done in the SDK with
Pythonarthur_model.review()
This creates a dataframe that contains all the information about the model schema. An example can be shown below:
After reviewing everything, teams can save their model to Arthur. This is done with the command below.
Pythonarthur_model.save()
Once you call arthur_model.save()Arthur will handle creating the model and provisioning the necessary infrastructure to enable data ingestion for this model. If model creation fails, you may try re-saving the model or contact support if the problem persists.
(Optional) Monitoring Attributes for Bias
For some of the attributes in your model, you may want to pay particular attention to how your model’s outputs are potentially different for each subpopulation of that attribute. We refer to this as monitoring an attribute for bias.
When you set up to monitor a PipelineInput or NonInput attribute for bias enables using Fairness Metrics for those attributes within your model UI.
🚧Enable Monitoring for Bias Before Sending InferencesYou must enable monitoring attributes for bias before sending inferences to the platform.
Based on whether the attributes of interest are categorical or continuous, you can follow the steps below for each attribute of interest you'd like to monitor for bias.
Categorical Attributes
For a categorical variable, each possible level of the attribute will be treated as a distinct sub-population for analysis. For example, if you had an attribute for “Gender,” which comprised the three possible values Male, Female, and Non-Binary, then you would simply add the following to your model onboarding.
Pythonarthur_model.get_attribute(""SEX"", stage=Stage.ModelPipelineInput
).monitor_for_bias = True
Continuous Attributes
For a continuous variable, you need to break the continuous range into a fixed number of groupings so that we can create sub-populations. You can do this by providing cutoff thresholds for each grouping. For example, if we have a continuous attribute called AGE, we can create three age brackets, such as < 35, 35 - 55, and > 55. We create these groups by providing the upper-cutoff values for each group.
Pythonarthur_model.get_attribute(""AGE"", stage=Stage.ModelPipelineInput
).monitor_for_bias = True
arthur_model.get_attribute(""AGE"", stage=Stage.ModelPipelineInput
).set(bins = [None, 35, 55, None])
Updated 2 months ago What’s NextMove onto enabling enrichments in Arthur or learn more about specifics around model attributes and stagesEnabling EnrichmentsAttributes and StagesRegistering Model Attributes ManuallyTable of Contents
TL;DR
Create a Production Ready Model
Creating Arthur Connection
Prepare Model Schema / Reference Dataset for Arthur
Some key things to keep in mind when onboarding models schemas are:
Selecting a Reference Dataset
Define Arthur Model Object
User-Defined Information
Structural Information
Register Arthur Model Object
Non-Input Attributes (Optional)
Verify Model and Onboard to Arthur
(Optional) Monitoring Attributes for Bias",arthur_scope_docs,https://docs.arthur.ai/docs/creating-arthur-model-object
77,"SAML
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by SAMLSuggest Edits📘SSO configurations are only supported in on-prem Arthur installations
This page provides a walk-through for how to configure your Arthur installation to work with a SAML compatible IdP. In order to complete this guide, you need administrator access to your IdP and access to your Arthur installation's admin console configuration. Additionally, you will either need access to the Arthur superadmin user or be able to assume a role in your IdP to give yourself RBAC management permissions in Arthur.
This guide will walk through the following steps:
Configure the IdP user groups and SAML assertion
Configure the Arthur service provider URLs in the IdP
Configure Arthur to work with your IdP
Apply the Arthur IdP YAML configuration
Create organization user roles to match the IdP user groups
Test Access
Cleaning Up
1. Configure the IdP user groups and SAML assertion
In order to properly map users to permissions, Arthur requires an attribute in your SAML assertion that contains
information about the group memberships of the user. Each group in the IdP should correspond to a role in Arthur's
{doc}custom_rbac permission system.
This process can vary depending on your IdP, but most IdP's should have a user grouping mechanism, and a mechanism to configure attributes in the SAML assertions. For example using Okta, under the SAML application settings, admins can configure the SAML assertion attributes to include group information under SAML Settings -> Configure SAML -> Group Attribute Statements, then specifying a name for the attribute and a filter for the groups to include:
Setting this configuration produces the following attribute in the SAML assertion (in Okta click ""Preview the SAML Assertion"" button to see a sample):
XML
<saml2:AttributeStatement>
<saml2:Attribute Name=""groups"">
<saml2:AttributeValue>Everyone</saml2:AttributeValue>
<saml2:AttributeValue>admins</saml2:AttributeValue>
<saml2:AttributeValue>org-1-model-owners</saml2:AttributeValue>
</saml2:Attribute>
</saml2:AttributeStatement>
2. Configure the Arthur service provider URLs in the IdP
In order for your IdP to speak to Arthur, it needs to know where to find it. Enter the following URLs in your IdP's configuration to Arthur's SAML endpoints:
ACS URL (SSO URL): https://<HOSTNAME>/api/v3/saml/sso
Entity ID: https://<HOSTNAME>/api/v3/saml/sso
Start URL: https://<HOSTNAME>/
📘If your IdP will be sending signed assertions to Arthur, you will also need to generate and upload the public key
(certificate) Arthur will be using in your IdP. This will be the same certificate you set in the Arthur configuration
below. Please follow your own company policies to obtain a certificate
for Arthur. If you have no internal guidelines, then use a tool like
ssh-keygen to
generate them
3. Configure Arthur to work with your IdP
Additionally, Arthur needs to know how to handshake with your IdP. To do that, Arthur requires the following information:
Your IdP's metadata URL or the metadata XML payload (some IdPs require it be downloaded, either is fine)
One or more IdP administrator user groups that will be paired to global custom roles in Arthur
(see here for a description of what these are for)
An understanding of your SAML assertion and how to parse user information out of it
With those three things available, it is possible to fill out Arthur's IdP configuration YAML. The next subsections explain each section of the Arthur YAML configuration, and are followed by some complete examples further down.
Configuring the IdPs metadata URL
Some IdP's host their metadata XML at a public URL, while others only have it available for download privately. To support either option, Arthur has two configurations that can be used:
YAML# use this option if your IdP has a public URL for its metadata
metadataURL: ""link to IdP metadata goes here""
# use this option if your IdP does not have a public URL and include the XML payload
# make sure to indent the XML payload two spaces and make sure the X509Certificate lines
# do not have more than two leading whitespaces!
metadataXML: 
<?xml version=""1.0"" encoding=""UTF-8""?><md:EntityDescriptor ...>
<md:IDPSSODescriptor>
...
<ds:X509Certificate>CERTIFICATE LINE 1
CERT LINE 2
CERT LINE 3
CERT LINE 4
LAST CERT LINE</ds:X509Certificate>
</md:IDPSSODescriptor>
</md:EntityDescriptor>
{warning}If using the `metadataXML` configuration option, make sure to indent the entire XML payload two spaces. YAML expects
multi-line values to be indented under the key `metadataXML`.
{warning}Additionally, the assertion's `X509Certificate` XML attribute is a multi-line value within the XML.
Any new lines in the certificate value need to be indented only two spaces (all the way to the left of the YAML value).
Otherwise, the extra whitespaces introduces characters which will invalidate the certificate value.
Configure the Arthur Global Roles
Arthur has the ability to create roles for the cluster administrators during the configuration of the IdP. These roles are often needed by admins to configure RBAC and create organizations for other users in the system. See {ref}creating_global_roles_in_arthur_config for a deep dive on how to use global roles.
This section of the YAML config is under the globalRoleDefs field. It accepts a list of role definitions that will be created when the configuration is applied. The names of the roles in this section must match the user groups in your IdP in order to be able to assume them in Arthur.
YAML
globalRoleDefs:
# Here we can specify a list to define multiple global roles
- name: ""idp-admin"" # change this name to match the cluster administrator group name in your IdP
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
model:
- read
- write
- delete
Parsing the IdP SAML Assertion
In order for Arthur to communicate with your IdP, it needs to understand the format of the SAML assertion your IdP uses. This section of the config falls under the assertionAttributes YAML field. This section is designed to be flexible to support a variety of assertion formats, so it has a lot of options. At its core, the goal is to tell Arthur how to be able to extract the following information from the assertion:
user roles/groups
first name
last name
email
user ID
Each field has a corresponding YAML field the defines where to find the information in the SAML assertion XML. For example:
YAMLfirstNameAttribute:
name: ""employeeFirstName""
This configuration tells Arthur that it can find the user's first name under the ""employeeFirstName"" attribute in the XML assertion. Such an assertion might look like this:
XML
<saml2:AttributeStatement>
<saml2:Attribute Name=""firstName"">
<saml2:AttributeValue>Ian</saml2:AttributeValue>
</saml2:Attribute>
</saml2:AttributeStatement>
More examples of how to parse attributes out of the SAML assertion can be
found below.
Full Configuration Examples
Here is an example of a full configuration, combining each section described above.
YAMLversion: v1
kind: SAML
config:
# if your IdP hosts its metadata, provide the URL to it here
metadataURL: ""link to IdP metadata goes here""
# if the IdP does not host the metadata, provide the XML payload here and comment out metadataURL
#
metadataXML: 
#
<?xml version=""1.0"" encoding=""UTF-8""?><md:EntityDescriptor ...>
#
<md:IDPSSODescriptor>
#
...
#
</md:IDPSSODescriptor>
#
</md:EntityDescriptor>
# this section describes how Arthur will parse the SAML assertion from your IdP
# for each required attribute, Arthur will use the ""name"" field to match to an XML attribute in the SAML assertion
assertionAttributes:
# this roleAttribute configuration will use a ""groups"" attribute in the XML assertion which expects the
# roles in separate XML AttributeValues within the assertion Attribute
roleAttribute:
name: ""groups""
useAllAttributeValues: True
firstNameAttribute:
name: ""employeeFirstName""
lastNameAttribute:
name: ""employeeLastName""
emailAttribute:
name: ""company_email""
userIdAttribute:
name: ""companyUserID""
globalRoleDefs:
# Here we specify a global role for the IdP user group ""idp-admin"" to create and manage RBAC in Arthur
- name: ""idp-admin""
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
model:
- read
- write
- delete
4. Apply the Arthur IdP YAML configuration
Once you have your YAML configuration file ready, you need to add it to your Arthur installation. With the Arthur admin console open, navigate to the ""Use a 3rd Party Global Identity Provider"" section and select ""SAML"". This will expose a text box for you to paste the YAML config file assembled above. When pasting, make sure whitespace is preserved and the YAML document has consistent spacing (do not mix tabs and spaces). Here is a screenshot of the config section:
{note}If your IdP enforces signed authorization requests, this config page also provides the ability to upload a
certificate and private key for Arthur to use when making the requests. Click the ""Upload a file"" button for the Public
Certificate and Private Key sections of the config to upload the appropriate files for your IdP.
Once you have added your config files, scroll to the bottom and click ""Save"" to save the config. Then go to the latest version and click ""Deploy"" to roll out the change to the cluster.
5. Create organization user roles to match the IdP user groups
In order to complete this section, you will need access to the Arthur superadmin user credentials set during your install, or you will need to be able to assume the role defined in the Arthur IdP config YAML created above in the globalRoleDefs section.
In order to use the API example linked below, you will need a Bearer token (authentication token) to include with your API request. There are a few options available to retrieve a token:
Retrieve a SAML assertion from your IdP and exchange with Arthur - Most IdPs will have a method to retrieve a SAML assertion for users. Some companies make scripts or APIs to do so. If your IdP does not have an automated method to retrieve an assertion, use one of the other options below. Once you have an assertion, you can exchange it for an Arthur access token with the follow API call to Arthur:
Shellcurl --location --request POST 'https://<YOUR ARTHUR HOST>/api/v3/saml/sso' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'SAMLResponse=<INSERT URL ENCODED BASE64 ASSERTION>'
Retrieve a global role token from your browser cookies - If you sign in to Arthur as a user with a global role, the UI will not be fully functional, but it will have a valid access token in the cookies. If you navigate to your browser's developer console and then go to the Application Storage/Cookies section, you should see a cookie like Authentication. The authentication token is the value of that cookie.
Use the /login API endpoint with the superadmin user's credentials set during the Arthur install (only available on-prem).
Using either of those credentials, we can use the Arthur API to define roles in Arthur that match the user group names in your IdP. See the {ref}creating_organization_roles section for an example API request to create custom roles in Arthur. Importantly, the role names must uniquely match to a user group in your IdP in order for your users to be able to assume those permissions in Arthur. Therefore, the roles in Arthur must be globally unique in the entire Arthur installation.
6. Test Access
At this point everything should be configured correctly to sign in to Arthur via SSO. Either navigate to your IdP or the Arthur homepage to test logging in.
7. Cleaning Up
Once users are successfully able to log in to Arthur via the IdP, you should do the following to ensure proper security best-practices remain enforced:
Restrict any Arthur global roles to only allow access to essential admin functions
Set the Arthur superadmin user password securely, and either store the password in a vault, or discard the password entirely. superadmin shouldn't be used going forward.
Set up a policy to routinely rotate the superadmin password to keep it secure
Together, these practices will help ensure the security of your Arthur installation, and will give your IdP sole control over the platform and who is able to access it.
Common Troubleshooting
If after following the steps above, users are not able to log in via the IdP try some of these common troubleshooting tips:
Does the user properly redirected to the IdP's log in screen?
If not, there is likely a configuration error in the Arthur YAML config with the IdP metadata or the URL to access it. Another problem could be if your IdP expects Arthur to make signed requests to authenticate users. If that is the case, be sure you have correctly configured Arthur's certificate and private key as described above.
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
If not, there is likely a configuration error with the IdP and the URLs that it uses to communicate with Arthur. Double-check the ACS (SSO) URL is configured correctly for the Arthur installation
at https://<HOSTNAME>/api/v3/saml/sso.
A user can see the Arthur home page, but can't see any of the model in their organization
If a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to access models (see {doc}../../reference/permissions_by_endpoint) or they were not able to correctly assume the role in Arthur. Double-check the groups in their SAML assertion match the role names that have been configured in Arthur. A superadmin or global role user with permissions to manage RBAC can see a list of roles in the installation by using the following API call. Be sure to replace the HOST and AUTH TOKEN for your installation and user:
Shellcurl --location 'https://<HOST>/api/v3/authorization/custom_roles' \
--header 'Authorization: Bearer <INSERT AUTH TOKEN HERE>'
Appendix A: More examples of SAML assertion values and how to parse them
This section outlines some additional ways to use the assertionAttributes section of the Arthur IdP config YAML format. The below examples include sample SAML assertions, then corresponding YAML for how to parse them.
Basic Full Example
This example shows how to parse a user's information from a SAML assertion when each field is its own Attribute and the user's groups are each in their own AttributeValue.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name=""employeeFirstName"">
<saml:AttributeValue>John</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name=""employeeLastName"">
<saml:AttributeValue>Doe</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name=""employeeEmail"">
<saml:AttributeValue>[email protected]</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name=""employeeID"">
<saml:AttributeValue>1234567890</saml:AttributeValue>
</saml:Attribute>
<saml:Attribute Name=""userGroups"">
<saml:AttributeValue>group1</saml:AttributeValue>
<saml:AttributeValue>group2</saml:AttributeValue>
<saml:AttributeValue>group3</saml:AttributeValue>
</saml:Attribute>
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the user information field:
YAML
assertionAttributes:
roleAttribute:
name: ""userGroups""
useAllAttributeValues: True
firstNameAttribute:
name: ""employeeFirstName""
lastNameAttribute:
name: ""employeeLastName""
emailAttribute:
name: ""employeeEmail""
userIdAttribute:
name: ""employeeID""
Parsing User Groups from Multiple Attribute Values
This example shows how to parse a user's groups from a SAML assertion when each group is its own AttributeValue.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name=""Idp_user_groups"">
<saml:AttributeValue>role1</saml:AttributeValue>
<saml:AttributeValue>role2</saml:AttributeValue>
<saml:AttributeValue>role3</saml:AttributeValue>
</saml:Attribute>
...
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the roleAttribute field:
YAML
assertionAttributes:
roleAttribute:
name: ""Idp_user_groups""
useAllAttributeValues: True
Parsing User Groups from a String Attribute Value
This example shows how to parse a user's groups from a SAML assertion when the groups are in a single string AttributeValue.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name=""Idp_user_groups"">
<saml:AttributeValue>role1,role2,role3</saml:AttributeValue>
</saml:Attribute>
...
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the roleAttribute field:
YAML
assertionAttributes:
roleAttribute:
name: ""Idp_user_groups""
deliminator: "",""
Parsing Specific Fields in a Single Attribute's AttributeValue List
This example shows how to parse a user's information from a SAML assertion when all fields are in a single assertion Attribute's list of AttributeValues.
Example SAML assertion XML:
XML
<saml2:AttributeStatement>
<saml:Attribute Name=""employeeInfo"">
<saml:AttributeValue>John</saml:AttributeValue>
<saml:AttributeValue>Doe</saml:AttributeValue>
<saml:AttributeValue>(123) 456-7890</saml:AttributeValue>
<saml:AttributeValue>42 Wallaby Way, Sydney</saml:AttributeValue>
<saml:AttributeValue>[email protected]</saml:AttributeValue>
<saml:AttributeValue>5678987654</saml:AttributeValue>
</saml:Attribute>
...
</saml2:AttributeStatement>
Corresponding settings for the Arthur IdP config YAML assertionAttributes for the user information field:
YAML
assertionAttributes:
firstNameAttribute:
name: ""employeeInfo""
index: 0
lastNameAttribute:
name: ""employeeInfo""
index: 1
emailAttribute:
name: ""employeeInfo""
index: 4
userIdAttribute:
name: ""employeeInfo""
index: 5
Updated 3 months ago Table of Contents
1. Configure the IdP user groups and SAML assertion
2. Configure the Arthur service provider URLs in the IdP
3. Configure Arthur to work with your IdP
Configuring the IdPs metadata URL
Configure the Arthur Global Roles
Parsing the IdP SAML Assertion
Full Configuration Examples
4. Apply the Arthur IdP YAML configuration
5. Create organization user roles to match the IdP user groups
6. Test Access
7. Cleaning Up
Common Troubleshooting
Does the user properly redirected to the IdP's log in screen?
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
A user can see the Arthur home page, but can't see any of the model in their organization
Appendix A: More examples of SAML assertion values and how to parse them
Basic Full Example
Parsing User Groups from Multiple Attribute Values
Parsing User Groups from a String Attribute Value
Parsing Specific Fields in a Single Attribute's AttributeValue List",arthur_scope_docs,https://docs.arthur.ai/docs/saml
78,"Object Detection
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Object DetectionImage Object Detection Models within ArthurSuggest EditsObject detection models analyze and classify objects within an image by placing a bounding box over the objects. In Arthur, these models are listed under the object detection model type.
Some common examples of Object Detection are:
Animal detection with wildlife cameras
Quality control in manufacturing, detecting defective pieces on the factory line
Formatted Data in Arthur
Object detection models require two columns: image input and bounding box output. When onboarding a reference dataset (and setting a model schema), you need to specify the relationship between your prediction and ground truth bounding box columns. Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Formatting Bounding Boxes
When using an Object Detection model, bounding boxes should be formatted as lists in the form:
[class_id, confidence, top_left_x, top_left_y, width, height]
The first two components of the bounding box list represent the classification being made within the bounding box. The class_id represents the ID of the class detected within the bounding box, and the confidence represents the % confidence the model has in this prediction (0.0 for completely unconfident and 1.0 for completely confident).
The next four components of the bounding box list represent the location of the bounding box within the image: the top_left_x and top_left_y represent the X and Y pixel coordinates of the top-left corner of the bounding box. These pixel coordinates are calculated from the origin, which is in the top left corner of the image. This means that each coordinate is calculated by counting pixels from the image's left or the top, respectively. The width represents the number of pixels the bounding box covers from left to right, and the height represents the number of pixels the bounding box covers from top to bottom.
Attribute (Image Input)Prediction (bounding boxes)Ground Truth (bounding boxes)Non-Input Attribute (numeric or categorical)image_1.jpg45.3462.42High School Educationimage_2.jpg55.153.2Graduate Degree
Predict Function and Mapping
Teams must provide the relationship between the prediction and ground truth column to onboard their object detection models. This is defined to help set up your Arthur environment to calculate default performance metrics.
prediction to ground truth mapping## Single Column Ground Truth
output_mapping = {
'pred_bounding_box_column':'gt_bounding_box_column'}
# Build Function for this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
Available Metrics
When onboarding object detection models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a object detection model. Learn more about these metrics in the
Performance Metrics section.
MetricMetric TypeMAPEPerformanceInference CountIngestionAverage ConfidencePerformance
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Learn more about these metrics in the Drift and Anomaly section.
Of note, for unstructured data types (like text and image), feature drift is calculated for non-input attributes. The actual input to the model (in this case, image) drift is calculated with multivariate drift to accommodate the multivariate nature/relationships within the data type.
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate DriftAverage Raw Anomaly ScoreMultivariate Drift
Note: Teams can evaluate drift for inference data at different intervals with our Python SDK and query service (for example, data coming into the model now compared to a month ago).
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Formatting Bounding Boxes
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/image-object-detection
79,"Alerting
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by AlertingSuggest EditsAn alert is a message notifying you that something has occurred with your model. With alerts, Arthur Scope makes it easy to provide a continuous view of your model by highlighting important changes in model performance. When defining alerts in Arthur, there are a few things you need to consider:
Alert Severity
Two alert severities are available in Arthur; they are warning and critical.
Teams can set their own severity for alerts. We typically recommend that teams set two different thresholds for the same value, marking the less severe as Warning and the more as Critical.
Understanding Alert Rules
An alert is triggered based on an alert rule, which you define using a metric and a threshold. So to create an alert in Arthur, you need to:
Create a Metric in Arthur Scope
A metric in Arthur is a function for evaluating model performance. These can be common functions that data scientists or ML teams are familiar with, such as accuracy or F1 Score. Or they can be functions specific to a model's use case, such as Fairness Metrics or User Defined Metrics. This means that any metric you can create in Arthur (including segmentations, filters, or logical functions) can be transformed into an alert.
For the latter, following the proper steps to create the metric within Arthur for your specific model is important.
This is because it must first be created for your model to alert on a metric.
Define Threshold & Bound for Alert
After creating your metric, it is time to decide what level of underperformance you would like to be alerted to. This numeric value is called the alert threshold. In Arthur, alert threshold values have to be manually set. Users must also define whether they want to be alerted when their function exceeds that numeric threshold. This is the upper or lower bound of the alert.
Define Notification Timelines for Alert (For Streaming Models)
For batch models, alerts are calculated per batch of data. However, teams that are running streaming models need to decide how often they would like alerts to be calculated and how much data. To make these decisions, they have to clarify two values:
Lookback Period: How much data do they want to aggregate in their function? (Do they want to be alerted when the average of just one minute of data has passed the metric threshold, or do they only care if it's affected the average of a day or week).
Alert Wait Time: How often do you want to be alerted that something is happening? This is how often they would like the alert to be calculated (and triggered if the function threshold is met).
Creating Alerts
Default Alerts
Default data drift alerts for Feature and Prediction Drift are automatically created for every feature once reference and inference data are sent to Arthur. These alerts are created with dynamic threshold values specific to your reference dataset from the “data_drift” endpoint with “metric”: “Thresholds.”
Defining Custom Alert Rules in the UI
The UI provides a clickable walk-through guide for teams to make common performance, drift, and data-bound alerts. The common practice of alerting on segments (or filters) of your data when calculating the metric is also added as an optional step.
See below an example of defining an accuracy alert rule for women in the UI to be alerted every hour for the last 24 hours of inferences.
Defining Custom Alert Rules with the Python SDK
Our predefined alerting structure is not the only way teams can create alerts. Alerts can be made for any customizable User Defined Metric teams create for their model in Arthur. Teams must first create the user-defined metric, and then they can easily set the alert from their Python SDK notebook.
Alert Notifications
The latest alerts in your organization are shown on the homepage of your Arthur organization. However, beyond being highlighted in the online Arthur UI, alerts can be delivered to teams via email and/or via integrations such as PagerDuty and Slack. You can learn more about setting up those integrations here.
Alert Endpoint in the API
The dedicated alerts endpoint is /models/{model_id}/alert_rules.Updated 3 months ago Table of Contents
Alert Severity
Understanding Alert Rules
Create a Metric in Arthur Scope
Define Threshold & Bound for Alert
Define Notification Timelines for Alert (For Streaming Models)
Creating Alerts
Default Alerts
Defining Custom Alert Rules in the UI
Defining Custom Alert Rules with the Python SDK
Alert Notifications
Alert Endpoint in the API",arthur_scope_docs,https://docs.arthur.ai/docs/creating-alerts
80,"Airgap Kubernetes Cluster (K8s) Install with CLI
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Airgap Kubernetes Cluster (K8s) Install with CLISuggest EditsIf you prefer to install programmatically using CLI only, follow the steps below.
Prepare a config.yaml file using the Config Template
Deploy the application by running the below kubectl command:
Shellkubectl kots install arthur \
--no-port-forward \
--namespace arthur \
--shared-password [Provide an Admin Console password] \
--license-file ./license.yaml \
--config-values ./config.yaml \
--airgap-bundle ./arthur-x.x.x.airgap \
--kotsadm-registry [Your private container image repository] \
--kotsadm-namespace arthurai \
--registry-username [Read-Write Username] \
--registry-password [Read-Write Password]
shared-password is the Admin Console password.
Installing a specific version of Arthur
To install a specific version of Arthur, you would run the same command as above (following the same steps to prepare the configuration), with the inclusion of the --app-version-label flag. This flag allows you to specify which specific version of Arthur you want to install (e.g., to set up a sandbox environment on the same version
as production).
To determine which versions of Arthur are available, you can run the following:
kubectl kots get versions arthur -n <arthur namespace>
Updated 3 months ago Table of Contents
Installing a specific version of Arthur",arthur_scope_docs,https://docs.arthur.ai/docs/airgap-kubernetes-cluster-k8s-install-with-cli
81,"Sending Historical Data
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Sending Historical DataSuggest EditsAlthough optional, we've found that a useful step for teams after onboarding is to onboard historical model information to Arthur. This allows teams to immediately begin to dig into their model data and explore trends in the data, even before new sending new inferences.
To send historical data, we will need to do the following:
Collect Historical Data
Format It's Timestamp Information
Send it to your Arthur Model
Collect Historical Data
Now, we want to collect all the historical data and inferences our model has run on. Organizations may store this information in different tables, so you may need to query or merge information from wherever you store data. However, it is important to ensure that this information contains the following:
All Inputs / Outputs Expected By Arthur Model Object: During model creation, we explored the format the Arthur Model Object expects to receive information. To review, the Arthur model object expects:
All Feature Inputs to the Model
Predicted Model Outputs
(Optional) Ground Truth (True Label Output), if known
(Optional) Any Non-Input Attributes logged to the model if known
Inference TimeStamps: This is information about when the model ran the historical predictions. This is especially important to ensure that information logged into Arthur is logged in for the correct time so that you can accurately evaluate trends and/or diagnose previous issues.
Additional Inference Identification: This is discussed further in detail in Sending Inferences, but this information includes inference identification like Partner Inference ID or Batch ID (for batch models).
Format Timestamps
The Arthur model object expects timestamps to be in DateTime format. Below is a quick example of how to format timestamps into the correct format, but here are some references about converting Python information into DateTime.
Pythonimport pytz
from datetime import datetime
## Use Lambda Function to Convert to Date Time
def get_timestamps(x):
new_time = x.split('.')[0]
return datetime.strptime(new_time, '%Y-%m-%d %H:%M:%S')
historical_df['timestamps'] = historical_df['timestamps'].apply(lambda x: get_timestamps(x))
Send Inferences to Arthur
Now, we can send the inferences to Arthur. We must separate the Arthur model information from the timestamps to do this. We will see how this works below. Although we use a streaming model, we can see a commented-out example of sending historical inferences as a batch.
Python## Send Historical Streaming Data
hist_df = historical_df.drop(columns = ['timestamps'])
arthur_model.send_inferences(hist_df, inference_timestamps = historical_df['timestamps'])
When sending historical batch data, it is important to remember that you need not only to send historical timestamp information but also unique batch ids. For teams that do not have unique batch ids already stored, a common technique will be to create a unique timestamp based on the frequency that you run batches historically.
An example below is creating unique batches based on the day. (i.e., all inferences with the same date will belong to the same batch). Changing this based on your batch frequency (i.e., if you run data every hour, etc.) is important.
Python## Create Daily Batch Times
historical_df['batch_id'] = historical_df['timestamps'].apply(lambda x: 'batch_'+x.strftime('%m_%d_%Y'))
batch_df = historical_df.drop(columns = ['timestamps','batch_id'])
## Send Historical Batch Data
arthur_model.send_inferences(batch_df, batch_id=historical_df['batch_id'], inference_timestamps=historical_df['timestamps'])
Key Things to Keep in Mind
Alerts will backdate: Alerts will still trigger any alerts set up on historical data. This can be useful to evaluate when an alert may have occurred in the past. However, it may be best to make notes explore, and then mark and conclude this information in the alerts section. This way, you can gain all the information from backdated alerts but have a cleaned and operable alerting homepage for new alerts your teams will need to act on.
UI may show just a window: The UI within Arthur is automatically set to show the last month of data. It can be easiest to go into the filter and select ""all time"" to ensure all your data has been sent to the platform.
All enabled functionality should be available: You should be ready to explore your data using the Arthur UI or query service with all your data and any special enrichments you enabled during Arthur Model Object creation.
Beyond Arthur functionalities, when evaluating your historical data, it is also important to keep in mind that:
Your model will perform better on data it was trained on: While we encourage onboarding all the historical data you want to your model to view trends, teams often may not realize that this historical data consists of the data they used to train their more recent model. If that is the case, seeing higher performance for inferences included in your model training set is not unusual. We encourage teams to still onboard historical data to visualize trends in their feature set/performance overall, but remember to think critically when seeing high historical accuracy.
Updated 3 months ago Table of Contents
Collect Historical Data
Format Timestamps
Send Inferences to Arthur
Key Things to Keep in Mind",arthur_scope_docs,https://docs.arthur.ai/docs/sending-historical-data
82,"Querying Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Querying ExplainabilitySuggest EditsOne of the most popular enrichments within Arthur, Explainability values, can also be queried with Arthur's query language. Teams looking to query explainability will pull their values from the enriched table in Arthur. It's important to note that teams must also ensure that explainability has been enabled for their model; otherwise, they will be unable to pull this information.
Tabular Explainability
For tabular models, there are three ways to interact with and query explainability:
Local Explainability: Explainability is calculated for each inference in a model.
Global Explainability: This aggregates all inferences' explainability values.
Regional Explainability: This is a filtered aggregate of explainability values for a specified cohort (or region) of inferences. This is often used for comparison evaluation against the global explainability values or another cohort.
Local Explainability
The most common way to interact with local explainability is within the UI. However, teams that want to pull a local explanation score within a notebook often use the query below. In this query, we are filtering for a specific inference based on itspartner_inference_id, which is the name used for an organization's unique inference id. This is the most common way for teams to look up individual inferences.
Pythonquery = {
""select"": [
{""property"": ""explainer_algo""},
{""property"": ""explainer_predicted_attribute""},
{ ""property"": ""explainer_attribute""},
{""property"": ""explainer_score""}
],
""filter"": [
{""property"": ""[inference_idpartner_inference_id]"",
""comparator"": ""eq"",
""value"": ""<id> [string]""
}
],
""from"": ""enriched""
}
response = arthur_model.query(query)
response[{ ""explainer_algo"": ""lime"",
""explainer_attribute"": ""AGE"",
""explainer_predicted_attribute"": ""prediction_0"",
""explainer_score"": 0.0016485283063465874},
{""explainer_algo"": ""lime"",
""explainer_attribute"": ""BILL_AMT1"",
""explainer_predicted_attribute"": ""prediction_0"",
""explainer_score"": 0.0032036810960691183},
{ ""explainer_algo"": ""lime"",
explainer_attribute"": ""BILL_AMT2"",
""explainer_predicted_attribute"": ""prediction_0"",
""explainer_score"": 0.002008238656596662},
{ ""explainer_algo"": ""lime"",
""explainer_attribute"": ""AGE"",
""explainer_predicted_attribute"": ""prediction_1"",
""explainer_score"": -0.0016485283063465892},
{ ""explainer_algo"": ""lime"",
""explainer_attribute"": ""BILL_AMT1"",
""explainer_predicted_attribute"": ""prediction_1"",
""explainer_score"": -0.0032036810960691126},
{ ""explainer_algo"": ""lime"",
""explainer_attribute"": ""BILL_AMT2"",
""explainer_predicted_attribute"": ""prediction_1"",
explainer_score"": -0.0020082386565966667} ]
Global Explainability
Global explainability aggregates all the inferences sent to your production model. It is calculated by taking each inference's average value of all feature importance scores. Teams can pull this information using the query below:
Python## get global explanations
explanation_algo = model.explainability.explanation_algo
predicted_class_col = model.get_positive_predicted_class()
global_explanations = pd.DataFrame(model.query({
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""global_importance"",
""parameters"": {
""predicted_attribute_name"": predicted_class_col,
""explanation_algorithm"": explanation_algo
}
}
]})).rename(columns={'explainer_attribute': 'feature','global_importance':'global_importance'}).sort_values(by='global_importance', ascending=False)
feature
global_importance
0
PAY_0
0.092898
1
PAY_2
0.027737
2
LIMIT_BAL
0.025445
3
PAY_3
0.018831
4
PAY_AMT1
0.013233
5
BILL_AMT1
0.012518
6
PAY_AMT2
0.011359
7
PAY_AMT3
0.010639
8
BILL_AMT2
0.008891
9
BILL_AMT6
0.008058
10
PAY_6
0.007976
11
PAY_AMT6
0.007970
12
PAY_AMT4
0.007123
13
BILL_AMT4
0.006892
14
BILL_AMT5
0.006786
15
PAY_5
0.006765
16
BILL_AMT3
0.006730
17
PAY_4
0.006194
18
PAY_AMT5
0.004818
19
AGE
0.004392
20
EDUCATION
0.002111
21
MARRIAGE
0.001940
22
SEX
0.001188
Teams also often choose to plot these values using this plot function:
Pythonax = global_explanations.set_index('feature').plot(kind='bar')
ax.set_title(""Global Explanations"")
ax.set_ylabel(""feature importance"")
Regional Explainability
Regional explainability is similar to global explainability, with additional filters applied to define your region or cohort of interest. This can be seen in the example below:
Query Request:
Pythonmodel.query({
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""global_importance"",
""parameters"": {
""predicted_attribute_name"": predicted_class_col,
""explanation_algorithm"": explanation_algo
}
}
],
""filter"": [
{
""property"": ""AGE"",
""comparator"": ""gte"",
""value"": 18
},
{
""property"": ""AGE"",
""comparator"": ""lt"",
""value"": 40
}
]
})
Comparing Regional Importance to Global Importance
Pythondef regional_compare_global_explainability(model, regional_filters,title):
## get model attributes
explanation_algo = model.explainability.explanation_algo
predicted_class_col = model.get_positive_predicted_class()
## get regional explanations from filters
regional_explanations = pd.DataFrame(model.query({
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""global_importance"",
""parameters"": {
""predicted_attribute_name"": predicted_class_col,
""explanation_algorithm"": explanation_algo
}
}
],
""filter"": regional_filters
})).rename(columns={'explainer_attribute': 'feature','global_importance':'regional_importance'}).sort_values(by='regional_importance', ascending=False)
## get global explanations
global_explanations = pd.DataFrame(model.query({
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""global_importance"",
""parameters"": {
""predicted_attribute_name"": predicted_class_col,
""explanation_algorithm"": explanation_algo
}
}
]})).rename(columns={'explainer_attribute': 'feature','global_importance':'global_importance'}).sort_values(by='global_importance', ascending=False)
## combine dataframes
explanations = global_explanations.merge(regional_explanations, left_on='feature', right_on='feature')
ax = explanations.set_index('feature').plot(kind='bar')
ax.set_title(title)
ax.set_ylabel(""feature importance"")
Python## Running Regional Explainability
filters = [
{
""property"": ""AGE"",
""comparator"": ""gte"",
""value"": 18
},
{
""property"": ""AGE"",
""comparator"": ""lt"",
""value"": 40
}
]
regional_compare_global_explainability(arthur_model, filters,title =""Regional vs Global"")
Comparing Regional Importance to Regional Importance
Pythondef regional_compare_regional_explainability(model, regional_filters1,regional_filters2,title):
## get model attributes
explanation_algo = model.explainability.explanation_algo
predicted_class_col = model.get_positive_predicted_class()
## get regional explanations from filters
regional_explanations1 = pd.DataFrame(model.query({
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""global_importance"",
""parameters"": {
""predicted_attribute_name"": predicted_class_col,
""explanation_algorithm"": explanation_algo
}
}
],
""filter"": regional_filters1
})).rename(columns={'explainer_attribute': 'feature','global_importance':'regional_importance1'}).sort_values(by='regional_importance1', ascending=False)
## get regional explanations from filters
regional_explanations2 = pd.DataFrame(model.query({
""select"": [
{
""function"": ""regionalFeatureImportances"",
""alias"": ""global_importance"",
""parameters"": {
""predicted_attribute_name"": predicted_class_col,
""explanation_algorithm"": explanation_algo
}
}
],
""filter"": regional_filters2
})).rename(columns={'explainer_attribute': 'feature','global_importance':'regional_importance'}).sort_values(by='regional_importance', ascending=False)
## combine dataframes
explanations = regional_explanations1.merge(regional_explanations2, left_on='feature', right_on='feature')
ax = explanations.set_index('feature').plot(kind='bar')
ax.set_title(title)
ax.set_ylabel(""feature importance"")
Python## Running Regional Explainability
filters1 = [
{
""property"": ""SEX"",
""comparator"": ""eq"",
""value"": 1
}
]
filters2 = [
{
""property"": ""SEX"",
""comparator"": ""eq"",
""value"": 2
}
]
regional_compare_regional_explainability(arthur_model, filters1,filters2,title =""Regional vs Regional"")
back to top
NLP Explainability
The nlp_explanation function can be used to query and filter explanations for tokens in NLP inferences. Using this function, the user can filter and order tokens by importance. The following are available optional properties:
nlp_explanation.token - References a token within a specific inference.
nlp_explanation.location - References a token's absolute location within a specific inference.
nlp_explanation.value - References a token's explanation value within a specific inference.
Query Request:
JSON{
""select"": [
{
""function"": ""nlp_explanation"",
""alias"": ""<alias_name> [Optional]"",
""parameters"": {
""attribute_name"": ""<text_input_attribute_name> [string]"",
""nlp_predicted_attribute"": ""<predicted_attribute_name> [string]"",
""nlp_explainer"": ""[limeshap]""
}
}
],
""filter"": [
{
""property"": ""nlp_explanation.token"",
""comparator"": ""eq"",
""value"": ""<token>""
},
{
""property"": ""nlp_explanation.location"",
""comparator"": ""eq"",
""value"": ""<location>""
}
],
""order_by"": [
{
""property"": ""nlp_explanation.value"",
""direction"": ""desc""
}
]
}
Query Response:
JSON{
""query_result"": [
{
""inference_id"": ""<id> [string]"",
""nlp_explanation"": [
{
""algorithm"": ""[limeshap]"",
""predicted_attribute_name"": ""<predicted_attribute_name> [string]"",
""importance_scores"": [
{
""attribute_name"": ""<input_attribute_name> [string]"",
""tokens"": [
{
""token"": ""<token> [string]"",
""position"": ""<position_in_text> [int]"",
""value"": ""<explanation_score> [float]""
}
]
}
]
}
]
}
]
}
back to topUpdated 3 months ago Table of Contents
Tabular Explainability
Local Explainability
Global Explainability
Regional Explainability
NLP Explainability",arthur_scope_docs,https://docs.arthur.ai/docs/querying-explainability
83,"Binary Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Binary ClassificationSuggest EditsBinary classification models predict a binary outcome (i.e., one of two potential classes). In Arthur, these models fall into the classification category and are represented by the Multiclass model type.
Some common examples of Tabular binary classification are:
Is this credit card transaction fraud or not?
Will a customer click on an ad or not?
Frequently, these models output both a yes/no answer and a probability for each (i.e., prob_yes and prob_no). These probabilities are then categorized into yes/no based on a threshold. In these cases, during onboarding, teams will supply their classification threshold and continuously track the class probabilities (i.e., prob_yes, prob_no).
Formatted Data in Arthur
Tabular binary classification models require three things to be specified in their schema: all predicting model attributes (or features), predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (numeric or categorical)Attribute (numeric or categorical)Probability of Prediction AProbability of Prediction BGround TruthNon-Input Attribute (numeric or categorical)High School Education34.5.95.05AMaleGraduate Degree44.1.86.14BFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their binary classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 3 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below are common examples of the required runnable predict function (that outputs two values, the probability of each potential class).
prediction to ground truth mappingExample Prediction FunctionExample Prediction Function with Transformations## Option 1:
Single Prediction Column, Single Ground Truth Column
# Map PredictedValue Column to its corresponding GroundTruth value.
# This tells Arthur that the `pred_proba_credit_default` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map_1 = {'pred_proba_credit_default' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_1,
)
## Option 2:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
pred_to_ground_truth_map_2 = {'pred_0' : 0,
'pred_1' : 1}
# Building the Model with this technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=pred_to_ground_truth_map_2,
positive_predicted_attr = 'pred_1'
)
## Option 3:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
pred_to_ground_truth_map_3 = {'pred_0' : 'gt_0',
'pred_1' : 'gt_1'}
# Building the Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=pred_to_ground_truth_map_3,
positive_predicted_attr = 'pred_1'
)
# example_entrypoint.py
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(x)
# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Available Metrics
When onboarding tabular classification models, several default metrics are available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
When teams onboard a binary classification model, the following metrics are automatically available in the UI (out-of-the-box). Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments are able to be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXXXUpdated 3 months ago What’s NextLearn more about how to interact with models, including binary classification, in ArthurModel OnboardingModel Monitoring Metric TypesTable of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/binary-classification
84,"Deploying on Amazon AWS EKS
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Deploying on Amazon AWS EKSSuggest EditsThis is a guide with additional steps to help you prepare your existing Amazon Elastic Kubernetes Service (Amazon EKS) cluster for installing the Arthur platform.
Ensure the initial steps detailed Installing Arthur Pre-requisites have already been applied to the cluster.
Configure EKS EBS CSI driver
As of EKS 1.23, the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver needs to be installed explicitly. This driver allows EKS clusters to manage the lifecycle of EBS volumes for Persistent Volumes. For more information, see Amazon Docs.
If you are deploying Arthur on EKS 1.23+, you must follow the instructions on this page.
Verify that the Add-On is successfully installed by navigating to AWS Console → EKS → Cluster → Add-Ons or by running helm list -A, depending on your installation method.
Optimizing the AWS EKS StorageClass
Once the EKS EBS CSI driver is installed, you can take advantage of the gp3 StorageClass type. This StorageClass is more cost-effective and performant
than the previous gp2 StorageClass. Apply the below YAML definition to your cluster:
YAML{note}
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
annotations:
storageclass.kubernetes.io/is-default-class: ""true""
name: gp3
parameters:
type: gp3
encrypted: ""true"" # parameter ensures created AWS EBS volumes are encrypted using AWS Managed KMS Key
kmsKeyId: <KMS Key ARN> # optional parameter ensures created AWS EBS volumes are encrypted using Customer Managed KMS Key
provisioner: ebs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
Ensure there is **_only one_** default StorageClass on the cluster. This is controlled by the `storageclass.kubernetes.io/is-default-class` annotation.
Supported AWS Service Authentication Mechanisms
If using AWS services with Arthur, such as S3 or SES, you will need to configure Arthur to authenticate with AWS.
Arthur currently supports 3 authentication mechanisms:
AWS Access Keys
Access Keys only work with S3.
If you want to use Access Keys, you must provision an IAM user and a set of keys.
Via AWS IAM, you will need to grant this user read/write access to the S3 storage bucket you plan to use with Arthur.
Selecting the Access Keys option will expand the Blob Storage section of the config page, where you will be able to enter your Access key, Secret Access key ID, and the S3 bucket.
IRSA
We recommend using IRSA to authenticate Arthur with AWS as it is the most secure and the only mechanism supporting SES.
Using this methodology will require some AWS platform work in preparation for Arthur.
You can follow these AWS docs, which will show you how to do this setup via eksctl or the AWS CLI, or you can automate this via your internal Infrastructure as Code.
The role you create will need S3 read/write privileges on the bucket you want to use with Arthur and permissions to send email via your SES entity.
Example snippets are as below:
Sample IAM policies
We provide some sample IAM policy snippets so they can be referenced easily.
Sample IAM policy for S3 access
{
""Statement"": [
{
""Action"": [
""s3:PutObject"",
""s3:GetObject"",
...
],
""Effect"": ""Allow"",
""Resource"": [
""arn:aws:s3:::<insert-s3-bucket-name>/*"",
""arn:aws:s3:::<insert-s3-bucket-name>""
],
....
},
Sample IAM policy for SES access
""Action"": [
""ses:SendTemplatedEmail"",
""ses:SendEmail"",
""ses:SendCustomVerificationEmail"",
""ses:SendBulkTemplatedEmail"",
""ses:SendBulkEmail"",
""ses:SendBounce""
],
""Effect"": ""Allow"",
""Resource"": ""*"",
""Sid"": ""sesSendEmails""
},
This role will also need to have a trust relationship with the OIDC provider of your EKS cluster, specifying the Arthur service accounts.
See the linked docs above for a further explanation.
An example snippet of this is:
{
""Version"": ""2012-10-17"",
""Statement"": [
{
""Sid"": """",
""Effect"": ""Allow"",
""Principal"": {
""Federated"": ""arn:aws:iam::123456789012:oidc-provider/oidc.eks.us-east-2.amazonaws.com/id/ABDCEF......""
},
""Action"": ""sts:AssumeRoleWithWebIdentity"",
""Condition"": {
""StringEquals"": {
""oidc.eks.us-east-2.amazonaws.com/id/ABDCEF:sub"": [
""system:serviceaccount:<namespace>:arthurai-<namespace>"",
""system:serviceaccount:<namespace>:arthurai-<namespace>-helm-hook""
]
}
}
}
]
}
Once this is all set up, you can pass this role to Arthur via the config page.
This sets the role in the Arthur Service Accounts specified above, which enables Arthur's pods to authenticate with AWS via the role, and the permissions you created.
Be sure to use the exact formatting shown below:
Proceed to the Blob Storage section of the Arthur config page to specify the S3 bucket
SES
To utilize AWS SES for Arthur-generated emails, you must configure IRSA as outlined in the above section.
Once this is done, navigate to the email configuration section of Arthur's config page.
Select AWS SES, then enter the region in which your SES entity is configured. As outlined above, the role associated with the cluster must have permissions on this SES entity.
If the SES entity is in the same account as your cluster, and you do not need to utilize a different role, such as for cross-account permissions, do not enter a role in the second box.
If your SES entity is in another Arthur, account, you must set up cross-account privileges between roles.
In the account of your SES entity (Account A), you must create an IAM role (Role A) that has sent email permissions to SES, as depicted above.
Role A will also need to have a trust relationship with either the account that your cluster is in (Account B), the OIDC provider on your cluster as depicted above, or the IRSA role associated with your cluster.
Additionally, the IRSA role you created above in Account B, will also need to be granted STS assume role privileges on the role you are creating in Account A.
Once all of this is set up, enter the role in the account that contains the SES entity (Account A) that the IRSA role should assume to send emails:
Updated 3 months ago Table of Contents
Configure EKS EBS CSI driver
Optimizing the AWS EKS StorageClass
Supported AWS Service Authentication Mechanisms
AWS Access Keys
IRSA
Sample IAM policies
SES",arthur_scope_docs,https://docs.arthur.ai/docs/deploying-on-amazon-aws-eks
85,"Fundamentals
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by FundamentalsUnderstanding the Basics of Arthur Query EndpointsSuggest EditsArthur models are queried using a SQL-like wrapper, so a working query's endpoints expect a body that contains the following keys:
select (required)
from (optional)
subquery (optional)
filter (optional)
group_by (optional)
order_by (optional)
This page is going to go through these endpoints one-by-one to explain what is happening, as well as practice querying some standard information about our model.
Select
The select statement of our query allows us to choose what we want to grab out of our database. Typically, when we are grabbing information about our model, we would want to grab one of the ""regular"" properties within the Arthur database. These properties include:
all of the model's attributes
inference_timestamp
received_timestamp
inference_id
partner_inference_id
ground_truth_timestamp (if ground truth is included)
batch_id (if batch model)
Similar to SQL, you can also select all of these attributes at once using the * string
We can practice running the following query to select inference_id
Pythonquery = {
""select"": [
{
""property"": ""inference_id""}
]
}
response = arthur_model.query(query)
response
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b'},
{'inference_id': '3392ac21-3cf9-48ad-9e07-e0343b76cc50'},]
We can see that this grabbed all of the inference ids from the inference dataset. It is important to note that the inference dataset is the default dataset to select from. If we wanted to grab more than one parameter at once in the query, we could do the following:
Pythonquery = {
""select"": [
{
""property"": ""inference_id""},
{
""property"": ""inference_timestamp""}
]
}
response = arthur_model.query(query)
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b',
'inference_timestamp': '2022-11-10T15:22:14.959Z'},
{'inference_id': '3392ac21-3cf9-48ad-9e07-e0343b76cc50',
'inference_timestamp': '2022-11-10T15:23:14.959Z'},]
📘Transformations in Select FunctionsTeams often ask about running transformations in select functions, such as addition, subtraction, or ""OR"" and ""AND"" statements. More informationn about creating logic can be found in the Transformation Functions page.
Unique Model Types
Object Detection
Computer Vision models with an Output Type of Object Detection have some special fields you can use when querying. Bounding boxes are sent using the following form: [class_id, confidence, top_left_x, top_left_y, width, height]. While the fields aren't named when sending data, you can access these nested fields when querying.
Here we can see:
Pythonquery = {
""select"": [
{""property"": ""inference_id""},
{""property"": ""objects_detected""}
]
}
arthur_model.query(query)
response
Or, to grab each value within the bounding box individually:
Pythonquery = {
""select"": [
{""property"": ""inference_id"" },
{""property"": ""objects_detected.class_id"" }
]
}
arthur_model.query(query)
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b',
'class_id': '0'},
{'inference_id': '3392ac21-3cf9-48ad-9e07-e0343b76cc50',
'class_id': '1'},]
Generative Text with Token Likelihoods
TokenLikelihoods attributes yield two queryable columns for that attribute with suffixes “_tokens” and “_likelihoods” appended to the attribute's name. For example, a model with a TokenLikelihoods attribute named summary_token_probs yields two queryable columns: summary_token_probs_tokens and summary_token_probs_likelihoods which represent an array of the selected tokens and an array of their corresponding likelihoods.
Pythonquery =
{""select"": [
{""property"": ""summary_token_probs_tokens""},
{""property"": ""summary_token_probs_likelihoods""}
]}
response[{ ""summary_token_probs_likelihoods"": [
0.3758265972137451,
0.6563436985015869,
0.32000941038131714,
0.5629857182502747],
""summary_token_probs_tokens"": [
""this"",
""is"",
""a"",
""summary""] }]
Property Types
There are two property types within Arthur.
""regular"" properties - listed above. These are properties available for any table queried.
""enriched"" properties - you must specify these by name to include them in the response and use the from value enriched:
anomaly_score
lime_importance
shap_importance
We'll learn more about querying from different datasets next.
From
In the example above, we could pull data from the inference dataset. The inference dataset is the dataset that is most often queried and therefore set as the default. However, it is only one of the datasets from where we can pull data. All of our sources include:
inference - The latest raw inference data sent to the platform. This is the default.
enriched - Every value from the inference data, with additional fields for anomaly scores and explanations. This data has some insert latency compared to the raw table.
reference - The reference data set uploaded for the model.
We will talk about the enriched dataset later. So, we can practice pulling an attribute from our reference dataset below
Pythonquery = {
""select"": [
{
""property"": ""age""}
]
, ""from"": ""reference""
}
response = arthur_model.query(query)
response[{'age': 23},
{'age': 48}]
Filter
The next feature we can add to queries within Arthur is filters. These allow us to specify a subset of data we are interested in from a database and grab only them. This filter command works similarly to where in traditional SQL query queries. We create this specification by using comparators to create rules within the data.
Available Comparators in Arthur
Numerical Comparison
eq - Filters where the property field equals the value field.
ne - Filters where the property field is not equal to the value field.
lt - Filters where the property field is less than the value field. Only valid for number values.
gt - Filters where the property field is greater than the value field. Only valid for number values.
lte - Filters where the property field is less than or equal to the value field. Only valid for number values.
gte - Filters where the property field is greater than or equal to the value field. Only valid for number values.
Examplequery = {
""select"": [
{
""property"": ""*""
}
],
""filter"": [
{
""property"": ""inference_id"",
""comparator"": ""eq"",
""value"": ""509f34eb-19ae-4a18-b454-6517fed4e76b""
}
]
}
response[{'inference_id': '509f34eb-19ae-4a18-b454-6517fed4e76b'},
{'age': '22'},
{""inference_timestamp"":""2020-07-22T10:01:23Z""},
{""attr1"":""something}]
The numerical comparators can also be used to investigate
Pythonquery = {
""select"": [
{ ""property"": ""*"" },
{""property"": ""anomaly_score""}
],
""from"": ""enriched"",
""filter"": [
{""property"": ""inference_timestamp"",
""comparator"": ""gte"",
""value"": ""2020-07-22T10:00:00Z"" },
{ ""property"": ""inference_timestamp"",
""comparator"": ""lt"",
""value"": ""2020-07-22T11:00:00Z""
}
]
}
response[{ ""inference_id"": ""0001"",
""attr1"": ""something"",
""anomaly_score"": 0.34,
""inference_timestamp"": ""2020-07-22T10:01:23Z""
},
{""inference_id"": ""0002"",
""attr1"": ""something"",
""anomaly_score"": 0.67,
""inference_timestamp"": ""2020-07-22T10:02:55Z"" } ]
In
in - Filters where the property field is equal to any value in a list of possible values
This comparator is typically used in count or rate functions to identify values that are in a set list of expected values.
Pythoncategory_list = ['no college', 'some college', 'bachelors', 'masters','phd',null]
query= {""select"":[{
""function"": ""rate"",
""alias"": ""OldCatRate"",
""parameters"": {
""property"": feature_of_interest,
""comparator"": ""in"",
""value"":category_list
}
}
]}
arthur_model.query(query)
response[{""OldCatRate"":0.89}]
Like
like - Filters where the property field is like the value field. This filter is only valid for property types of unstructured text.
Python
Null Values
NotNull - Filters where the property field is not null. The value field should be empty.
IsNull - Filters where the property field is null. The value field should be empty.
Python## Rate of Null Values in Feature
query = {""select"": [
{""function"": ""rate"",
""alias"": ""MarriageNullRate"",
""parameters"": {
""property"": ""Marriage"",
""comparator"": ""IsNull"",
""value"":""""
}
}
]
}
response = arthur_model.query(query)
response[{""MarriageNullRate"":0.89}]
Group By
The group_byendpoint is used in queries to group rows based on one or more columns. It allows for data aggregation by specifying the criteria for grouping and can be used in conjunction with any of the different function types we allow in Arthur queries.
These function types are further discussed in Querying Functions, but at a high level, they contain:
Default Performance Metrics: All default performance metrics available within Arthur
Transformation Functions: Functions done on the properties selected
Aggregation Functions: Provides common data aggregation measures like average, sum, count, quantiles, etc.
Composing Functions: This enables teams to create their own function logic
Pythonquery = {
""select"":[
{""function"":""count"",
""alias"":""count""},
{""property"":""age""}],
""filter"":[
{""property"":'income',
""comparator"":""gt"",
""value"":30000}],
""group_by"":[
{""property"":""age""}],
}
response[{""count"":2500},{""age"":25}]
Order By
The order_by endpoint enables teams to order the outputs of their query. Within the order_by command teams will specify what property they would like to sort the output by and in which direction. The options for direction are asc and desc for ascending and descending, respectively.
Pythonquery = {
""select"":[
{""property"":""age""},
{""property"":""income""}
],
""order_by"": [
{
""property"": ""income"",
""direction"": ""desc""
}
}
arthur_model.query(query)
Subquery
Subqueries are a powerful feature of modern SQL. The query endpoint can support subqueries via the subquery field in the request body. The format of the subquery is exactly the same as the full request body and can even support recursive subqueries! Here are some helpful examples that show how to use them.
Concise Queries with Subqueries
Sometimes you may have a calculation that must be aggregated in multiple ways. One option would be to repeat the calculation in each aggregation’s select, which can lead to lots of repeated JSON. Subqueries can be used to reduce duplicated expressions. In this example, we use a subquery to square the property Home_Value, then aggregate with min, max, and avg without repeating the calculation.
query = {
""select"": [
{
""function"": ""max"",
""alias"": ""max"",
""parameters"": {
""property"": {
""alias_ref"": ""hv_squared""
}
}
},
{
""function"": ""min"",
""alias"": ""min"",
""parameters"": {
""property"": {
""alias_ref"": ""hv_squared""
}
}
},
{
""function"": ""avg"",
""alias"": ""avg"",
""parameters"": {
""property"": {
""alias_ref"": ""hv_squared""
}
}
}
],
""subquery"": {
""select"": [
{
""function"": ""multiply"",
""alias"": ""hv_squared"",
""parameters"": {
""left"": ""Home_Value"",
""right"": ""Home_Value""
}
}
]
}
}
response[{""avg"": 33413668226.974968,
""max"": 17640000000000,
""min"": 0 }]
Subqueries for grouping
Subqueries can also be used to perform operations on grouped data. In this example, we get the count of the inferences in each batch in the subquery, then average those counts.
Python{
""select"": [
{
""function"": ""avg"",
""alias"": ""avg_count"",
""parameters"": {
""property"": {
""alias_ref"": ""batch_count""
}
}
},
{
""function"": ""count"",
""alias"": ""total_batches""
}
],
""subquery"": {
""select"": [
{
""function"": ""count"",
""alias"": ""batch_count""
},
{
""property"": ""batch_id""
}
],
""group_by"": [
{
""property"": ""batch_id""
}
]
}
}
response[{""avg_count"": 5930.2558139534885,
""total_batches"": 86}]
Updated 3 months ago Table of Contents
Select
Unique Model Types
Property Types
From
Filter
Available Comparators in Arthur
Group By
Order By
Subquery
Concise Queries with Subqueries
Subqueries for grouping",arthur_scope_docs,https://docs.arthur.ai/docs/fundamentals
86,"Common Queries Quickstart
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Common Queries QuickstartSuggest EditsTo access information about a model's performance, drift, bias, or other enabled enrichments, write a query object and submit it with the Arthur SDK using arthur_model.query(query)
For a general overview of this endpoint, including a more thorough description of its rules, power, and customizability, see the Fundamentals.
In each of the following examples, let our model be a binary classifier, and let GT1 and PRED1 be the names of our model's ground truth attribute and predicted value.
Accuracy
This is usually the simplest way to check for classifier performance. We can fetch a model's accuracy rate by querying a select on the function accuracyRate using the typical threshold 0.5.
Given the following query:
PythonGT1 = 'gt_isFraud'
PRED1 = 'pred_isFraud'
query = {
""select"": [
{
""function"": ""accuracyRate"",
""parameters"":
{
""threshold"" : 0.5,
""ground_truth_property"" : GT1,
""predicted_property"" : PRED1
}
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'accuracyRate': 0.999026947368421}]
Accuracy by batch
To expand the accuracy query by batch, add the batch_id property to the query's select, and add a group_by to the query using batch_id.
Given the following query:
PythonGT1 = 'gt_isFraud'
PRED1 = 'pred_isFraud'
query = {
""select"": [
{
""function"": ""accuracyRate"",
""parameters"":
{
""threshold"" : 0.5,
""ground_truth_property"" : GT1,
""predicted_property"" : PRED1
}
},
{
""property"": ""batch_id""
}
],
""group_by"": [
{
""property"": ""batch_id""
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'accuracyRate': 0.999704, 'batch_id': 'newbatch3'},
{'accuracyRate': 0.999744, 'batch_id': 'newbatch0'},
{'accuracyRate': 0.992952, 'batch_id': 'newbatch19'},
{'accuracyRate': 0.999616, 'batch_id': 'newbatch5'},
{'accuracyRate': 0.999144, 'batch_id': 'newbatch6'},
...]
Batch IDs
Querying accuracy by batch includes the batch_id values in the query result. But to query the batch_ids on their own, only select and group_by the batch_id.
Given the following query:
Pythonquery = {
""select"": [
{
""property"": ""batch_id""
}
],
""group_by"": [
{
""property"": ""batch_id""
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'batch_id': 'newbatch19'},
{'batch_id': 'newbatch18'},
{'batch_id': 'newbatch13'},
{'batch_id': 'newbatch12'},
{'batch_id': 'newbatch16'},
...]
Accuracy (single batch)
To query the accuracy for only one batch, add a filter to the query according to the rule batch_id == BATCHNAME
Given the following query (for a specified batch name):
PythonGT1 = 'gt_isFraud'
PRED1 = 'pred_isFraud'
BATCHNAME = ""newbatch19""
query = {
""select"": [
{
""function"": ""accuracyRate"",
""parameters"":
{
""threshold"" : 0.5,
""ground_truth_property"" : GT1,
""predicted_property"" : PRED1
}
},
{
""property"": ""batch_id""
}
],
""group_by"": [
{
""property"": ""batch_id""
}
],
""filter"": [
{
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": BATCHNAME
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'accuracyRate': 0.992952, 'batch_id': 'newbatch19'}]
Confusion Matrix
A confusion matrix counts the number of true positive, true negative, false positive, and false negative classifications; knowing these values is usually more useful than just accuracy when it is time to improve your model.
To query a confusion matrix, we use the confusionMatrix function in our query's select.
📘For the confusionMatrix function, the ground_truth_property and predicted_property parameters are optional.
Given the following query:
Pythonquery = {
""select"": [
{
""function"": ""confusionMatrix"",
""parameters"":
{
""threshold"" : 0.5
}
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'confusionMatrix':
{'false_negative': 4622,
'false_positive': 0,
'true_negative': 4745195,
'true_positive': 183}}]
Confusion Matrix (single batch)
As we did with accuracy, to get a confusion matrix for a single batch we add the property batch_id to the query's select, add a group_by using batch_id, and then add a filter according to the rule batch_id == BATCHNAME
Given the following query (for a specified batch name):
PythonBATCHNAME = 'newbatch19'
query = {
""select"": [
{
""function"": ""confusionMatrix"",
""parameters"":
{
""threshold"" : 0.5
}
},
{
""property"": ""batch_id""
}
],
""group_by"": [
{
""property"": ""batch_id""
}
],
""filter"": [
{
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": BATCHNAME
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'batch_id': 'newbatch19',
'confusionMatrix':
{'false_negative': 1762,
'false_positive': 0,
'true_negative': 248238,
'true_positive': 0}}]
Confusion Matrix (by group)
Instead of querying for metrics and grouping by batch, we can group by other groupings as well. Here, we use the model's non-input attribute race so that we can compare model performance across different demographics. To do this, we add the group name race to our query's select and to its group_by
Given the following query:
PythonGROUP = 'race'
query = {
""select"": [
{
""function"": ""confusionMatrix"",
""parameters"": {
""threshold"" : 0.5
}
},
{
""property"": GROUP
}
],
""group_by"": [
{
""property"": GROUP
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'confusionMatrix': {'false_negative': 1162,
'false_positive': 0,
'true_negative': 1184707,
'true_positive': 44},
'race': 'hispanic'},
{'confusionMatrix': {'false_negative': 1145,
'false_positive': 0,
'true_negative': 1186659,
'true_positive': 49},
'race': 'asian'},
{'confusionMatrix': {'false_negative': 1137,
'false_positive': 0,
'true_negative': 1187500,
'true_positive': 38},
'race': 'black'},
{'confusionMatrix': {'false_negative': 1178,
'false_positive': 0,
'true_negative': 1186329,
'true_positive': 52},
'race': 'white'}]
Predictions
Here we aren't querying any metrics - we are just accessing all the predictions that have output by the model.
Given the following query:
PythonPRED1 = 'pred_isFraud'
query = {
""select"": [
{
""property"": PRED1
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'pred_isFraud_1': 0.005990342859493804},
{'pred_isFraud_1': 0.02271116879043313},
{'pred_isFraud_1': 0.15305224676085477},
{'pred_isFraud_1': 0},
{'pred_isFraud_1': 0.03280797449330532},
...]
Predictions (average)
To only query the average value across all these predictions (since querying all predictions and then averaging locally can be slow for production-sized query results), we only need to add the avg function to our query's select, with our predicted value PRED1 now being a parameter of avg instead of a property we directly select.
Given the following query:
PythonPRED1 = 'pred_isFraud'
query = {
""select"": [
{
""function"": ""avg"",
""parameters"": {
""property"": PRED1
}
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'avg': 0.016030786000398464}]
Predictions (average over time)
To get the average predictions on each day, we add the function roundTimestamp to our select using a time_interval of day - this groups the timestamp information according to day instead of options like hour or week. Then, we add a group_by to the query using the alias (DAY) specified in the roundTimestamp function.
Given the following query:
PythonPRED1 = 'pred_isFraud'
query = {
""select"": [
{
""function"": ""avg"",
""parameters"": {
""property"": PRED1
}
},
{
""function"": ""roundTimestamp"",
""alias"": ""DAY"",
""parameters"": {
""property"": ""inference_timestamp"",
""time_interval"": ""day""
}
}
],
""group_by"": [
{
""alias"": ""DAY""
}
]
}
query_result = arthur_model.query(query)
The query_result will be:
Python[{'avg': 0.016030786000359423, 'DAY': '2022-07-11T00:00:00Z'},
{'avg': 0.018723459201003300, 'DAY': '2022-07-12T00:00:00Z'},
{'avg': 0.014009919280009284, 'DAY': '2022-07-13T00:00:00Z'},
{'avg': 0.016663649020394829, 'DAY': '2022-07-14T00:00:00Z'},
{'avg': 0.017791902929210039, 'DAY': '2022-07-15T00:00:00Z'},
...]
Updated 3 months ago Table of Contents
Accuracy
Accuracy by batch
Batch IDs
Accuracy (single batch)
Confusion Matrix
Confusion Matrix (single batch)
Confusion Matrix (by group)
Predictions
Predictions (average)
Predictions (average over time)",arthur_scope_docs,https://docs.arthur.ai/docs/query-quickstart
87,"Bias Mitigation
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Bias MitigationPost Process Technique to Determine the Fairest ThresholdSuggest EditsIf you are interested in bias mitigation capabilities, we’re happy to discuss your needs and what approaches would work best for you. Within Arthur Scope, we offer postprocessing methods; we encourage exploring alternate (pre- or in-processing) methods if your data science team has the bandwidth to do so.
Our currently available postprocessing method for use is the Threshold Mitigator. It automatically evaluates for demographic parity, equalized odds, and equal opportunity constraints.
Enabling Bias Mitigation will automatically train a mitigation model for all attributes marked as.monitor_for_bias=True, for the constraints of demographic parity, equalized odds, and equal opportunity.
The Onlys of Bias Mitigation
Bias Mitigation is an enrichment in Arthur with a few only's.
Bias Mitigation is only available for binary classification models
It can only be enabled if at least one model attribute is marked as.monitor_for_bias=True
It is the only enrichment that is only available in the Python SDK. This also means that it is the only enrichment you run in a notebook each time you want to use it.
Bias Mitigation with the Python SDK
As mentioned above, bias mitigation is only available through our Python SDK. Here is an example notebook that we have put together on how to use the bias mitigation capabilities: Bias Mitigation Notebook on Arthur GitHub
Understanding the Algorithm
To learn more about the algorithm used for bias mitigation. Please refer to the Arthur Algorithms documentation section.Updated 3 months ago What’s NextLearn about enabling enrichments in the model onboarding sectionEnabling EnrichmentsTable of Contents
The Onlys of Bias Mitigation
Bias Mitigation with the Python SDK
Understanding the Algorithm",arthur_scope_docs,https://docs.arthur.ai/docs/bias-mitigation
88,"Welcome to Platform Administration
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Welcome to Platform AdministrationSuggest EditsThe second half of our documentation is dedicated to Platform Administration resources.
Platform administration isUpdated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/welcome-to-platform-administration
89,"Examples
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ExamplesGet started with Arthur Scope using one our many examplesSuggest EditsThroughout our website, documentation, public GitHub, and YouTube channel, we are dedicated to providing useful and intuitive examples of how to use the Arthur platform.
Core Example Groups: We have broken out our core examples resources into four sections:
Model Onboarding
Notebook Tutorials
Interested in something else?: Here are some of our other most commonly referred to references:
Arthur Glossary
Arthur YouTube Channel
📘Can't Find What You're Looking For?If you can't find what you're looking for, please let us know either through the feedback section in the top right corner of this documentation website or submit a page review at the bottom of this page.
Model Onboarding
Getting Started - Quickstart Guides
ExampleResource PageQuickstart (Tabular)QuickstartQuickstart NLPNLP OnboardingQuickstart Computer VisionCV OnboardingQuickstart Generative Text (LLM)Generative TextOnboarding Model with APIRegistering A Model with the API
Model Input / Output Type
Model TypeBatchStreamingTabular Binary ClassificationCredit Risk Batch ExampleCredit Risk ExampleTabular Multi-class ClassificationTabular RegressionBoston Housing ExampleNLP Binary ClassificationNLP Multi-class ClassificationMedical Transcript ExampleNLP RegressionNLP - Generative Text (LLM)OpenAI ExamplesCV Binary ClassificationSatelite Image ExampleCancer Classification ExampleCV Multi-class ClassificationCV RegressionCV Object DetectionMars Rover ExampleTime Series Ranked List (Recommender System)
Partnerships and Integrations
NameResource PageSpark MLSpark MLSageMaker Data CaptureSageMakerLangchainLangchain
Extra Model Onboarding Tutorials
NameDescriptionResource Page or NotebookTesting Arthur Explainer LocallyBefore enabling explainability, teams can test their explainer locally. Ensure that it will work when onboarded to Arthur.Test Explainability LocallyConnecting Arthur Model Object in NotebookTeams must first connect to the Arthur platform to run any functionality in a notebook.Creating a Connection to ArthurOnboarding Historical Data to ArthurWant to backfill your new model on Arthur with existing data? Onboard historical inferencesSending Historical DataModel VersioningExample of creating mutliple model versionsModel Versioning Notebook
Notebook Tutorials
Metrics and Alerting
NameNotebookCreate Custom Metrics in a NotebookComing SoonCreate Custom Alerts in a NotebookComing SoonDeleting Alerts in a NotebookComing Soon
Enrichments
NameDescriptionNotebookBias MitigationRun our bias mitigation enrichment in a notebookBias Mitigation Notebook
Querying Guides
NameDescriptionNotebook or Resource PageQuery Guide NotebookOverview of querying in a notebookQuery Jumpstart NotebookQuerying ExplainabilityQuery multiple levels of explainability in a notebookQuerying ExplainabilityQuerying Data DriftQuery data drift comparing different distributionsQuery Data DriftUpdated 2 months ago Table of Contents
Model Onboarding
Getting Started - Quickstart Guides
Model Input / Output Type
Partnerships and Integrations
Extra Model Onboarding Tutorials
Notebook Tutorials
Metrics and Alerting
Enrichments
Querying Guides",arthur_scope_docs,https://docs.arthur.ai/docs/examples
90,"Time Series Onboarding
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Time Series OnboardingSuggest EditsThis page walks through the basics of setting up a time series input model and onboarding it to Arthur Scope to monitor performance.
Getting Started
The first step is to import functions from the arthurai package and establish a connection with Arthur Scope.
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"")
Registering a Time Series Model
Each time series model is created with a name and with input_type = InputType.TimeSeries. Here, we register a time series model:
Pythonarthur_model = arthur.model(name=""RecSysQuickstart"",
input_type=InputType.TimeSeries,
model_type=OutputType.RankedList)
Formatting Reference/Inference Data
Column names can contain only alphanumeric and underscore characters.
Time series data can be uploaded to Arthur either in a DataFrame or a JSON file. Typically, a JSON file is a more natural formatting for time series data. For a time series model tracking credit card balance over time as input, the reference data might look like this:
JSON{
""reference_data"": {
""credit_card_balance"": [
{
""timestamp"": ""2023-10-05T00:00:00Z"",
""value"": 3004.18
},
{
""timestamp"": ""2023-10-06T00:00:00Z"",
""value"": 150.19
}
],
""id"": ""6euQxGJai11qr0gENGgvgh"",
""account_id"": ""8klQSGJil78qr4gLJKklsy""
},
... // more inferences here
}
Data Requirements
Arthur requires that all times will be present in a given series according to a regular interval (eg. one value each day).
There is an upper bound of 500 timestamps in a single time series inference.
Reviewing the Model Schema
Before you register your model with Arthur by calling arthur_model.save(), you can call arthur_model.review() on the model schema to check that your data was parsed correctly in your call to arthur_model.build().
For a time series model, the model schema should look like this:
Python		 name
stage
value_type
categorical
is_unique
0
time_series_attr
PIPELINE_INPUT
TIME_SERIES
False
True
1
non_input_1
NON_INPUT_DATA
FLOAT
False
False
...
Finishing Onboarding
Once you have finished formatting your reference data and your model schema looks correct using arthur_model.review(), you are finished registering your model and its attributes - so you are ready to complete onboarding your model.
See this guide for further details on how to save your model, send inferences, and get performance results from Arthur. These steps are the same for time series models as for models of any InputType and OutputType .Updated about 2 months ago Table of Contents
Getting Started
Registering a Time Series Model
Formatting Reference/Inference Data
Data Requirements
Reviewing the Model Schema
Finishing Onboarding",arthur_scope_docs,https://docs.arthur.ai/docs/ranked-list-output-onboarding
91,"User-Defined Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by User-Defined MetricsTrack and communicate the unique ways your internal team or external stakeholders define performanceSuggest EditsBeyond Arthur's UI capabilities and APIs, Arthur's Python SDK has a built-in Query Functionality. This SQL-like query structure allows teams the ability to turn any functions they create into alert-able metrics within the Arthur platform.
Pieces of User-Defined Metrics
There are three pieces to define when creating user-defined metrics:
Metric Name: The name refers to how the user identifies or calls the metric.
Metric Type: The Arthur platform has four unique metric types. These are:
Metric TypePerformanceDataDriftDataBoundModelOutput
Typically, Arthur infers the type of metric from the function provided. However, for more advanced metrics that Arthur cannot infer or for teams that want to ensure a specific metric type, teams should define the specific metric type.
Arthur Query Function: Finally, the most important aspect of defining a metric is creating the mathematical function the metric will track. These functions are built with the Arthur Query structure. More about building out this Arthur Query function can be found below:
Building an Arthur Query Function
A more in-depth querying guide can be found in the query section. However, there are some key things to keep in mind:
The query must be built with the information contained within Arthur (per inference). Additional information beyond model inputs (features) and outputs (predictions) can be added to Arthur as Non-Input Attributes.
The query should return a single value. For example, the query should not return a row for each inference, time-series data, or score for multiple attributes.
The query should generally not include filters unless it is to define a very specific segment you wish to track (and not a further filter). This is because filters can easily be defined when evaluating a metric. Keeping the metric definition general allows different segmentations to be easily applied.
The query may include parameters, which are denoted by {{ param_name }} template values in the query definition and have corresponding entries in the parameters field on the metric definition.
Custom Data Drift Metrics
Of note for teams looking to create custom metrics for data drift, data drift uses a special query structure. So, these metrics would need to be defined with this special structure. Teams also need to add in is_data_drift = True to their definition.
Defining Metrics Examples
Using Python SDK
We typically recommend that teams create custom metrics within a notebook with our Python SDK. This is because teams can easily craft and validate their queries in a notebook before onboarding them to Arthur.
Follow this notebook example to learn more: Coming Soon
Using API
You can define a custom metric for your model by sending a POST request to the dedicated metrics endpoint at /models/{model_id}/metrics. Check out the API Reference guide for the full specification.Updated 3 months ago Table of Contents
Pieces of User-Defined Metrics
Building an Arthur Query Function
Defining Metrics Examples
Using Python SDK
Using API",arthur_scope_docs,https://docs.arthur.ai/docs/custom
92,"OIDC
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by OIDCSuggest Edits📘SSO configurations are only supported in on-prem Arthur installations
This page provides a walk-through for how to configure your Arthur installation to work with an OIDC compatible IdP.
In order to complete this guide, you need administrator access to your IdP and access to your Arthur installation's admin console configuration. Additionally, you will either need access to the Arthur superadmin user or be able to assume a role in your IdP to give yourself RBAC management permissions in Arthur.
This guide will walk through the following steps:
Configure the IdP user groups and OIDC token claims
Configure the IdP OIDC Settings
Configure Arthur to work with your IdP
Apply the Arthur IdP YAML configuration
Create organization user roles to match the IdP user groups
Test Access
Cleaning Up
1. Configure the IdP user groups and OIDC token claims
In order to properly map users to permissions, Arthur requires a claim in your OIDC JSON Web Token (JWT) that contains information about the group memberships of the user. Each group in the IdP should correspond to a role in Arthur's {doc}custom_rbac permission system.
This process can vary depending on your IdP, but most IdP's should have a user grouping mechanism, and a mechanism to configure attributes in the JWT claims. For example using Okta, admins can configure the JWT claims to include group information under their account's Security -> API -> default -> Claims then the ""Add Claim"" button. From the popup, give the claim a name, ""Groups"" in this example, set the ""Include in token type"" to ""Access Token"", select ""Value type"" as ""Groups"", and include a ""Matches regex"" filter to select the groups to include:
Retrieving an example OIDC token is IdP-specific and may involve completing the sign-in flow via a script or API client. An alternative can be to use a 3rd party like https://oidcdebugger.com, but that will require whitelisting https://oidcdebugger.com/debug as a valid redirect URL for your Arthur SSO app (this could be enabled temporarily for debugging, then removed). Here is an example JWT after setting the group claims field:
JSON{
""iss"": ""https://dev.okta.com/oauth2/default"",
""aud"": ""api://default"",
""scp"": [
""openid""
],
""Groups"": [
""idp-admin"",
""org-1-model-owner"",
""org-2-model-owner""
],
""FirstName"": ""John"",
""LastName"": ""Doe"",
""Login"": ""[email protected]""
}
As the example token shows, the user's groups in the IdP are populated as a string list in the ""Groups"" field in the token. Arthur will use this list of groups to match the user to the corresponding roles in Arthur by name.
2. Configure the IdP OIDC Settings
In order for your IdP to speak to Arthur, it needs to know where to find it. Enter the following URL in your IdP's configuration to whitelist Arthur's callback endpoint (sign-in redirect
URL): https://<YOUR ARTHUR HOST>/login/callback.
Additionally, the IdP will need to know what OIDC protocol to speak with Arthur. Today, Arthur supports two protocol flows:
implicit
PKCE
Both flows are intended to be used with ""Single Page Applications"" or SPAs. Follow the configuration for your IdP that matches one of those two flows with SPAs. Additionally, note the following settings from your IdP in order to use in the Arthur configuration below:
Client ID
Resource ID (if available)
OIDC flow (PKCE or implicit)
Audience (value that is set in the token by the IdP)
3. Configure Arthur to work with your IdP
Next, Arthur needs to know how to handshake with your IdP. To do that, Arthur requires the following
information:
Your IdP's discovery URL, typically in the format <IdP path>/.well-known/openid-configuration URL that contains the relevant endpoints for your IdP.
{note}If this page isn't accessible to Arthur due to CORS or other restrictions, the values can be provided manually.
One or more IdP administrator user groups that will be paired to global custom roles in Arthur
(see here for a description of what these are for)
An understanding of your OIDC token claims (values) and how to parse user information out of it
The four configuration values captured above from your IdP
With that information available, it is possible to fill out Arthur's IdP configuration YAML. The next subsections explain each subsection of the Arthur YAML configuration, and is followed by some complete examples further down.
Configuring the IdPs discovery URL
Almost all OIDC IdP's have accessible discovery URLs, but some do not provide CORS support for them, so their contents need to be filled out manually. To support either option, Arthur has two configurations that can be used, discoveryBaseURL or endpointOverrides. If your IdP has CORS restrictions, see Appendix B for additional setup that is required.
YAML# use this option if your IdP has an accessible discovery URL
# IMPORTANT: don't include the /.well-known/openid-configuration suffix!!
# for example, if the full URL is https://<HOST>/oauth2/default/.well-known/openid-configuration
# only specify: https://<HOST>/oauth2/default
discoveryBaseURL: ""https://<HOST>/oauth2/default""
# use this option if your IdP has CORS restrictions on the discovery URL, otherwise comment this out.
# fill in the values manually from the discovery endpoint's contents
endpointOverrides:
issuer: ""issuer string for the IDP""
authorization_endpoint: ""URL ending in /authorize""
token_endpoint: ""URL ending in /token""
jwks_uri: ""URL ending in /keys""
userinfo_endpoint: ""URL ending in /userinfo""
# note not all IdPs will have the following endpoints, fill in as many as you can
end_session_endpoint: ""URL ending in /logout""
device_authorization_endpoint: ""URL ending in /devicecode""
revocation_endpoint: ""URL ending in /revoke""
introspection_endpoint: ""URL ending in /introspect""
registration_endpoint: ""URL ending in /clients""
Configure the Arthur Global Roles
Arthur has the ability to create roles for the cluster administrators during the configuration of the IdP. These roles are often needed by admins to configure RBAC and create organizations for other users in the system. See {ref}creating_global_roles_in_arthur_config for a deep dive on how to use global roles.
📘Global Roles in the UIGlobal Roles only provide access for the Arthur API, and therefore, cannot be used to grant universal access to workflows in the Arthur UI. A user that belongs to a group that maps to a Global Role will not be able to see all organizations in the UI, nor enter and read/write data in a specific organization unless they are bound to a role within that organization through an Organization-Scoped Role.
This section of the YAML config is under the globalRoleDefs field. It accepts a list of role definitions that will be created when the configuration is applied. The names of the roles in this section must match the user groups in your IdP in order to be able to assume them in Arthur.
YAML
globalRoleDefs:
# Here we can specify a list to define multiple global roles
- name: ""idp-admin"" # change this name to match the cluster administrator group name in your IdP
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
model:
- read
- write
- delete
Parsing the IdP JWT claims
In order for Arthur to communicate with your IdP, it needs to understand the format of the JWT claims your IdP uses. This section of the config falls under the accessTokenValidation YAML field. This section is designed to be flexible to support a variety of claim formats, so it has a lot of options. At its core, the goal is to tell Arthur how to be able to extract the following information from the claims:
user roles/groups
first name
last name
email
user ID
Each field has a corresponding YAML configuration that defines where to find the information in the JWT claims. For example:
YAMLclaimMapping:
firstName: FirstName
This configuration tells Arthur that it can find the user's first name under the ""FirstName"" claim in the JWT. Such a token might look like this:
JSON{
""iss"": ""https://dev.okta.com/oauth2/default"",
""aud"": ""api://default"",
""scp"": [
""openid""
],
""Groups"": [
""idp-admin"",
""org-1-model-owner"",
""org-2-model-owner""
],
""FirstName"": ""John"",
""LastName"": ""Doe"",
""Login"": ""[email protected]""
}
More examples of how to parse user information out of the JWT claims can be
found below.
Full Configuration Examples
Here is an example of a full configuration, combining each section described above.
YAMLversion: v2
kind: OIDC
config:
# discovery URL without the /.well-known/openid-configuration suffix
discoveryBaseURL: https://example.com/oauth2/default
# Either ""implicit"" or ""PKCE""
flowType: PKCE
# client ID from your IdP for the Arthur SSO application
clientID: ""client id string""
# optional: resource ID from your IdP for the Arthur SSO application if required by the IdP
resourceID: """"
authorizeScopes:
- openid
# required for OIDC
# use this section to define global roles
# one example role would be to give the cluster admin permissions to create organizations and manage custom roles
globalRoleDefs:
- name: ""iam-admin""
# change this to match the user group name in your IdP for administrators
permissions:
custom_roles:
- read
- write
- delete
organization_global:
- read
- write
organization:
- read
- delete
# this section describes how to parse the user information out of the JWT returned from the IdP
# this is used by Arthur to understand who the user is and what their roles are
accessTokenValidation:
type: JWT
# only JWT is supported today
# fields in the token Arthur will use to extract the authentication information
claimMapping:
roles: Groups
# this is telling Arthur to look in the ""Groups"" claim to find the list of user's roles
userID: EmployeeID
username: Login
firstName: FirstName
lastName: LastName
email: Login
# one or more audiences to validate, this should match your IdP's configuration
audience:
- api://default
# optional override signature algo
# signatureAlgo: RS256
Here is an additional descriptions of the fields that need to be set in the config YAML above:
discoveryBaseURL: This is the base URL for your Identity Provider. Your IdP should have
a /.well-known/openid-configuration endpoint and the
discoveryBaseURL is simply that url minus the /.well-known/openid-configuration part.
flowType: We support both implicit and PKCE flows. Consult with your team to decide which OIDC flow type is right
for your organization.
clientID: When you create the application integration in your IdP, a Client ID will be provided to you. Paste that
value into this field.
resourceID: This is optional. If your IdP gives you a Resource ID when creating your application integration, paste
the value here.
claimMapping: We extract various pieces of authentication information from the provided JWT access token. However,
there is no common standard for
how these pieces of information should be formatted in the token. For us to extract this information from the token,
we need you to explicitly tell Arthur
where this information is stored in the token. For example, a username could be stored in a field called username
or login or email or userID.
In order to get this user information, a mapping needs to be provided for the following items
roles: This is the field for where either a single authorization role or a list of authorization roles will be
specified.
Note that this is not where you paste a list of roles, this is the name of a field in the JWT where the
user's roles are specified.
For help with role definitions, see {doc}custom_rbac.(Required)
userID: This is the field for a unique identifier for the user; this is frequently the same as username
and/or email. (Optional, omit if empty)
username: This is the field for the user's unique username; this is frequently the same as username
and/or email. (Optional, omit if empty)
firstName: This is the field for the user's first name. (Optional, omit if empty)
lastName: This is the field for the user's last name. (Optional, omit if empty)
email: This is the field for the user's email. (Optional, omit if empty)
audience: This is part of the JWT standard. The aud field for any JWT you create must be a value in this list. For
example in the above configuration, any token that has an aud field that is not set to api://defaults, the token
will be automatically rejected by Arthur. If you are having trouble finding this value, it is frequently the same as
your resourceID. Remember to format this as a list, not a single value.
{note}If your IdP has CORS restrictions see
[Appendix B](#appendix-b-setup-for-idps-with-cors-restrictions) below for a workaround.
4. Apply the Arthur IdP YAML configuration
Once you have your YAML configuration file ready, you need to add it to your Arthur installation. With the Arthur admin console open, navigate to the ""Use a 3rd Party Global Identity Provider"" section and select ""OIDC"". This will expose a text box for you to paste the YAML config file assembled above. When pasting, make sure whitespace is preserved and the YAML document has consistent spacing (do not mix tabs and spaces). Here is a screenshot of the config section:
Once you have added your config files, scroll to the bottom and click ""Save"" to save the config. Then go to the latest
version and click ""Deploy"" to roll out the change to the cluster.
5. Create organization user roles to match the IdP user groups
In order to complete this section, you will need access to the Arthur superadmin user credentials set during your install, or you will need to be able to assume the role defined in the Arthur IdP config YAML created above in the globalRoleDefs section.
In order to use the API example linked below, you will need a Bearer token (authentication token) to include with your API request. There are a few options available to retrieve a token:
Retrieve a global role token directly from your IdP - Most IdPs will have a method to retrieve tokens for users. Some companies make scripts or APIs that allow retrieving a token. If your IdP does not have an automated method to retrieve a token, you can try setting up a tool like https://oidcdebugger.com (this may involve adding https://oidcdebugger.com as an allowed URL in your IdP settings).
Retrieve a global role token from your browser cookies - If you sign in to Arthur as a user with a global role, the UI will not be fully functional, but it will have a valid access token in the cookies. If you navigate to your browser's developer console and then go to the Application Storage/Cookies section, you should see a cookie like ArthurAuth0, which is your authentication token. Note: if your user has a large number of groups, there may be multiple cookies of the form ArthurAuthN. In this case your token was too large to fit in the browser cookie, so it had to be split. You can assemble the full token by concatenating the values of the ArthurAuthN cookies in order.
Use the /login API endpoint with the superadmin user's credentials set during the Arthur install (only available on-prem).
Using either of those credentials, we can use the Arthur API to define roles in Arthur that match the user group names in your IdP. See the {ref}creating_organization_roles section for an example API request to create custom roles in Arthur. Importantly, the role names must uniquely match to a user group in your IdP in order for your users to be able to assume those permissions in Arthur. Therefore, the roles in Arthur must be globally unique in the entire Arthur installation.
6. Test Access
At this point everything should be configured correctly to sign in to Arthur via SSO. Either navigate to your IdP or the Arthur homepage to test logging in.
7. Cleaning Up
Once users are successfully able to log in to Arthur via the IdP, you should do the following to ensure proper security best-practices remain enforced:
Restrict any Arthur global roles to only allow access to essential admin functions
Set the Arthur superadmin user password securely, and either store the password in a vault, or discard the password entirely. superadmin shouldn't be used going forward.
Set up a policy to routinely rotate the superadmin password to keep it secure
Together, these practices will help ensure the security of your Arthur installation, and will give your IdP sole control over the platform and who is able to access it.
Common Troubleshooting
If after following the steps above, users are not able to log in via the IdP try some of these common troubleshooting tips:
Does the user properly redirected to the IdP's log in screen?
If not, there is likely a configuration error in the Arthur YAML config with the IdP discovery URL. Double check that the url entered resolved correctly when you append /.well-known/openid-configuration to the end of it. The full URL should be viewable in your browser or via a REST client.
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
If not, there is likely a configuration error with the IdP and the URLs that it uses to communicate with Arthur. Double-check the redirect (whitelisted) URL is configured correctly for the Arthur installation at https://<HOSTNAME>/login/callback.
A user can see the Arthur home page, but can't see any of the model in their organization
If a user cannot see any of the models in their organization, it means they either don't have the necessary permissions to access models (see {doc}../../reference/permissions_by_endpoint) or they were not able to correctly assume the role in Arthur. Double-check the groups in their JWT claims match the role names that have been configured in Arthur. A superadmin or global role user with permissions to manage RBAC can see a list of roles in the installation by using the following API call. Be sure to replace the HOST and AUTH TOKEN for your installation and user:
Shellcurl --location 'https://<HOST>/api/v3/authorization/custom_roles' \
--header 'Authorization: Bearer <INSERT AUTH TOKEN HERE>'
Appendix A: More examples of JWT claims and how to parse them
This section outlines some additional ways to use the accessTokenValidation section of the Arthur IdP config YAML format. The below examples include sample JWTs, then corresponding YAML for how to parse them.
Basic Full Example
This example shows how to parse a user's information from JWT claims in a typical format.
Example parsed JWT claims JSON:
JSON{
""iss"": ""https://dev.okta.com/oauth2/default"",
""aud"": ""api://default"",
""scp"": [
""openid""
],
""Groups"": [
""idp-admin"",
""org-1-model-owner"",
""org-2-model-owner""
],
""FirstName"": ""John"",
""LastName"": ""Doe"",
""Login"": ""[email protected]"",
""EmployeeID"": ""1234567890""
}
Corresponding settings for the Arthur IdP config YAML accessTokenValidation for the user information field:
YAMLaccessTokenValidation:
type: JWT
claimMapping:
roles: Groups
userID: EmployeeID
username: Login
firstName: FirstName
lastName: LastName
email: Login
Minimal Example
This example shows how to parse a user's information from JWT claims when many fields are missing.
Example parsed JWT claims JSON:
JSON{
""iss"": ""https://dev.okta.com/oauth2/default"",
""aud"": ""api://default"",
""scp"": [
""openid""
],
""Groups"": [
""idp-admin"",
""org-1-model-owner"",
""org-2-model-owner""
],
""user"": ""[email protected]""
}
Corresponding settings for the Arthur IdP config YAML accessTokenValidation for the user information field:
YAMLaccessTokenValidation:
type: JWT
claimMapping:
roles: Groups
userID: user
username: user
firstName: """"
lastName: """"
email: user
Appendix B: Setup for IdPs with CORS Restrictions
Completing this will require access to the Kubernetes cluster Arthur is running in, and the ability to create ingress resources in that cluster.
If your OIDC Identity Provider does not support CORS (common with Microsoft Azure AD), you will need to proxy requests via the Arthur backend. The following examples show how this can be done with a cluster using the NGINX ingress controller.
This first example YAML configures a route on NGINX that will proxy OIDC connections to your IdP. You'll need to replace the <IDP HOST> and <ARTHUR HOST> placeholders, then apply it to your cluster with kubectl apply -n <NAMESPACE> -f file.yaml. There should be two places to fill in each variable below.
YAMLapiVersion: v1
kind: Service
metadata:
name: external-idp
spec:
type: ExternalName
externalName: ""<IDP HOST>""
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: external-idp
annotations:
kubernetes.io/ingress.class: nginx
nginx.ingress.kubernetes.io/rewrite-target: /$2
nginx.ingress.kubernetes.io/backend-protocol: ""HTTPS"" #important
nginx.ingress.kubernetes.io/upstream-vhost: ""<IDP HOST>""
spec:
rules:
- host: ""<ARTHUR HOST>""
http:
paths:
- backend:
service:
name: external-idp
port:
number: 443
path: /oidc(/$)(.*)
pathType: Prefix
tls:
- hosts:
- ""<ARTHUR HOST>""
secretName: kotsadm-tls
After you've applied the above configuration to your cluster, you should be able to visit your IdP's /.well-known/openid-configuration endpoint at the following URL: https://<ARTHUR HOST>/oidc/<your IdP's .well-known endpoint path>. Once that is accessible, we need to modify the OIDC YAML configuration file. Fill in the following example with the correct values in the endpointOverrides section. Note, the issuer and authorization_endpoint fields should match what is in your IdP's /.well-known spec. The rest of the values should use the same path as shown in the IdP's /.well-known spec, but with the value of <ARTHUR HOST>/oidc/ substituted for the host of the IdP. The following example shows a proper CORS config for an IdP at the https://XXXX.okta.com address.
YAMLversion: v2
kind: OIDC
config:
discoveryBaseURL: https://XXXX.okta.com/oauth2/default
# if your IdP has CORS restrictions with the metadata URL,
# specify this block to prevent using the metadata endpoint to look them up
endpointOverrides:
# these first two match the IdP's .well-known spec
issuer: ""https://XXXX.okta.com/oauth2/default""
authorization_endpoint: ""https://XXXX.okta.com/oauth2/default/authorize""
# notice the following are all modified to add the <ARTHUR HOST>/oidc prefix in the URL
token_endpoint: ""https://<ARTHUR HOST/oidc/oauth2/default/tokens""
jwks_uri: ""https://<ARTHUR HOST/oidc/oauth2/default/keys""
userinfo_endpoint: ""https://<ARTHUR HOST/oidc/oauth2/default/user_info""
end_session_endpoint: ""https://<ARTHUR HOST/oidc/oauth2/default/logout""
# the rest of this file is unchanged from the examples above...
Once you have modified this YAML file accordingly, follow the steps
above to save it to your installation.Updated 3 months ago Table of Contents
1. Configure the IdP user groups and OIDC token claims
2. Configure the IdP OIDC Settings
3. Configure Arthur to work with your IdP
Configuring the IdPs discovery URL
Configure the Arthur Global Roles
Parsing the IdP JWT claims
Full Configuration Examples
4. Apply the Arthur IdP YAML configuration
5. Create organization user roles to match the IdP user groups
6. Test Access
7. Cleaning Up
Common Troubleshooting
Does the user properly redirected to the IdP's log in screen?
Once the user authenticates with the IdP, are they redirected to the Arthur homepage?
A user can see the Arthur home page, but can't see any of the model in their organization
Appendix A: More examples of JWT claims and how to parse them
Basic Full Example
Minimal Example
Appendix B: Setup for IdPs with CORS Restrictions",arthur_scope_docs,https://docs.arthur.ai/docs/okta
93,"Tabular
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by TabularSuggest EditsTabular input models are a type of machine learning model that operates on tabular data, which is data that is arranged in a table or spreadsheet format. These models are used to make predictions or classifications based on the input data, which may include features such as numerical or categorical variables. Tabular input models are commonly used in fields such as finance, healthcare, and marketing, where data is often structured in a tabular format.Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/tabular
94,"Integrations
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by IntegrationsSuggest EditsCurrently there are two available integrations for Single-Sign-On. Find these in the Single Sign On (SSO) Integrations page.Updated 3 months ago",arthur_scope_docs,https://docs.arthur.ai/docs/integrations
95,"Glossary
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by GlossarySuggest EditsThe following definitions are specific to the Arthur platform, though in most cases apply to ML more broadly.
Arthur Inference
Container class for inferences uploaded to the Arthur platform. An inference comprises input features, prediction values, and (optionally) ground truth values and any Non-Input data.
Example:
Pythonground_truth = {
""Consumer Credit Score"": 652.0
}
inference = arthur_model.get_inference(external_id)
inference.update(ground_truth)
Related terms: inference, ArthurModel
Arthur Model
A model object sends and retrieves data pertinent to a deployed ML system. The ArthurModel object is separate from the underlying model trained and makes predictions; it serves as a wrapper for the underlying model to access Arthur platform functionality.
An ArthurModel contains at least aname, an InputType and a OutputType.
Examples:
Pythonarthur_model = connection.model(name=""New_Model"",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
Pythonarthur_model = connection.get(model_id)
arthur_model.send_inference(...)
Arthur Model Group
Arthur Model Groups are an organizational construct the Arthur platform uses to track different versions of an Arthur model. Every
Arthur Model is a version of one Model Group, and a Model Group will always have at least one Arthur Model. The Model Group for an Arthur Model can only be specified during onboarding, and once the Arthur Model is saved, its group cannot be changed. If an Arthur Model is created without specifying a Model Group, a new Model Group will be created automatically with the new model as its single version.
When adding a model to a model group, the model is assigned a unique, incrementing Version Sequence Number (starting at 1) corresponding to the order in which it was added to the model group. Additionally, you can provide a Version Label to store a custom version string label along with the Version Sequence Number.
Example:
Python# retrieve the first version of a model
arthur_model_v1 = connection.get(model_id)
model_group = arthur_model_v1.model_group
# create the new version of the model
arthur_model_v2 = connection.model(name=""Model_V2"",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
# add the new model to the model group
model_group.add_version(arthur_model_v2, label=""2.0.1"")
arthur_model_v2.save()
Related terms: Version Label, Version Sequence Number
Attribute
A variable associated with a model. , be input, prediction, ground truth or ancillary information (these groupings are known as Stages in the Arthur platform).
It can be categorical or continuous.
Example:
The attribute age is an input to the model, whereas the attribute creditworthy is the target for the model.
Synonyms: variable, {predictor, input}, {ouput, target}, prediction.
Related terms: input, stage, prediction, ground truth
Bias
While bias is an overloaded term in stats&ML, we refer specifically to where the model's outcomes have the potential to lead to discriminatory outcomes.
Example:
This credit approval model tends to lead to biased outcomes: men are approved for loans at a rate 50% higher than women are.
Related terms: bias detection, bias mitigation, disparate impact
Bias Detection
The detection and quantification of {ref}algorithmic bias <bias_monitoring> in an ML system, typically as evaluated on a model's outputs (predictions) across different definitions of a sensitive attribute.
Many definitions of algorithmic bias have been proposed, including group and individual fairness definitions.
Group fairness definitions are often defined by comparing group-conditional statistics about the model's predictions. In the below definitions, the group membership feature is indicated by G, and a particular group membership value is simplified by g.
Example:
Common metrics for group fairness include Demographic Parity, Equalized Odds, and Equality of Opportunity.
Related terms: bias mitigation
Demographic Parity
A fairness metric that compares group-conditional selection rates. The quantity being compared is:
P(Y^ = 1  G = g)
There is not necessarily a normative ideal relationship between the selection rates for each group: in some situations, such as allocating resources, it may be important to minimize the disparity in selection rates across groups; in others, metrics based on group-conditional accuracy may be more relevant. However, even in the latter case, understanding group-conditional selection rates, especially when compared against the original training data, can be useful contextualization for the model and its task as a whole.
Related term: disparate impact
Equal Opportunity
A fairness metric that compares group-conditional true positive rates. The quantity being compared is:
P(Y^ = 1  Y =1, G = g)
For all groups, a true positive rate closer to 1 is better.
Equalized Odds
A fairness metric that incorporates both group-conditional true positive rates and false positive rates, equivalently, true positive and negative rates. There are a variety of implementations
due to the fact that some quadrants of the confusion matrix are complements of one another); here is one possible quantity to compare across groups:
P(Y^ = 1  Y = 1, G = g) + P(Y^ = 0  Y= 0, G = g)
In this implementation, this quantity should be as close to 2 as possible for all groups.
Bias Mitigation
Automated techniques to mitigate bias in a discriminatory model. Can be characterized by where the technique sits in the model lifecycle:
Pre-Processing: Techniques that analyze datasets and often modify/resample training datasets to make the learned classifier less discriminatory.
In-Processing: Techniques for training a fairness-aware classifier (or regressor) that explicitly trades off optimizing for accuracy and maintaining fairness across sensitive groups.
Post-Processing: Techniques that only adjust the output predictions from a discriminatory classifier without modifying the training data or the classifier.
Related terms: bias detection
Binary Classification
A modeling task where the target variable belongs to a discrete set with two possible outcomes.
Example:
This binary classifier will predict whether or not a person is likely to default on their credit card.
Related terms: output type, classification, multilabel classification
Categorical Attribute
An attribute whose value is taken from a discrete set of possibilities.
Example:
A person's blood type is a categorical attribute: it can only be A, B, AB, or O.
Synonyms: discrete attribute
Related terms: attribute, continuous, classification
Continuous Attribute
An attribute whose value is taken from an ordered continuum can be bounded or unbounded.
Example:
A person's height, weight, income, IQ can all be through of as continuous attributes.
Synonyms: numeric attribute
Related terms:
attribute, continuous, regression
Classification
A modeling task where the target variable belongs to a discrete set with a fixed number of possible outcomes.
Example:
This classification model will determine whether an input image is of a cat, a dog, or fish.
Related terms: output type, binary classification, multilabel classification
Data Drift
Refers to the problem arising when, after a trained model is deployed, changes in the external world lead to degradation of model performance and the model becoming stale.
Detecting data drift will provide a le ing indicator of data stability and integrity.
Data drift can be quantified with respect to a specific reference set (e.g., the model's training data) or, more generally, the model's temporal shifts in a variable with respect to past time windows.
Your project can {ref}query data drift metrics through the Arthur API <data_drift>. This section will provide an overview o
the available data drift metrics in Arthur's query service.
Related terms: ou Arthur'stribution
Multivariate
Arthur also offers a multivariate Anomaly Score, which you can configure via the steps detailed in Enabling Enrichments.
See Anomaly Detection for an explanation of how these scores are used and Arthur Algorithms for how they're calculated.
Disparate Impact
Legal terminology originally from Fair Lending case law. This constraint is strictly harder than Dispara e Treatment and asserts that model outcomes must not be discriminatory across protected groups. That is, the outcome of a decision process should not be substantially higher (or lower) for one group of a protected class over another.
While there does not exist a single threshold for establishing the presence or absence of disparate impact, the so-called ""80% rule"" is commonly referenced. However, harm certain subgroups of a population differentially r, we strongly recommend against adopting this rule-of-thumb, as these analyses should be grounded in use-case-specific analysis and the legal framework pertinent to a given industry.
Example:
Even though the model didn't take gender as input, it still results in disparate impact when we compare outcomes for males and females.
Related terms: bias, disparate treatment
Disparate Treatment
Legal terminology originally from Fair Lending case law. Disparate Treatment asserts that you are not allowed to consider protected variables (e.g., race, age, gender) when approving or denying a credit card loan application.
In practical terms, a data scientist cannot include these attributes as inputs to a credit decision model.
Adherence to Disparate Treatment is not a sufficient condition for actually achieving a fair model (see proxy and bias detedefinitionstion). ""Fairness through ""unawareness"" is not good enough.
Related terms: bias, disparate impact
Enrichment
Generally used to describe data or metrics added to raw data after ingestion. Arthur provides various enrichments such as Anomaly Detection and Explainability. See entity enrichments for details about using enrichments within Arthur.
Feature
An individual attribute that is an input to a model
Example:
The credit scoring model has features like “home_value”, “zip_code”, “height"".
Ground Truth
The true label or target variable (Y) corresponds to inputs (X) for a dataset.
Examples:
pred = sklearn_model.predict_proba(X)
arthur_model.send_inference(
model_pipeline_input=X,
predicted_values={1:pred, 0: 1-pred})
Related terms: prediction
Image Data
Imagery data is commonly used for computer vision models.
Related terms: attribute, output type, Stage
Inference
One row of a dataset. Inference refers to passing a single input into a model and the model's prediction. Data associated with that inference might include (1) the input to the model, (2) the model's prediction and (3) the corresponding ground truth.
With respect to the Arthur platform, the term inference denotes any and all of those related components of data for a single input&prediction.
Related terms: ArthurInference, stage
Input
A single data instance upon which a model can calculate an output prediction. The input consists of all relevant features together.
Example:
The input features for the credit scoring model consist of “home_value”, “zip_code”, “height"".
Related terms: feature, model
Input Type
For an ArthurModel, this field declares what kind of input datatype will be flowing into the system.
Allowable values are defined in the InputType enum:
Tabular
Image
NLP
Example:
Pythonarthur_model = connection.model(name=""New_Model"",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
Related terms: output type, tabular data, nlp data
Model Health Score
On the UI dashboard, you will see a model health score between 0-100 for each of your models. This
score averages over a 30-day window of the following normalized metrics: performance, drift, and ingestion.
Performance:
Regression: 1 - Normalized MAE
Classification: F1 Score
Drift
1 - Average Anomaly Score
Ingestion
The variance of normalized periods between ingestion events
The variance of normalized volume differences between ingestion events
You can also extract the health score via an API call.
Model Onboarding
Model onboarding refers to the process of defining an ArthurModel, preparing it with the necessary reference dataset, passing it through a validation check, and saving it to the Arthur system.
Once your model is onboarded onto Arthur, you can use the Arthur system to track the model and view all its performance and analytics in your online Arthur dashboard.
Related terms: ArthurModel, reference dataset
Multiclass Classification
A modeling task where each input is associated with one label from a fixed set of possible labels.
Often this is a binary classifier (the output is either 0 or 1), but the output can also have more than 2 possible labels.
Example:
This NLP model applies the most relevant tag to news articles. The model is trained on example articles which are tagged with a topic like Congress.
Related terms: multilabel clasification, output type,
Multilabel Classification
A modeling task where each input is associated with two or more labels from a fixed set of possible labels.
Example:
This NLP model applies relevant tags to news articles. The model is trained on example articles which are tagged with multiple topics like Politics, Elections, Congress.
Related terms: output type, multiclass clasification
NLP Data
Unstructured text sequences are commonly used for Natural Language Processing models.
Related terms: attribute, output type, Stage
Non-Input Attribute
A non-input attribute is an attribute that an ArthurModel will track that does not actually enter the model as an input.
Common non-input attributes are protected class attributes such as age, race, or sex. By The model ending such non-input attributes to Arthur, you can track model performance based on these groups in your data to evaluate model bias and fairness.
Related terms: attribute, bias
Object Detection
The OutputType is for computer vision models to detect an object within an image and output a box that bounds the object.
This bounding box is used to identify where the object resides in the image.
Related terms: image
Out of Distribution Detection
Refers to the challenge of detecting when an input (or set of inputs) is substantially different from the distribution of a larger set of reference inferences. This term commonly arises in data drift, where we want to detect if new inputs differ from the training data (and distribution thereof) for a particular model. OOD Detection is a relevant challenge for Tabular data and unstructured data such as images and sequences.
Related terms: {ref}glossary_data_drift
Output Type
For an ArthurModel, this field declares what kind of output predictions will flow out of the system.
Allowable values are defined in the OutputType enum:
Regression
appropriate for continuous-valued targets
Multiclass
appropriate for both binary classifiers and multiclass classifiers
Multilabel
appropriate for multilabel classifiers
ObjectDetection
only available for computer vision models
Example:
Pythonarthur_model = connection.model(name=""New_Model"",
input_type=InputType.Tabular,
output_type=OutputType.Regression)
Related terms: input type
Prediction
The output prediction (y_hat) of a trained model for any input.
Related terms: ground truth
Protected Attribute
An attribute of an inference that is considered sensitive with respect to model bias. Common examples include race, age, and gender. The term ""protected"" comes from the Civil Rights Act of 1964.
Synonyms: sensitive attribute
Related terms: bias, proxy
Proxy
An input attribute in a model (or combination thereof) is highly correlated with a protected attribute such as race, age, or gender. The presence of proxies in a dataset makes it difficult to rely only on [Disparate Treatment] as a standard for fair ML.
Example:
In most US cities, zip code is a strong proxy for race. Therefore, one must be cautious when using zip code as an input to a model.
Related terms: bias, disparate impact, disparate treatment
Reference
The dataset is used as a baseline reference for an Arthur model.
A reference dataset must include a sample of the input features a model receives.
A reference dataset can optionally include a sample of model outputs, ground truth values, and other non-input attributes as metadata.
The reference dataset for a model is used to compute drift: the distribution of input features in the reference dataset makes up the baseline against which future inferences are compared to compute anomaly scores.
Related terms: inference
Regression
A modeling task (or model) where the target variable is a continuous variable.
Example:
This regression model predicts what the stock price of $APPL will be tomorrow.
Related terms: output type
Sensitive Attribute
See protected attribute
Stage
The Arthur platform uses taxonomy to delineate how attributes contribute to the model computations.
Allowable values are defined in the Stage enum:
ModelPipelineInput: Input to the entire model pipeline. This will most commonly be the Stage used to represent all model inputs. Will contain base input features familiar to the data scientist: categorical and continuous columns of a tabular dataset.
PredictFunctionInput: Potential alternative input source, representing direct input into the model's predi t() method. Therefore, data in the specific models have already undergone all relevant transformations, including scaling, one-hot encoding, or embedding.
PredictedValue: The predictions coming out of the model.
GroundTruth: The ground truth (or target) attribute or a model. Must be one-hot for classifiers
GroundTruthClass: The ground truth class for classification models, not one-hot encoded
NonInput: Ancillary data that can be associated with each inference but not necessarily a direct input t
the model. For example, sensitive attributes like age, sex, or race might not be direct model inputs, but will be useful to associate with each prediction.
Tabular Data
The data type for model inputs where the data can be thought of as a table (or spreadsheet) composed o
rows and columns. Each column represents an input attribute for the model, and each row represents a separate record that composes the training data. In supervised learning, exactly one of the columns acts as the target.
Example:
This credit scoring model is trained on tabular data. The input attributes are income, country, and age and the target is FICO score.
Related terms: Attribute, output type, Stage
Token Likelihood
The token likelihood is a number between 0 and 1 that quantifies the model’s level of surprise that this token was the next predicted token of the sentence. If a token has a low likelihood (close to 0), the model is more unsure about selecting this token. While a likelihood close to 1 indicates that the model is very confident in predicting this token.
Version Label
A Version Label is a string representing a custom version of your Arthur Model within its A thur Model Group. Version Labels are not required, and the platform will default to using the Version Sequence Number when not provided.
Example:
Python# retrieve the model group
model_group = connection.get_model_group(model_group_id)
# create the new version of the model
arthur_model_v2 = connection.model(name=""Model_V2"",
input_type=InputType.Tabular,
model_type=OutputType.Regression)
# add the new model to the model group
model_group.add_version(arthur_model_v2, label=""2.0.1"")
label = arthur_model_v2.version_label
arthur_model_v2.save()
# label == ""2.0.1""
Related terms: Arthur Model, Arthur Model Group, Version Sequence Number
Version Sequence Number
A Version Sequence Number is a unique, auto-incrementing (starting at 1) integer assigned to Arthur Models in an A thur Model Group. This number uniquely represents an Arthur Model’s Version with the Model Group. If a Version Label is not provided, the platform will show the Version Sequence Number instead.
Example:
Python# retrieve the first version of a model
arthur_model_v1 = connection.get(model_id)
num = arthur_model_v1.version_sequence_num
# num == 1
# retrieve the second version of a model
model_group = arthur_model_v1.model_group
arthur_model_v2 = model_group.get_version(sequence_num=2)
num = arthur_model_v2.version_sequence_num
# num == 2
Related terms: Arthur Model, Arthur Model Group, Version LabelUpdated 3 months ago Table of Contents
Arthur Inference
Arthur Model
Arthur Model Group
Attribute
Bias
Bias Detection
Demographic Parity
Equal Opportunity
Equalized Odds
Bias Mitigation
Binary Classification
Categorical Attribute
Continuous Attribute
Classification
Data Drift
Multivariate
Disparate Impact
Disparate Treatment
Enrichment
Feature
Ground Truth
Image Data
Inference
Input
Input Type
Model Health Score
Model Onboarding
Multiclass Classification
Multilabel Classification
NLP Data
Non-Input Attribute
Object Detection
Out of Distribution Detection
Output Type
Prediction
Protected Attribute
Proxy
Reference
Regression
Sensitive Attribute
Stage
Tabular Data
Token Likelihood
Version Label
Version Sequence Number",arthur_scope_docs,https://docs.arthur.ai/docs/glossary
96,"Langchain
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by LangchainSuggest EditsThis guide walks through how to use the ArthurCallbackHandler, an integration that allows you to send LLM inferences to the Arthur platform through LangChain.
Register your ArthurModel
You can skip this step if your generative text model is already registered with Arthur.
If you do not have a model currently onboarded to Arthur, follow the steps on our Generative Text Onboarding Guide
You will need your model's ID registered to the platform before you create an ArthurCallbackHandler with your LangChain LLM.
Create your LangChain LLM with the ArthurCallbackHandler
First, get your Arthur login info and the ID of your registered ArthurModel:
Pythonarthur_url = ""<https://app.arthur.ai"">
arthur_login = ""your-arthur-login-username-here""
arthur_model_id = ""your-arthur-model-id-here""
Next, we create a LangChain ChatOpenAI LLM with your Arthur credential info passed to the ArthurCallbackHandler
Note that we are also configuring the LLM with the useful StreamingStdOutCallbackHandler from LangChain, which returns responses as a token-by-token stream instead of returning the entire result at once - this typically makes for a better UX for development & testing.
Pythonfrom langchain.callbacks import ArthurCallbackHandler
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chat_models import ChatOpenAI
chatgpt = ChatOpenAI(
streaming=True,
temperature=0.1,
callbacks=[
StreamingStdOutCallbackHandler(),
ArthurCallbackHandler.from_credentials(
arthur_model_id,
arthur_url=arthur_url,
arthur_login=arthur_login)
])
You can now use the LLM in your LangChain application, with its input text, output text, and other monitored attributes recorded to the Arthur platform for each inference.
Note that the attributes from each LLM response will only be saved to Arthur if you have registered those attributes with your ArthurModel. For more information on registered generative text models with additional attributes besides input text & output text, visit the Generative Text Onboarding Guide.
Run the model to log inferences to the Arthur platform
Here we define a run function that executes a loop for a chat between a user and an LLM until the user types q to quit
Note that this function is not required to use the ArthurCallbackHandler - it is just meant as a quick demonstration of how to use a LangChain LLM
Pythonfrom langchain.schema import HumanMessage
def run(llm):
history = []
while True:
user_input = input(""\n>>> input >>>\n>>>: "")
if user_input == ""q"":
break
history.append(HumanMessage(content=user_input))
history.append(llm(history))
Each subsequent user <> LLM back-and-forth response will be logged as its own inference in the Arthur platform.
For example, here is an execution of this run function with the ChatGPT LLM.
Pythonrun(chatgpt)
Textinput >>>
> > > : What is a callback handler?
> > > A callback handler, also known as a callback function or callback method, is a piece of code that is executed in response to a specific event or condition. It is commonly used in programming languages that support event-driven or asynchronous programming paradigms.
The purpose of a callback handler is to provide a way for developers to define custom behavior that should be executed when a certain event occurs. Instead of waiting for a result or blocking the execution, the program registers a callback function and continues with other tasks. When the event is triggered, the callback function is invoked, allowing the program to respond accordingly.
Callback handlers are commonly used in various scenarios, such as handling user input, responding to network requests, processing asynchronous operations, and implementing event-driven architectures. They provide a flexible and modular way to handle events and decouple different components of a system.
input >>>
> > > : What do I need to do to get the full benefits of this
> > > To get the full benefits of using a callback handler, you should consider the following:
1. Understand the event or condition: Identify the specific event or condition that you want to respond to with a callback handler. This could be user input, network requests, or any other asynchronous operation.
2. Define the callback function: Create a function that will be executed when the event or condition occurs. This function should contain the desired behavior or actions you want to take in response to the event.
3. Register the callback function: Depending on the programming language or framework you are using, you may need to register or attach the callback function to the appropriate event or condition. This ensures that the callback function is invoked when the event occurs.
4. Handle the callback: Implement the necessary logic within the callback function to handle the event or condition. This could involve updating the user interface, processing data, making further requests, or triggering other actions.
5. Consider error handling: It's important to handle any potential errors or exceptions that may occur within the callback function. This ensures that your program can gracefully handle unexpected situations and prevent crashes or undesired behavior.
6. Maintain code readability and modularity: As your codebase grows, it's crucial to keep your callback handlers organized and maintainable. Consider using design patterns or architectural principles to structure your code in a modular and scalable way.
By following these steps, you can leverage the benefits of callback handlers, such as asynchronous and event-driven programming, improved responsiveness, and modular code design.
input >>>
> > > : q
Updated 3 months ago Table of Contents
Register your ArthurModel
Create your LangChain LLM with the ArthurCallbackHandler
Run the model to log inferences to the Arthur platform",arthur_scope_docs,https://docs.arthur.ai/docs/langchain
97,"Administration
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by AdministrationSuggest EditsBy default, the installer creates a new organization, ""My Organization,"" for convenience. You can also create new organizations using the API with the superadmin user. Full instructions for creating new users and organizations can be found Organizations and Users.
To access the UI for the default organization dashboard, visit thehttps://your_arthur_domain from your Web browser. Login with admin username and SuperSecret password. Make sure to change the password as soon as possible.
Refer to the Quickstart guide to start onboarding your models.
Admin Console
The Admin Console can be made available via the ingress controller on port 443 by creating a subdomain DNS record that starts with admin. (e.g., admin.arthur.mydomain.com). This eliminates the port 8800 egress requirement for VM installation on the firewall.
We recommend that you rotate your Admin Console password often. You can reset the password using this command:
Shellkubectl kots reset-password -n <namespace>
Update Admin password for Embedded Postgres
The 'Postgres' admin user manages the embedded Postgres database. If you would like to update the password for this admin user, you can execute the following commands on the primary database pod:
Shellkubectl exec -it database-master-0 -- psql -U postgres
Password for user postgres: <type_current_secret>
psql (11.13)
Type ""help"" for help.
postgres=# ALTER ROLE postgres WITH PASSWORD '<insert_new_secret>';
postgres=# \q
$
Updated 3 months ago Table of Contents
Admin Console
Update Admin password for Embedded Postgres",arthur_scope_docs,https://docs.arthur.ai/docs/administration
98,"Regression
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by RegressionSuggest EditsRegression models predict a numeric outcome. In Arthur, these models are listed under the regression model type.
Some common examples of Text regression are:
What is the predicted review score for written restaurant reviews?
Predict house price from description text
Formatted Data in Arthur
Tabular regression models require
two columns: text input and numeric output. When onboarding a reference dataset (and setting a model schema), you need to specify a target column for each inference's ground truth. Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (numeric or categorical)Attribute (numeric or categorical)Prediction (numeric)Ground Truth (numeric)Non-Input Attribute (numeric or categorical)45Graduate Degree45.3462.42Female22Bachelor's Degree55.153.2Male
Predict Function and Mapping
These are some examples of common values teams need to onboard for their regression models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics.
Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Single Column Ground Truth
output_mapping = {
'prediction_column':'gt_column'}
# Build Arthur Model with this technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping
)
## Example prediction function for binary classification
def predict(x):
return model.predict(x)
Available Metrics
When onboarding tabular regression models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a regression model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeRoot Mean Squared ErrorPerformanceMean Absolute ErrorPerformanceR SquaredPerformanceInference CountIngestionAverage PredictionIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/regression
99,"Release Notes
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchArthur Platform 3.12.0 23 days ago by ReadMe APIRelease notes for Arthur Platform 3.12.0Arthur Platform 3.11.0 about 2 months ago by ReadMe APIRelease notes for Arthur Platform 3.11.0Arthur Platform 3.10.0 about 2 months ago by ReadMe APIRelease notes for Arthur Platform 3.10.0Arthur Platform 3.9.0 4 months ago by ReadMe APIRelease notes for Arthur Platform 3.9.0Arthur Platform 3.8.0 5 months ago by ReadMe APIRelease notes for Arthur Platform 3.8.0Arthur Platform 3.7.0 7 months ago by ReadMe APIRelease notes for Arthur Platform 3.7.0addedMay 2023 10 months ago by Haley MassaNew FeaturesWelcome to Arthur about 1 year ago by Haley MassaWelcome to the developer hub and documentation for Arthur.",arthur_scope_docs,https://docs.arthur.ai/changelog
100,"Email
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by EmailSuggest EditsOne of the most popular alert notification techniques is through email. This notification technique can easily be set up in the organization or model-level UI.
Organization Level
They are typically used when teams want to manage organization-wide email alert notification access over multiple models quickly. Teams can go into the organization's setting page and configure which users will receive email alerts for each model that has alerts enabled.
Model Level
If you do not want to look at alerts at an organizational level, you can also set up email alerts per model within an individual model's Alerts tab. Here you can navigate to manage the alert rules of the model. Within this management center, you can select and edit the notification channels for this model. This is where emails are entered and saved for users that wish to receive alerts.
Resulting Alert
After configuring your alert email settings, teams are ready to receive alerts for these models to their designated emails.
An example of an email alert notification sent by Arthur
Configuring Email Alert Notifications on SSO
For users in SSO environments to subscribe other users up to email alerts notifications, configure your email domain whitelist through the KOTS admin console to be able to send email alert notifications.
In the KOTS Admin console, check “Show Other Advanced Options” navigate to the “Other Advanced Options” section
Under ""Other Advanced Options,"" set a comma-separated list of domains in the “Email Domain Whitelist” field.
An example of the Email Domain Whitelist within the Admin ConsoleUpdated 3 months ago Table of Contents
Organization Level
Model Level
Resulting Alert
Configuring Email Alert Notifications on SSO",arthur_scope_docs,https://docs.arthur.ai/docs/email
101,"Ranked List (Recommender Systems)
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Ranked List (Recommender Systems)Suggest EditsRanked List output data is typically used in recommender systems, which is a type of machine learning model that generates suggestions about “relevant” ranked items based on some input data. An example of a recommender system using ranked list data is a model that recommends relevant movies to a viewer based on metadata generated from their watch history.
Formatted Data in Arthur
Ranked List output models require the following data formatting:
JSON[
{ // first recommended item
""item_id"": ""item1"",
""score"": 0.324,
""label"": ""apple""
},
{ // second recommended item
""item_id"": ""item2"",
""score"": 0.024,
""label"": ""banana""
]
In this formatting, the score must be a float value, whereas the label and item_id must be string values. The label is an optional, readable version of item_id and score is an optional score/probability for a given item. If one of these optional metadata fields are specified in one inference, it must be specified for all of them.
Arthur expects the list of ranked items to be sorted in rank order, such that the highest ranked item is first. Each ranked list output model in Arthur can have max 1000 total unique recommended items in its reference dataset. Additionally, each ranked list output model can have max 100 recommendations per inference/ground truth.
Recommender Systems Ground Truth
The ground truth for ranked list output models is an array of strings representing the items that have been determined “relevant” for a given inference.
Available Metrics
When onboarding recommender system models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) when teams onboard a ranked list model. Find out more about these metrics in the Performance Metrics section.
MetricMetric TypePrecision at kPerformanceRecall at kPerformancenDCG at kPerformanceMean Reciprocal RankPerformanceRanked List AUCPerformanceInference CountIngestion
Drift Metrics
In the Arthur platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftTime Series DriftFeature DriftPrediction DriftPrediction Drift
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined data segments, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.Updated about 2 months ago Table of Contents
Formatted Data in Arthur
Available Metrics",arthur_scope_docs,https://docs.arthur.ai/docs/ranked-list-recommender-systems
102,"Image
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ImageSuggest EditsImage input models are a type of machine learning model that operates on image data, such as digital photographs, medical scans, and satellite imagery. These models are designed to recognize patterns and features within images and can perform tasks such as object detection, image segmentation, and image classification. Image input models can be built using a variety of techniques, including convolutional neural networks, which are specifically designed to work with image data.
Sending Images to Arthur
Arthur takes in the raw images as input. This means that instead of taking in images as matrixes of pixel values, we take the images in with [.gif , .jpeg , .png , .tiff] format. After sending these values to Arthur, they are stored in an AWS S3 bucket.Updated 3 months ago What’s NextBinary ClassificationTable of Contents
Sending Images to Arthur",arthur_scope_docs,https://docs.arthur.ai/docs/image
103,"Pre-Requisites
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Pre-RequisitesSuggest EditsThe following configurations should be applied to the platform to use Arthur's Backup and Restore capabilities:
Arthur must be configured to use external object storage, specifically, AWS S3
The access to external storage must be configured using IRSA annotations
In order to use IRSA annotations, the cluster must be deployed using Amazon EKS
If the above are not true/possible for your deployment, please reach out to Arthur Support.
Configuring Velero
The only component that needs to be installed separately from Arthur to perform backup and restores is Velero. Instructions are provided below for setting up Velero to store backups in S3 using IRSA.
The general overview of the installation is as follows:
Setup permissions for Velero
Install Velero
Confirm Velero is installed and configured correctly
Configure the Backup Storage Destination to point to S3
Velero permissions
Generate the below policy which will grant Velero the necessary permissions:
TextShell$ cat > velero-policy.json <<EOF
{
""Version"": ""2012-10-17"",
""Statement"": [
{
""Effect"": ""Allow"",
""Action"": [
""ec2:DescribeVolumes"",
""ec2:DescribeSnapshots"",
""ec2:CreateTags"",
""ec2:CreateVolume"",
""ec2:CreateSnapshot"",
""ec2:DeleteSnapshot""
],
""Resource"": ""*""
},
{
""Effect"": ""Allow"",
""Action"": [
""s3:GetObject"",
""s3:DeleteObject"",
""s3:PutObject"",
""s3:AbortMultipartUpload"",
""s3:ListMultipartUploadParts""
],
""Resource"": [
""arn:aws:s3:::${BUCKET}/*""
]
},
{
""Effect"": ""Allow"",
""Action"": [
""s3:ListBucket""
],
""Resource"": [
""arn:aws:s3:::${BUCKET}""
]
},
{
""Effect"": ""Allow"",
""Action"": [
""kms:CreateGrant*"",
""kms:ReEncrypt*"",
""kms:GenerateDataKey*"",
""kms:Encrypt*"",
""kms:DescribeKey*"",
""kms:Decrypt*""
],
""Resource"": ""*""
}
]
}
EOF
$ aws iam create-policy \
--policy-name velero-perms \
--policy-document file://velero-policy.json
Attach this IAM policy to the IAM role that the Arthur service account (IRSA) assumes
Shell$ aws iam attach-role-policy \
--role-name <IRSA-Role-name> \
--policy-arn <policy-ARN>
📘Encrypting data at restArthur highly recommends that your EBS volumes are encrypted with KMS.
In addition to giving Velero the permission for KMS, please make sure that the IAM roles assumed by Arthur service account also have access to KMS so the restored encrypted volumes can be re-attached.
If you're using separate KMS keys on the cluster you backed-up and the cluster you're restoring to, the EBS volume snapshots must be copied with the new KMS key so the new cluster can work with the snapshots.
Install Velero
Velero can be installed on the Kubernetes cluster using helm.
Generate a velero-values file as follows (taken from the official source with defaults removed for brevity):
YAML$ cat > velero-values.yaml <<EOF
resources:
requests:
cpu: 500m
memory: 128Mi
limits:
cpu: 1000m
memory: 512Mi
initContainers:
- name: velero-plugin-for-aws
image: velero/velero-plugin-for-aws:v1.6.1
imagePullPolicy: IfNotPresent
volumeMounts:
- mountPath: /target
name: plugins
podSecurityContext:
runAsNonRoot: true
runAsUser: 1000
runAsGroup: 1000
upgradeCRDs: true
cleanUpCRDs: false
configuration:
# Cloud provider being used (e.g. aws, azure, gcp).
provider: aws
backupStorageLocation:
# name is the name of the backup storage location where backups should be stored.
name: <insert-bsl-name-here>
provider: aws
# bucket is the name of the bucket to store backups in. Required.
bucket: <insert-s3-bucket-name-here>
config:
region: us-east-2
volumeSnapshotLocation:
# name is the name of the volume snapshot location where snapshots are being taken. Required.
name: <insert-vsl-name-here>
config:
region: us-east-2
# These are server-level settings passed as CLI flags to the `velero server` command.
logLevel: debug
namespace: <insert-velero-namespace-here>
rbac:
# Whether to create the Velero role and role binding to give all permissions to the namespace to Velero.
create: true
# Whether to create the cluster role binding to give administrator permissions to Velero
clusterAdministrator: true
# Name of the ClusterRole.
clusterAdministratorName: cluster-admin
# Information about the Kubernetes service account Velero uses.
serviceAccount:
server:
create: true
name: velero
annotations:
eks.amazonaws.com/sts-regional-endpoints: ""true""
eks.amazonaws.com/role-arn: <insert-IAM-Role-ARN-here>
labels:
credentials:
useSecret: false
backupsEnabled: true
snapshotsEnabled: true
deployNodeAgent: false
EOF
Install the Velero helm chart with the above values file:
Shell{note}$ velero_namespace=<insert-velero-namespace-here>
$ helm install velero vmware-tanzu/velero \
--create-namespace \
--namespace $velero_namespace \
--version 3.2.0 \
-f velero-values.yaml
Arthur recommends installing Velero in a different namespace from Arthur, so Velero can be managed separately from Arthur.
📘Where to install veleroArthur recommends installing Velero in a different namespace from Arthur, so Velero can be managed separately from Arthur.
Verify Velero Installation
To confirm that Velero is installed and configured correctly:
Open the Kots Admin Interface and navigate to the ""Snapshots"" tab
Click the ""Check for Velero"" button (see the screenshot below)
Validate the Backup Storage Location
The Backup Storage Location is a Velero resource that points to the S3 Bucket where backups will be stored. Use kubectl to validate connectivity/access to AWS S3, which should say ""Available"".
Shell$ velero_namespace=<insert-velero-namespace-here>
$ kubectl get backupstoragelocation -n $velero_namespace
Please do not proceed until the Backup Storage Location is Available.
Configuring clickhouse-backup
Configuring clickhouse-backup to store backups in remote storage (e.g., S3) can be done in the Admin Console.
Once your cluster is set up for Backup and Restore, you should see the ""Enable OLAP Database Backup Capabilities"" option in the ""OLAP Database"" section.
Ensure that:
The configuration that points to the bucket is correct
The Bucket Name
The Bucket Region
The ServiceAccount is the same ServiceAccount that you've configured with the IRSA annotation (if you are not sure, enter the default value)
The IAM Role that you are using for the IRSA annotation has the appropriate permissions to read/write/list from the S3 bucket
The S3 Path is where you want to be storing backups
Updated 3 months ago Table of Contents
Configuring Velero
Velero permissions
Install Velero
Verify Velero Installation
Validate the Backup Storage Location
Configuring clickhouse-backup",arthur_scope_docs,https://docs.arthur.ai/docs/pre-requisites
104,"Performance Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Performance MetricsMonitor and communicate model performanceSuggest EditsWhether a performance alert has been triggered requiring the ML team's attention or an external stakeholder has requested a performance breakdown for a specific segment, Arthur provides a single location to report on and explore model performance across an organization.
Performance Metrics in Arthur
Out-of-the-Box
It can be overwhelming for external stakeholders to balance too many technical definitions for model accuracy, primarily if each team in an organization provides different baseline metrics. Onboarded model schema automatically infers performance metrics of interest for your models, easily standardizing top performance metrics across the organization.
Each model types out-of-the-box performance metrics are defined for them in their Model Types section.
User-Defined Metrics
Different stakeholders define performance differently and accommodate all measures of performance utilizing our ability to curate performance metrics by data slices and user-defined custom metrics.
Performance Metric UI Guide
Performance metrics for specific models can be found within that model's Overview tab.
Time Display
The default option for seeing performance charts in the UI is in time series mode. These charts provide an average of each metric overtime at the time interval specified at the top of the chart.
Snapshot Mode
For teams that are not interested in viewing their metrics over time, they can select Snapshot mode in the top corner.
This mode creates bar graphs of the average metric over the specified range from the global filters (discussed more below).
Segment Sections of Interest
As referenced above, users can also segment data of interest.
Global Filters
Global Filters are available in the top corner of the UI. Teams can apply time or attribute-based filter rules to generate representative charts for those groups.
Note: Global Filter Are Applied Globally. Global filters do not apply just to the Performance graph or Overview Tab. These filters are applied globally across the UI. This means that they can be used to explore the inferences in the inference deep dive for example. However, it is important to remember that the filters are applied before navigating to other tabs.
Require Ground Truth
One key thing to keep in mind is that many performance metrics require ground truth (or labels). Teams that have a lag between prediction and ground truth should look into Drift and Anomaly metrics within Arthur.Updated 3 months ago What’s NextSet alerts on your performance metrics, or learn more about different metrics within Arthur.AlertingDrift and AnomalyFairness MetricsUser Defined MetricsTable of Contents
Performance Metrics in Arthur
Out-of-the-Box
User-Defined Metrics
Performance Metric UI Guide
Time Display
Segment Sections of Interest
Global Filters
Require Ground Truth",arthur_scope_docs,https://docs.arthur.ai/docs/performance-metrics
105,"Versioning
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by VersioningSuggest EditsVersioning is crucial for production systems in machine learning as it allows for the following:
seamless updates and rollbacks of machine learning models
enables performance monitoring and A/B testing, facilitating data-driven decisions for model improvements.
and ensures reproducibility and auditability of deployed models, meeting compliance requirements and providing transparency in machine learning systems.
Model Groups == Model Tasks
We can think of versions relating to a particular model task (or use case). For example, a “Real Time Transaction Fraud Model” will be a years-long project with much evolution. Many small and large changes may occur over the years, from retraining with new data to building completely new architectures.
When you represent these new versions in Arthur Scope, they go into a
model group. Within Arthur, a model group refers to a group of versions of the models that are used for the same model task.
Creating a New Model Version with the Python SDK
New model versions within Arthur are completely new ArthurModel Objects. This means that building out a new model version consists of building a new ArthurModel object and linking it to your existing ArthurModel object through its Arthur model group. That may seem a bit confusing, so we'll break the steps down below:
Step 0: Have an Initial Model on Arthur
To put the first version of your model onto Arthur, you do not need to specify its version. You just need to follow the Creating Arthur Model Object section of the documentation.
Get the Model Group for ArthurModel you want to version
Once you have been running that version of your model on Arthur for a while, you may decide that it is time to onboard a new model version. To ensure that this new version is connected with the old version, you will use the model_group_id.
Here is a code example of how to grab the model_group_id from a running ArthurModel object.
Python# get the model group for v1 (we will save v2 in the same model group)
model_group = connection.get_model_group(arthur_model_v1)
Build a new ArthurModel Object for this version
Build a new Arthur model object for this version, following the same steps as Creating Arthur Model Object. Just make sure that you do not save the model object until you do the next step.
Python# Register V2 of the model with Arthur
arthur_model_v2 = connection.model(display_name=""Magic_Model"", # The model name can be the same or different
input_type=InputType.Tabular,
output_type=OutputType.Multiclass)
# creating mapping from predictions to ground truth
pred_to_ground_truth_map = {}
for i, name in enumerate(pred_df_2.columns):
pred_to_ground_truth_map[name]= i
arthur_model_v2.build(ref_df_2,
ground_truth_column='label',
pred_to_ground_truth_map=pred_to_ground_truth_map)
arthur_model_v2.get_attribute('label').set(categorical=True, categories=list(ref_df_2['label'].unique()))
arthur_model_v2.review()
Save your new version
The connection between your old ArthurModel object and the new ArthurModel object happens when you save the model. Here we can see how we are assigning our new model object the same model_group_id as our original ArthurModel object. We can also provide a version_label which will represent what this new version is in the platform.
When we finish linking all that, we can save the model to the platform.
Python# Add v2 of the model to the same model group as v1
arthur_model_v2.model_group_id = model_group.id
# Assign a version label to v2 of the Model
arthur_model_v2.version_label = ""V2""
# Save v2 of the model to Arthur
model_id = arthur_model_v2.save()
Update Inference Sending Techniques
With all that done, your new ArthurModel object is now on the Arthur platform. However, you are not done just yet. You need to ensure that any system you set up for sending inferences to your old model (or want to set up for sending inferences to your new model) is implemented to begin effectively monitoring.
To do this, teams should follow the techniques listed in Sending Inferences. Know that for model versioning, teams often follow a few different patterns:
Replacing Old Inference Connection to New Model ID: If you are switching over completely to this new model version, replacing the old ArthurModel ID connection with this new version's model id can be the easiest.
Setting Up Validation in Production Pipelines: Many teams use versioning before completely switching which model is being served. Teams may run different types of validation tests in production (such as A/B testing, shadow deployments, or canary tests). These are all possible within Arthur, you will just need to create different ArthurModel objects for each test and set up inference sending to the platform for them to be effective.
Updated 3 months ago Table of Contents
Model Groups == Model Tasks
Creating a New Model Version with the Python SDK
Step 0: Have an Initial Model on Arthur
Get the Model Group for ArthurModel you want to version
Build a new ArthurModel Object for this version
Save your new version
Update Inference Sending Techniques",arthur_scope_docs,https://docs.arthur.ai/docs/versioning
106,"Troubleshooting Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Troubleshooting ExplainabilitySuggest EditsTroubleshooting
AttributeError When Loading Predict Function
While this can be an issue with any model type, it is common to see when using sk-learn objects that take in custom user functions.
We will use TfidfVectorizer as an example, which is a commonly used vectorizer for NLP models, that often utilizes custom user functions.
A TfidfVectorizer accepts a user defined tokenize function, which is used to split a text string into tokens.
Problem
Say this code was used to create your model.
Python# make_model.py
def tokenize(text):
# tokenize and lemmatize
doc = nlp(txt)
tokens = []
for token in doc:
if not token.is_stop and not token.is_punct \
and not token.is_space and token.lemma_ != '-PRON-':
tokens.append(token.lemma_)
return tokens
def make_model():
# here we pass a custom function to an sklearn object
vectorizer = TfidfVectorizer(tokenizer=tokenize)
vectorizer.fit(X_train)
model = LogisticRegression()
model.fit(vectorizer.transform(X_train))
pipeline = make_pipeline(vectorizer, model)
joblib.dump(pipeline, 'model.pkl')
if __name__ == ""__main__"":
make_model()
Now you create this entrypoint file to enable explainability:
Python# entrypoint.py
model = joblib.load(""./model.pkl"")
def predict(fv):
return model.predict_proba(fv)
Now when the SDK imports entrypoint to test the function, the following error gets thrown:
AttributeError: module '__main__' has no attribute 'tokenize'
What happens is that Python failed to serialize the custom function, only the reference to how it was imported. Which in this case, it was just top level in the model creation script (hence __main__.tokenize in the error).
This function doesn't exist in entrypoint, and so the error is thrown.
Solution
To solve, you need to pull out tokenize into its own module, that can be imported from both create_model.py
and also in entrypoint.py.
Python# model_utils.py
def tokenize(text):
# tokenize and lemmatize
doc = nlp(txt)
tokens = []
for token in doc:
if not token.is_stop and not token.is_punct \
and not token.is_space and token.lemma_ != '-PRON-':
tokens.append(token.lemma_)
return tokens
Python# create_model.py
from model_utils import tokenize
def make_model():
# here we pass a custom function to an sklearn object
vectorizer = TfidfVectorizer(tokenizer=tokenize)
vectorizer.fit(X_train)
model = LogisticRegression()
model.fit(vectorizer.transform(X_train))
pipeline = make_pipeline(vectorizer, model)
joblib.dump(pipeline, 'model.pkl')
if __name__ == ""__main__"":
make_model()
Python# entrypoint.py
from model_utils import tokenize
model = joblib.load(""./model.pkl"")
def predict(fv):
return model.predict_proba(fv)
Now, when Python serializes the model, it stores the reference as model_utils.tokenize, which is also imported within entrypoint.py and therefore no error is thrown.
Now everything will work, but both model_utils.py AND entrypoint.py must be included in the directory passed to enable_explainability().Updated 3 months ago Table of Contents
Troubleshooting
AttributeError When Loading Predict Function",arthur_scope_docs,https://docs.arthur.ai/docs/troubleshooting-explainability
107,"Registering A Model with the API
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Registering A Model with the APIThis page describes the process of registering models through standard REST API calls.Suggest EditsIntroduction
As an API-first solution, Arthur Scope allows the entire model onboarding process to be fully automated, either through our SDK, or through standard REST API calls. This allows Arthur to be integrated with basically any ML platform, workflow management or automation software. This is possible because every step of the onboarding process can be achieved through API calls.
In this page, we will describe the process of onboarding a model using standard API calls. We will also be providing hints about how the SDK can be leveraged to help with some of these steps, for python-based automation environments.
The main steps required to onboard a model are:
Create the model schema definition and set basic metadata
Save the model
Upload reference dataset for model
Manage model Enrichments (Bias monitoring, Explainability, Hotspots, etc)
Create model Alert Rules (optional)
Create the model schema definition and basic metadata
To onboard a model, Arthur Scope needs information about the attributes (input, non-input, prediction and ground truth), as well as some basic metadata about the model.
When onboarding a model through the API, this information is sent in JSON format, as in the example below:
json{
""display_name"": ""Credit Risk"",
""partner_model_id"": ""CreditRiskModel_FG_20230523115857"",
""description"": ""Credit Risk Model Created Through REST API"",
""input_type"": ""TABULAR"",
""output_type"": ""MULTICLASS"",
""attributes"": [
{
""name"": ""LIMIT_BAL"",
""value_type"": ""INTEGER"",
""stage"": ""PIPELINE_INPUT"",
""position"": 0,
""categorical"": false,
""min_range"": 10000,
""max_range"": 1000000,
""monitor_for_bias"": false,
""is_unique"": true,
""is_positive_predicted_attribute"": false
},
...
],
""tags"": [""Tabular"", ""Classification"", ""Credit""],
""is_batch"": true,
""version_label"": ""Credit Risk v1""
}
PS: most of the attributes were removed from this code block, for readability.
Please refer to the documentation for more details on which metadata parameters are required. Do keep in mind that the partner_model_id attribute must be absolutely unique, even across different versions of the model, or for models that were deleted and re-created. The recommendation is to add a timestamp to the ID, in order to avoid potential duplication.
When preparing the model schema, the bulk of the work will go to the task of preparing the list of attributes. As we can see from the example above, the model attributes are sent as a list inside the JSON payload. This list must include all input, non-input (optionally), prediction, and ground truth attributes. Attributes cannot be added once the model is saved, so this list must be complete at model saving time.
Every attribute has several parameters that can be set. This is a more complete list of parameters from the API documentation:
JSON{
""name"": ""string"",
""label"": ""string"",
""value_type"": ""BOOLEAN"",
""stage"": ""GROUND_TRUTH"",
""position"": 0,
""categorical"": false,
""categories"": [],
""min_range"": 0,
""max_range"": 0,
""monitor_for_bias"": false,
""bins"": [],
""is_unique"": false,
""is_positive_predicted_attribute"": false,
""attribute_link"": ""string"",
""gt_class_link"": ""string"",
""implicit"": false
}
Do note that some of these parameters only apply to certain types of attributes. A brief description of each attribute can be found in the table below:
AttributeDescriptionnameName of the attribute. Must be unique within the model.labelA friendly label that can be set for attribute names that are encoded.value_typeType of data managed by the attribute (integer, float, etc). A list of supported value types can be found here.stageDetermines whether this attribute is input, non-input, prediction or ground truth.positionThis is an incremental counter that should start with 0 and increase by 1 for each attribute (with the exception of prediction and ground truth attributes, that must start at zero).categoricalSet it to true if the attribute has a limited number of potential values.categoriesList all the categories (potential values) for the attribute. Only relevant if categorical is set to true.min_rangeFor non-categorical, numerical attributes. Lowest numerical value this attribute should receive (PS: this will not be enforced as a threshold limit).max_rangeFor non-categorical, numerical attributes. Highest numerical value this attribute should receive (PS: this will not be enforced as a threshold limit).monitor_for_biasTrue or False. Determines whether or not this attribute should be monitored for bias.binsFor non-categorical, numerical attributes that are being monitored for bias. Describes the bins (or buckets) that Arthur should use to group inferences as it checks for bias.is_uniqueUsed to determined if the values of the attribute are unique (PS: this will not be enforced through unique constraint validation).is_positive_predicted_attributeOnly used for the predicted attribute of a binary classification model (where there's a 'positive' and 'negative' prediction.attribute_linkUsed to associate prediction attributes to their corresponding ground truth attributes.gt_class_linkUsed for single ground truth class models, where the prediction attribute is associated with the corresponding string value of the ground truth class.
Next, let's examine some attributes to see how they can be configured.
JSON{
""name"": ""LIMIT_BAL"",
""value_type"": ""INTEGER"",
""stage"": ""PIPELINE_INPUT"",
""position"": 0,
""categorical"": false,
""min_range"": 10000,
""max_range"": 1000000,
""monitor_for_bias"": false,
""is_unique"": false,
""is_positive_predicted_attribute"": false
},
This is an integer input attribute, non-categorical, that can range between 10,000 and 1,000,000. It will not be monitored for bias.
JSON{
""name"": ""AGE"",
""value_type"": ""INTEGER"",
""stage"": ""PIPELINE_INPUT"",
""position"": 1,
""monitor_for_bias"": true,
""bins"" : [
{
""continuous_start"" : 0,
""continuous_end"" : 35
},
{
""continuous_start"" : 35,
""continuous_end"" : 55
},
{
""continuous_start"" : 55,
""continuous_end"" : 100
}
],
""categorical"": false,
""min_range"": 21,
""max_range"": 79,
""is_unique"": false,
""is_positive_predicted_attribute"": false
},
This is an integer input attribute, non-categorical, being monitored for bias. Because this attribute is non-categorical, we need to specify the bins for bias monitoring, so that Arthur knows how to group the inferences.
JSON{
""name"": ""SEX"",
""value_type"": ""INTEGER"",
""stage"": ""PIPELINE_INPUT"",
""position"": 2,
""monitor_for_bias"": true,
""categorical"": true,
""categories"": [
{
""value"": ""1"",
""label"": ""Male""
},
{
""value"": ""2"",
""label"": ""Female""
}
],
""is_unique"": false,
""is_positive_predicted_attribute"": false
},
This is an integer input, categorical attribute. It will be monitored for bias; in this case, Arthur will used the defined categories to group inferences for bias monitoring.
JSON{
""name"": ""UNIT_CODE"",
""value_type"": ""STRING"",
""stage"": ""PIPELINE_INPUT"",
""position"": 3,
""categorical"": true,
""monitor_for_bias"": false,
""categories"": [
{ ""value"": ""DW00"" }, { ""value"": ""A800"" }, { ""value"": ""YZ00"" }, { ""value"": ""M 00"" }, { ""value"": ""H 00""},
{ ""value"": ""H100"" }, { ""value"": ""N 00"" }, { ""value"": ""T800"" }, { ""value"": ""A 00"" }, { ""value"": ""GX00""},
{ ""value"": ""RG00"" }, { ""value"": ""JL00"" }, { ""value"": ""TC00"" }, { ""value"": ""R 00"" }, { ""value"": ""LV00""},
{ ""value"": ""E 00"" }, { ""value"": ""T 00"" }
],
""is_unique"": false,
""is_positive_predicted_attribute"": false
},
This is a string input. All string input attributes should be categorical.
JSON{
""name"": ""RISK_AMT"",
""value_type"": ""FLOAT"",
""stage"": ""NON_INPUT_DATA"",
""position"": 4,
""categorical"": false,
""min_range"": 10152.23,
""max_range"": 999990.0,
""monitor_for_bias"": false,
""is_unique"": false,
""is_positive_predicted_attribute"": false
},
This is a float, non-input attribute. It is non-categorical and it will not be monitored for bias.
JSON{
""name"": ""prediction_1"",
""value_type"": ""FLOAT"",
""stage"": ""PREDICTED_VALUE"",
""position"": 1,
""categorical"": false,
""min_range"": 0,
""max_range"": 1,
""monitor_for_bias"": false,
""is_unique"": false,
""is_positive_predicted_attribute"": true,
""attribute_link"": ""ground_truth_1""
},
{
""name"": ""ground_truth_1"",
""value_type"": ""INTEGER"",
""stage"": ""GROUND_TRUTH"",
""position"": 1,
""categorical"": true,
""categories"": [
{ ""value"": ""0"" },
{ ""value"": ""1"" }
],
""is_unique"": false,
""monitor_for_bias"": false,
""is_positive_predicted_attribute"": false,
""attribute_link"": ""prediction_1""
}
This is a pair of matching prediction and ground truth attributes. They must be explicitly correlated for Arthur to be able to calculate performance metrics. Regression models will have one prediction and one ground truth attributes, while multiclass models will have many. For every prediction attribute declared, a corresponding ground truth attribute must also be provided, even if this model is not expected to receive ground truth data at all.
The position of these elements should match as well. In this case, the prediction_1 is the 'positive' prediction made by the model, so we will mark it as such.
Arthur expects the prediction attribute to always be a float (the probability of this class), while the ground truth should be either an integer (0 or 1) or a String.
📘The Arthur SDK can be used to help with the process of mapping model attributes, especially for models with dozens or hundreds of attributes. The SDK provides a build() function that will create a dataframe with the model attributes, based on the reference data provided. This dataframe can then be converted to JSON and used on a direct REST API call. Consider leveraging the Arthur SDK to do the basic mapping of attributes, and then apply your specific logic to ensure the proper value types, ranges, categories, etc.
Save the Model
With the model schema and metadata in place, saving the model is a simple call to:
POST {{hostname}}/api/v3/models
Documentation: https://docs.arthur.ai/api-documentation/v3-api-docs.html#tag/models/paths/~1models/post
PS: this will require credentials with the Model Owner role, or a custom role allowed to create models.
If the call is successful, the return message will include the model ID. This model ID will be required for all subsequent calls, so make sure to save it to an environment variable.
A number of backend operations are executed at model saving time, including creating the database tables for the model, along with Kafka topics and other components. This process should only take a few seconds.
At this time, the model is saved and it has the necessary infrastructure to receive data. However, since no reference dataset is available, Drift and Anomaly Scores will not be calculated. Also, none of the other enrichments will have been enabled.
Upload Reference Dataset for Model
The reference dataset is required to calculate Drift metrics, as well as to train the Anomaly Score model that Arthur will create for each model being monitored.
When models are saved through the Arthur SDK, several tasks happen automatically behind the scenes: the SDK will save the reference data to a parquet or json file, and will upload it to Arthur once the model is done saving. Those steps must be executed explicitly when saving the model through direct API calls.
The reference data must contain all input, non-input, prediction, and ground truth attributes. It must also include the column headers:
It should also be representative of all expected values for the inferences; otherwise, the Drift scores might be misleading. For instance, if the reference dataset is only comprised of records for customers between 20 and 50 years old, any inference data about 80-year-old customers will receive a high drift score.
Setting the reference data is done in 2 steps:
Upload the reference data
Close the reference data
Upload the Reference Data
The parquet or json file can be uploaded to the following API endpoint:
POST {{hostname}}/api/v3/models/{{model_id}}/reference_data
The file should be attached as reference_data. This is a python example for that call:
payload = {}
files=[
('reference_data',('dataset.parquet',open('/Users/.../dataset.parquet','rb'),'application/octet-stream'))
]
headers = {
'Arthur-Organization-ID': '{{organization_id}}',
'Authorization': '{{access_token}}'
}
response = requests.request(""POST"", url, headers=headers, data=payload, files=files)
PS: The Arthur-Organization-ID header is required for environments with multiple organizations.
The return message will include the number of records that were successfully uploaded. Note this number, as you will need it for the next call.
Close the Reference Data
Arthur will wait until the reference data is closed before doing the backend processing (which includes training the anomaly score model and other tasks). The reference data can not be modified once it's closed, so ensure the proper data is in place before executing the next call.
Reference data can be closed through a PATCH call to the same endpoint as before:
PATCH {{hostname}}/api/v3/models/{{model_id}}/reference_data
The body must include the number of records uploaded, which must match the number of successful records uploaded in the previous step:
JSON{
""status"": ""uploaded"",
""total_record_count"": 30000
}
In this case, the reference data uploaded had 30,000 records.
Manage Model Enrichments
Arthur provides a set of standard Enrichments that are available for all model types. Other Enrichments will be specific to certain model types. In this section, we will review the available API endpoints for the different Enrichment capabilities.
Main Documentation page: https://docs.arthur.ai/api-documentation/v3-api-docs.html#tag/enrichments
🚧Keep in mind that most Enrichments will only be applied to new inferences. Make sure to have all Enrichments enabled before the model gets populated with data.
Retrieving a list of current Enrichments
The following API endpoint can be used to fetch a list of Enrichments configured for a model:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments
The return message (for a fully configured model) will look like this:
JSON{
""anomaly_detection"": {
""enabled"": true,
""config"": {}
},
""bias_mitigation"": {
""enabled"": false
},
""explainability"": {
""enabled"": true,
""config"": {
""sdk_version"": ""3.25.0"",
""python_version"": ""3.8"",
""explanation_algo"": ""shap"",
""model_server_cpu"": ""1000m"",
""model_server_memory"": ""1Gi"",
""explanation_nsamples"": 2000,
""shap_expected_values"": [
0.8227832963996526,
0.17721670360034752
],
""inference_consumer_cpu"": ""100m"",
""inference_consumer_memory"": ""256Mi"",
""model_server_max_replicas"": 2,
""inference_consumer_score_percent"": 1,
""streaming_explainability_enabled"": true,
""user_predict_function_import_path"": ""entrypoint"",
""inference_consumer_thread_pool_size"": 2
}
},
""hotspots"": {
""enabled"": true,
""config"": {}
}
}
In this example, we can see that this model has Anomaly Detection, Explainability and Hotspots enabled, while Bias Mitigation is currently disabled.
It is possible to update the configuration for all Enrichments with a single call. The URL would be the same as described above, only using the PATCH method instead of GET. The JSON block above would be sent as part of the form-multipart request. Do keep in mind that Explainability requires additional assets to be attached (this will be covered further down in this page).
Anomaly Detection
Anomaly Detection is automatically enabled for every model other than Time Series input models, once the Reference Data is uploaded (and closed). Because of this, it does not need to be explicitly enabled after the model is saved. It can be disabled and re-enabled at any time (at which point, the anomaly score model will be re-trained from the reference data).
This is the API endpoint to check the status of the Anomaly Detection Enrichment:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments/anomaly_detection
The return payload will look like this:
JSON{
""enabled"": true,
""config"": {}
}
To modify the status (enable/disable), the same endpoint can be used, with the PATCH method:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/anomaly_detection
The request body will contain the desired status for the Enrichment:
JSON{
""enabled"": false
}
In this case, Anomaly Detection is being disabled for this model.
Bias Mitigation
To check the status of Bias Mitigation, use this API endpoint:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments/bias_mitigation
The return payload will be similar to this:
JSON{
""enabled"": false
}
To update the status, use the same endpoint, with the PATCH method:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/bias_mitigation
The request body will contain the desired status for this Enrichment:
JSON{
""enabled"": false
}
Hotspots
Hotspots is a capability that aims to identify and surface regions of underperformance in the model. It is currently only available for Tabular - Classification models. Hotspots will not be automatically enabled once the model is saved. It can, however, be enabled at any time after that, and it does not require any additional data or assets.
To check the status of Hotspots, use this API endpoint:
GET {{hostname}}/api/v3/models/{{model_id}}/enrichments/hotspots
The return payload will be similar to this:
JSON{
""enabled"": true,
""config"": {}
}
To update the status, use the same endpoint, with the PATCH method:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/hotspots
The request body will contain the desired status for this Enrichment:
JSON{
""enabled"": false
}
Explainability
With Explainability enabled, Arthur is able to provide explanations for each inference. This data is also used for Global Explainability (feature importance, etc). This capability requires Arthur to be able to generate predictions on demand, which means Arthur needs a working model that can be called at any time.
Because of that, there are a number of assets required to enable Explainability, including the model assets, and a python function that can call the model's predict function and return the probability arrays.
Also, some Explainability assets need to be generated by the Arthur SDK, which makes the SDK a required part of the Explainability process.
🚧Arthur uses LIME or SHAP, which are industry accepted algorithms, to produce explanations. This means that the Explainability assets will be python-based. These algorithms will be packaged in an Explainer object, that must be created with the Arthur SDK. This Explainer is uploaded as part of the call to enable Explainability.
In order to enable Explainability for a model, the following assets and files are required:
user_project.zip file
This is a zip file that contains all required model assets, and an 'entrypoint' file
The model assets include anything required to create a prediction: .pkl files, tokenizers, etc. Arthur must be able to generate a prediction from the inference data received.
The entrypoint file is a python file that will run the predict() function. This is the appropriate place for any data manipulation that might be required for the input data: transformations, scaling, one-hot encoding, etc. This file must contain a predict() function and return an array of probabilities.
The model assets must be in the root of the zip file (not in a subfolder). This is important, as it will fail to generate predictions otherwise.
user_requirements_file.txt (requirements.txt file)
This file contains a list of python requirements to run the model. Make sure it includes all pre-reqs to your model, or the entrypoint file will fail to load (with 'package not found' exceptions)
explainer.pkl
This is the Explainer object created by Arthur SDK. It will include the LIME or SHAP algorithm to produce feature importance data.
config
This is a json block with additional settings. It's the same as can be seen in the GET /enrichments example above.
Preparing the user_project zip file
Move all required model assets to a folder. Keep in mind that Arthur will automatically install the packages listed in the requirements.txt file, from a public or private repository.
In that folder, create the entrypoint.py file. This file must contain a predict() function, which Arthur will call at runtime. The predict() function will received a 2-D numpy array, where each item represents a perturbation of the original attribute input data. The default number of perturbations is 5000 (it is a configurable parameter). So the entrypoint file must be able to process an array of input elements. The expected return is a 2-D numpy array with the probability scores. The size of this array should match the size of the input data.
This is an example of entrypoint file:
Pythonimport joblib
import os
from pathlib import Path
model_path = os.path.join(Path(__file__).resolve().parents[0], ""credit_model.pkl"")
sk_model = joblib.load(model_path)
def predict(x):
return sk_model.predict_proba(x)[:,1]
In this case, the entrypoint file loads the model and uses the predict() function to wrap the predict_proba() function from the model. More complicated cases might require data transformation, one-hot encoding, etc. Keep in mind that the input of the predict() function will always be an array of input elements, and Arthur expects to receive as return an array of probabilities.
Next, prepare the requirements.txt file with all the necessary packages.
Finally, zip the folder, ensuring that the entrypoint.py and requirements.txt files are at the root.
Creating the explainer.pkl file
The easiest way to create this file is by using the Arthur SDK. Depending on the environment, it might be easier to use the enable_explainability() function of the SDK. Some customer environments, however, will prefer to run these functions as pure REST calls, without having to load the Arthur SDK. So the Explainability assets can be prepared in advance and then uploaded through direct REST calls.
The basic steps to create the Explainer object are:
Connect to the Arthur instance
Load the model definition from Arthur
Prepare the (unzipped) user project folder, with the entrypoint and requirements files
Load reference data (can be a small subset)
Create a packager object using the SDK
Extract the Explainer from the packager
Save the Explainer to a .pkl file in the user project folder
Save the model (regressor/classifier/etc) to a .pkl file in the user project folder
This code example walks through those steps:
Pythonconnection = ArthurAI(url = url, login = login, password = password)
arthur_model = connection.get_model(model_id)
import os
project_dir = os.path.join(os.getcwd() + ""/explain"")
from arthurai.explainability.explanation_packager import ExplanationPackager
packager = ExplanationPackager(
arthur_model,
df=df_reference,
project_directory=project_dir,
user_predict_function_import_path='entrypoint',
streaming_explainability_enabled=True,
requirements_file=""requirements.txt"",
explanation_algo='lime')
packager.create()
arthur_explainer = packager.explainer
# Test the Explainer by producing 1 explanation
# The input for the Explainer is a list of input values --not a dataframe
# remove non-input, prediction and ground truth attributes from this list
def get_sample(num_samples):
sample_inf = df_reference.sample(num_samples)
del sample_inf['pred']
del sample_inf['gt']
return sample_inf.values.tolist()
sample_inf = get_sample(1)
explanation = arthur_explainer.explain_tabular_lime(sample_inf)
print(explanation)
# Save Model to .pkl file
import dill as pickle
with open(""./explain/model.pkl"", ""wb"") as file:
pickle.dump(regressor, file)
# Save Explainer to .pkl file
with open(""./explain/explainer.pkl"", ""wb"") as file:
pickle.dump(arthur_explainer, file)
Currently, the project folder will contain the model .pkl file, the Explainer, the entrypoint, and the requirements.file. These are all the assets required to enable Explainability.
Enabling Explainability
This is the API endpoint to enable explainability:
PATCH {{hostname}}/api/v3/models/{{model_id}}/enrichments/explainability
This is a multipart/form-data call that must include the elements described above.
The following python code is an example of how this call can be configured:
Pythonheaders = {
'Arthur-Organization-ID': '{{organization_id}}',
'Authorization': '{{access_token}}'
}
files = [
('user_project.zip',('explain.zip',open('./explain.zip','rb'),'application/zip')),
('user_requirements_file.txt',('requirements.txt',open('./explain/requirements.txt','rb'),'text/plain')),
('explainer.pkl',('explainer.pkl',open('./explain/explainer.pkl','rb'),'application/octet-stream'))
]
payload = {
'config': '{""enabled"": true, ""config"":{""python_version"": ""3.8"",""sdk_version"": ""3.25.0"",""streaming_explainability_enabled"": true,""user_predict_function_import_path"": ""entrypoint"",""shap_expected_values"": [0],""model_server_cpu"": ""2"",""model_server_memory"": ""1500Mi"",""model_server_max_replicas"": 30,""explanation_nsamples"": 2000,""explanation_algo"": ""lime"",""inference_consumer_cpu"": ""500m"",""inference_consumer_memory"": ""512Mi"",""inference_consumer_score_percent"": 1,""inference_consumer_thread_pool_size"": 5}}'
}
exp_response = session.request(""PATCH"", f'{url}/models/{model_id}/enrichments/explainability',
headers = headers,
data = payload,
files = files)
The expected response is an ""ok"" string.
In the backend, Arthur will provision a model server, which will be able to provide explanations on demand. This process should take a few minutes to complete.
Appendix: Sending Inferences
Once the model is saved to Arthur and all enrichments are enabled, everything should be ready to receive and process inference data.
Inferences can be sent to Arthur in several different ways: individually or in batches; in JSON format or as parquet, and when using the Arthur SDK, directly as pandas dataframes.
When sending inference data, the following information is required:
Inference timestamp
Inference ID
Inference data (all input values)
Non-input data (optional, but it cannot be uploaded after the fact)
Prediction attributes
Optionally, ground truth data can also be sent (when available).
This is a JSON payload example sending 2 inferences:
JSONpayload = json.dumps([
{
""partner_inference_id"": ""inf_"" + str(uuid.uuid4()),
""inference_timestamp"": ""2023-06-07T12:00:13.656449Z"",
""inference_data"": {
""input_1"": 1000,
""input_2"": ""ABDC1234"",
""input_3"": 9999,
""pred"" : 0.85
},
""ground_truth_timestamp"": ""2023-06-07T12:00:13.656449Z"",
""ground_truth_data"": {
""gt"": 1
}
},
{
""partner_inference_id"": ""inf_"" + str(uuid.uuid4()),
""inference_timestamp"": ""2023-05-07T12:01:13.656449Z"",
""inference_data"": {
""input_1"": 2000,
""input_2"": ""ABDC12345"",
""input_3"": 8888,
""pred"" : 0.65
},
""ground_truth_timestamp"": ""2022-05-07T12:01:13.656449Z"",
""ground_truth_data"": {
""gt"": 0
}
}
])
PS: for batch models, the batch_id parameter is required.
In this example, ground truth data is being sent along with inference data. Arthur supports uploading ground truth data at any moment after receiving the inference data.
This is the API endpoint that can be used to send inferences in JSON format:
POST {{hostname}}/api/v3/models/{{model_id}}/inferences
Sending inferences as parquet files
A different API endpoint is available to process inferences in parquet files:
POST {{hostname}}/api/v3/models/{{model_id}}/inferences/file
The parquet file must contain the headers, as well as the timestamps and inference IDs:
In this case, the parquet file does not contain ground truth data.
The file must be attached to the request as inference_data, as shown in the following example:
Pythonfiles = [
('inference_data',('data.parquet', open('./data.parquet','rb'), 'application/octet-stream'))
]
Uploading Ground Truth Data
When uploading ground truth data, only the ground truth timestamp, ground truth attributes, and inference IDs are required. The inference IDs must match the existing inferences:
The endpoint to upload ground truth data is the same one used to upload inference data:
POST {{hostname}}/api/v3/models/{{model_id}}/inferences/file
The difference will be that the file should be sent as ground_truth_data, instead of inference_data:
Pythonfiles = [
('ground_truth_data',('gt.parquet', open('./gt.parquet','rb'), 'application/octet-stream'))
]
Updated 2 months ago Table of Contents
Introduction
Create the model schema definition and basic metadata
Save the Model
Upload Reference Dataset for Model
Appendix: Sending Inferences",arthur_scope_docs,https://docs.arthur.ai/docs/registering-a-model-with-the-api
108,"Full Directory of Arthur Permissions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Full Directory of Arthur PermissionsSuggest EditsThese can be used to specify permissions in the custom role mapping configuration JSON under ""resource"" and ""action"".
EX:
* ""Resource""
* ""Action""
* ""Action""
* ""Action""
Org and User Related Permissions
organization_global
read
write
organization
read
delete
custom_roles
read
write
delete
system_config
write
user
read
write
delete
user_invite
write
organization_metrics
read
Model Related Permissions
model
read
write
delete
tag
read
write
delete
enrichment_config
read
write
metric_query
read
write
delete
raw_data
read
write
ground_truth
write
query
execute
Alert Related Permissions
alert
read
resolve
notify
insight
read
resolve
alert_rule
read
write
delete
alert_notification_config
read
write
delete
alert_summary
notify
alert_summary_config
read
write
delete
alert_summary_subscriber
read
write
delete
Updated about 2 months ago Table of Contents
Org and User Related Permissions
Model Related Permissions
Alert Related Permissions",arthur_scope_docs,https://docs.arthur.ai/docs/full-directory-of-arthur-permissions
109,"Default Evaluation Functions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Default Evaluation FunctionsSuggest EditsArthur provides common default metrics for all Model Input / Output Types. A list of each default metric available can be found within each model types page. This page will provide an overview of all of them to show how to query Arthur.
Regression
All regression evaluation metrics will follow the below request body structure.
Query Request:
JSON{
""select"": [
{
""function"": ""[rmsemaerSquared]"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""ground_truth_property"": ""<attribute_name> [string]"",
""predicted_property"": ""<attribute_name> [string]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<evaluation_value> [float]""
}
]
}
RMSE
Get the RMSE between a prediction attribute and a ground truth attribute.
Sample Request:
JSON{
""select"": [
{
""function"": ""rmse"",
""alias"": ""error"",
""parameters"": {
""ground_truth_property"": ""FICO_actual"",
""predicted_property"": ""FICO_predicted""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""error"": 0.76
}
]
}
back to top
MAE
Get the Mean Absolute Error between a prediction and ground truth attributes.
This function takes an optional parameter aggregation that allows swapping the aggregation from ""avg"" to either ""min"" or ""max"". This can be helpful if you're looking for extremes, such as the lowest or highest absolute error. Additionally, this function supports optional params normalizationMax and normalizationMin that accept numbers and will perform min/max normalization on the values before aggregation if both params are provided.
Query Request:
JSON{
""select"": [
{
""function"": ""mae"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""predicted_property"": ""<predicted_property_name> [string]"",
""ground_truth_property"": ""<ground_truth_property_name> [string]"",
""aggregation"": ""[avgminmax] (default avg, optional)"",
""normalizationMin"": ""<value> [optional number]"",
""normalizationMax"": ""<value> [optional number]""
}
}
]
}
Sample Request:
JSON{
""select"": [
{
""function"": ""mae"",
""alias"": ""error"",
""parameters"": {
""ground_truth_property"": ""FICO_actual"",
""predicted_property"": ""FICO_predicted""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""error"": 0.76
}
]
}
back to top
R Squared
Get the R Squared value between a prediction and ground truth attributes.
Sample Request:
JSON{
""select"": [
{
""function"": ""rSquared"",
""alias"": ""rsq"",
""parameters"": {
""ground_truth_property"": ""FICO_actual"",
""predicted_property"": ""FICO_predicted""
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""rsq"": 0.94
}
]
}
back to top
Binary Classification
When using binary classification evaluation functions with a multiclass model, outputs will be calculated assuming a one vs. all approach.
Confusion Matrix
Calculates the confusion matrix for a classification model. For binary classifiers, users must specify a probability threshold to count a prediction as a positive class.
Query Request:
JSON{
""select"": [
{
""function"": ""confusionMatrix"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""threshold"": ""<value [float]> [required only for binary classifiers]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": {
""true_positive"": ""<count> [int]"",
""false_positive"": ""<count> [int]"",
""true_negative"": ""<count> [int]"",
""false_negative"": ""<count> [int]""
}
}
]
}
Sample Request: Calculate the confusion matrix for a binary classifier with a threshold of 0.5 (the standard threshold for confusion matrix).
JSON{
""select"": [
{
""function"": ""confusionMatrix"",
""parameters"": {
""threshold"": 0.5
}
}
]
}
Sample Response:
JSON{
""query_result"": [
{
""confusionMatrix"": {
""true_positive"": 100480,
""false_positive"": 100076,
""true_negative"": 100302,
""false_negative"": 99142
}
}
]
}
back to top
Confusion Matrix Rate
Calculates the confusion matrix rates for a classification model. For binary classifiers, users must specify a probability threshold to count a prediction as a positive class.
Query Request:
JSON{
""select"": [
{
""function"": ""confusionMatrixRate"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""threshold"": ""<value [float]> [required only for binary classifiers]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": {
""true_positive_rate"": ""<rate> [float]"",
""false_positive_rate"": ""<rate> [float]"",
""true_negative_rate"": ""<rate> [float]"",
""false_negative_rate"": ""<rate> [float]"",
""accuracy_rate"": ""<rate> [float]""
}
}
]
}
Sample Request: Calculate the confusion matrix for a binary classifier with a threshold of 0.5 (the standard threshold for confusion matrix).
JSON{
""select"": [
{
""function"": ""confusionMatrixRate"",
""parameters"": {
""threshold"": 0.5
}
}
]
}
Response:
JSON{
""query_result"": [
{
""confusionMatrixRate"": {
""true_positive_rate"": 0.5033513340213003,
""false_positive_rate"": 0.49943606583557076,
""true_negative_rate"": 0.5005639341644292,
""false_negative_rate"": 0.4966486659786997
}
}
]
}
back to top
Confusion Matrix Variants
If you only want a specific metric derived from a confusion matrix, you can use one of the following functions:
truePositiveRate
falsePositiveRate
trueNegativeRate
falseNegativeRate
accuracyRate
balancedAccuracyRate
f1
sensitivity
specificity
precision
recall
For example, to return the truePositiveRate:
JSON{
""select"": [
{
""function"": ""truePositiveRate"",
""parameters"": {
""threshold"": 0.5,
""ground_truth_property"":""class_a"",
""predicted_property"":""ground_truth_a""
}
}
]
}
Response:
JSON{
""query_result"": [
{
""truePositiveRate"": 0.5033513340213003
}
]
}
back to top
AUC
The Area Under the ROC Curve can also be computed for binary classifiers.
Sample Query:
JSON{
""select"": [
{
""function"": ""auc"",
""parameters"": {
""ground_truth_property"":""class_a"",
""predicted_property"":""ground_truth_a""
}
}
]
}
Response:
JSON{
""query_result"": [
{
""auc"": 0.9192331426352897
}
]
}
Multi-class Classification
Multi-class Accuracy Rate
Calculates the global accuracy rate.
Query Request:
JSON{
""select"": [
{
""function"": ""accuracyRateMulticlass"",
""alias"": ""<alias_name> [optional string]""
}
]
}
Query Response:
JSON{
""query_result"": [
{
""accuracyRateMulticlass"": ""<rate> [float]""
}
]
}
Example:
JSON{
""select"": [
{
""function"": ""accuracyRateMulticlass""
}
]
}
Response:
JSON{
""query_result"": [
{
""accuracyRateMulticlass"": 0.785
}
]
}
back to top
Multi-class Confusion Matrix
Calculates the confusion matrix for a multi-class model with regard to a single class.
The predicted attribute and ground truth attribute must be passed as parameters.
Query Request:
JSON{
""select"": [
{
""function"": ""confusionMatrixMulticlass"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""predicted_property"": ""<predicted_property_name>"",
""ground_truth_property"": ""<ground_truth_property_name>""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": {
""true_positive"": ""<count> [int]"",
""false_positive"": ""<count> [int]"",
""true_negative"": ""<count> [int]"",
""false_negative"": ""<count> [int]""
}
}
]
}
Example:
JSON{
""select"": [
{
""function"": ""confusionMatrixMulticlass"",
""parameters"": {
""predicted_property"": ""predicted_class_A"",
""ground_truth_property"": ""gt_predicted_class_A""
}
}
]
}
Response:
JSON{
""query_result"": [
{
""confusionMatrix"": {
""true_positive"": 100480,
""false_positive"": 100076,
""true_negative"": 100302,
""false_negative"": 99142
}
}
]
}
back to top
Multi-class Confusion Matrix Rate
Calculates the confusion matrix rates for a multi-class classification model in regards to a single predicted class.
Query Request:
JSON{
""select"": [
{
""function"": ""confusionMatrixRateMulticlass"",
""alias"": ""<alias_name> [optional string]"",
""parameters"": {
""predicted_property"": ""predicted_class_A"",
""ground_truth_property"": ""gt_predicted_class_A""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": {
""true_positive_rate"": ""<rate> [float]"",
""false_positive_rate"": ""<rate> [float]"",
""true_negative_rate"": ""<rate> [float]"",
""false_negative_rate"": ""<rate> [float]"",
""accuracy_rate"": ""<rate> [float]"",
""balanced_accuracy_rate"": ""<rate> [float]"",
""precision"": ""<rate> [float]"",
""f1"": ""<rate> [float]""
}
}
]
}
Example calculating the confusion matrix rates:
JSON{
""select"": [
{
""function"": ""confusionMatrixRateMulticlass"",
""parameters"": {
""predicted_property"": ""predicted_class_A"",
""ground_truth_property"": ""gt_predicted_class_A""
}
}
]
}
Response:
JSON{
""query_result"": [
{
""confusionMatrixRateMulticlass"": {
""true_positive_rate"": 0.6831683168316832,
""false_positive_rate"": 0.015653220951234198,
""true_negative_rate"": 0.9843467790487658,
""false_negative_rate"": 0.31683168316831684,
""accuracy_rate"": 0.9378818737270875,
""balanced_accuracy_rate"": 0.8337575479402245,
""precision"": 0.8884120171673819,
""f1"": 0.7723880597014925
}
}
]
}
back to top
If you only want a specific value from the confusion matrix rate function, you can use one of the following functions:
truePositiveRateMulticlass
falsePositiveRateMulticlass
trueNegativeRateMulticlass
falseNegativeRateMulticlass
For example, to return the truePositiveRate:
JSON{
""select"": [
{
""function"": ""truePositiveRateMulticlass"",
""parameters"": {
""predicted_property"": ""predicted_class_A"",
""ground_truth_property"": ""gt_predicted_class_A""
}
}
]
}
Response:
JSON{
""query_result"": [
{
""truePositiveRate"": 0.5033513340213003
}
]
}
back to top
Multi-class F1
Calculates the components needed to compute a F1 score for a multi-class model.
In this example, the model has 3 classes: class-1, class-2, class-3 and the corresponding ground truth labels class-1-gt, class-2-gt, class-3-gt.
Query Request:
JSON{
""select"": [
{
""function"": ""count"",
""alias"": ""count""
},
{
""function"": ""confusionMatrixRateMulticlass"",
""alias"": ""class-1"",
""parameters"": {
""predicted_property"": ""class-1"",
""ground_truth_property"": ""class-1-gt""
}
},
{
""function"": ""countIf"",
""alias"": ""class-1-gt"",
""parameters"": {
""property"": ""multiclass_model_ground_truth_class"",
""comparator"": ""eq"",
""value"": ""class-1-gt""
},
""stage"": ""GROUND_TRUTH""
},
{
""function"": ""confusionMatrixRateMulticlass"",
""alias"": ""class-2"",
""parameters"": {
""predicted_property"": ""class-2"",
""ground_truth_property"": ""class-2-gt""
}
},
{
""function"": ""countIf"",
""alias"": ""class-2-gt"",
""parameters"": {
""property"": ""multiclass_model_ground_truth_class"",
""comparator"": ""eq"",
""value"": ""class-2-gt""
},
""stage"": ""GROUND_TRUTH""
},
{
""function"": ""confusionMatrixRateMulticlass"",
""alias"": ""class-3"",
""parameters"": {
""predicted_property"": ""class-3"",
""ground_truth_property"": ""class-3-gt""
}
},
{
""function"": ""countIf"",
""alias"": ""class-3-gt"",
""parameters"": {
""property"": ""multiclass_model_ground_truth_class"",
""comparator"": ""eq"",
""value"": ""class-3-gt""
},
""stage"": ""GROUND_TRUTH""
}
]
}
Query Response:
JSON{
""query_result"": [
{
""count"": 7044794,
""class-1-gt"": 2540963,
""class-2-gt"": 2263918,
""class-3-gt"": 2239913,
""class-1"": {
""true_positive_rate"": 0.4318807475748368,
""false_positive_rate"": 0.3060401245073361,
""true_negative_rate"": 0.6939598754926639,
""false_negative_rate"": 0.5681192524251633,
""accuracy_rate"": 0.5994314383074935,
""balanced_accuracy_rate"": 0.5629203115337503,
""precision"": 0.4432575070302042,
""f1"": 0.437495178612114
},
""class-2"": {
""true_positive_rate"": 0.42177322676881407,
""false_positive_rate"": 0.3514795196528837,
""true_negative_rate"": 0.6485204803471163,
""false_negative_rate"": 0.578226773231186,
""accuracy_rate"": 0.5756528863725469,
""balanced_accuracy_rate"": 0.5351468535579652,
""precision"": 0.3623427088234848,
""f1"": 0.38980575845890253
},
""class-3"": {
""true_positive_rate"": 0.26144274353512836,
""false_positive_rate"": 0.2805894672521546,
""true_negative_rate"": 0.7194105327478454,
""false_negative_rate"": 0.7385572564648716,
""accuracy_rate"": 0.5737983254017079,
""balanced_accuracy_rate"": 0.4904266381414869,
""precision"": 0.3028268576818381,
""f1"": 0.2806172238153916
}
}
]
}
With this result, you can calculate the weighted F1 score by multiplying each classes's F1 score by the count of the ground truth and dividing by the total count.
In this example, that would be
(class-1.f1 * class-1-gt + class-2.f1 * class-2-gt + class-3.f1 * class-3-gt) / count
and with numbers:
(0.437495178612114 * 2540963 +
0.38980575845890253 * 2263918 +
0.2806172238153916 * 2239913) / 7044794
= 0.3722898785
back to top
Object Detection
Objects Detected
For multiclass, multilabel, and regression models, querying model performance works the same for Arthur computer vision models as more tabular and NLP models. But Object Detection computer vision has some special fields you can use when querying.
Example query fetching all bounding box fields:
JSON{
""select"": [
{
""property"": ""inference_id""
},
{
""property"": ""objects_detected""
}
]
}
The response will have 1 object per bounding box.
JSON{
""query_result"": [
{
""inference_id"": ""1"",
""objects_detected.class_id"": 0,
""objects_detected.confidence"": 0.6,
""objects_detected.top_left_x"": 23,
""objects_detected.top_left_y"": 45,
""objects_detected.width"": 20,
""objects_detected.height"": 30
},
{
""inference_id"": ""1"",
""objects_detected.class_id"": 1,
""objects_detected.confidence"": 0.6,
""objects_detected.top_left_x"": 23,
""objects_detected.top_left_y"": 45,
""objects_detected.width"": 20,
""objects_detected.height"": 30
},
{""inference_id"": 2,
""..."": ""...""}
]
}
You can also specify only a single nested field:
JSON{
""select"": [
{
""property"": ""inference_id""
},
{
""property"": ""objects_detected.class_id""
},
{
""property"": ""objects_detected.confidence""
}
]
}
The response will have 1 object per bounding box.
JSON{
""query_result"": [
{
""inference_id"": ""1"",
""objects_detected.class_id"": 0,
""objects_detected.confidence"": 0.6
},
{
""inference_id"": ""1"",
""objects_detected.class_id"": 1,
""objects_detected.confidence"": 0.6
},
{""inference_id"": 2,
""..."":
""...""}
]
}
When supplying the bounding box specific fields in filters, group bys, or order bys the columns must also be supplied in the select clause in order for the query to succeed.
Mean Average Precision
Calculates Mean Average Precision for an object detection model. This is used as measure of accuracy for object detection models.
threshold determines the minimum IoU value to be considered a match for a label. predicted_property and ground_truth_property are optional parameters and should be the names of the predicted and ground truth attributes for the model.
They default to ""objects_detected"" and ""label"" respectively if nothing is specified for these parameters.
Query Request:
JSON{
""select"": [
{
""function"": ""meanAveragePrecision"",
""alias"": ""<alias_name> [Optional]"",
""parameters"": {
""threshold"": ""<threshold> [float]"",
""predicted_property"": ""<predicted_property> [str]"",
""ground_truth_property"": ""<ground_truth_property> [str]""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<result> [float]""
}
]
}
Example:
JSON{
""select"": [
{
""function"": ""meanAveragePrecision"",
""parameters"": {
""threshold"": 0.5,
""predicted_property"": ""objects_detected"",
""ground_truth_property"": ""label""
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""meanAveragePrecision"": 0.78
}
]
}
Generative Text
Token Likelihood
TokenLikelihoods attributes yield two queryable columns for that attribute with suffixes “_tokens” and “_likelihoods” appended to the attribute's name. For example, a model with a TokenLikelihoods attribute named summary_token_probs yields two queryable columns: summary_token_probs_tokens and summary_token_probs_likelihoods which represent an array of the selected tokens and an array of their corresponding likelihoods.
Pythonquery =
{""select"": [
{""property"": ""summary_token_probs_tokens""},
{""property"": ""summary_token_probs_likelihoods""}
]}
response[{ ""summary_token_probs_likelihoods"": [
0.3758265972137451,
0.6563436985015869,
0.32000941038131714,
0.5629857182502747],
""summary_token_probs_tokens"": [
""this"",
""is"",
""a"",
""summary""] }]
Bias
Bias Mitigation
Calculates mitigated predictions based on conditional thresholds, returning 0/1 for each inference.
This function returns null for inferences that don't match any of the provided conditions.
Query Request:
JSON{
""select"":
[
{
""function"": ""biasMitigatedPredictions"",
""alias"": ""<alias_name> [Optional]"",
""parameters"":
{
""predicted_property"": ""<predicted_property> [str]"",
""thresholds"":
[
{
""conditions"":
{
""property"": ""<attribute_name> [string or nested]"",
""comparator"": ""<comparator> [string] Optional: default 'eq'"",
""value"": ""<string or number to compare with property>""
},
""threshold"": ""<threshold> [float]""
}
]
}
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<function_name/alias_name>"": ""<result> [int]""
}
]
}
Example:
JSON{
""select"":
[
{
""function"": ""biasMitigatedPredictions"",
""parameters"":
{
""predicted_property"": ""prediction_1"",
""thresholds"":
[
{
""conditions"":
[
{
""property"": ""SEX"",
""value"": 1
}
],
""threshold"": 0.4
},
{
""conditions"":
[
{
""property"": ""SEX"",
""value"": 2
}
],
""threshold"": 0.6
}
]
}
}
]
}
Response:
JSON{
""query_result"":
[
{
""SEX"": 1,
""biasMitigatedPredictions"": 1
},
{
""SEX"": 2,
""biasMitigatedPredictions"": 0
},
{
""SEX"": 1,
""biasMitigatedPredictions"": 0
}
]
}
Ranked List Outputs
Performance Metrics
Precision at k
Precision is an indicator of the efficiency of a supervised machine learning model. If one model gets all the relevant items by recommending fewer items than another model, it has a higher precision. For item recommendation models, precision at k measures the fraction of all relevant items among top-k recommended items.
Query Request
JSON{
""select"": [
{
""function"": ""precisionAtK"",
""parameters"": {
""predicted_property"": ""predicted_items"",
""ground_truth_property"": ""relevant_items"",
""k"": 5
}
}
]
}
Query Response
JSON{
""query_result"": [
{
""precisionAtK"": 0.26666666666666666
}
]
}
Recall at k
Recall is an indicator of the effectiveness of a supervised machine learning model. The model which correctly identifies more of the positive instances gets a higher recall value. In case of recommendations, the recall at k is measured as the fraction of all relevant items that were recovered in top k recommendations.
Query Request
JSON{
""select"": [
{
""function"": ""recallAtK"",
""parameters"": {
""predicted_property"": ""predicted_items"",
""ground_truth_property"": ""relevant_items"",
""k"": 5
}
}
]
}
Query Response
JSON{
""query_result"": [
{
""recallAtK"": 0.26666666666666666
}
]
}
Mean Average Precision at k (MAP @ k)
The MAP@K metric is the most commonly used metric for evaluating recommender systems. It calculates the precision at every location 1 through k where there is a relevant item. Average Precision is calculated per inference, then the per inference values are averaged across a group of inferences to create Mean Average Precision.
Query Request
JSON{
""select"": [
{
""function"": ""mapAtK"",
""parameters"": {
""predicted_property"": ""predicted_items"",
""ground_truth_property"": ""relevant_items"",
""k"": 5
}
}
]
}
Query Response
JSON{
""query_result"": [
{
""mapAtK"": 0.26666666666666666
}
]
}
Normalized Discounted Cumulative Gain at k (nDCG @ k)
nDCG measures the overall reward at all positions that hold a relevant item. The reward is an inverse log of the position (i.e. higher ranks for relevant items would lead to better reward, as desired).
Similar to MAP@k, this metric calculates a value per inference, then is aggregated across inferences using an average.
Query Request
JSON{
""select"": [
{
""function"": ""nDCGAtK"",
""parameters"": {
""predicted_property"": ""predicted_items"",
""ground_truth_property"": ""relevant_items"",
""k"": 5
}
}
]
}
Query Response
JSON{
""query_result"": [
{
""nDCGAtK"": 0.26666666666666666
}
]
}
AUC
In the case of ranked list metrics, AUC measures the likelihood that a random relevant item is ranked higher than a random irrelevant item. Higher the likelihood of this happening implies a higher AUC score meaning a better recommendation system. We calculate this likelihood empirically based on the ranks given by the algorithm to all items — out of all possible pairs of type (relevant-item, non-relevant-item), AUC is a proportion of pairs where the relevant item was ranked higher than the irrelevant item from that pair.
This metric is calculated per-inference, then aggregated as an average over a group of inferences.
Query Request
JSON{
""select"": [
{
""function"": ""rankedListAUC"",
""parameters"": {
""predicted_property"": ""predicted_items"",
""ground_truth_property"": ""relevant_items""
}
}
]
}
Query Response
JSON{
""query_result"": [
{
""rankedListAUC"": 0.26666666666666666
}
]
}
Mean Reciprocal Rank (MRR)
Mean Reciprocal Rank quantifies the rank of the first relevant item found in the recommendation list. It takes the reciprocal of this “first relevant item rank”, meaning that if the first item is relevant (i.e. the ideal case) then MRR will be 1, otherwise it will be lower.
Query Request
JSON{
""select"": [
{
""function"": ""meanReciprocalRank"",
""parameters"": {
""predicted_property"": ""predicted_items"",
""ground_truth_property"": ""relevant_items""
}
}
]
}
Query Response
JSON{
""query_result"": [
{
""meanReciprocalRank"": 0.26666666666666666
}
]
}
back to topUpdated 3 months ago Table of Contents
Regression
RMSE
MAE
R Squared
Binary Classification
Confusion Matrix
Confusion Matrix Rate
Confusion Matrix Variants
AUC
Multi-class Classification
Multi-class Accuracy Rate
Multi-class Confusion Matrix
Multi-class Confusion Matrix Rate
Multi-class F1
Object Detection
Objects Detected
Mean Average Precision
Generative Text
Token Likelihood
Bias
Bias Mitigation
Ranked List Outputs
Performance Metrics",arthur_scope_docs,https://docs.arthur.ai/docs/querying-performance
110,"Multiclass Classification
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Multiclass ClassificationSuggest EditsMulticlass classification models predict one class from more than two potential classes. In Arthur, these models fall into the category of classification and are represented by the Multiclass model type.
Some common examples of Tabular multiclass classification are:
What breed of dog is in this photo?
What part of the car is damaged in this photo?
Similar to binary classification, these models frequently output not only the predicted class but also a probability for each class predicted. The highest probability class is then the predicted output. In these cases, a threshold does not need to be provided to Arthur and it will automatically track the highest probability class as the predicted output.
Formatted Data in Arthur
Tabular binary classification models require three things to be specified in their schema: all predicting model attributes (or features), predicted probability of outputs, and a column for the inference's true label (or ground truth). Many teams also choose to onboard metadata for the model (i.e. any information you want to track about your inferences) as non-input attributes.
Attribute (numeric or categorical)Attribute (numeric or categorical)Probability of Prediction AProbability of Prediction BProbability of Prediction CGround TruthNon-Input Attribute (numeric or categorical)High School Education34.5.90.05.05AMaleGraduate Degree44.1.46.14.40BFemaleGraduate Degree33.5.16.17.71CFemale
Predict Function and Mapping
These are some examples of common values teams need to onboard for their multi-class classification models.
The relationship between the prediction and ground truth column must be defined to help set up your Arthur environment to calculate default performance metrics. There are 2 options for formatting this, depending on your reference dataset. Additionally, if teams wish to enable explainability, they must provide a few Assets Required For Explainability. Below is an example of the runnable predict function, which outputs a single numeric prediction.
prediction to ground truth mappingExample Prediction Function## Option 1:
Multiple Prediction Columns, Single Ground Truth Column
# Map each PredictedValue attribute to its corresponding GroundTruth value.
output_mapping_1 = {
'pred_class_one_column':'one',
'pred_class_two_column':'two',
'pred_class_three_column':'three'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
ground_truth_column='ground_truth',
pred_to_ground_truth_map=output_mapping_1
)
## Option 2:
Multiple Prediction and Ground Truth Columns
# Map each PredictedValue attribute to its corresponding GroundTruth attribute.
output_mapping_2 = {
'pred_class_one_column':'gt_class_one_column',
'pred_class_two_column':'gt_class_two_column',
'pred_class_three_column':'gt_class_three_column'}
# Build Arthur Model with this Technique
arthur_model.build(reference_data,
pred_to_ground_truth_map=output_mapping_2
)
## Example prediction function for classification
def predict(x):
return model.predict_proba(x)
Available Metrics
When onboarding multiclass classification models, you have a number of default metrics available to you within the UI. You can learn more about each specific metric in the metrics section of the documentation.
Out-of-the-Box Metrics
The following metrics are automatically available in the UI (out-of-the-box) per class when teams onboard a multiclass classification model. Find out more about these metrics in the
Performance Metrics section.
MetricMetric TypeAccuracy RatePerformanceBalanced Accuracy RatePerformanceAUCPerformanceRecallPerformancePrecisionPerformanceSpecificity (TNR)PerformanceF1PerformanceFalse Positive RatePerformanceFalse Negative RatePerformanceInference CountIngestionInference Count by ClassIngestion
Drift Metrics
In the platform, drift metrics are calculated compared to a reference dataset. So, once a reference dataset is onboarded for your model, these metrics are available out of the box for comparison. Find out more about these metrics in the Drift and Anomaly section.
Note: Teams are able to evaluate drift for inference data at different intervals with our Python SDK and query service (for example data coming into the model now, compared to a month ago).
PSIFeature DriftKL DivergenceFeature DriftJS DivergenceFeature DriftHellinger DistanceFeature DriftHypothesis TestFeature DriftPrediction DriftPrediction DriftMultivariate DriftMultivariate Drift
Fairness Metrics
As further described in the Fairness Metrics section of the documentation, fairness metrics are available for any tabular Arthur attributes manually selected to monitor for bias.
MetricMetric TypeAccuracy RateFairnessTrue Positive Rate (Equal Opportunity)FairnessTrue Negative RateFairnessFalse Positive RateFairnessFalse Negative RateFairness
User-Defined Metrics
Whether your team uses a different performance metric, wants to track defined segments of data, or needs logical functions to create a metric for external stakeholders (like product or business metrics). Learn more about creating metrics with data in Arthur in the User-Defined Metrics section.
Available Enrichments
The following enrichments can be enabled for this model type:
Anomaly DetectionHot SpotsExplainabilityBias MitigationXXXUpdated 3 months ago Table of Contents
Formatted Data in Arthur
Predict Function and Mapping
Available Metrics
Out-of-the-Box Metrics
Drift Metrics
Fairness Metrics
User-Defined Metrics
Available Enrichments",arthur_scope_docs,https://docs.arthur.ai/docs/multi-class-classification
111,"Quickstart
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by QuickstartSuggest EditsFrom a Python environment with the arthurai package installed, this quickstart code will:
Make binary classification predictions on a small dataset
Onboard the model with reference data to Arthur
Log batches of model inference data with Arthur
Get performance results for our model
Imports
The arthurai package can be pip-installed from the terminal, along with numpy and pandas:
Bashpip install arthurai numpy pandas
Then you can import the functionality we'll use from the arthurai package like this:
Python# Arthur imports
from arthurai import ArthurAI
from arthurai.common.constants import InputType, OutputType, Stage
from arthurai.util import generate_timestamps
# Other libraries used in this example
import numpy as np
import pandas as pd
Model Predictions
We write out samples from a Titanic survival prediction dataset explicitly in Python,
giving the age of each passenger, the cost of their ticket, the passenger class of their ticket, and the ground-truth label of whether they survived. Our model's outputs are given by a predict function using only the age variable. We split the data into
reference_data for onboarding the model
inference_data for in-production inferences the model processes
Python# Define Titanic sample data
titanic_data = pd.DataFrame({
""age"":[19.0,37.0,65.0,30.0,22.0,24.0,16.0,40.0,58.0,32.0],
""fare"":[8.05,29.7,7.75,7.8958,7.75,49.5042,86.5,7.8958,153.4625,7.8958],
""passenger_class"":[3,1,3,3,3,1,1,3,1,3],
""survived"":[1,0,0,0,1,1,1,0,1,0]})
# Split into reference and inference data
reference_data, inference_data = titanic_data[:6].copy(), titanic_data[6:].copy()
# Predict the probability of Titanic survival as inverse percentile of age
def predict(age):
nearest_age_index = np.argmin(np.abs(np.sort(reference_data['age']) - age))
return 1 - (nearest_age_index / (len(reference_data) - 1))
# reference_data and inference_data contain the model's inputs and outputs
reference_data['pred_survived'] = reference_data['age'].apply(predict)
inference_data['pred_survived'] = inference_data['age'].apply(predict)
Onboarding
This code will only run once you enter a valid username.
First we connect to the Arthur API and create an arthur_model with some high-level metadata: a classification model operating on tabular data with the name ""TitanicQuickstart"".
Python# Connect to Arthur
arthur = ArthurAI(url=""https://app.arthur.ai"",
login=""<YOUR_USERNAME_OR_EMAIL>"",
password=os.environ['ARTHUR_PASSWORD'])
# Register the model type with Arthur
arthur_model = arthur.model(display_name=""Example: Titanic Quickstart"",
input_type=InputType.Tabular,
output_type=OutputType.Multiclass)
Next, we infer the model schema from thereference_data, specifying which attributes are in which {ref}stage <basic_concepts_attributes_and_stages>. Additionally, we configure extra settings for the passenger_class attribute. Then we save the model to the platform.
Python# Map PredictedValue attribute to its corresponding GroundTruth attribute value.
# This tells Arthur that the `pred_survived` column represents
# the probability that the ground truth column has the value 1
pred_to_ground_truth_map = {'pred_survived' : 1}
# Build arthur_model schema on the reference dataset,
# specifying which attribute represents ground truth
# and which attributes are NonInputData.
# Arthur will monitor NonInputData attributes even though they are not model inputs.
arthur_model.build(reference_data,
ground_truth_column='survived',
pred_to_ground_truth_map=pred_to_ground_truth_map,
non_input_columns=['fare', 'passenger_class'])
# Configure the `passenger_class` attribute
# 1. Turn on bias monitoring for the attribute.
# 2. Specify that the passenger_class attribute has possible values [1, 2, 3],
# since that information was not present in reference_data (only values 1 and 3 are present).
arthur_model.get_attribute(name='passenger_class').set(monitor_for_bias=True,
categories=[1,2,3])
# onboard the model to Arthur
arthur_model.save()
Sending Inferences
Here we send inferences from inference_data to Arthur. We'll oversample inference_data and use Arthur's utility function to generate some fake timestamps as though the inferences were made over the last five days.
Python# Sample the inference dataset with predictions
inferences = inference_data.sample(100, replace=True)
# Generate mock timestamps over the last five days
timestamps = generate_timestamps(len(inferences), duration='5d')
# Send the inferences to Arthur
arthur_model.send_inferences(inferences, inference_timestamps=timestamps)
Inferences usually become available for analysis in seconds, but it can take up to a few minutes. You can wait until they're ready for your analysis like this:
Python# Wait until some inferences land in Arthur
arthur_model.await_inferences()
Performance Results
With our model onboarded and inferences sent, we can get performance results from Arthur. View your model in your Arthur dashboard, or use the code below to fetch the overall accuracy rate:
Python# Query overall model accuracy
query = {
""select"": [
{
""function"": ""accuracyRate""
}
]
}
query_result = arthur_model.query(query)
print(query_result)
You should see [{'accuracyRate': 0.8}] or a similar value depending on the random sampling of your inference set.Updated 3 months ago What’s NextLearn more about important terms with the Core Concepts in Arthur page, try out in-depth examples in our Arthur Github Sandbox, or start your in-depth onboarding walkthrough with the Data Preparation for Arthur page.Core Concepts in ArthurArthur Sandbox GitHub RepositoryData Preparation for ArthurTable of Contents
Imports
Model Predictions
Onboarding
Sending Inferences
Performance Results",arthur_scope_docs,https://docs.arthur.ai/docs/quickstart
112,"Grouped Inference Queries
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Grouped Inference QueriesSuggest EditsInitial analyses that treat inferences as independent of one another can provide tremendous value. But over time, models often make multiple predictions about the same real-world entities. No matter what you're predicting, it can be helpful to compare the inputs and outputs of your model on an entity-by-entity basis.
For example, let's say that your model makes predictions about whether customers will make a purchase in the next 30 days. You might have the following attributes:
customer_id: a non-input attribute
will_purchase_pred: the prediction attribute: whether a customer will make a purchase in the next 30 days
will_purchase_gt: the ground truth attribute: whether a customer actually did make a purchase within 30 days
recent_purchase_count: an input attribute with the total number of purchases the customer made in the last 90 days
newsletter_subscriber: an input attribute depicting whether the customer subscribes to the deals newsletter
Your model might be run on the full universe of Customer IDs at some regular interval. With Arthur's powerful Query API, you can follow inferences for each Customer ID through time and answer questions like:
How does recent_purchase_count tend to change for each customer, from the first to last time inference is conducted?
What is the per-customer variance of recent_purchase_count across time?
How many customers changed their newsletter subscription status, from one month ago to today?
What is the distribution of the lifetimes of Customer IDs?
Example Queries
We'll walk through some example queries for these entity-by-entity comparisons, exploring the sample case outlined above.
Per-Customer Variance
We can look at how consistent recent_purchase_count is for each customer across time. We'll compute the variance in recent_purchase_count for each customer across all their inferences, and then roll those individual variances up into a distribution.
JSON{
""select"": [
{
""function"": ""distribution"",
""alias"": ""recent_purchase_count_variance_distribution"",
""parameters"": {
""property"": {
""nested_function"": {
""function"": ""variance"",
""parameters"": {
""property"": ""recent_purchase_count""
}
}
},
""num_bins"": 20
}
}
],
""subquery"": {
""select"": [
{
""property"": ""recent_purchase_count""
},
{
""property"": ""customer_id""
}
],
""group_by"": [
{
""property"": ""customer_id""
}
]
}
}
Change Across Batches
If our model is a batch model, we might want to compare the values for each customer between two difference batches. We'll again look at the distribution of change in the recent_purchase_count, but this time look at the difference for each customer between two specific batches.
JSON{
""select"": [
{
""function"": ""distribution"",
""alias"": ""recent_purchase_count_difference_distribution"",
""parameters"": {
""property"": {
""nested_function"": {
""function"": ""subtract"",
""parameters"": {
""left"": ""batch1_recent_purchase_count"",
""right"": ""batch2_recent_purchase_count""
}
}
},
""num_bins"": 20
}
}
],
""subquery"": {
""select"": [
{
""property"": ""customer_id""
},
{
""property"": ""batch1_recent_purchase_count""
},
{
""property"": ""batch2_recent_purchase_count""
}
],
""subquery"": {
""select"": [
{
""property"": ""customer_id""
},
{
""function"": ""anyIf"",
""parameters"": {
""result"": ""recent_purchase_count"",
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""batch1""
},
""alias"": ""batch1_recent_purchase_count""
},
{
""function"": ""anyIf"",
""parameters"": {
""result"": ""recent_purchase_count"",
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""batch2""
},
""alias"": ""batch2_recent_purchase_count""
}
],
""group_by"": [
{
""property"": ""customer_id""
}
]
},
""where"": [
{
""property"": ""batch1_recent_purchase_count"",
""comparator"": ""NotNull""
},
{
""property"": ""batch2_recent_purchase_count"",
""comparator"": ""NotNull""
}
]
}
}
Change Across First to Last Inference Per Customer
We can again compare the difference between two absolute points, but instead of comparing fixed batches compute it for the earliest and latest inference for each customer:
JSON{
""select"": [
{
""function"": ""distribution"",
""alias"": ""recent_purchase_count_difference_distribution"",
""parameters"": {
""property"": {
""nested_function"": {
""function"": ""subtract"",
""parameters"": {
""left"": ""newest_recent_purchase_count"",
""right"": ""oldest_recent_purchase_count""
}
}
},
""num_bins"": 20
}
}
],
""subquery"": {
""select"": [
{
""property"": ""customer_id""
},
{
""function"": ""argMax"",
""parameters"": {
""argument"": ""inference_timestamp"",
""value"": ""recent_purchase_count""
},
""alias"": ""newest_recent_purchase_count""
},
{
""function"": ""argMin"",
""parameters"": {
""argument"": ""inference_timestamp"",
""value"": ""recent_purchase_count""
},
""alias"": ""oldest_recent_purchase_count""
}
],
""group_by"": [
{
""property"": ""customer_id""
}
]
}
}
Change in Categorical Variables
We can also look at change in categorical variables on an entity-by-entity basis. Let's look at the distribution of customers who remained subscribed, remained unsubscribed, newly subscribed, or newly unsubscribed from one batch to the next.
JSON{
""select"": [
{
""alias"": ""batch1_not_subscribed"",
""function"": ""equals"",
""parameters"": {
""left"": ""batch1_newsletter_subscriber"",
""right"": 0
}
},
{
""alias"": ""batch1_is_subscribed"",
""function"": ""equals"",
""parameters"": {
""left"": ""batch1_newsletter_subscriber"",
""right"": 1
}
},
{
""alias"": ""batch2_not_subscribed"",
""function"": ""equals"",
""parameters"": {
""left"": ""batch2_newsletter_subscriber"",
""right"": 0
}
},
{
""alias"": ""batch2_is_subscribed"",
""function"": ""equals"",
""parameters"": {
""left"": ""batch2_newsletter_subscriber"",
""right"": 1
}
},
{
""alias"": ""stayed_unsubscribed_count"",
""function"": ""and"",
""parameters"": {
""left"": {
""alias_ref"": ""batch1_not_subscribed""
},
""right"": {
""alias_ref"": ""batch2_not_subscribed""
}
}
},
{
""alias"": ""did_subscribe_count"",
""function"": ""and"",
""parameters"": {
""left"": {
""alias_ref"": ""batch1_not_subscribed""
},
""right"": {
""alias_ref"": ""batch2_is_subscribed""
}
}
},
{
""alias"": ""stayed_subscribed_count"",
""function"": ""and"",
""parameters"": {
""left"": {
""alias_ref"": ""batch1_is_subscribed""
},
""right"": {
""alias_ref"": ""batch2_is_subscribed""
}
}
},
{
""alias"": ""did_unsubscribe_count"",
""function"": ""and"",
""parameters"": {
""left"": {
""alias_ref"": ""batch1_is_subscribed""
},
""right"": {
""alias_ref"": ""batch2_not_subscribed""
}
}
}
],
""subquery"": {
""select"": [
{
""property"": ""customer_id""
},
{
""property"": ""batch1_newsletter_subscriber""
},
{
""property"": ""batch2_newsletter_subscriber""
}
],
""subquery"": {
""select"": [
{
""property"": ""customer_id""
},
{
""function"": ""anyIf"",
""parameters"": {
""result"": ""newsletter_subscriber"",
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""batch1""
},
""alias"": ""batch1_newsletter_subscriber""
},
{
""function"": ""anyIf"",
""parameters"": {
""result"": ""newsletter_subscriber"",
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""batch2""
},
""alias"": ""batch2_newsletter_subscriber""
}
],
""group_by"": [
{
""property"": ""customer_id""
}
]
},
""where"": [
{
""property"": ""batch1_newsletter_subscriber"",
""comparator"": ""NotNull""
},
{
""property"": ""batch2_newsletter_subscriber"",
""comparator"": ""NotNull""
}
]
}
}
Updated 3 months ago Table of Contents
Example Queries
Per-Customer Variance
Change Across Batches
Change Across First to Last Inference Per Customer
Change in Categorical Variables",arthur_scope_docs,https://docs.arthur.ai/docs/grouped-inference-queries
113,"Audit Log
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Audit LogSuggest EditsThe Arthur platform has the ability to produce an audit log of all calls to sensitive endpoints that include models, organizations, RBAC, and uploading / modifying data.
Event Format
Each event in the audit log has the following fields:
FieldTypeNotesevent_categorystringA description of the overarching category for this event. See the table below for a breakdown of the various categories.event_typestringAn explanation of what kind of event occurred within the event_category. See the table below for a breakdown of the various types.event_idstringA unique ID for this event, currently in UUID format but this may change in the future.timestamp[string, int]A timestamp in either Unix Epoch millisecond integer format or RFC 3339 string format, depending on the point of integration.organization_id[string, null]A string UUID of the organization if there is one associated with the event.model_id[string, null]A string UUID of the model if there is one associated with the event.user_id[string, null]A string ID of the user if there is one associated with the event.user_type[string, null]A string description of the kind of user if there is one associated with the event. This can be one of: service-account, arthur-managed, or idp-managed.http_path[string, null]A string HTTP path of the request that triggered the event if one exists.http_method[string, null]A string HTTP method of the request that triggered the event if one exists.http_status_code[int, null]An integer HTTP status code of the request that triggered the event if one exists.
Logged Endpoints
When enabled, Audit Logging will track all requests made to the following endpoints and set the Event Category and Event Type respectively in the audit log events.
EndpointMethodEvent CategoryEvent Type/organizationsPOSTevents.arthur.ai/organizationcreated/organizations/{organization_id}DELETEevents.arthur.ai/organizationdeleted/modelsPOSTevents.arthur.ai/modelcreated/models/{model_id}PUTevents.arthur.ai/modelupdated/models/{model_id}DELETEevents.arthur.ai/modeldeleted/alerts/{alert_id}/notificationsPOSTevents.arthur.ai/alertcreated/models/{model_id}/inferencesPOSTevents.arthur.ai/ingestioninference_data_received/models/{model_id}/inferencesPATCHevents.arthur.ai/ingestionground_truth_data_received/models/{model_id}/inferences/filePOSTevents.arthur.ai/ingestioninference_data_received/models/{model_id}/reference_dataPOSTevents.arthur.ai/ingestionreference_data_received/models/{model_id}/batches/{batch_id}PATCHevents.arthur.ai/ingestioninference_data_batch_completed/models/{model_id}/reference_dataPATCHevents.arthur.ai/ingestionreference_data_upload_completed/models/{model_id}/metricsPOSTevents.arthur.ai/metricscreated/models/{model_id}/metrics/{metric_id}PUTevents.arthur.ai/metricsupdated/models/{model_id}/metrics/{metric_id}DELETEevents.arthur.ai/metricsdeleted/authorization/custom_rolesPOSTevents.arthur.ai/rbacupdated/authorization/custom_rolesDELETEevents.arthur.ai/rbacupdated
A more thorough description of these endpoints is available at our API Documentation.
Integration with EventBridge
The on-prem installation provides support for shipping the Audit Log to AWS EventBridge. To configure this, you will need the following:
Bus Name: Required. The name of the EventBridge bus. This should not be the full ARN of the bus.
Region: Required. This is the AWS region where your EventBridge bus is located.
Source: Optional. This value will be added to the EventBridge events ""source"" for all events. This defaults to ""arthur-audit-log"".
Detail Type: Optional. This value will be added to the EventBridge events ""detail-type"" for all events. This defaults to ""events.arthur.ai.""
An example of the events that are written to EventBridge looks like the following (this was captured via an EventBridge to CloudWatch Log Group rule and target):
JSON{
""version"": ""0"",
""id"": ""b87f2a3a-6be1-e1d9-bc94-720d60e0a9d8"",
""detail-type"": ""events.arthur.ai"",
""source"": ""arthur-audit-log"",
""account"": ""1234567890"",
""time"": ""2022-07-21T22:07:00Z"",
""region"": ""us-east-2"",
""resources"": [],
""detail"": {
""event_type"": ""created"",
""event_category"": ""events.arthur.ai/model"",
""event_id"": ""da2ec82d-f581-4e72-bb66-fc82504f2a7e"",
""timestamp"": ""2022-07-21T22:06:59.683+0000"",
""organization_id"": ""d579359a-7259-4397-a08b-3e36c212350f"",
""model_id"": ""a950c9ad-6a1e-4042-8e47-461d13072da5"",
""user_id"": ""df3fe374-26d7-4bd8-bf62-e04a6e078e2b"",
""user_type"": ""arthur-managed"",
""http_path"": ""/api/v3/models"",
""http_method"": ""POST"",
""http_status_code"": 200
}
}
Configuration
The EventBridge integration can be enabled on the Admin Console Config Page by:
Checking ""Show Other Advanced Options"" under the Other Advanced Options section
After that is checked, a new section will appear called ""Audit Logging""
Check ""Enable Audit Log""
Next, a choice of persistence methods appears. Choose ""AWS EventBridge""
Fill out the ""Bus Name,"" ""Region,"" ""Event Source,"" and ""Detail Type"" fields that appear.
Click ""Save config"" and deploy the updated version
Required IAM Permissions
To send events to AWS EventBridge, the Arthur IAM credentials or role will require the events:PutEvents permission. Here is an example policy that grants that permission on an EventBridge bus called arthur-events in the us-east-2 region, in the 0123456789 AWS account.
JSON{
""Statement"": [
{
""Action"": ""events:PutEvents"",
""Effect"": ""Allow"",
""Resource"": ""arn:aws:events:us-east-2:0123456789:event-bus/arthur-events"",
""Sid"": """"
}
],
""Version"": ""2012-10-17""
}
Updated 3 months ago Table of Contents
Event Format
Logged Endpoints
Integration with EventBridge
Configuration
Required IAM Permissions",arthur_scope_docs,https://docs.arthur.ai/docs/audit-log
114,"Online Kubernetes Cluster (K8s) Install
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Online Kubernetes Cluster (K8s) InstallSuggest EditsMake sure your K8s cluster is ready for Arthur platform installation by following the Kubernetes Preparation guide.
Download Installation Files
Go to the download portal using the URL and the password provided by Arthur.
Click the ""Download license"" button to download your license in the YAML file.
Setup for Installation
Make sure you're in the correct kubectl environment context before running the installer.
Install the KOTS kubectl extension on your local machine:
Shellcurl https://kots.io/install  bash
Start Installation
Run the Admin Console installer and login on to your browser at localhost:8800 via the provided port forwarding tunnel:
Shellkubectl kots install arthur
For Namespace-Scoped Installs, follow this {doc}guide <k8s_install_namespace_scoped>.
When you need to re-create the tunnel to Admin Console, run:
Shellkubectl kots admin-console --namespace <your_name_space>
Upload your license file.
On the following screen, click on the link to install Arthur from the Internet.
Configure Arthur.
Review the preflight checks to make sure that your machine meets the minimum requirements before you proceed with the installation.
Verify Installation
Monitor the admin console dashboard for the application status to become Ready.
To see the progress of the deployment, monitor the deployment status with thekubectl CLI:
Shellkubectl get deployment,statefulset,pod -n <yournamespace>
If anything is showing Pending, it is likely you need to add more/bigger nodes to your cluster.
Customize Installation
Configure graphs on Admin Console by clicking on the Configure graphs button and providing your Prometheus endpoint (e.g.,http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090).Updated 3 months ago Table of Contents
Download Installation Files
Setup for Installation
Start Installation
Verify Installation
Customize Installation",arthur_scope_docs,https://docs.arthur.ai/docs/online-kubernetes-cluster-k8s-install
115,"Querying Data Drift
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Querying Data DriftSuggest EditsQuerying Drift in Python
The basic format of a drift query using the Python SDK involves specifying that the
query_type parameter has the value 'drift':
Pythonquery = {...}
arthur_model.query(query, query_type='drift')
Data Drift Endpoint
Data drift has a dedicated endpoint at Query Data Drift.
Returns the data drift metric between a base dataset with a target dataset. This endpoint can support up to 100 properties in one request.
num_bins - Specifies the granularity of bucketing for continuous distributions and will be ignored if the attribute is categorical.
metric - Specify one metric among {ref}the data drift metrics Arthur offers <glossary_data_drift>.
filter - Optional blocks specific to either reference or inference set to specify which data should be used in the data drift calculation.
group_by - Global and applies to both the base and target data.
rollup - Optional parameter aggregating the calculated data drift value by the supported time dimension.
For HypothesisTest, the returned value is transformed as -log_10(P_value) to maintain directional parity with the other data drift metrics. A lower P_value is more significant and implies data drift, reflected in a higher -log_10(P_value). Further mathematical details are in the Glossary
Query Request:
JSON{
""properties"": [
""<attribute1_name> [string]"",
""<attribute2_name> [string]"",
""<attribute3_name> [string]""
],
""num_bins"": ""<num_bins> [int]"",
""metric"": ""[PSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest]"",
""base"": {
""source"": ""[inferencereference]"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""target"": {
""source"": ""[inferencereferenceground_truth]"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""group_by [Optional]"": [
{
""property"": ""<group_by_attribute_name> [string]""
}
],
""rollup [Optional]"": ""minutehourdaymonthyearbatch_id""
}
Query Response:
JSON{
""query_result"": [
{
""<attribute1_name>"": ""<attribute1_data_drift> [float]"",
""<attribute2_name>"": ""<attribute2_data_drift> [float]"",
""<attribute3_name>"": ""<attribute3_data_drift> [float]"",
""<group_by_attribute_name>"": ""<group_by_attribute_value> [stringintnull]"",
""rollup"": ""<rollup_attribute_value> [stringnull]""
}
]
}
Example: Reference vs. Inference
Sample Request: Calculate data drift for males, grouped by country, rolled up by hour.
JSON{
""properties"": [
""age""
],
""num_bins"": 10,
""metric"": ""PSI"",
""base"": {
""source"": ""reference"",
""filter"": [
{
""property"": ""gender"",
""comparator"": ""eq"",
""value"": ""male""
}
]
},
""target"": {
""source"": ""inference"",
""filter"": [
{
""property"": ""gender"",
""comparator"": ""eq"",
""value"": ""male""
},
{
""property"": ""inference_timestamp"",
""comparator"": ""gte"",
""value"": ""2020-07-22T10:00:00Z""
},
{
""property"": ""inference_timestamp"",
""comparator"": ""lt"",
""value"": ""2020-07-23T10:00:00Z""
}
]
},
""group_by"": [
{
""property"": ""country""
}
],
""rollup"": ""hour""
}
Sample Response:
JSON{
""query_result"": [
{
""age"": 2.3,
""country"": ""Canada"",
""rollup"": ""2020-07-22T10:00:00Z""
},
{
""age"": 2.4,
""country"": ""United States"",
""rollup"": ""2020-07-22T10:00:00Z""
}
]
}
Example: Inference vs. Inference
Sample Request: Compare data drift between two batches, with no grouping, filters, or rollups.
JSON{
""properties"": [
""age""
],
""num_bins"": 10,
""metric"": ""PSI"",
""base"": {
""source"": ""inference"",
""filter"": [
{
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""5""
}
]
},
""target"": {
""source"": ""inference"",
""filter"": [
{
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""6""
}
]
}
}
Sample Response:
JSON{
""query_result"": [
{
""age"": 2.3
}
]
}
back to top
Example: Reference vs. Ground Truth
Sample Request: Calculate data drift for individual ground truth class prediction probabilities, rolled up by hour.
JSON{
""properties"": [
""gt_1""
],
""num_bins"": 10,
""metric"": ""PSI"",
""base"": {
""source"": ""reference""
},
""target"": {
""source"": ""ground_truth"",
""filter"": [
{
""property"": ""ground_truth_timestamp"",
""comparator"": ""gte"",
""value"": ""2020-07-22T10:00:00Z""
},
{
""property"": ""ground_truth_timestamp"",
""comparator"": ""lt"",
""value"": ""2020-07-23T10:00:00Z""
}
]
},
""rollup"": ""hour""
}
Sample Response:
JSON{
""query_result"": [
{
""gt_1"": 0.03,
""rollup"": ""2020-07-22T10:00:00Z""
},
{
""gt_1"": 0.4,
""rollup"": ""2020-07-22T11:00:00Z""
}
]
}
back to top
Data Drift PSI Bucket Table Values
This metric has a dedicated endpoint at Query PSI Bucket Table.
Returns the PSI scores by bucket using the reference set data. This query for this endpoint omits the need for metric and takes in a single property but otherwise is identical to the data drift endpoint
Note when using this endpoint with categorical features, the bucket_min and bucket_max fields will not be
returned in the response. Instead, the bucket field will contain the category name.
Query Request:
JSON{
""property"": ""<attribute_name> [string]"",
""num_bins"": ""<num_bins> [int]"",
""base"": {
""source"": ""[inferencereference]"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""target"": {
""source"": ""[inferencereference]"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""group_by [Optional]"": [
{
""property"": ""<group_by_attribute_name> [string]""
}
],
""rollup [Optional]"": ""minutehourdaymonthyearbatch_id""
}
Query Response:
JSON{
""query_result"": [
{
""bucket"": ""string"",
""rollup"": ""stringnull"",
""group_by_property_1"": ""stringnull"",
""base_bucket_max"": ""number"",
""base_bucket_min"": ""number"",
""base_count_per_bucket"": ""number"",
""base_ln_probability_per_bucket"": ""number"",
""base_probability_per_bucket"": ""number"",
""base_total"": ""number"",
""target_bucket_max"": ""number"",
""target_bucket_min"": ""number"",
""target_count_per_bucket"": ""number"",
""target_ln_probability_per_bucket"": ""number"",
""target_probability_per_bucket"": ""number"",
""target_total"": ""number"",
""probability_difference"": ""number"",
""ln_probability_difference"": ""number"",
""psi"": ""number""
}
]
}
Sample Request: Calculate data drift bucket components for males, grouped by country, rolled up by hour.
JSON{
""property"": ""age"",
""num_bins"": 2,
""base"": {
""source"": ""reference"",
""filter"": [
{
""property"": ""gender"",
""comparator"": ""eq"",
""value"": ""male""
}
]
},
""target"": {
""source"": ""inference"",
""filter"": [
{
""property"": ""gender"",
""comparator"": ""eq"",
""value"": ""male""
},
{
""property"": ""inference_timestamp"",
""comparator"": ""gte"",
""value"": ""2020-07-22T10:00:00Z""
},
{
""property"": ""inference_timestamp"",
""comparator"": ""lt"",
""value"": ""2020-07-23T10:00:00Z""
}
]
},
""group_by"": [
{
""property"": ""country""
}
],
""rollup"": ""hour""
}
Sample Response:
JSON{
""query_result"": [
{
""bucket"": ""bucket_1"",
""rollup"": ""2020-01-01T00:00:00Z"",
""country"": ""Canada"",
""base_bucket_max"": 0.9999971182990177,
""base_bucket_min"": 0.5009102069226075,
""base_count_per_bucket"": 4988,
""base_ln_probability_per_bucket"": -0.6955500651756032,
""base_probability_per_bucket"": 0.4988,
""base_total"": 10000,
""target_bucket_max"": 0.9999971182990177,
""target_bucket_min"": 0.5009102069226075,
""target_count_per_bucket"": 2487,
""target_ln_probability_per_bucket"": -0.6701670131762315,
""target_probability_per_bucket"": 0.5116231228142357,
""target_total"": 4861,
""probability_difference"": -0.012823122814235699,
""ln_probability_difference"": -0.025383051999371742,
""psi"": 0.00032548999318807485
},
{
""bucket"": ""bucket_2"",
""rollup"": ""2020-01-01T00:00:00Z"",
""country"": ""United States"",
""base_bucket_max"": 0.9999971182990177,
""base_bucket_min"": 0.5009102069226075,
""base_count_per_bucket"": 4988,
""base_ln_probability_per_bucket"": -0.6955500651756032,
""base_probability_per_bucket"": 0.4988,
""base_total"": 10000,
""target_bucket_max"": 0.9999971182990177,
""target_bucket_min"": 0.5009102069226075,
""target_count_per_bucket"": 2487,
""target_ln_probability_per_bucket"": -0.6701670131762315,
""target_probability_per_bucket"": 0.5116231228142357,
""target_total"": 4861,
""probability_difference"": -0.012823122814235699,
""ln_probability_difference"": -0.025383051999371742,
""psi"": 0.00032548999318807485
},
{
""bucket"": ""bucket_1"",
""rollup"": ""2020-01-01T01:00:00Z"",
""country"": ""Canada"",
""base_bucket_max"": 0.9999971182990177,
""base_bucket_min"": 0.5009102069226075,
""base_count_per_bucket"": 4988,
""base_ln_probability_per_bucket"": -0.6955500651756032,
""base_probability_per_bucket"": 0.4988,
""base_total"": 10000,
""target_bucket_max"": 0.9999971182990177,
""target_bucket_min"": 0.5009102069226075,
""target_count_per_bucket"": 2487,
""target_ln_probability_per_bucket"": -0.6701670131762315,
""target_probability_per_bucket"": 0.5116231228142357,
""target_total"": 4861,
""probability_difference"": -0.012823122814235699,
""ln_probability_difference"": -0.025383051999371742,
""psi"": 0.00032548999318807485
},
{
""bucket"": ""bucket_2"",
""rollup"": ""2020-01-01T01:00:00Z"",
""country"": ""United States"",
""base_bucket_max"": 0.9999971182990177,
""base_bucket_min"": 0.5009102069226075,
""base_count_per_bucket"": 4988,
""base_ln_probability_per_bucket"": -0.6955500651756032,
""base_probability_per_bucket"": 0.4988,
""base_total"": 10000,
""target_bucket_max"": 0.9999971182990177,
""target_bucket_min"": 0.5009102069226075,
""target_count_per_bucket"": 2487,
""target_ln_probability_per_bucket"": -0.6701670131762315,
""target_probability_per_bucket"": 0.5116231228142357,
""target_total"": 4861,
""probability_difference"": -0.012823122814235699,
""ln_probability_difference"": -0.025383051999371742,
""psi"": 0.00032548999318807485
}
]
}
Sample Request: Compare data drift bucket components between two batches, with no grouping, no filters, and no rollups.
JSON{
""property"": ""age"",
""num_bins"": 10,
""base"": {
""source"": ""inference"",
""filter"": [
{
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""5""
}
]
},
""target"": {
""source"": ""inference"",
""filter"": [
{
""property"": ""batch_id"",
""comparator"": ""eq"",
""value"": ""6""
}
]
}
}
Sample Response:
JSON{
""query_result"": [
{
""bucket"": ""bucket_1"",
""base_bucket_max"": 0.9999971182990177,
""base_bucket_min"": 0.5009102069226075,
""base_count_per_bucket"": 4988,
""base_ln_probability_per_bucket"": -0.6955500651756032,
""base_probability_per_bucket"": 0.4988,
""base_total"": 10000,
""target_bucket_max"": 0.9999971182990177,
""target_bucket_min"": 0.5009102069226075,
""target_count_per_bucket"": 2487,
""target_ln_probability_per_bucket"": -0.6701670131762315,
""target_probability_per_bucket"": 0.5116231228142357,
""target_total"": 4861,
""probability_difference"": -0.012823122814235699,
""ln_probability_difference"": -0.025383051999371742,
""psi"": 0.00032548999318807485
},
{
""bucket"": ""bucket_2"",
""base_bucket_max"": 0.9999971182990177,
""base_bucket_min"": 0.5009102069226075,
""base_count_per_bucket"": 4988,
""base_ln_probability_per_bucket"": -0.6955500651756032,
""base_probability_per_bucket"": 0.4988,
""base_total"": 10000,
""target_bucket_max"": 0.9999971182990177,
""target_bucket_min"": 0.5009102069226075,
""target_count_per_bucket"": 2487,
""target_ln_probability_per_bucket"": -0.6701670131762315,
""target_probability_per_bucket"": 0.5116231228142357,
""target_total"": 4861,
""probability_difference"": -0.012823122814235699,
""ln_probability_difference"": -0.025383051999371742,
""psi"": 0.00032548999318807485
}
]
}
back to top
Data Drift for Classification Outputs
For classification outputs, one may want to examine drift among a collection of different classes, i.e., the system of outputs, instead of the drift of the probability predictions of a single class. The query uses one of ""predicted_classes"": [""*""] or ""ground_truth_classes"": [""*""] but otherwise is identical to a standard data drift query. Rather than using the star operator to select all prediction or ground truth classes, respectively, in a model, a list of string classes can be provided for looking at the drift of a subset of multiclass outputs.
predicted_classes - Specifies which prediction classes to use for predictedClass data drift.
ground_truth_classes - Specifies which prediction classes to use for groundTruthClass data drift.
properties can be included in the same query as long as the target sourcecorresponds to the classification output tag. For example, one can query drift on input attributes and predictedClass in the same query with target source of inference; one can query drift on individual ground truth labels and groundTruthClass in the same query with target source of ground_truth.
Query Request:
JSON{
""properties [Optional]"": [
""<attribute1_name> [string]"",
""<attribute2_name> [string]"",
""<attribute3_name> [string]""
],
""[predicted_classesground_truth_classes]"": [
""<class0_name> [string]""
""<class1_name> [string]""
],
""num_bins"": ""<num_bins> [int]"",
""metric"": ""[PSIKLDivergenceJSDivergenceHellingerDistanceHypothesisTest]"",
""base"": {
""source"": ""[inferencereference]"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""target"": {
""source"": ""[inferencereferenceground_truth]"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""group_by [Optional]"": [
{
""property"": ""<group_by_attribute_name> [string]""
}
],
""rollup [Optional]"": ""minutehourdaymonthyearbatch_id""
}
Query Response:
JSON{
""query_result"": [
{
""<attribute1_name>"": ""<attribute1_data_drift> [float]"",
""<attribute2_name>"": ""<attribute2_data_drift> [float]"",
""<attribute3_name>"": ""<attribute3_data_drift> [float]"",
""[predictedClassgroundTruthClass]"": ""<classification_data_drift> [float]"",
""<group_by_attribute_name>"": ""<group_by_attribute_value> [stringintnull]"",
""rollup"": ""<rollup_attribute_value> [stringnull]""
}
]
}
Sample Request: Calculate data drift on all prediction classes.
JSON{
""predicted_classes"": [
""*""
],
""num_bins"": 20,
""base"": {
""source"": ""reference""
},
""target"": {
""source"": ""inference""
},
""metric"": ""PSI""
}
Sample Response:
JSON{
""query_result"": [
{
""predictedClass"": 0.021
}
]
}
Sample Request: Calculate data drift on ground truth using the first and third ground truth classes.
JSON{
""predicted_classes"": [
""gt_1"",
""gt_3""
],
""num_bins"": 20,
""base"": {
""source"": ""reference""
},
""target"": {
""source"": ""ground_truth""
},
""metric"": ""PSI""
}
Sample Response:
JSON{
""query_result"": [
{
""groundTruthClass"": 0.021
}
]
}
back to top
Automated Data Drift Thresholds
What is a sufficiently high data drift value to suggest that the target data has actually drifted from the base data? For HypothesisTest, we can reverse engineer -log_10(P_value) and plug in the conventional .05 alpha level to establish a lower bound of -log_10(.05).
For the other data drift metrics, pining a constant is insufficient. We abstract this away for the user and allow queries to obtain automatically generated data drift thresholds (lower bounds) based on a model's data. These thresholds can be used in alerting. For more information, see: Automating Data Drift Thresholding in Machine Learning Systems.
The query uses the""metric"": ""Thresholds"" and does not require nor use ""target"" and ""rollup"" fields but otherwise is identical to a standard data drift query.
Query Request:
JSON{
""properties"": [
""<attribute1_name> [string]"",
""<attribute2_name> [string]"",
""<attribute3_name> [string]""
],
""num_bins"": ""<num_bins> [int]"",
""metric"": ""Thresholds"",
""base"": {
""source"": ""reference"",
""filter [Optional]"": [
{
""property"": ""<filter_attribute_name> [string]"",
""comparator"": ""<comparator> [string]"",
""value"": ""<filter_threshold_value> [stringintfloat]""
}
]
},
""group_by [Optional]"": [
{
""property"": ""<group_by_attribute_name> [string]""
}
]
}
Query Response:
JSON{
""query_result"": [
{
""<attribute1_name>"": {
""HellingerDistance"": ""<threshold> [float]"",
""JSDivergence"": ""<threshold> [float]"",
""KLDivergence"": ""<threshold> [float]"",
""PSI"": ""<threshold> [float]""
},
""<attribute2_name>"": {
""HellingerDistance"": ""<threshold> [float]"",
""JSDivergence"": ""<threshold> [float]"",
""KLDivergence"": ""<threshold> [float]"",
""PSI"": ""<threshold> [float]""
}
}
]
}
Sample Request:
JSON{
""properties"": [
""AGE""
],
""num_bins"": 20,
""base"": {
""source"": ""reference""
},
""metric"": ""Thresholds""
}
Sample Response:
JSON{
""query_result"": [
{
""AGE"": {
""HellingerDistance"": 0.00041737395239735647,
""JSDivergence"": 2.959228131592643,
""KLDivergence"": 0.001893866910388703,
""PSI"": 0.0018945640055550161
}
}
]
}
back to topUpdated 3 months ago Table of Contents
Querying Drift in Python
Data Drift Endpoint
Data Drift PSI Bucket Table Values
Data Drift for Classification Outputs
Automated Data Drift Thresholds",arthur_scope_docs,https://docs.arthur.ai/docs/querying-data-drift
116,"Alert Summary Reports
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Alert Summary ReportsSuggest EditsAn alert summary is a way for teams to send alert reports about different models in their organization to members of their internal team or external stakeholders interested in alert governance.
What Is Sent Out with An Alert Summary?
An alert summary report contains information about the status of alerts triggered for a particular model.
What Needs to Be Configured?
An alert summary report in Arthur is an aggregated report that can be shared with members of your organization on some regular cadence. The report can be configured to control the following:
Name of Your Alert Summary Report: Teams can create as many unique alert summary configurations as they want. Each alert configuration will have a unique ID, which will be used when maintaining the report (keeping up-to-date subscriber lists, maintaining models to be alerted on, etc).
Which Arthur Models Are Included: List of Arthur Model UUIDs
Who Gets Sent the Report: known as a subscriber, this contains email addresses of users who would like to receive the report
Frequency of Sent Reports: There are currently two options: daily and weekly. For weekly, teams can also configure what day of the week they would like to receive the report.
Time of Day to Send Report: Do you want to receive the report on Monday mornings at a specific time? Set up the time of day you would like your report to be sent
This can be configured through the Create Alert Summary Configuration.
This will return:
Alert Summary Configuration ID: This unique ID is used when maintaining the report (keeping up-to-date subscriber lists, maintaining models to be alerted on, etc.).
Maintaining Your Alert Summary Report
After creating a specific alert summary report, teams can maintain these reports to keep them current. A few common API calls and actions that teams will make include:
Adding and Deleting Alert Summary Subscribers: Teams will use these API calls to add new subscribers
and remove subscribers from specific alert summary configurations.
Updating Alert Summary Configuration: Another piece of alert summary maintenance is being able to make changes to an existing configuration. One of the most common reasons teams make changes is to add in a newly onboarded model to the group's alert summary. This can be done with the Update Alert Summary Configuration API call.
Pulling Reports on Your Alert Summary Report: The level of Meta-governance often needed in governance systems. Teams can call the Arthur API to pull reports on their alert summary configurations, such as lists of alert summaries past ,
get subscribers,, and subscriber notification configurations.
Updated 3 months ago Table of Contents
What Is Sent Out with An Alert Summary?
What Needs to Be Configured?
Maintaining Your Alert Summary Report",arthur_scope_docs,https://docs.arthur.ai/docs/alert-summary-3
117,"ServiceNow
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by ServiceNowService Now Integration GuideSuggest EditsWith the Arthur + ServiceNow integration, you can set up email automation to notify on-call teams in ServiceNow of alerts Arthur triggers. To set up this integration, follow these steps:
Step 1: Set up your email integration in ServiceNow
An inbound email action in ServiceNow can be configured to receive Arthur alerts and create incidents in response to those alerts. Please see the ServiceNow Email Actions Guide for details on how to do this. Once you have set up an email action to handle incoming Arthur alerts and generate incidents from those alerts, retrieve the ServiceNow instance email address for Step 2.
Step 2: Configure your integration in Arthur
To configure the ServiceNow integration for a model in Arthur, you can send a POST request to the/alert_notification_configurations.
model_id - UUID of the model this alert notification configuration belongs to.
type - Type of notification to send. In this case, ""ServiceNow"".
destination - The integration email address obtained in Step 1.
enabled - Whether or not the notification configuration is enabled. It defaults to true.
Example Query Request:
JSON{
""model_id"" : ""<model_id> [string]"",
""type"" : ""[ServiceNow]"",
""destination"" : ""<[email protected]> [string]"",
}
For more information on alert notifications, please see the notifications section of the
Alerting page.
Step 3: Start monitoring!
Your integration is now ready to use! When an alert is triggered in Arthur for this model, an incident will be created in your ServiceNow instance.Updated 3 months ago Table of Contents
Step 1: Set up your email integration in ServiceNow
Step 2: Configure your integration in Arthur
Step 3: Start monitoring!",arthur_scope_docs,https://docs.arthur.ai/docs/servicenow
118,"Arthur Scope FAQ
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Arthur Scope FAQSuggest Edits1. Can I use Arthur Scope without using the Python SDK?
Yes! The Arthur Scope platform is API-first. You can use our Arthur API to onboard models, send predictions, and query metrics and insights.
2. Does Arthur need a copy of my model?
Arthur doesn’t generally need access to your actual model but only captures the inputs to the model and the predictions it makes. This means that you can even use Arthur to monitor models you cannot access, such as models hosted by third-party services.
To enable explainability, Arthur does need access to your model. When enabling explainability, you will need to provide access to the modelspredict function.
3. What if my data is proprietary? Can I still use Arthur?
Yes! Arthur offers on-premises installation for customers with data security requirements. By integrating Arthur into your business's on-premises stack, you can be confident that all security requirements are met while still getting the benefits of the computation and analytics Arthur provides.
4. What if I don’t have ground truth labels for my data? Or what if I will have the ground truth labels in the future, but they are not available yet?
You don't need ground truth labels to log your model's inferences with Arthur.
If your ground truth labels become available after your model's inferences, whether seconds later or years later, Arthur can link these new ground truth values to your model's past predictions, linking the new values by ID to their corresponding inferences already in the Arthur system.
In the meantime, Arthur’s data drift metrics can offer leading indicators of model underperformance to keep you covered if your ground truth labels are delayed or never become available.
5. I got an error using the SDK. What do I do?
If the error message says ""an internal exception occurred, please report to Arthur"" that means there was a problem on our side. Please email the Arthur support team at [email protected] to let us know what happened.
Otherwise, the error message should provide helpful instructions for how to resolve the issue. If you don’t find the error message actionable, please let Arthur know so we can improve it.
6. Do I have to type my credentials in every time I use the SDK?
No! Instead of manually entering them, you can specify an ARTHUR_ENDPOINT_URL and ARTHUR_API_KEY environment variable to be used to create the ArthurAI connection object.
7. What are streaming and batch models?
Streaming and batch are two model types with different patterns of ingesting data to send to Arthur.
A streaming model processes data as a stream of individual inferences: data is logged with Arthur directly as individual inferences when the data flows into the model.
A batch model processes data as a sequence of grouped inferences, which are usually grouped over time: data is logged with Arthur as a group of inferences as the model processes the batch.
8. Which drift metric should I use?
Population stability index (PSI) is typically a good default drift metric.
In some cases, one wants a drift metric with a certain property, e.g., using a drift metric with the unit nats for interpretability or using a drift metric bounded between 0 and 1 so that drift values don't increase arbitrarily for outliers. In these cases, other metrics may be preferable to PSI.
For a review of the data drift metrics Arthur offers and their properties, see the data drift section of our
Glossary. Furthermore, see our blog post for an overview of data on how Arthur automates the choice of thresholding for drift metrics.Updated about 2 months ago Table of Contents
1. Can I use Arthur Scope without using the Python SDK?
2. Does Arthur need a copy of my model?
3. What if my data is proprietary? Can I still use Arthur?
4. What if I don’t have ground truth labels for my data? Or what if I will have the ground truth labels in the future, but they are not available yet?
5. I got an error using the SDK. What do I do?
6. Do I have to type my credentials in every time I use the SDK?
7. What are streaming and batch models?
8. Which drift metric should I use?",arthur_scope_docs,https://docs.arthur.ai/docs/arthur-faq
119,"Backup and Restore
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Backup and RestoreSuggest EditsWARNINGS
🚧Only tested on AWS, as writtenThese instructions have been tested as written for an AWS deployment. If you find they do not work for your use-case, please reach out to Arthur Support before modifying them. We cannot guarantee reliable operation if these instructions are not followed exactly as written.
🚧Ensure no network connection between backup/restore environmentsWhen restoring into a new cluster, you must ensure that the new cluster is unable to communicate with any services or data store in the old cluster.
If you took a backup on cluster Apple, and performed a restore into cluster Banana, cluster Banana must point to its own RDS Instance, ClickHouse Database, and Kafka Store (note: it is ok if clusters share the S3 bucket).
To ensure this, you must re-configure via the Admin Interface when restoring into a new cluster. Failure to do this WILL CAUSE DATA CORRUPTION on both clusters that is unrecoverable.
🚧Backup everything at the same timeIf you are either manually taking a backup or scheduling a backup, you MUST take a backup of the full platform. You CANNOT use a ClickHouse snapshot taken at midnight with an RDS snapshot taken at 0400 AM (or any other time). All backup operations must be performed at the same time, and when restoring, the data you are using must all belong to the same backup operation. This is to ensure data consistency across the different data stores. IGNORING THIS WILL CAUSE DATA CORRUPTION.
Overview
The overall backup and restore process for the Arthur Platform is as follows:
Backing up the Arthur platform
Take a backup of ClickHouse Data
Take a backup of Kubernetes Deployment State and Persistent Volumes
Enrichments infrastructure
Model Servers
Data Pipeline Services
Enrichment / Delete Enrichment Workflows
Kafka Deployment State and EBS Volumes (using EBS Snapshots)
Take a backup of RDS Postgres
Restore the Arthur platform
Restore RDS Postgres
Update configuration and install the platform
Restore ClickHouse Data
Restore the Kafka Deployment State and Persistent Volumes
Restore Enrichments infrastructure
Restore Workflows
Smoke Tests and Validation
Overview - clickhouse-backup
The Arthur Platform stores inference data, data built from the enrichments pipeline, reference and ground truth data in ClickHouse. ClickHouse is an open-source OLAP Database which enables SQL-like query execution, replication, sharding and many additional features.
To backup ClickHouse, the Arthur Platform uses a tool called clickhouse-backup. clickhouse-backup is a sidecar-container included on the ClickHouse pods and is responsible for taking backups, performing restores, and coordinating with remote storage (in this case S3) to store and retrieve backups. clickhouse-backup uses built-in functionality of ClickHouse to take backups and perform restores.
Overview - Velero
The Arthur Platform uses Velero, which is an industry-standard, battle-tested tool for backing up Kubernetes Resources including Persistent Volumes.
Arthur uses Velero to backup necessary namespaced Kubernetes resources, as well as the EBS Volume Snapshot backups for each PersistentVolumes claimed by the StatefulSets (eg: via PVCs).
Backup data (not including EBS Volume Snapshots) is stored in an S3 bucket which is accessible via a ServiceAccount that is provisioned for the Backup and Restore agent. Backups and Restores are managed by Velero using Kubernetes Custom Resource Definitions (CRDs), which are consumed by the Velero Backup Controller.
Velero has a feature which also allows backups to be scheduled, using a cron-like configuration. It also provides ServiceMonitors which expose metrics via Prometheus, so that operators can monitor backup and restore status and set up alerts for when backup or restore operations fail.
Overview - Arthur (Argo)Workflows
The Arthur Platform uses Argo Workflows as a workflow orchestration engine for running certain jobs. Argo installs a handful of Custom Resource Definitions (CRDs) which enable the Argo Workflow services to schedule, execute and update these jobs.
Workflows are dynamically managed, meaning that their definitions are not stored in the Arthur installer script. The Backup and Restore operation accounts for this by treating restoration of Workflows on a case-by-case basis, as follows:
Enrichments and Delete Enrichments workflows
These workflows are created to stand-up and tear-down infrastructure necessary for processing enrichments data (eg: kafka topics, pods which manage the data pipeline for enrichments, etc.)
These workflows are idempotent and safe to recover
Therefore, these workflows are backed up and restored just like any other Kubernetes Resource during the backup stage
Batch workflows
These workflows are created to manage batch jobs, which are used by clients when uploading large data files to models (inferences and/or ground truths).
These workflows are sometimes safe to recover
Therefore, these workflows are restored selectively based on what state they were in when the backup was taken
Workflows for which Arthur received all the data from the client are resumed by manually re-submitting them (this is done via an Administrative HTTP endpoint that needs to be invoked manually)
Workflows for which Arthur did not receive all the data from the client will need to be re-submitted. Operators restoring the cluster will need to reach out to affected clients to communicate that their batch workflows should be re-submitted.
Reference and Cron Workflows
Reference Workflows are created for monitoring the upload of reference datasets to S3
Reference datasets that were in-flight during a backup will need to be re-uploaded via the SDK.
Cron Workflows are scheduled workflows which perform some regular processing (eg: triggering alerts for non-batch inferences)
Cron Workflows are meant to be run on a regular schedule. It is safe to wait for the next workflow to be triggered, and therefore, these workflows are not backed up nor restored.
Overview - S3
The Arthur Platform uses AWS S3 object storage for storing inference data, reference data, as well as data and trained models for the enrichments pipeline.
Arthur recommends enabling Cross-Region Replication on the AWS S3 buckets, so that objects are available in the rare event of an AWS region outage.
The Arthur Backup solution does not manage consistency with the S3 bucket and other backup data.
The data in S3 is only used in conjunction with data that is stored in Postgres (eg: model definitions), so it's ok if there's data in S3 that isn't represented in Postgres.
Therefore, the S3 bucket for a cluster will always reflect the most up-to-date state, regardless of when a backup was taken.Updated 3 months ago Table of Contents
WARNINGS
Overview
Overview - clickhouse-backup
Overview - Velero
Overview - Arthur (Argo)Workflows
Overview - S3",arthur_scope_docs,https://docs.arthur.ai/docs/backup-and-restore
120,"Welcome to Arthur Scope!
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchDiscardSubmit Suggested EditsWelcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by SuggestAs your team's data science operations center, Arthur helps enterprise teams monitor, measure and optimize AI performance at scale.",arthur_scope_docs,https://docs.arthur.ai/edit/getting-started
121,"Fairness Metrics
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Fairness MetricsMonitor and track fairness metrics in production to take action on underperformance in sensitive segments.Suggest EditsEnabling Fairness Sections
A dedicated fairness section within the UI enables teams to track fairness performance between groups easily. The UI does not infer these trackable groups and must be explicitly defined. For attributes to show up (and be tracked) within the fairness section of the dashboard, they must be enabled for bias monitoring. This can be done with the Python SDK and is further explained in Enabling Enrichments under bias mitigation.
Tracking Fairness in the UI
Marked sensitive attributes are tracked in the Fairness section in the model's Overview Tab. This section operationalizes systematic comparisons for critical groups.
Metrics: Easily compare different accuracy rates between groups by selecting from standard fairness metrics within our drop-down selection. These metrics include:
MetricDescriptionAccuracy Ratethe proportion of correctly classified instances out of the total number of instancesTrue Positive Ratethe proportion of actual positive instances correctly identified by a machine learning model out of the total number of actual positive instancesTrue Negative Ratethe proportion of actual negative instances correctly identified by a machine learning model out of the total number of actual negative instancesFalse Positive Ratethe proportion of actual negative instances that are incorrectly classified as positive by a machine learning model out of the total number of actual negative instancesFalse Negative Ratethe proportion of actual positive instances incorrectly classified as negative by a machine learning model out of the total number of positive instances
Baseline: Within Arthur, the fairness section allows comparisons within groups of different attributes of interest. The baseline group is the group that all other groups in that attribute will be compared against. This selection is made by clicking on the Set Baseline button, and the selected group can be seen in the fairness table under the baseline column for each comparison.
Fairness Threshold Toggle: A fairness threshold is the acceptable rate of disparate performance. An appropriate fairness threshold heavily depends on the team and use case, so Arthur does not apply strict parameters. Instead, teams can toggle the threshold to their model's acceptable disparate performance rate.
Fairness Status: Based on the threshold provided, a visual representation of whether a group's performance rate has passed the allowed threshold is provided. A green check represents that all groups' performance for the rate selected is within the threshold compared to the baseline group. On the other hand, red exclamation points mean that one or more comparison groups have points beyond the specified fairness threshold.
Tracking Attribute Disparity
By default, attribute disparity rates are tracked over time.
Using Snap Shot Mode
While tracking performance over time is incredibly helpful for debugging or evaluating when disparate impact occurred, another popular way teams use the fairness section in reporting is with Snapshot mode. Snapshot mode is a toggle at the top of the UI that converts all charts from time series to average bar charts (for the time range selected in the Global Filters). This allows teams to easily create shareable charts for reports on the average impact rate between different groups.
Querying Fairness in a Notebook
Beyond the metrics enabled in the Arthur UI, Arthur can query additional fairness metrics in a notebook. Using the bias metrics submodule, teams can call demographic_parity, group_confusion_matrices, or group_positivity_rates to be calculated on a specified attribute.
Pythonarthur_model.bias.metrics.demographic_parity('<attr_name>')
arthur_model.bias.metrics.group_confusion_matrices('<attr_name>')
A description of these bias metrics:
MetricDescriptionDemographic ParityGet group-conditional positivity rates for all inferences, with the option to filter for a batch_id or a particular chunk of time.Group Confusion MatricesGet group-conditional confusion matrices for all inferences, with the option to filter for a batch_id or a particular chunk of time.Group Positivity RatesGet group-conditional positivity rates for all inferences, with the option to filter for a batch_id or a particular chunk of time.
Available Model Types
Since fairness metrics are calculated with accuracy rates, they are only available for classification models within Arthur. Additionally, since fairness metrics are a different visual way of tracking accuracy between sensitive groups, they require ground truth labels.Updated 3 months ago Table of Contents
Enabling Fairness Sections
Tracking Fairness in the UI
Tracking Attribute Disparity
Using Snap Shot Mode
Querying Fairness in a Notebook
Available Model Types",arthur_scope_docs,https://docs.arthur.ai/docs/fairness-metrics
122,"Model Metric Definitions
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Model Metric DefinitionsUnderstanding Model Metrics Monitored with ArthurSuggest EditsPerformance
Accuracy Rate
Accuracy is the most common metric for classification tasks. Accuracy is the measure of how many predictions were correct out of all the predictions made.
Accuracy Rate = (# correct predictions) / (total # predictions)
We can also think of accuracy in terms of common confusion matrix rates
Accuracy Rate = (TP + TN) / (TP + TN + FP + FN)
AUC
The Area Under the Curve (AUC) is a metric that measures the performance of a classification model by calculating the area under the receiver operating characteristic (ROC) curve. It provides a single score that summarizes the trade-off between the true positive rate and false positive rate across different thresholds for defining positive cases.
Average Prediction
This is a metric used by regression models. It returns the average prediction your model has output over the
Average Token Likelihood
The token likelihood is a number between 0 and 1 that quantifies the model’s level of surprise that this token was the next predicted token of the sentence. In token sequence models, this metric quantifies the average
In Arthur, this metric is available for token sequence models.
Average Sequence Length
This metric is available for token sequence models. This is the average count of tokens for each inference ingested by Arthur.
Balanced Accuracy Rate
The balanced accuracy rate is a metric used for classification tasks.
Balanced Accuracy Rate = (Sensitivity + Specificity) / 2
Confusion Matrix Rates
A confusion matrix is a table that summarizes the performance of a classification model by comparing its predicted outputs to the true outputs across different classes. It contains information about the number of true positives, true negatives, false positives, and false negatives, which can be used to calculate various metrics for evaluating model performance.
False Negative Rate
The false negative rate is a metric that measures the proportion of actual positive cases that are incorrectly predicted as negative by a classification model. A high false negative rate indicates that the model is missing a significant number of positive cases, which can have serious consequences in some applications.
False Positive Rate
The false positive rate is a metric that measures the proportion of negative cases that are incorrectly predicted as positive by a classification model. A high false positive rate indicates that the model is producing a large number of false alarms, which can be costly or lead to unnecessary actions in some applications.
True Negative Rate
The true negative rate, also known as specificity, is a metric that measures the proportion of actual negative cases that are correctly identified as negative by a classification model. A high true negative rate indicates that the model is able to effectively identify negative cases, which is important in applications where avoiding false positives is critical.
True Positive Rate
The true positive rate, also known as sensitivity or recall, is a metric that measures the proportion of actual positive cases that are correctly identified as positive by a classification model. A high true positive rate indicates that the model is able to effectively identify positive cases, which is especially important in applications where detecting all positive cases is critical.
F1
The F1 score is a single metric that combines precision and recall, two performance metrics used in classification tasks. It ranges from 0 to 1, with higher values indicating better model performance in balancing precision and recall.
F1 = (precision * recall) / (precision + recall)
F1 = TP / (TP + 0.5 * (FP + FN))
Likelihood Stability
This metric is available for token sequence models.
Mean Average Error (MAE)
Mean Absolute Error (MAE) is a metric that measures the average magnitude of the differences between the predicted and actual values of a numerical variable in a regression model. It is calculated as the average absolute difference between the predicted and actual values and is one of the most popular measures of the model's predictive accuracy.
Mean Average Precision
In object detection, Mean Average Precision (MAP) is a commonly used metric that measures the precision and recall of a model in detecting objects of different classes in an image. It takes into account the localization accuracy of the predicted bounding boxes and is often used to compare the performance of different object detection models.
Recall
Recall, also known as sensitivity or true positive rate, is a metric that measures the proportion of true positive cases among all actual positive cases in a classification model. It provides information about the model's ability to detect positive cases and is especially important in applications where detecting all positive cases is critical.
RSME
The Root Mean Squared Error (RMSE) is a metric that measures the average magnitude of the differences between the predicted and actual values of a numerical variable in a regression model. It is calculated as the square root of the average squared difference between the predicted and actual values and is often used as a measure of the model's predictive accuracy.
R Squared
R-squared, also known as the coefficient of determination, is a statistical metric that measures the proportion of variation in the dependent variable that is explained by the independent variable(s) in a regression model. It ranges from 0 to 1, with higher values indicating a better model fit to the data.
Inference Count
Available for all model types. This is the number of predictions that have been sent to Arthur.
Inference Count = # of predictions
Inference Count by Class
For classification models, this metric counts the number of predictions ingested by Arthur per each class label.
Overall Accuracy Rate
The overall accuracy rate is a metric that measures the proportion of correctly classified cases in a classification model, across all classes. It provides a general sense of how well the model is performing, but it may not capture the performance of individual classes or the cost of misclassification errors.
Precision
Precision is a metric that measures the proportion of true positive cases among all predicted positive cases in a classification model. It provides information about the model's ability to minimize false positives and is especially important in applications where avoiding false positives is critical.
Precision = TP / (TP + FP)
Data Drift
Background
P and Q
We establish some mathematical housekeeping for the below metrics. Let P be the reference distribution and Q
be the target distribution. Binning the underlying reference and target sets can approximate these probability distributions. Generally, P is an older dataset, and Q is a new dataset of interest. We'd like to quantify how far the distributions differ to see if the reference set has gone stale, and algorithms trained on it should not be used to perform inferences on the target dataset.
Entropy
Let H(P) be the entropy of distribution P. It is interpreted as the expected (i.e., average) number of bits (if log base 2) or nats (if log base e) required to encode information of a datapoint from the distribution P. Arthur applications use log base e, so interpretation will be in nats.
KL Divergence
Let D(PQ) be the Kullback-Leibler (KL) Divergence from P to Q. It is interpreted as the nats of information we expect to lose using Q instead of P for modeling data X, discretized over probability space K. KL Divergence is not symmetrical, i.e., D(PQ) does not equal D(QP) and should not be used as a distance metric.
Population Stability Index (PSI)
Let PSI(P,Q) be the Population Stability Index (PSI) between P and Q. It is interpreted as the roundtrip loss of na s of information we expect to lose from P to Q and then from Q returning back to P, and vice versa. PSI smooths out KL Divergence since the return"" trip information loss is included, and this metric is popular in financial applications.
JS Divergence
Let JSD(P,Q) be the Jensen-Shannon (JS) Divergence between P and Q. It smooths out KL divergence using a mixture of the base and target distributions and is interpreted as the entropy of the mixture M = (P+Q)_/2 minus the mixture of the entropies of the individual distributions.
Hellinger Distance
Let HE(P,Q) be the Hellinger Distance between P and Q. It is interpreted as the Euclidean norm of the difference of the square root distributions of P and Q.
Hypothesis Test
Hypothesis testing uses different tests depending on whether a feature is categorical or continuous.
For categorical features, let the equation below be the chi-squared test statistic for P and Q, with K being the number of categories of the feature, i.e., K-1 are the degrees of freedom. Let NPK and NQK be the count of feature occurrences being k, with 1<= k <= K, for P and Q, respectively. The chi-squared test statistic summarizes the standardized differences of expected counts between P and Q.
For continuous features, let KS(P, Q) be the Kolmogorov-Smirnov test statistic for P and Q. Let FP and FQ be the empirical cumulative density for P and Q, respectively. The Kolmogorov-Smirnov test is a nonparametric,
i.e., distribution-free test that compares the empirical cumulative density functions of P and Q.
The returned test statistic is then compared to cutoffs for significance. A higher test statistic indicates more data drift. We've abstracted the calculations away within our query endpoint.
For HypothesisTest, the returned value is transformed as -log_10(P_value) to maintain directional parity with the other data drift metrics. A lower P_value is more significant and implies data drift, reflected in a higher -log_10(P_value).
Fairness
Demographic Parity
Demographic parity is a fairness metric that measures whether the proportion of positive outcomes is the same across different demographic groups in a classification model. It aims to ensure that the model is not systematically biased towards or against certain groups based on demographic characteristics like race or gender.
Equalized Odds
Equalized Odds is a fairness metric that measures the true positive and false positive rates for a given group, such as a protected group defined by a demographic characteristic like race or gender, in a classification model. It ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or unequal treatment issues in both true positive and false positive rates.
Equal Opportunity
Equal Opportunity is a fairness metric that measures the true positive rate for a given group, such as a protected group defined by a demographic characteristic like race or gender, in a classification model. It ensures that the model is not systematically biased against certain groups and can help identify and address potential discrimination or unequal treatment issues.Updated 3 months ago Table of Contents
Performance
Accuracy Rate
AUC
Average Prediction
Average Token Likelihood
Average Sequence Length
Balanced Accuracy Rate
Confusion Matrix Rates
F1
Likelihood Stability
Mean Average Error (MAE)
Mean Average Precision
Recall
RSME
R Squared
Inference Count
Inference Count by Class
Overall Accuracy Rate
Precision
Data Drift
Background
KL Divergence
Population Stability Index (PSI)
JS Divergence
Hellinger Distance
Hypothesis Test
Fairness
Demographic Parity
Equalized Odds
Equal Opportunity",arthur_scope_docs,https://docs.arthur.ai/docs/model-metric-definitions
123,"On-Prem Deployment Requirements
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by On-Prem Deployment RequirementsSuggest EditsGeneral
A DNS hostname
TLS private key & certificate
SMTP server (StartTLS supported)
The minimum compute resource requirements in this documentation is for running a few small models in a non-production environment. Your production deployment will likely use more compute resources to achieve higher availability, performance and scalability.
Arthur’s horizontally elastic architecture allows high throughput processing in both streaming and batch. The platform's auto-scaler mechanism self-manages resource utilization in optimized and cost-effective fashion. It automatically scales up and down based on compute resource requests by the platform activities as well as the lag observed in the data pipeline queue within the limits of the allocated hardware. This works best in a cloud infrastructure with a managed Kubernetes service that enables Arthur to also auto-scale the provisioned hardware (e.g. AWS EKS, Azure ASK).
Storage volumes used for Arthur deployment should be encrypted with a data key using industry-standard data encryption (e.g. AES-256). This applies to the mounted disk volumes as well as the externalized storage, such as the S3 object storage and the relational database if any.
Kubernetes Install
Kubectl-ing workstation: Linux or MacOS
Kubernetes: 1.25 to 1.27
Runtime: containerd or Docker
Namespace
Storage class
Minimum Node Group Resource
16 CPUs
32 GB RAM
Storage with at least 3000 IOPS (>100GB recommended)
Permissions
When Arthur platform is installed, Kubernetes RBAC resources are created to allow the Admin Console to manage the application.
The kubectl-ing user who installs Arthur must have the wildcard privileges in the cluster.
Refer to this documentation for the ClusterRole and ClusterRoleBinding that
will be created for the Admin Console.
Components
Prometheus
Ingress Controller (Nginx or Ambassador)
Kubernetes Metrics Server
Velero with Restic (Optional for managed backup and restore feature)
For Airgapped installation only:
An existing private container registry
Existing private Python registries (PyPI, Anaconda) - only required for the model explanation feature
VM Install
Minimum Server Resource
16 CPUs
32 GB RAM
Storage with at least 3000 IOPS (>100GB recommended)
Supported Operating Systems
The latest versions of the following Linux operating systems are supported.
Ubuntu
RHEL
Please do the following before running the installer on your VM for a smoother deployment experience:
If SELinux is enabled, set it to the permissive mode
Make sure the VM doesn't have any container runtime pre-installed, such as Docker or containerd
Firewall Configurations
Ingress
The TCP port 443 is the only entry point that Arthur exposes.
Egress
The platform requires access to any integrations (e.g. SMTP, IdP) as well as the components you externalize (e.g. Postgres, S3).
For Airgap Installation
Your private container and Python registries must be accessible.
(requirements_for_online_installation)=
For Online Installation
Access to container images and deployment manifest files from the below public registries are required.
HostExisting ClusterEmbedded ClusterDocker HubRequiredRequiredproxy.replicated.comRequiredRequiredreplicated.appRequiredRequiredk8s.kurl.shNot RequiredRequiredamazonaws.comNot RequiredRequiredUpdated about 2 months ago Table of Contents
General
Kubernetes Install
Minimum Node Group Resource
Permissions
Components
VM Install
Minimum Server Resource
Supported Operating Systems
Firewall Configurations
Ingress
Egress",arthur_scope_docs,https://docs.arthur.ai/docs/on-prem-deployment-requirements
124,"Assets Required For Explainability
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Assets Required For ExplainabilitySuggest EditsArthur can automatically calculate explanations (feature importances) for every prediction your model makes. To make this possible, we package up your model in a way that allows us to call it's predict function, which allows us to calculate explanations. We require a few things from your end:
A Python script that wraps your models predict function
For Image models, a second function, load_image is also required (see CV Explainability).
A directory containing the above file, along with any serialized model files, and other supporting code
A requirements.txt with the dependencies to support the above
This guide will set everything up and then use the SDK to enable explainability.
Setting up Project Directory
Project Structure
Here is an example of what your project directory might look like.
-- model_folder/

-- data/


-- training_data.csv


-- testing_data.csv

-- requirements.txt

-- model_entrypoint.py

-- utils.py

-- serialized_model.pkl
Requirements File
Your project requirements and dependencies can be stored in any format you like, such as the typical requirements.txt file, or another form of dependency management.
This should contain all packages your model and predict function need to run.
📘You do not need to include the arthurai package in this requirements file. We supply that
# example_requirements.txt
pandas==0.24.2
numpy==1.16.4
scikit-learn==0.21.3
torch==1.3.1
torchvision==0.4.2
It is advised to pin the specific versions your model requires. If no version is pinned, we will use the latest version. This can cause issues if the latest version is incompatible with the version used to build your model.
Prediction Function
We need to be able to send new inferences to your model to get predictions and generate explanations. For us to have access to your model, you need to create an entrypoint file that defines a predict() method.
The exact name of the file isn't strict, so long as you specify the correct name when you enable explainability (see below). The only thing that does matter is that this file implements a predict() method. In most cases, if you have a previously trained model, this predict() method will likely just invoke the prediction from your trained model.
Python# example_entrypoint.py
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(x)
This predict method can be as simple or complicated as you need, so long as you can go from raw input data to a model output prediction.
Specifically, in the case of a binary classifier, we expect a 2-d array where the first column indicates probability_0 for each input, and the second column indicates probability_1 for each input. In the case of a multiclass classifier with n possible labels, we expect an n-d array where column i corresponds to the predicted probability that each input belongs to class i.
Preprocessing for Prediction
Commonly, a fair amount of feature processing and transformation will need to happen before invoking your actual model.predict(). This might include normalizations, rescaling, one-hot encoding, embedding, etc. Whatever those transformations are, you can make them a part of this predict() method. Alternatively, you can wrap all those transformations into a helper function.
Python# example_entrypoint.py
from utils import pipeline_transformations
sk_model = joblib.load(""./serialized_model.pkl"")
def predict(x):
return sk_model.predict_proba(pipeline_transformations(x))
Enabling Explainability
Enabling explainability can be done using the SDK function arthur_model.enable_explainability, which takes as input a sample of your model's data (to train the explainer), and which takes as input the files that contain your model's predict function and necessary environment.
Pythonarthur_model.enable_explainability(
df=X_train.head(50),
project_directory=""/path/to/model_folder/"",
requirements_file=""requirements.txt"",
user_predict_function_import_path=""model_entrypoint"",
ignore_dirs=[""folder_to_ignore""] # optionally exclude directories within the project folder from being bundled with predict function
)
The above provides a simple example. For a list of all configuration options and details around them, see the explainability section in Enabling Enrichments.
Notes about the above example:
joblib is a Python library allowing you to reconstruct your model from a serialized pickle file.
X_train is your trained model data frame.
user_predict_function_import_path is the Python path to import the entry point file as if you imported it into the Python program running enable_explainability.
Configuration Requirements
When going from disabled to enabled, you will need to include the required configuration settings. Once the explainability enrichment has been enabled, you can update the non-required configuration settings without re-supplying the required fields.
You must not pass in any config settings when disabling the explainability enrichment.
Configuration
SettingRequiredDescriptiondfXThe dataframe passed to the explainer. It should be similar to, or a subset of, the training data. Typically small, ~50-100 rows.project_directoryXThe path to the directory containing your predict function, requirements file, model file, and any other resources needed to support the predict function.user_predict_function_import_pathXThe name of the file containing the predict function. Do not include .py extension. Used to import the predict function.requirements_fileXThe name of the file containing pip requirements for the predict function.python_versionXThe Python version to use when executing the predict function. This is automatically set to the current Python version when usingmodel.enable_explainability().sdk_versionXThe arthurai version used to make the enable request. This is automatically set to the currently installed SDK version when using themodel.enable_explainability().explanation_algoThe explanation algorithm to use. Valid options are 'lime' or 'shap'. The default value of 'lime'.explanation_nsamplesThe number of perturbed samples used to generate the explanation. The result will be calculated more quickly for a smaller number of samples but may be less robust. It is recommended to use at least 100 samples. The default value of 2000.inference_consumer_score_percentThe number between 0.0 and 1.0 sets the percent of inferences for which to compute an explanation score. Only applicable when streaming_explainability_enabled is set to true. The default value of 1.0 (all inferences explained).streaming_explainability_enabledIf true, every inference will have an explanation generated for it. If false, explanations are available on-demand only.ignore_dirsList of paths to directories within project_directory that will not be bundled and included with the predict function. Use to prevent including irrelevant code or files in larger directories.
CV Explainability
📘Explainability is currently available as an enrichment for classification, multi-labeling, and regression CV models, but not object detection CV models.
In your model_entrypoint.py for Multiclass Image models, in addition to the predict() function, there is a second function which is required: load_image(). This function should take in a string, which is a path to an image file. The function should return the image in a numpy array. Any image processing, such as converting to greyscale, should also happen in this function. This is because Lime (the explanation algorithm used behind the scenes) will create variations of this array to generate explanations. However, any transformation resulting in a non-numpy array should happen in the predict function, such as converting to a Tensor.
No image resizing is required. As part of onboarding an image model, pixel_height and pixel_width are set as metadata on the model. When ingesting, Arthur will automatically resize the image to the configured size and pass this resized image path to the load_image function.
Below is a full example file for an Image model, with both load_image and predict defined.
Imports and class definitions are omitted for brevity.
Python# example_entrypoint.py
import ...
class MedNet(nn.Module):
...
# load model using custom user defined class
net = MedNet()
path = pathlib.Path(__file__).parent.absolute()
net.load_state_dict(torch.load(f'{path}/pretrained_model'))
# helper function for transforming image
def quantize(np_array):
return np_array + (np.random.random(np_array.shape) / 256)
def load_image(image_path):
""""""Takes in single image path, and returns single image in format predict expects
""""""
return quantize(np.array(Image.open(image_path).convert('RGB')) / 256)
def predict(images_in):
""""""Takes in numpy array of images, and returns predictions in numpy array.
Can handle both single image in `numpy` array, or multiple images.
""""""
batch_size, pixdim1, pixdim2, channels = images_in.shape
raw_tensor = torch.from_numpy(images_in)
processed_images = torch.reshape(raw_tensor, (batch_size, channels, pixdim1, pixdim2)).float()
net.eval()
with torch.no_grad():
return net(processed_images).numpy()
👍Note on Enabling Explainability for CV ModelsExplainability for CV, at least for CV models, should be configured with 4 CPUs and 4 GB RAM (default 1) to avoid long explanation times (which could break the UI). It’s done per model when enabling explainability in the notebook.
This enabling explainability configuration can be seen here:
Pythonarthur_model.enable_explainability(
project_directory=project_dir,
user_predict_function_import_path='entrypoint',
streaming_explainability_enabled=False,
requirements_file=""requirements.txt"",
explanation_algo='lime',
explanation_nsamples=2000,
model_server_num_cpu=""4"",
model_server_memory=""4Gi""
)
NLP Explainability
Enabling explainability for NLP models follows the same process for Tabular models
📘An important choice for NLP explainability is the text_demiliter parameter, since this delimiter determines how tokens will be perturbed when generating explanations.
Here is an example entrypoint.py file which loads our NLP model and defines a predict function that the explainer will use:
Pythonmodel_path = os.path.join(os.path.dirname(__file__), ""model.pkl"")
model = joblib.load(model_path)
def predict(fvs):
# our model expects a list of strings, no nesting
# if we receive nested lists, unnest them
if not isinstance(fvs[0], str):
fvs = [fv[0] for fv in fvs]
return model.predict_proba(fvs)
Updated 3 months ago Table of Contents
Setting up Project Directory
Project Structure
Requirements File
Prediction Function
Preprocessing for Prediction
Enabling Explainability
Configuration Requirements
CV Explainability
NLP Explainability",arthur_scope_docs,https://docs.arthur.ai/docs/assets-required-for-explainability
125,"Monitoring Best Practices
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by Monitoring Best PracticesSuggest EditsThe Arthur Scope product is used to monitor machine learning models. It runs on Kubernetes and is able to scale on-demand. There are several components that should be monitored so the platform stays healthy.
Some recommended best practices for monitoring the various Scope components are as follows:
Kubernetes
Pods
CPU/Memory utilization
Pods are the smallest building blocks in Kubernetes. It's always advised to ensure pods have sufficient resources(CPU and Memory) available for them to run.
Number of Restarts
Pods getting restarted frequently is a sign of an buggy code or bad configuration.
Pods in Pending/Unknown/Unavailable/CLBO state
Pods not in a Ready state is a sign of hardware degradation or connectivity failures to external systems.
Persistent Volumes
IOPS
Ensure the storage backing Persistent Volumes have enough throughput provisioned and there is no throttling being experienced.
Available Disk Space
Ensure attached Persistent Volumes have enough disk space.
VolumeAttachment Errors
Ensure there are no VolumeAttachment errors observed in Persistent Volumes. This is particularly critical in multi-AZ deployments.
Nodes
Sufficient nodes in each AZ
Ensure there are required number of **nodes per AZ for each deployment.
Max nodes per cluster
Monitoring the total number of nodes a cluster is scaled to ensures performance and costs are in optimal.
Datastores
Meta Database(External)
Disk Space
Ensure there is enough disk space for the database.
IOPS
Monitor for any throttling of performance for the database disk and adjust IOPS accordingly.
CPU
Monitor for any throttling of performance for the database cpu and adjust it accordingly.
OLAP Database
Replication Lag
The OLAP database is usually deployed in a 3 node setup, which are synced via replication. A lag happens when data is not consistent across all nodes.
Delayed/Rejected Inserts
This usually happens when a large number of INSERTS are sent too quickly. This can lead to data loss or corruption.
ZooKeeper Exceptions
These should generally not happen and is sometimes an indication of bad hardware.
Messaging Middleware
Kafka
Consumer Lag
Producers write data and Consumers read data from the messaging middleware. If consumers are not able to keep up with the producers, it will lead to a lag which can mean poor performance for the platform.
Under Replicated partitions
Follower replicas get data from Leader replicas using replication. Due to resource exhaustion or Leader failure, it is possible the Follower replicas don’t keep up with the Leader replicas.
Kafka Connect
Connector failures
These failures mean data is not being written to data stores, which can lead to data loss.
Task failures
These failures mean data is not matching the configurations, which can lead to data loss/corruption.
ZooKeeper
Outstanding Requests
This is the number of requests waiting to be processed by ZooKeeper.
Workflow Scheduler
Failed Steps
This usually implies a bad configuration or being unable to communicate with external systems.
Failed Workflows
Failed Steps or bad configurations could lead to failed workflows.
Queued Workflows
Workflows being queued could mean there is a lack of resources on the cluster.
Microservices
Rate of 4XX/5XX HTTP response status
Bad HTTP status codes could happen due to various reasons (bugs, pod restarts, invalid creds, access etc.).
Response times
Elevated response times can happen due to various reasons (bugs, pod restarts etc.).
Updated about 2 months ago Table of Contents
Kubernetes
Datastores
Messaging Middleware
Workflow Scheduler
Microservices",arthur_scope_docs,https://docs.arthur.ai/docs/monitoring-best-practices
126,"Overview
Jump to ContentProduct DocumentationAPI and Python SDK ReferenceRelease Notesv3.6.0v3.7.0v3.8.0v3.9.0v3.10.0v3.11.0v3.12.0Schedule a DemoSchedule a DemoMoon (Dark Mode)Sun (Light Mode)v3.12.0Product DocumentationAPI and Python SDK ReferenceRelease NotesSearchLoading…Welcome to ArthurWelcome to Arthur Scope!Pages in the Arthur Scope PlatformExamplesArthur SDKArthur APIModel TypesModel Input / Output TypesTabularBinary ClassificationMulticlass ClassificationRegressionTextBinary ClassificationMulticlass ClassificationRegressionToken Sequence (LLM)ImageBinary ClassificationMulticlass ClassificationRegressionObject DetectionRanked List (Recommender Systems)Time SeriesCore ConceptsMetricsPerformance MetricsData Drift MetricsFairness MetricsUser-Defined MetricsAlertingManaging AlertsAlert Summary ReportsEnrichmentsAnomaly DetectionBias MitigationExplainabilityHot SpotsToken LikelihoodVersioningModel OnboardingQuickstartCV OnboardingNLP OnboardingGenerative TextRanked List Outputs OnboardingTime Series OnboardingRegistering A Model with the APIData Preparation for ArthurCreating Arthur Model ObjectRegistering Model Attributes ManuallyEnabling EnrichmentsAssets Required For ExplainabilityTroubleshooting ExplainabilitySending InferencesSending Historical DataSending Ground TruthIntegrations and ExamplesAlerting ServicesEmailPagerDutyServiceNowData PipelinesSageMakerML PlatformsLangchainSingle Sign On (SSO)OIDCSAMLSpark MLArthur Query GuideOverviewCreating QueriesFundamentalsCommon Queries QuickstartQuerying FunctionsDefault Evaluation FunctionsAggregation FunctionsTransformation FunctionsComposing Advanced FunctionsEnrichments + Data DriftQuerying Data DriftQuerying ExplainabilityAdvanced Walk ThroughsGrouped Inference QueriesResourcesArthur Scope FAQGlossaryModel Metric DefinitionsArthur AlgorithmsPlatform AdministrationWelcome to Platform AdministrationInstallation OverviewOn-Prem Deployment RequirementsPlatform Readiness for Existing Cluster InstallsVirtual Machine InstallationConfiguring for High AvailabilityExternalizing the Relational DatabaseInstalling KubernetesInstalling Arthur Pre-requisitesDeploying on Amazon AWS EKSKubernetes Cluster (K8s) Install with Namespace Scope PrivilegesOnline Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) InstallAirgap Kubernetes Cluster (K8s) Install with CLIPlatform Access ControlAccess ControlDefault Access ControlCustom RBACIntegrationsOngoing Platform MaintenanceWhat does ongoing maintenance look like?Audit LogAdministrationOrganizations and UsersUpgradingMonitoring Best PracticesPlatform ResourcesConfig TemplateExporting Platform ConfigurationsFull Directory of Arthur PermissionsArthur Permissions by Standard RolesArthur Permissions by EndpointBackup and RestorePre-RequisitesBacking Up the Arthur PlatformRestoring the Arthur PlatformAppendixPowered by OverviewSuggest EditsThe Arthur Query service is a feature within Arthur that enables teams to interact with and analyze their data within Arthur Scope. Built with a SQL-like wrapper, the functionality can be used to create metric functions, pull data, create graphs for custom reports, and much more.
Interacting with Arthur Query Service
Python SDK
One of the most common ways teams can query is through the Python SDK.
Building a Queryfrom arthurai import ArthurAI
## Create Connection to Model of interest
url = ## arthur url
access_key = ## arthur access key
connection = ArthurAI(url = url, access_key = access_key, verify_ssl=False)
model_id = ## model id
arthur_model.connection.get_model(model_id)
## Build Query Function
query_function = {
""select"":[ ## .....
]
}
arthur_model.query(query_function)
API
API calls that take in a query expect a JSON-formatted query.
JSON{""select"":[
{""property"":""*""}
],
""from"":""inference""
}
Quick Common Examples
Some of the most common use cases for querying include:
Pulling Custom Data: Teams often use the query service to pull data that they are interested in experimenting with / reporting on further with custom graphs / reports.
Creating Metrics to track and alert within Arthur: User-Defined Metrics are created based on the Arthur query language. Teams can also set alerts based on these metrics in the UI, Python SDK, or API after creating a metric.
Comparing Cohorts: Finally, another common workflow of the query service is to pull cohorts of data to compare in a notebook. This can be done for any function, but some of the most common are performance, drift, or regional explainability.
Updated 3 months ago Table of Contents
Interacting with Arthur Query Service
Python SDK
API
Quick Common Examples",arthur_scope_docs,https://docs.arthur.ai/docs/overview
127,"Add Scorer Configurations - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Add Scorer Configurations#
Many scorers included in the Bench package have optional parameters that provide flexibility for users to match scorers with their use case. Please visit our SDK documentation to view the optional configurations avaiable for each scorer.
As an example, in the quickstart, we showed how to use the exact_match scorer. By default, the exact_match scorer is case sensitive. This means, the scorer returns the value of 1.0 only when the candidate output matches the content and the capitalization of the reference output.
If we want to ignore capitalization differences, we can add a configuration to the exact_match scorer.
Creating the test suite#
Instantiate a test suite with a name, scorer, input text, and reference outputs. For our use case, instead of invoking the scorer using the string representation (which corresponds to the default config), we will explicitly call the scorer and add optional configurations.
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import ExactMatch
suite = TestSuite(
name='bench_quickstart',
scoring_method=ExactMatch(case_sensitive=False),
input_text_list=[""What year was FDR elected?"", ""What is the opposite of down?""],
reference_output_list=[""1932"", ""up""]
)
Running the test#
To create a test run, we need to specify the candidate responses.
run = suite.run('quickstart_run', candidate_output_list=[""1932"", ""Up""])
print(run.test_cases)
>>> [TestCaseOutput(output='1932', score=1.0), TestCaseOutput(output='Up', score=1.0)]
We have now logged the results for both test cases as 1.0 even though the capitalization doesn’t match the reference. This is non-default behavior for which we needed to configure the scorer while creating the test suite.
Additional resources#
We also support creating custom scorers that provide even more flexibility. Please view the guide here to learn how custom scorers can be created.
Next
Custom Scoring
Previous
Compare Generation Settings
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Add Scorer Configurations
Creating the test suite
Running the test
Additional resources",arthur_bench_docs,https://bench.readthedocs.io/en/latest/add_scorer_config.html
128,"Code Evaluation - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Code Evaluation#
Basic Usage#
Code evaluation refers to the process of checking whether LLM-written code passes unit tests
To use a code evaluation scoring method, instantiate the scorer with the unit tests you want to attach to the suite, and proceed with test suite creation / test case running as usual.
Here we show the basic usage for the PythonUnitTesting code evaluation scorer. See the Data Requirements and Example Walkthrough sections below for more details on preparing unit tests and candidate solutions.
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import PythonUnitTesting
# create scorer from unit_tests: List[str]
python_scorer = PythonUnitTesting(unit_tests=unit_tests)
# create test suite
# we explain how to prepare the data for python_unit_test_df below
python_suite = TestSuite(
""python_testsuite"",
python_scorer,
reference_data=python_unit_test_df
)
Data Requirements#
Unit tests#
Unit tests must be compatible with the code_eval evaluator metric from HuggingFace, which is what the PythonUnitTesting scorer uses under the hood.
Format
Each unit test is expected to invoke the candidate function by name and assert its output
The general format of the unit test expected by bench is as follows (the name check is not required)
def check(candidate):
assert candidate(test_input_0) = test_output_0
assert candidate(test_input_1) = test_output_1
assert candidate(test_input_2) = test_output_2
# ...
check(candidate_function_name)
For example, here is the unit test for the greatest_common_divisor task from the HumanEval dataset:
def check(candidate):
assert candidate(3, 7) == 1
assert candidate(10, 15) == 5
assert candidate(49, 14) == 7
assert candidate(144, 60) == 12
check(greatest_common_divisor)
Provide unit tests as strings
Unit tests can be passed to the PythonUnitTesting scorer as a list of strings, which is likely the simpler option if you are loading tests from a benchmark dataset (e.g. HumanEval as we do in the example below):
# create scorer from unit_test: List[str]
python_scorer = PythonUnitTesting(unit_tests=unit_tests)
Provide unit tests as directory
Alternatively you can load unit tests from a directory to evaluate your candidate solutions. Given a directory of unit test scripts:
unit_test_dir_name:
- unit_test_0.py
- unit_test_1.py
...
The PythonUnitTesting scorer can be created just from that directory name:
python_scorer = PythonUnitTesting(unit_test_dir=unit_test_dir_name)
Solutions#
Candidate solutions will only be evaluated to be correct if they contain:
a function to call (in the HumanEval dataset, this is called the entry_point)
any necessary imports
This is correct:
import math
def greatest_common_divisor(a: int, b: int) -> int:
return math.gcd(a, b)
This will be scored as incorrect due to missing the math import
def greatest_common_divisor(a: int, b: int) -> int:
return math.gcd(a, b)
This will be scored as incorrect due to missing a function entrypoint for the unit test to invoke:
import math
return math.gcd(a, b)
Input prompts & reference outputs#
Input prompts and reference outputs (AKA canonical / golden solutions) have no requirements in Bench. These components are only for your own analysis, and are not used by the scorers under the hood in code evaluation.
Example Walkthrough#
Here is some example code that you can use to generate and compare python coding solutions using OpenAI’s GPT-3.5 and Anthropic’s Claude-2 on the HumanEval dataset from HuggingFace
Environment setup#
First we set environment variables for OPENAI_API_KEY and ANTHROPIC_API_KEY before running our generation code
pip install openai anthropic
export ""OPENAI_API_KEY""=""sk-...""
export ""ANTHROPIC_API_KEY""=""sk-ant-...""
Data preparation#
Our dataset is the HumanEval dataset from HuggingFace loaded into a pandas DataFrame
from datasets import load_dataset
import pandas as pd
humaneval_code_dataset = load_dataset(""openai_humaneval"")
humaneval_df = pd.DataFrame(humaneval_code_dataset[""test""])
humaneval_df_sample = humaneval_df.sample(20, random_state=278487)
Prepare unit tests#
We prepare the unit tests to invoke each candidate function using the test and entry_point fields of the HumanEval dataset:
unit_tests = [
f'\n{humaneval_df_sample.test.values[i]}\ncheck({humaneval_df_sample.entry_point.values[i]})'
for i in range(len(humaneval_df_sample))
]
Generate solutions#
from langchain.chat_models import ChatOpenAI, ChatAnthropic
gpt35 = ChatOpenAI()
claude = ChatAnthropic()
prompt_template = """"""
You are a bot that gives answers to coding tasks only. If the task is a coding task, give an expert python solution.
If the task is unrelated, give the response ""I don't know.""
ALWAYS mark the beginning and end of your solution with ```python markdown markers.
Without these markers, the code cannot be extracted. Therefore the markers are required.
===
<text>
===
Solution:
""""""
# used to extract the portion of an LLM response which is python code
extract_python = lambda x : x.replace('python\n', '').replace('```', '').replace(' def', 'def')
def get_solutions(model):
filled_prompt_templates = [
prompt_template.replace(""<text>"", humaneval_df_sample.prompt.values[i])
for i in range(len(humaneval_df_sample))
]
return [extract_python(model.predict(x)) for x in filled_prompt_templates]
gpt35_solutions = get_solutions(gpt35)
claude_solutions = get_solutions(claude)
Create and run test suite
Now that you have generated solutions for each model, we can create a test suite and a run for each LLM
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import PythonUnitTesting
python_scorer = PythonUnitTesting(unit_tests=unit_tests)
python_suite = TestSuite(
""humaneval_testsuite"",
python_scorer,
input_text_list=list(humaneval_df_sample.prompt.values),
reference_output_list=list(humaneval_df_sample.canonical_solution.values),
)
python_suite.run(""gpt-3.5-turbo"", candidate_output_list=gpt35_solutions)
python_suite.run(""claude-2"", candidate_output_list=claude_solutions)
Best practices#
Prompt templating for code extraction#
Evaluation becomes more straightforward if you can easily extract the part of an LLM response which is its actual code solution. The simplest way to do that seems to be including an instruction in your prompt or system message that specifies to place code in between “```python markers” in markdown, as we did in the example above.
Function signature#
Performance tends to improve on coding when your task description contains an explicit function signature that you want the solution to adhere to, as well as including example input/output behavior in its docstring.
As an example, here is the input prompt for the greatest_common_divisor coding task from HumanEval:
def greatest_common_divisor(a: int, b: int) -> int:
"""""" Return a greatest common divisor of two integers a and b
>>> greatest_common_divisor(3, 5)
1
>>> greatest_common_divisor(25, 15)
5
""""""
Note that the HumanEval dataset prompts all contain docstrings like this one
Next
Python API Reference
Previous
Custom Scoring
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Code Evaluation
Basic Usage
Data Requirements
Unit tests
Solutions
Input prompts & reference outputs
Example Walkthrough
Environment setup
Data preparation
Prepare unit tests
Generate solutions
Best practices
Prompt templating for code extraction
Function signature",arthur_bench_docs,https://bench.readthedocs.io/en/latest/code_evaluation.html
129,"Compare Generation Settings - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Compare Generation Settings#
In this guide we compare LLM-generated answers to questions using different temperature settings. Higher temperature improves creativity and diversity of answers, but increases the likelihood that responses veer into nonsense.
We use a custom scorer that compares each LLM temperature setting based on how many typos each response contains
Environment setup#
In this guide, we use the OpenAI API and use the pyspellchecker package for a custom scorer
pip install openai pyspellchecker
export OPENAI_API_KEY=""sk-...""
Data preparation#
We write out some basic questions which we will use to test how much temperature impacts the responses
inputs = [""What planet are we on?"", ""What time is it?"", ""What day is it?"", ""What is love?""]
LLM response generation#
We use different temperature settings to generate three different lists of responses:
from langchain.chat_models import ChatOpenAI
chatgpt_zero_temp = ChatOpenAI(temperature=0.0, max_tokens=100)
chatgpt_low_temp = ChatOpenAI(temperature=0.5, max_tokens=100)
chatgpt_med_temp = ChatOpenAI(temperature=1.2, max_tokens=100)
chatgpt_high_temp = ChatOpenAI(temperature=1.9, max_tokens=100)
baseline_responses = [chatgpt_zero_temp.predict(x) for x in inputs]
low_temp_responses = [chatgpt_low_temp.predict(x) for x in inputs]
med_temp_responses = [chatgpt_med_temp.predict(x) for x in inputs]
high_temp_responses = [chatgpt_high_temp.predict(x) for x in inputs]
Create test suite#
For this test suite, we want to measure how corrupted the responses get as we increase the generation temperature.
Let’s define a quick custom scorer that uses the pyspellchecker package to scan for typos in the response, and we will then see how much the typo score changes between the low, medium, and high temperature model generations.
from arthur_bench.run.testsuite import TestSuite
from arthur_bench.scoring import Scorer
from spellchecker import SpellChecker
import string
from typing import List, Optional
class CustomSpellingScore(Scorer):
""""""
Custom scoring which scores each LLM response with the formula 1 / (2 ^ number of typos)
This gives a typo-free response a score of 1, and each additional typo further decreases the score
""""""
def __init__(self):
self.spell_checker = SpellChecker()
@staticmethod
def name() -> str:
return ""spell_checker""
@staticmethod
def requires_reference() -> bool:
return False
def run_batch(self, candidate_batch: List[str], reference_batch: Optional[List[str]] = None,
input_text_batch: Optional[List[str]] = None, context_batch: Optional[List[str]] = None) -> List[float]:
res = []
for s in candidate_batch:
# remove punctuation
s = ''.join(ch for ch in s if ch not in string.punctuation)
# get number of typos in s
num_typos = len(self.spell_checker.unknown(s.split()))
# custom score is 1/(2^num_typos)
res.append(1.0 / (2**num_typos))
return res
my_suite = TestSuite(
""test-spelling"",
CustomSpellingScore(),
input_text_list=inputs,
reference_output_list=baseline_responses
)
Run test suite#
my_suite.run(""low_temp_responses"", candidate_output_list=low_temp_responses)
my_suite.run(""med_temp_responses"", candidate_output_list=med_temp_responses)
my_suite.run(""high_temp_responses"", candidate_output_list=high_temp_responses)
View results#
Run bench from your command line to visualize the run results comparing the different temperature settings.
Next
Add Scorer Configurations
Previous
Compare Prompts
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Compare Generation Settings
Environment setup
Data preparation
LLM response generation
Create test suite
Run test suite
View results",arthur_bench_docs,https://bench.readthedocs.io/en/latest/compare_generation_settings.html
130,"Compare LLM Providers - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Compare LLM Providers#
In this guide we compare LLMs answers at summarizing text.
Environment setup#
In this guide, we use the OpenAI API and the Cohere API
pip install openai cohere
export OPENAI_API_KEY=""sk-...""
export COHERE_API_KEY=""...""
Data preparation#
We load a publically available congressional bill summarization dataset from HuggingFace.
import pandas as pd
from datasets import load_dataset
billsum = load_dataset(""billsum"", split=""ca_test"")
billsum_df = pd.DataFrame(billsum).sample(10, random_state=278487)
LLM response generation#
We use OpenAI and Cohere to generate summaries of these bills:
from langchain.llms import OpenAI, Cohere
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
gpt3 = OpenAI(temperature=0.0, max_tokens=100)
command = Cohere(temperature=0.0, max_tokens=100)
prompt_template = PromptTemplate(
input_variables=[""text""],
template=""""""
You are an expert summarizer of text. A good summary
captures the most important information in the text and doesnt focus too much on small details.
Text: {text}
Summary:
""""""
)
gpt3_chain = LLMChain(llm=gpt3, prompt=prompt_template)
command_chain = LLMChain(llm=command, prompt=prompt_template)
# generate summaries with truncated text
gpt3_summaries = [gpt3_chain.run(bill[:3000]) for bill in billsum_df.text]
command_summaries = [command_chain.run(bill[:3000]) for bill in billsum_df.text]
Create test suite#
For this test suite, we want to compare gpt-3 against command. We will use the SummaryQuality scoring metric to A/B test each set of candidate responses against the reference summaries from the dataset
from arthur_bench.run.testsuite import TestSuite
my_suite = TestSuite(
""congressional_bills"",
""summary_quality"",
input_text_list=list(billsum_df.text),
reference_output_list=list(billsum_df.summary)
)
Run test suite#
my_suite.run(""gpt3_summaries"", candidate_output_list=gpt3_summaries)
my_suite.run(""command_summaries"", candidate_output_list=command_summaries)
View results#
Run bench from your command line to visualize the run results comparing the different temperature settings.
Next
Compare Prompts
Previous
Creating test suites
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Compare LLM Providers
Environment setup
Data preparation
LLM response generation
Create test suite
Run test suite
View results",arthur_bench_docs,https://bench.readthedocs.io/en/latest/compare_llm_providers.html
131,"Compare Prompts - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Compare Prompts#
In this guide we compare prompts
Environment setup#
In this guide, we use the OpenAI API
pip install openai
export OPENAI_API_KEY=""sk-...""
Data preparation#
We load a publically available congressional bill summarization dataset from HuggingFace.
We also prepare an example bill with its summary to include in a prompt as an example response.
import pandas as pd
from datasets import load_dataset
billsum = load_dataset(""billsum"")
billsum_df = pd.DataFrame(billsum[""ca_test""]).sample(10, random_state=278487)
example_bill = billsum[""test""][6][""text""]
example_bill_summary = billsum[""test""][6][""summary""]
LLM response generation#
We use two different prompt templates to generate responses
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
gpt35 = ChatOpenAI(temperature=0.0, max_tokens=100)
prompt0_template= PromptTemplate(
input_variables=[""text""],
template=""""""
Text: {text}
Summary:
""""""
)
prompt1_template = PromptTemplate(
input_variables=[""text"", ""example_bill"", ""example_bill_summary""],
template=""""""
You are an expert summarizer of legal text. A good summary
captures the most important information in the text and doesnt focus too much on small details.
Make sure to use your expert legal knowledge in summarizing.
===
Text: {example_bill}
Summary: {example_bill_summary}
===
Text: {text}
Summary:
""""""
)
prompt0_chain = LLMChain(llm=gpt35, prompt=prompt0_template)
prompt1_chain = LLMChain(llm=gpt35, prompt=prompt1_template)
# generate summaries with truncated text
prompt0_summaries = [prompt0_chain.run(bill[:3000]) for bill in billsum_df.text]
prompt1_summaries = [
prompt1_chain({""text"" : bill[:3000], ""example_bill"" : example_bill, ""example_bill_summary"" : example_bill_summary})[""text""]
for bill in billsum_df.text
]
Create test suite#
For this test suite, we will use BERTScore to measure how much the candidate summaries approach the reference summaries by upgrading our prompt with task-specific detail and an example.
from arthur_bench.run.testsuite import TestSuite
my_suite = TestSuite(
""congressional_bills_to_reference"",
""bertscore"",
input_text_list=list(billsum_df.text),
reference_output_list=list(billsum_df.summary)
)
Run test suite#
my_suite.run(""prompt0_summaries"", candidate_output_list=prompt0_summaries)
my_suite.run(""prompt1_summaries"", candidate_output_list=prompt1_summaries)
View results#
Run bench from your command line to visualize the run results comparing the different temperature settings.
Next
Compare Generation Settings
Previous
Compare LLM Providers
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Compare Prompts
Environment setup
Data preparation
LLM response generation
Create test suite
Run test suite
View results",arthur_bench_docs,https://bench.readthedocs.io/en/latest/compare_prompts.html
132,"Concepts - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Concepts#
Data#
Testing LLMs involves preparing the following data for your use case:
Inputs to the LLM. Depending on the task at hand, these inputs are likely formatted to follow a prompt template.
Reference Outputs: these are your baseline outputs, which are optional in Arthur Bench but recommended to get a comprehensive understanding of your model’s performance relative to its expected outputs. These reference outputs would likely be either ground truth responses to the inputs, or could be outputs from a baseline LLM that you are evaluating against.
Candidate Outputs: these are the outputs from your candidate LLM that you are scoring.
Context: contextual information used to produce the candidate output, e.g. for retrieval-augmented Question & Answering tasks.
As an example, consider the task of Question & Answering about specific documents:
Input: “What war was referred to in the Gettysburg Address?”
Reference Output: American Civil War
Candidate Output: The war referenced in the Gettysburg Address is the American Civil War
Context: (Wikipedia) “The Gettysburg Address is a speech that U.S. President Abraham Lincoln delivered during the American Civil War at the dedication of the Soldiers’ National Cemetery, now known as Gettysburg National Cemetery, in Gettysburg, Pennsylvania on the afternoon of November 19, 1863, four and a half months after the Union armies defeated Confederate forces in the Battle of Gettysburg, the Civil War’s deadliest battle.”
Testing#
Test Suites#
A Test Suite stores the input & reference output data for your testing use case along with a scorer.
For example, for a summarization use case, your test suite could be created with:
the documents to summarize
baseline summaries as reference outputs to evaluate against
the SummaryQuality scorer
Test suites allow you to save and reuse your evaluation datasets over time with a consistent scorer to help you understand what drives changes in performance.
To view how to create test suites from various data formats, view our creating test suites guide
Test runs#
When a test suite is run, its scorer evaluates the candidate outputs provided in the run and assigns a score to each test case.
To run your test suite on candidate data, pass the data to the run() function of your test suite, along with any additional metadata you want to be logged for that run. To view the metadata you can save with your test runs, see the SDK docs
To view how to create test runs from various data formats, visit our test suites guide
Next
Creating test suites
Previous
Guides
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Concepts
Data
Testing
Test Suites
Test runs",arthur_bench_docs,https://bench.readthedocs.io/en/latest/concepts.html
133,"Contributing - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Contributing#
We welcome contributions and feedback from the community!
Creating a custom scorer#
All scorers should inherit from the Scorer base class and provide a custom implementation of the run_batch method.
A scorer can leverage any combination of input texts, context texts, and reference texts to score candidate generations. All computed scores must be float values where a higher value indicates a better score. If you have a scorer that does not fit these constraints, please get in touch with the Arthur team.
Steps for adding a custom scorer:
Install bench from source, in development mode:
pip install -e .
Add your Scorer implementation in a new file in arthur_bench/scoring. For scorers that require prompt templating, we use the LangChain library.
Register your scorer by adding it to the scorer enum in arthur_bench/models/models.py
At this point, you should be able to create test suites with your new scorer and test your implementation locally.
Contributing your scorer:
Fork the bench repository and create a pull request from your fork. This Github guide provides more in depth instructions.
Your scorer docstring should use Sphinx format for compatibility with documentation.
Provide unit tests for the scorer in a separate file in the test directory.
Next
Usage Data Collection
Previous
arthur_bench.utils
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Contributing
Creating a custom scorer",arthur_bench_docs,https://bench.readthedocs.io/en/latest/contributing.html
134,"Creating test suites - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Creating test suites#
What data should I use?#
It is best to use data that is as close to your production use case as possible. We recommend sampling some historic data and manually validating a set of 25+ cases.
Public datasets on HuggingFace like the Dolly dataset and the HumanEval dataset can be a great starting place to benchmark on your use case before you have data that is closer to your actual production setting.
When no baseline examples or labels easily exist for the inputs you want to evaluate LLM performance on, you can use an existing LLM to generate a baseline for the task and then iterate from there.
Ways to create a test suite#
The TestSuite class is the main touch point for creating and running tests in Arthur Bench. No matter how you prepare your data for a test suite, you use the common interface provided by importing the TestSuite class:
from arthur_bench.run.testsuite import TestSuite
You can provide data for your TestSuite via the following options, each of which we give examples of in the sections below:
List[str]
pd.DataFrame
CSV file
HuggingFace Dataset
To see the exact specifications for the TestSuite class, visit our SDK docs.
List[str] -> TestSuite#
You can create and run a test suite by passing lists of strings directly as the test suite data:
suite = TestSuite(
""bench_quickstart"",
""exact_match"",
input_text_list=[""What year was FDR elected?"", ""What is the opposite of down?""],
reference_output_list=[""1932"", ""up""]
)
suite.run('quickstart_run', candidate_output_list=[""1932"", ""up is the opposite of down""])
This path also allows you to pass LLM responses directly into a test suite as a set of baseline reference outputs and/or as a run of candidate outputs. For example, you can use gpt-4 outputs as a baseline for a test suite, and then run gpt-3.5-turbo as a candidate to see how it compares.
Creating and running the test suite directly from LLM-generated strings:
from langchain.chat_models import ChatOpenAI
gpt35 = ChatOpenAI()
gpt4 = ChatOpenAI(model_name=""gpt-4"")
inputs = [""What year was FDR elected?"", ""What is the opposite of down?""]
baseline_outputs = [gpt4.predict(x) for x in inputs]
candidate_outputs = [gpt35.predict(x) for x in inputs]
suite = TestSuite(
""bench_llm_quickstart"",
""exact_match"",
input_text_list=inputs,
reference_output_list=baseline_outputs
)
suite.run('quickstart_llm_run', candidate_output_list=candidate_outputs)
DataFrame -> TestSuite#
If you have your test suite and/or model response data in a pandas DataFrame you can create test suites and runs directly from those dataframes
Here is an example test suite built from a dataframe with the default reference data and candidate data column names that TestSuite expects (you can also use other column names as we show below)
Creating and running the default DataFrame test suite:
import pandas as pd
df = pd.DataFrame({
""input"": [""What year was FDR elected?"", ""What is the opposite of down?""],
""reference_output"": [""1932"", ""up""],
""candidate_output"": [""1932"", ""up is the opposite of down""]
})
test_suite = TestSuite(
""suite_from_df"",
""exact_match"",
reference_data=df
)
test_suite.run(""candidate_from_df"", candidate_data=df)
Alternatively you can create and run test suites from dataframes with custom column names.
Creating and running the custom DataFrame test suite:
import pandas as pd
df = pd.DataFrame({
""my_input"": [""What year was FDR elected?"", ""What is the opposite of down?""],
""baseline_output"": [""1932"", ""up""],
""gpt35_output"": [""1932"", ""up is the opposite of down""]
})
test_suite = TestSuite(
""suite_from_df_custom"",
""exact_match"",
reference_data=df,
input_column=""my_input"",
reference_column=""baseline_output""
)
test_suite.run(
""candidate_from_df_custom"",
candidate_data=df,
candidate_column=""gpt35_output""
)
.csv -> TestSuite#
If your test suite and/or model response data already exists in CSV files you can create test suites and runs directly from those files
Here is an example test suite CSV with the default reference data and candidate data column names that TestSuite expects (you can also use other column names as we show below)
test_suite_data_default_columns.csv
input, reference_output, candidate_output
What year was FDR elected?, 1932, 1932
What is the opposite of down?, up, up is the opposite of down
Creating and running the default csv test suite:
test_suite = TestSuite(
""suite_from_csv"",
""exact_match"",
reference_data_path=""/path/to/test_suite_data_default_columns.csv""
)
test_suite.run(
""candidate_from_csv"",
candidate_data_path=""/path/to/test_suite_data_default_columns.csv""
)
Alternatively you can create and run test suites from .csv files with custom column names:
test_suite_data_custom_columns.csv
my_input, baseline_output, gpt35_output
What year was FDR elected?, 1932, 1932
What is the opposite of down?, up, up is the opposite of down
Creating and running the custom csv test suite:
test_suite = TestSuite(
""suite_from_csv_custom"",
""exact_match"",
reference_data_path=""/path/to/test_suite_data_custom_columns.csv"",
input_column=""my_input"",
reference_column=""baseline_output""
)
test_suite.run(
""candidate_from_csv_custom"",
candidate_data_path=""/path/to/test_suite_data_custom_columns.csv"",
candidate_column=""gpt35_output""
)
HuggingFace dataset -> DataFrame -> TestSuite#
Here we create a small question-answering test suite from the Dolly dataset downloaded from HuggingFace. We set up the test suite to use BERTScore to measure similarity between candidate answers and reference answers
Creating and running the dolly test suite:
# get dolly dataset from huggingface into a pandas dataframe
import pandas as pd
from datasets import load_dataset
dolly = load_dataset(""databricks/databricks-dolly-15k"")
dolly_df = pd.DataFrame(dolly[""train""])
# make test suite from a question-answering subset of the data
dolly_df_sample = dolly_df[dolly_df[""category""]==""open_qa""].sample(25, random_state=278487)
dolly_suite = TestSuite(
""suite_from_huggingface_dolly"",
""bertscore"",
reference_data=dolly_df_sample,
input_column=""instruction"",
reference_column=""response""
)
# run test suite on gpt-3.5-turbo generated answers to the questions
from langchain.chat_models import ChatOpenAI
gpt35 = ChatOpenAI()
dolly_suite.run(
""gpt-3.5"",
candidate_output_list=[gpt35.predict(x) for x in dolly_df_sample.instruction])
Next
Compare LLM Providers
Previous
Concepts
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Creating test suites
What data should I use?
Ways to create a test suite
List[str] -> TestSuite
DataFrame -> TestSuite
.csv -> TestSuite
HuggingFace dataset -> DataFrame -> TestSuite",arthur_bench_docs,https://bench.readthedocs.io/en/latest/creating_test_suites.html
135,"Custom Scoring - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Custom Scoring#
In this guide, we will walk through the process of evaluating LLM performance using a custom scorer. We will
Define a custom scorer
Create a test suite with that scorer
Run the test suite and view scores
Define a custom scorer#
To create a custom scorer that satisfies the Scorer interface (defined in the section below), implement the scoring logic in the run_batch method. Additionally, provide your scorer a name in the name() method.
This example custom scorer is called TrigramRepetition, which scores responses with a 0.0 if they contain repeated trigrams above a thresholded number of times. For our scorer, we override the requires_reference() method to return False instead of True, since this custom scorer evaluates the candidate outputs without the need for a reference.
Make sure nltk is installed as a package to your environment, which our custom scorer uses.
pip install nltk
Make sure nltk is installed as a package to your environment, which our custom scorer uses.
pip install nltk
from arthur_bench.scoring import Scorer
import nltk
# make sure corpus is downloaded
nltk.download('punkt')
from nltk import trigrams
from typing import List, Optional
class TrigramRepetition(Scorer):
def __init__(self, threshold: int = 3):
self.threshold = threshold
@staticmethod
def name() -> str:
return ""trigram_repetition""
@staticmethod
def requires_reference() -> bool:
return False
def run_batch(self, candidate_batch: List[str], reference_batch: Optional[List[str]] = None,
input_text_batch: Optional[List[str]] = None, context_batch: Optional[List[str]] = None) -> List[float]:
repeat_scores = []
for text in candidate_batch:
tokens = [t.lower() for t in nltk.word_tokenize(text)]
all_trigrams = trigrams(tokens)
counts = {}
for tri in all_trigrams:
if tri in counts:
counts[tri] += 1
else:
counts[tri] = 1
max_repeat = max(counts.values())
repeat_scores.append(float(max_repeat < self.threshold))
return repeat_scores
Using a custom scorer in a test suite#
We pass in our custom scorer as the scoring_method parameter to the test suite:
from arthur_bench.run.testsuite import TestSuite
repetition_test = TestSuite(
'custom_trigram_suite',
scoring_method=TrigramRepetition(),
input_text_list=[
""Talk to me but don't repeat yourself too much"",
""Talk to me but don't repeat yourself too much""
]
)
Run the test suite#
Now that we’ve loaded in our custom scorer, our test suite can be run as usual on any candidate generations.
run = repetition_test.run(
'test_run',
candidate_output_list=[
'a great response with no repetition!',
'a bad response that repeats response that repeats response that repeats response that repeats'
]
)
print(run.scores)
>>> [1.0, 0.0]
Scorer Validation#
Test suites expect scorer configurations to remain consistent from run to run, so that each runs scores can be compared and reliably tracked throughout time. Let’s see what happens if we attempt to use this suite at a later time, but edit the underlying parameters.
scorer = TrigramRepetition(threshold=7)
repetition_test = TestSuite('custom_trigram_suite', scoring_method=scorer)
We see the following warning:
scoring method configuration has changed from test suite creation.
By default, bench will save the json serializable attributes of your scorer as the configuration. If you need more advanced serialization for validation or re-initialization, implement the to_dict() and from_dict() methods on your custom class. You can find the full scorer spec here.
Scorer interface#
All scorers in bench implement the scorer interface. Let’s take a look at that interface:
class Scorer(ABC):
""""""
Base class for all scorers.
""""""
@staticmethod
@abstractmethod
def name() -> str:
""""""
Get the name of this Scorer
:return: the Scorer name
""""""
raise NotImplementedError
@staticmethod
def requires_reference() -> bool:
return True
@abstractmethod
def run_batch(self, candidate_batch: List[str], reference_batch: Optional[List[str]] = None,
input_text_batch: Optional[List[str]] = None, context_batch: Optional[List[str]] = None) -> List[float]:
""""""
Score a batch of candidate generations.
:param candidate_batch: candidate generations to score
:param reference_batch: reference strings representing target outputs
:param input_text_batch: optional corresponding inputs
:param context_batch: optional corresponding contexts, if needed by scorer
""""""
raise NotImplementedError
To create a custom scorer, you need to implement the name and run_batch methods, and optionally override the requires_reference method if your scorer doesn’t require reference or target data.
Contributing#
If you think you’ve got a useful scorer, please consider contributing!
Next
Code Evaluation
Previous
Add Scorer Configurations
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Custom Scoring
Define a custom scorer
Using a custom scorer in a test suite
Run the test suite
Scorer Validation
Scorer interface
Contributing",arthur_bench_docs,https://bench.readthedocs.io/en/latest/custom_scoring.html
136,"Quickstart - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Quickstart#
Make sure you have completed installation from the setup guide before moving on to this quickstart.
Environment setup#
The environment variable BENCH_FILE_DIR points to the local directory where your test data is saved and visualized by Arthur Bench.
If you are running this quickstart right after completing the setup guide, then take a moment to reset BENCH_FILE_DIR to its default value, ""./bench_runs"". This will direct the bench UI to point to your new quickstart test suite instead of the examples from the setup.
export BENCH_FILE_DIR=""./bench_runs""
Creating your first test suite#
Instantiate a test suite with a name, data, and scorer.
This example creates a test suite from lists of strings directly with the exact_match scorer.
from arthur_bench.run.testsuite import TestSuite
suite = TestSuite(
'bench_quickstart',
'exact_match',
input_text_list=[""What year was FDR elected?"", ""What is the opposite of down?""],
reference_output_list=[""1932"", ""up""]
)
You can create test suites from a pandas DataFrame or from a path to a local CSV file. See the test suite creation guide to view all the ways you can create test suites.
You can view all scorers available out of the box with bench here on our scoring page, as well as customize your own.
Running your first test suite#
To create a Test Run, you only need to specify the candidate responses. See the test suite creation guide to view all the ways you can run test suites.
run = suite.run('quickstart_run', candidate_output_list=[""1932"", ""up is the opposite of down""])
print(run)
>>> [TestCaseOutput(output='1932', score=1.0), TestCaseOutput(output='up is the opposite of down', score=0.0)]
You should now have logged test case results with scores of 1.0 and 0.0, respectively.
View results in local UI#
Now run bench from the command line to launch the local UI and explore the test results.
bench
Next steps#
Now that you have set up and ran your first test suite, check out the rest of the scorers available in Arthur Bench out of the box.
To learn more about the basic concepts around data and testing in Arthur Bench, visit our basic concepts guide.
Next
Scoring
Previous
Setup
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Quickstart
Environment setup
Creating your first test suite
Running your first test suite
View results in local UI
Next steps",arthur_bench_docs,https://bench.readthedocs.io/en/latest/quickstart.html
137,"Scoring - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Scoring#
A Scorer is the criteria used to quantitatively evaluate LLM outputs. When you test LLMs with Arthur Bench, you attach a Scorer to each test suite you create - this defines how performance will be measured consistently across that test suite.
For a walkthrough on how to extend the Scorer class to create your own scorer specialized to your data and/or use-case to use with Arthur Bench, check out the custom scoring guide
If you would like to contribute scorers to the open source Arthur Bench repo, check out our contributing guide
Here is a list of all the scorers available by default in Arthur Bench (listed alphabetically):
Scorer
Tasks
Type
Requirements
BERT Score (bertscore)
any
Embedding-Based
Reference Output, Candidate Output
Exact Match (exact_match)
any
Lexicon-Based
Reference Output, Candidate Output
Hallucination (hallucination)
any
Prompt-Based
Candidate Output, Context
Hedging Language (hedging_language)
any
Embedding-Based
Candidate Output
Python Unit Testing (python_unit_testing)
Python Generation
Code Evaluator
Candidate Output, Unit Tests (see the code eval guide)
QA Correctness (qa_correctness)
Question-Answering
Prompt-Based
Input, Candidate Output, Context
Readability (readability)
any
Lexicon-Based
Candidate Output
Specificity (specificity)
any
Lexicon-Based
Candidate Output
Summary Quality (summary_quality)
Summarization
Prompt-Based
Input, Reference Output, Candidate Output
Word Count Match (word_count_match)
any
Lexicon-Based
Reference Output, Candidate Output
For better understandability we have broken down the Scorers based on the type of procedure each Scorer uses.
Prompt-Based Scorers#
qa_correctness#
The QA correctness scorer evaluates the correctness of an answer, given a question and context. This scorer does not require a reference output, but does require context. Each row of the Test Run will receive a binary 0, indicating an incorrect output, or 1, indicating a correct output.
summary_quality#
The Summary Quality scorer evaluates a summary against its source text and a reference summary for comparison. It evaluates summaries on dimensions including relevance and syntax. Each row of the test run will receive a binary 0, indicating that the reference output was scored higher than the candidate output, or 1, indicating that the candidate output was scored higher than the reference output.
hallucination#
The Hallucination scorer takes a response and a context (e.g. in a RAG setting where context is used to ground an LLM’s responses) and identifies when information in the response is not substantiated by the context . The scorer breaks down the response into a list of claims and checks the claims against the context for support. This binary score is 1 if all claims are supported, and 0 otherwise.
Embedding-Based Scorers#
bertscore#
BERTScore is a quantitative metric to compare the similarity of two pieces of text. Using bertscore will score each row of the test run as the bert score between the reference output and the candidate output.
hedging_language#
The Hedging Language scorer evaluates whether a candidate response is similar to generic hedging language used by an LLM (“As an AI language model, I don’t have personal opinions, emotions, or beliefs”). Each row of the Test Run will receive a score between 0.0 and 1.0 indicating the extent to which hedging language is detected in the response (using BERTScore similarity to the target hedging phrase). A score above 0.5 typically suggests the model output contains hedging language.
Lexicon-Based Scorers#
exact_match#
The Exact Match scorer evaluates whether the candidate output exactly matches the reference output. This is case sensitive. Each row of the Test Run will receive a binary 0, indicating a non-match, or 1, indicating an exact match.
readability#
The Readability scorer evaluates the reading ease of the candidate output according to the Flesch Reading Ease Score. The higher the score, the easier the candidate output is to read: scores of 90-100 correlate to a 5th grade reading level, while scores less than 10 are classified as being “extremely difficult to read, and best understood by university graduates.”
specificity#
The Specificity scorer outputs a score of 0 to 1, where smaller values correspond to candidate outputs with more vague language while higher values correspond to candidate outputs with more precise language. Specificity is calculated through 3 heuristic approaches: identifying the presence of predefined words that indicate vagueness, determing how rare the words used are according to word frequencies calculated by popular NLP corpora, and detecting the use of proper nouns and numbers.
word_count_match#
For scenarios where there is a preferred output length, word_count_match calculates a corresponding score on the scale of 0 to 1. Specifically, this scorers calculates how similar the number of words in the candidate output is to the number of words in the reference output, where a score of 1.0 indicates that there are the same number of words in the candidate output as in the reference output. Scores less than 1.0 are calculated as ((len_reference-delta)/len_reference) where delta is the absolute difference in word lengths between the candidate and reference outputs. All negative computed values are truncated to 0.
Code Evaluators#
python_unit_testing#
The Python Unit Testing scorer evaluates candidate solutions to coding tasks against unit tests. This scorer wraps the code_eval evaluator interface from HuggingFace. It is important to note that this function requires that solution code uses standard python libraries only.
Next
Guides
Previous
Quickstart
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Scoring
Prompt-Based Scorers
qa_correctness
summary_quality
hallucination
Embedding-Based Scorers
bertscore
hedging_language
Lexicon-Based Scorers
exact_match
readability
specificity
word_count_match
Code Evaluators
python_unit_testing",arthur_bench_docs,https://bench.readthedocs.io/en/latest/scoring.html
138,"arthur_bench.client.auth - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.auth#
Submodules#
arthur_bench.client.auth.helpers.get_arthur_internal_user_org(api_http_host: str, auth_token: str, verify_ssl: bool = True) → str  None#
Get the current organization for the provided Arthur auth token belonging to anArthur internal user
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
auth_token – auth token to pass to the API
verify_ssl – if True, verify that the SSL certificate is valid and not
self-signed
Returns:
the organization ID associated with the provided access key, None if no
such organization exists
Permissions:
N/A
arthur_bench.client.auth.helpers.get_auth_info(api_http_host: str, auth_token: str, verify_ssl: bool = True) → AuthenticationInfo#
Get the AuthInfo struct associated with the provided access key
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
:param auth_token: Token to fetch authentication info for
:param verify_ssl: Boolean for whether requests should verify that the SSL
certificate is valid and not self-signed
:return: the AuthInfo associated with the provided access key
:permissions: N/A
arthur_bench.client.auth.helpers.get_current_org(api_http_host: str, auth_token: str, verify_ssl: bool = True) → str  None#
Get the current organization for the provided access key
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
:param auth_token: API Key to pass to the API
:param verify_ssl: Boolean for whether requests should verify that the SSL
certificate is valid and not self-signed
Returns:
the organization ID associated with the provided access key, None if no
such organization exists
arthur_bench.client.auth.helpers.user_login(api_http_host: str, login: str, password: str, verify_ssl: bool = True) → str#
Static convenience function to get a new auth token for the provided username andpassword
Parameters:
api_http_host – base url of the host to connect to, including protocol
(e.g. “https://app.arthur.ai”)
login – the username or password to use to log in
password – password for the user
verify_ssl – Boolean for whether requests should verify that the SSL
certificate is valid and not self-signed
Returns:
an access_key
class arthur_bench.client.auth.refresh.AuthRefresher(url: str, login: str, password: str, verify_ssl: bool)#
Bases: object
ALGORITHMS = ['HS256']#
AUTH_KEY = 'Authorization'#
MINS_BEFORE_EXPIRY_TO_REFRESH = 5#
refresh() → Tuple[Dict[str, str], timedelta]#
Authorization header update function for an HTTPClient
Fetches a new session token and returns the new token, and how long to wait
before refreshing it (by calling this method again)
:return: Headers to update (Authorization), and time to wait before refreshing
again
Next
arthur_bench.client.http
Previous
arthur_bench.client
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.auth
Submodules
get_arthur_internal_user_org()
get_auth_info()
get_current_org()
user_login()
AuthRefresher
AuthRefresher.ALGORITHMS
AuthRefresher.AUTH_KEY
AuthRefresher.MINS_BEFORE_EXPIRY_TO_REFRESH
AuthRefresher.refresh()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.auth.html
139,"arthur_bench.client - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client#
Subpackages#
arthur_bench.client.auth
Submodules
get_arthur_internal_user_org()
get_auth_info()
get_current_org()
user_login()
AuthRefresher
AuthRefresher.ALGORITHMS
AuthRefresher.AUTH_KEY
AuthRefresher.MINS_BEFORE_EXPIRY_TO_REFRESH
AuthRefresher.refresh()
arthur_bench.client.http
Submodules
construct_url()
HTTPClient
HTTPClient.delete()
HTTPClient.get()
HTTPClient.patch()
HTTPClient.post()
HTTPClient.put()
HTTPClient.send()
HTTPClient.set_path_prefix()
validate_multistatus_response_and_get_failures()
validate_response_status()
arthur_bench.client.local
Submodules
LocalBenchClient
LocalBenchClient.create_new_test_run()
LocalBenchClient.create_test_suite()
LocalBenchClient.delete_test_run()
LocalBenchClient.delete_test_suite()
LocalBenchClient.get_runs_for_test_suite()
LocalBenchClient.get_summary_statistics()
LocalBenchClient.get_test_run()
LocalBenchClient.get_test_suite()
LocalBenchClient.get_test_suite_by_name()
LocalBenchClient.get_test_suites()
PageInfo
PageInfo.end
PageInfo.page
PageInfo.page_size
PageInfo.sorted_pages
PageInfo.start
PageInfo.total_count
PageInfo.total_pages
arthur_bench.client.rest
Subpackages
arthur_bench.client.rest.admin
Submodules
arthur_bench.client.rest.bench
Submodules
Submodules
ArthurClient
Submodules#
class arthur_bench.client.bench_client.BenchClient#
Bases: ABC
Base class for saving and loading bench data
check_run_exists(suite_id: str, run_name: str) → bool#
Check if run with given name if it exists for suite with id suite_id
Parameters:
client – BenchClient object for fetching test suite data
suite_id – the id of the test suite to check run names
run_name – the test run name to check for
Returns:
True if run with name is found, False otherwise
Raises:
ArthurInternalError – if using a client that does not support pagination
abstract create_new_test_run(test_suite_id: str, json_body: CreateRunRequest) → CreateRunResponse#
Create a new run for a test suite.
Parameters:
test_suite_id – the uuid of the test suite to log a run for
json_body – run request containing run_metadata and scored model
generations
abstract create_test_suite(json_body: TestSuiteRequest) → PaginatedTestSuite#
Create a new test suite.
Parameters:
json_body – test suite request object consisting of test suite metadata
and test cases
abstract delete_test_run(test_suite_id: str, test_run_id: str)#
Delete a test run from a suite.
abstract delete_test_suite(test_suite_id: str)#
Delete a test suite. All associated runs will also be deleted
abstract get_runs_for_test_suite(test_suite_id: str, sort: CommonSortEnum  TestRunSortEnum = CommonSortEnum.CREATED_AT_ASC, page: int = 1, page_size: int = 5) → PaginatedRuns#
Get runs for a given test suite.
Parameters:
test_suite_id – the uuid of the test suite
sort – optional sort key. possible values are ‘name’, ‘avg_score’, and ‘
created_at’.
use ‘-’ prefix for descending sort. defaults to ‘created_at’
page – the page to fetch
page_size – page size to fetch
get_suite_if_exists(name: str) → PaginatedTestSuite  None#
Get a full test suite with name if it exists.
Parameters:
client – BenchClient object for fetching test suite data
Returns:
complete test suite with all test cases joined,
or None if no suite with name exists
Raises:
ArthurInternalError – if using a client that does not support pagination
abstract get_summary_statistics(test_suite_id: str, run_ids: list[str]  None = None, page: int = 1, page_size: int = 5) → TestSuiteSummary#
Fetch aggregate statistics of a test suite. Returns averages and score
distributions for runs in test suite.
Parameters:
test_suite_id – uuid of the test suite
run_id – optional run id. run will be included in response regardless of
page information if provided
page – the page to fetch
page_size – page size to fetch
abstract get_test_run(test_suite_id: str, test_run_id: str, page: int = 1, page_size: int = 5, sort: TestCaseSortEnum  None = None) → PaginatedRun#
Get a test run by id.
Parameters:
test_suite_id – uuid of the test suite
test_run_id – uuid of the test run
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
sort – sort key to sort the retrieved results
abstract get_test_suite(test_suite_id: str, page: int = 1, page_size: int = 5) → PaginatedTestSuite#
Get a test suite by id.
Parameters:
test_suite_id – the uuid of the test suite to fetch
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
abstract get_test_suites(name: str  None = None, sort: CommonSortEnum  TestSuiteSortEnum = TestSuiteSortEnum.LAST_RUNTIME_ASC, scoring_method: List[str]  None = None, page: int = 1, page_size: int = 5) → PaginatedTestSuites#
Get metadata for all test suites.
Parameters:
name – filter test suites by name if provided
sort – optional sort key. possible values are ‘name’, ‘last_run_time’,
‘created_at’, use ‘-’ prefix for descending sort.
defaults to ‘last_run_time’
method (scoring) – optional filter on scoring method name,
multiple names may be provided
page – the page to fetch
page_size – page size to fetch
Next
arthur_bench.client.auth
Previous
Python API Reference
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client
Subpackages
Submodules
BenchClient
BenchClient.check_run_exists()
BenchClient.create_new_test_run()
BenchClient.create_test_suite()
BenchClient.delete_test_run()
BenchClient.delete_test_suite()
BenchClient.get_runs_for_test_suite()
BenchClient.get_suite_if_exists()
BenchClient.get_summary_statistics()
BenchClient.get_test_run()
BenchClient.get_test_suite()
BenchClient.get_test_suites()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.html
140,"arthur_bench.client.http - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.http#
Submodules#
arthur_bench.client.http.helper.construct_url(*parts: str, validate=True, default_https=True) → str#
Construct a URL from various parts
Useful for joining pieces which may or may not have leading and/or trailing
slashes. e.g. construct_url(”https://arthur.ai/”, “/api/v3”, “/users”) will yield
the same valid url as construct_url(”https://arthur.ai”, “api/v3/”, “users/”):
“https://arthur.ai/api/v3/users”.
Parameters:
validate – if True, validate that the URL is valid
default_https – if True, allow urls without a scheme and use https by default
parts – strings from which to construct the url
Returns:
a fully joined url, with NO trailing slash
class arthur_bench.client.http.requests.HTTPClient(base_url: str, path_prefix: str  None = None, default_headers: Dict[str, str]  None = None, verify_ssl: bool = True, timeout_sec: float = 300.0, allow_insecure: bool = True, header_refresh_func: Callable[[], Tuple[Dict[str, str], timedelta]]  None = None)#
Bases: object
A requests-based HTTP Client intended for interacting with JSON-based APIs.
Supports response validation, retries, connection reuse, and multipart requests.
delete(endpoint: str, headers: Dict[str, str]  None = None, return_raw_response: bool = False, params: Dict  bytes  None = None, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP DELETE request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
params – query parameters to add to the request
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the
status code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
get(endpoint: str, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP GET request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, allow any 2XX
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
Raises:
ArthurUserError – failed due to user error
ArthurInternalError – failed due to an internal error
patch(endpoint: str, json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP POST request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
json – data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize.
if files is also supplied, this should be a map from name to content,
to be sent along with the files as a multipart request
files – a map from file names to file-like objects, to be sent as
multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the
status code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
post(endpoint: str, json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP POST request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
json – data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize. if files is also supplied, this should be a
map from name to content, to be sent along with the files as a multipart
request
files – a map from file names to file-like objects, to be sent as
multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
put(endpoint: str, json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP PUT request
Parameters:
endpoint – the specific endpoint to append to the client URL
headers – headers to use for this request in addition to the client
default headers
json – data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize. if files is also supplied, this should be a
map from name to content, to be sent along with the files as a
multipart request
files – a map from file names to file-like objects,
to be sent as multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, don’t validate
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
send(endpoint: str, method: str = 'GET', json: Dict  List  str  bytes  None = None, files: Dict[str, BinaryIO]  List[Tuple]  Dict[str, Tuple]  None = None, headers: Dict[str, str]  None = None, params: Dict  bytes  None = None, return_raw_response: bool = False, retries: int = 0, validate_response_status: bool = True, validation_response_code: int  None = None) → Dict  List  bytes  BytesIO  Response#
Send an HTTP request
Parameters:
endpoint – the specific endpoint to append to the client URL
method – the HTTP method to use
headers – headers to use for this request in addition to the client
d
efault headers
:param json: data to send as JSON, either a string/bytes to send directly or a
dictionary/list to serialize. if
files is also supplied, this should be a map from name to content,
to be sent along with the files as a multipart request
Parameters:
files – a map from file names to file-like objects, to be sent as
multipart/form-data
params – query parameters to add to the request
return_raw_response – if true, return the requests.Response object
received; otherwise attempt to parse the response
retries – number of times to retry the request on failure.
uses exponential backoff
validate_response_status – if True, raise an ArthurException if the status
code is not 2XX or does not match validation_response_code
validation_response_code – expected status code of the response to
validate. if None, allow any 2XX
Returns:
if return_raw_response is true, return the requests.Response object
received; otherwise attempt to parse the response
Raises:
ArthurUserError – failed due to user error
ArthurInternalError – failed due to an internal error
set_path_prefix(path_prefix: str) → None#
Update the client’s path prefix
This update the path prefix which is prepended to ‘endpoint’ paths.
arthur_bench.client.http.validation.validate_multistatus_response_and_get_failures(response: Response, raise_on_failures: bool = False) → Tuple[List[dict], List[dict]]#
Validate a 207 MultiStatus response and return the failures it contains.
Parameters:
response – requests.Response object to validate, with the following body format:
{
""counts"": {
""success"": 0,
""failure"": 0,
""total"": 0
},
""results"": [
{
""message"": ""success"",
""status"": 200
}
]
}
raise_on_failures – if True, raise an exception if the response contains any
failures
:return: a tuple of two lists: user-caused failures and internal failures
:raises ArthurInternalValueError: If the response does not have 207 status code, or
is incorrectly formatted,
or ‘counts’ and ‘results’ do not agree
Raises:
ResponseClientError – if raise_on_failures and the response contains only
client errors
:raises ResponseServerError: if raise_on_failures and the response contains server
errors
arthur_bench.client.http.validation.validate_response_status(response_or_code: Response  int, expected_status_code: int  None = None, allow_redirects: bool  None = False) → None#
Validate the status code of a requests.Response object or (int) status code.
:param response_or_code: the requests.Response object or status code to validate
:param expected_status_code: the expected status code to check for. If None, all
codes <300 will be valid, and 3XX codes will be subject to allow_redirects
:param allow_redirects: if True will not raise an exception for 3XX status codes
:return: None
:raises InternalValueError: if expected_status_code is not None and does not match
the response code
:raises ResponseServerError: if the response has a 5XX status code
:raises ResponseClientError: if the response has a 4XX status code
:raises ResponseRedirectError: if the response has a 3XX status code
Next
arthur_bench.client.local
Previous
arthur_bench.client.auth
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.http
Submodules
construct_url()
HTTPClient
HTTPClient.delete()
HTTPClient.get()
HTTPClient.patch()
HTTPClient.post()
HTTPClient.put()
HTTPClient.send()
HTTPClient.set_path_prefix()
validate_multistatus_response_and_get_failures()
validate_response_status()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.http.html
141,"arthur_bench.client.local - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.local#
Submodules#
class arthur_bench.client.local.client.LocalBenchClient(root_dir: str  Path  None = None)#
Bases: BenchClient
Client for managing local file system test suites and runs
create_new_test_run(test_suite_id: str, json_body: CreateRunRequest) → CreateRunResponse#
Create a new run for a test suite.
Parameters:
test_suite_id – the uuid of the test suite to log a run for
json_body – run request containing run_metadata and scored model
generations
create_test_suite(json_body: TestSuiteRequest) → PaginatedTestSuite#
Create a new test suite.
Parameters:
json_body – test suite request object consisting of test suite metadata
and test cases
delete_test_run(test_suite_id: str, test_run_id: str)#
Delete a test run from a suite.
delete_test_suite(test_suite_id: str)#
Delete a test suite. All associated runs will also be deleted
get_runs_for_test_suite(test_suite_id: str, sort: CommonSortEnum  TestRunSortEnum = CommonSortEnum.CREATED_AT_ASC, page: int = 1, page_size: int = 5) → PaginatedRuns#
Get runs for a given test suite.
Parameters:
test_suite_id – the uuid of the test suite
sort – optional sort key. possible values are ‘name’, ‘avg_score’, and ‘
created_at’.
use ‘-’ prefix for descending sort. defaults to ‘created_at’
page – the page to fetch
page_size – page size to fetch
get_summary_statistics(test_suite_id: str, run_ids: list[str]  None = None, page: int = 1, page_size: int = 5) → TestSuiteSummary#
Fetch aggregate statistics of a test suite. Returns averages and score
distributions for runs in test suite.
Parameters:
test_suite_id – uuid of the test suite
run_id – optional run id. run will be included in response regardless of
page information if provided
page – the page to fetch
page_size – page size to fetch
get_test_run(test_suite_id: str, test_run_id: str, page: int = 1, page_size: int = 5, sort: TestCaseSortEnum  None = None) → PaginatedRun#
Get a test run by id.
Parameters:
test_suite_id – uuid of the test suite
test_run_id – uuid of the test run
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
sort – sort key to sort the retrieved results
get_test_suite(test_suite_id: str, page: int = 1, page_size: int = 5) → PaginatedTestSuite#
Get a test suite by id.
Parameters:
test_suite_id – the uuid of the test suite to fetch
page – the page to fetch, pagination refers to the test cases
page_size – page size to fetch, pagination refers to the test cases
get_test_suite_by_name(test_suite_name: str) → PaginatedTestSuite#
Additional getter to maintain backwards compatibility with non-identified
local files
get_test_suites(name: str  None = None, sort: CommonSortEnum  TestSuiteSortEnum = TestSuiteSortEnum.LAST_RUNTIME_ASC, scoring_method: List[str]  None = None, page: int = 1, page_size: int = 5) → PaginatedTestSuites#
Get metadata for all test suites.
Parameters:
name – filter test suites by name if provided
sort – optional sort key. possible values are ‘name’, ‘last_run_time’,
‘created_at’, use ‘-’ prefix for descending sort.
defaults to ‘last_run_time’
method (scoring) – optional filter on scoring method name,
multiple names may be provided
page – the page to fetch
page_size – page size to fetch
class arthur_bench.client.local.client.PageInfo(sorted_pages: List, start: int, end: int, page: int, page_size: int, total_pages: int, total_count: int)#
Bases: object
end: int#
page: int#
page_size: int#
sorted_pages: List#
start: int#
total_count: int#
total_pages: int#
Next
arthur_bench.client.rest
Previous
arthur_bench.client.http
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.local
Submodules
LocalBenchClient
LocalBenchClient.create_new_test_run()
LocalBenchClient.create_test_suite()
LocalBenchClient.delete_test_run()
LocalBenchClient.delete_test_suite()
LocalBenchClient.get_runs_for_test_suite()
LocalBenchClient.get_summary_statistics()
LocalBenchClient.get_test_run()
LocalBenchClient.get_test_suite()
LocalBenchClient.get_test_suite_by_name()
LocalBenchClient.get_test_suites()
PageInfo
PageInfo.end
PageInfo.page
PageInfo.page_size
PageInfo.sorted_pages
PageInfo.start
PageInfo.total_count
PageInfo.total_pages",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.local.html
142,"arthur_bench.client.rest.admin - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.rest.admin#
Submodules#
class arthur_bench.client.rest.admin.client.ArthurAdminClient(http_client: HTTPClient)#
Bases: object
A Python client to interact with the Arthur Admin API
authenticate() → AuthenticationInfo#
Returns authentication info for the calling, token-bearing user
get_current_user() → UserResponse#
Returns the currently authenticated user
login(json_body: LoginRequest) → Tuple[User, RequestsCookieJar]#
If the login attempt is successful, the user will be returned in the response
body and an HttpOnly, set-cookie “Authorization” header will be returned
that contains a JWT to be used in subsequent requests to the API in either
the “Authorization” or cookie header
Parameters:
json_body –
class arthur_bench.client.rest.admin.models.AuthenticationInfo(*, organization_ids: List[str], issuer: str, external_user_id: str  None = None, internal_user_id: str  None = None, service_account_id: str  None = None, username: str, first_name: str, last_name: str, email: str, roles: List[str])#
Bases: BaseModel
email: str#
The email of the Arthur authenticated user or the email of the external IDP user ifthe IDP is configured with that claim. For Arthur service accounts, this will be
empty.
external_user_id: str  None#
An identifier for an external-IdP token bearer. Populated if this user’s token came
from an IDP and the IDP configuration specified an oidc.CustomClaimNames that mapped
UserID to a claim.
first_name: str#
The first name of the Arthur authenticated user, or the first name claim if the
external IDP is configured with one.
internal_user_id: str  None#
An identifier for an Arthur-internal user. Populated for Arthur-authenticated users
with user tokens.
issuer: str#
The identifier of the IDP managing this user.
last_name: str#
The last name of the Arthur authenticated user, or the last name claim if the
external IDP is configured with one.
organization_ids: List[str]#
A list of organization IDs.
roles: List[str]#
The list of roles that this user has. For Arthur tokens, there will always be onerole in this array; however, there can be more than one for external providers.
service_account_id: str  None#
An identifier for an Arthur service account. Populated for Arthur-authenticated
service account tokens.
username: str#
Either the Arthur username or the username specified by an external IDP. This will
be set to arthur.ServiceAccountName for service account tokens.
class arthur_bench.client.rest.admin.models.LoginRequest(*, login: str, password: str)#
Bases: BaseModel
login: str#
either an email or a username
password: str#
class arthur_bench.client.rest.admin.models.User(*, id: str, first_name: str  None = None, last_name: str  None = None, email: str, username: str  None = None, roles: List[str]  None = None, alert_notifications_enabled: bool  None = None, show_intro_sequence: bool  None = None, help_mode_enabled: bool  None = None, created_at: datetime  None = None)#
Bases: BaseModel
alert_notifications_enabled: bool  None#
Whether or not the user will receive email notifications when alerts are triggered,
defaults to ‘false’
created_at: datetime  None#
UTC timestamp of when the user was created
email: str#
The user’s email
first_name: str  None#
The user’s first name
help_mode_enabled: bool  None#
Used by the Arthur dashboard to determine whether or not to show dashboard tooltips
id: str#
the unique id of the user
last_name: str  None#
The user’s last name
roles: List[str]  None#
The user’s roles in the current organization.
show_intro_sequence: bool  None#
Used by the Arthur dashboard to determine whether or not to show the user an intro
sequence upon login
username: str  None#
The username the user can use to login
class arthur_bench.client.rest.admin.models.UserContext(*, name: str  None = None, id: str  None = None)#
Bases: BaseModel
id: str  None#
UUID of the context.
name: str  None#
Name of the context.
class arthur_bench.client.rest.admin.models.UserResponse(*, id: str  None = None, organization_id: str, organization_name: str  None = None, first_name: str  None = None, last_name: str  None = None, email: str  None = None, username: str  None = None, roles: List[str]  None = None, alert_notifications_enabled: bool  None = None, show_intro_sequence: bool  None = None, help_mode_enabled: bool  None = None, plan: str  None = None, created_at: datetime  None = None, contexts: List[UserContext]  None = None)#
Bases: BaseModel
Represents an application user, if the client is using a service token then onlyorganization_id and roles will be populated in the object
alert_notifications_enabled: bool  None#
Whether or not the user will receive email notifications when alerts are triggered,defaults to ‘false’
contexts: List[UserContext]  None#
Contexts that the user has permissions in.
created_at: datetime  None#
UTC timestamp of when the user was created
email: str  None#
The user’s email
first_name: str  None#
The user’s first name
help_mode_enabled: bool  None#
Used by the Arthur dashboard to determine whether or not to show dashboard tooltips
id: str  None#
the unique id of the user
last_name: str  None#
The user’s last name
organization_id: str#
The ID of the users current context
organization_name: str  None#
The name of the users current context
plan: str  None#
string representation of what plan the org of the returned user is associated with(ie. self-service or paidSaas)
roles: List[str]  None#
The user’s roles
show_intro_sequence: bool  None#
used by the Arthur dashboard to determine whether the user should be shown the introsequence upon login
username: str  None#
The username the user can use to login
Next
arthur_bench.client.rest.bench
Previous
arthur_bench.client.rest
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.rest.admin
Submodules
ArthurAdminClient
ArthurAdminClient.authenticate()
ArthurAdminClient.get_current_user()
ArthurAdminClient.login()
AuthenticationInfo
AuthenticationInfo.email
AuthenticationInfo.external_user_id
AuthenticationInfo.first_name
AuthenticationInfo.internal_user_id
AuthenticationInfo.issuer
AuthenticationInfo.last_name
AuthenticationInfo.organization_ids
AuthenticationInfo.roles
AuthenticationInfo.service_account_id
AuthenticationInfo.username
LoginRequest
LoginRequest.login
LoginRequest.password
User
User.alert_notifications_enabled
User.created_at
User.email
User.first_name
User.help_mode_enabled
User.id
User.last_name
User.roles
User.show_intro_sequence
User.username
UserContext
UserContext.id
UserContext.name
UserResponse
UserResponse.alert_notifications_enabled
UserResponse.contexts
UserResponse.created_at
UserResponse.email
UserResponse.first_name
UserResponse.help_mode_enabled
UserResponse.id
UserResponse.last_name
UserResponse.organization_id
UserResponse.organization_name
UserResponse.plan
UserResponse.roles
UserResponse.show_intro_sequence
UserResponse.username",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.admin.html
143,"arthur_bench.client.rest.bench - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.rest.bench#
Submodules#
class arthur_bench.client.rest.bench.client.ArthurBenchClient(http_client: HTTPClient)#
Bases: BenchClient
A Python client to interact with the Arthur Bench API
create_new_test_run(test_suite_id: str, json_body: CreateRunRequest) → CreateRunResponse#
Creates a new test run with model version / associated metadata
Parameters:
test_suite_id –
json_body –
create_test_suite(json_body: TestSuiteRequest) → PaginatedTestSuite#
Creates a new test suite from reference data using specified scoring_method for
scoring
Parameters:
json_body –
delete_test_run(test_suite_id: str, test_run_id: str)#
Deletes a test run
Is idempotent.
Parameters:
test_suite_id –
test_run_id –
delete_test_suite(test_suite_id: str)#
Deletes test suite
Is idempotent.
Parameters:
test_suite_id –
get_runs_for_test_suite(test_suite_id: str, sort: CommonSortEnum  TestRunSortEnum = CommonSortEnum.CREATED_AT_ASC, page: int = 1, page_size: int = 5) → PaginatedRuns#
Get runs for a particular test suite (identified by test_suite_id)
Parameters:
test_suite_id –
sort –
get_summary_statistics(test_suite_id: str, run_ids: list[str]  None = None, page: int = 1, page_size: int = 5) → TestSuiteSummary#
Get paginated summary statistics of a test suite
Defaults to page size of 5.
Parameters:
test_suite_id –
run_id –
page –
page_size –
get_test_run(test_suite_id: str, test_run_id: str, page: int = 1, page_size: int = 5, sort: TestCaseSortEnum  None = TestCaseSortEnum.SCORE_ASC) → PaginatedRun#
Get a test run with input, output, and reference data
Parameters:
test_suite_id –
test_run_id –
page –
page_size –
sort – sort key to sort the retrieved results
get_test_suite(test_suite_id: str, page: int = 1, page_size: int = 5) → PaginatedTestSuite#
Get reference data for an existing test suite
Parameters:
test_suite_id –
get_test_suites(name: str  None = None, sort: CommonSortEnum  TestSuiteSortEnum = TestSuiteSortEnum.LAST_RUNTIME_ASC, scoring_method: List[str]  None = None, page: int = 1, page_size: int = 5) → PaginatedTestSuites#
Gets test suites
Sort by latest run by default.
If name query parameter is provided, filter on test suite name.
param name:
param sort:
param scoring_method:
score_hallucination(json_body: HallucinationScoreRequest) → HallucinationScoreResponse#
Next
arthur_bench.exceptions
Previous
arthur_bench.client.rest.admin
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.rest.bench
Submodules
ArthurBenchClient
ArthurBenchClient.create_new_test_run()
ArthurBenchClient.create_test_suite()
ArthurBenchClient.delete_test_run()
ArthurBenchClient.delete_test_suite()
ArthurBenchClient.get_runs_for_test_suite()
ArthurBenchClient.get_summary_statistics()
ArthurBenchClient.get_test_run()
ArthurBenchClient.get_test_suite()
ArthurBenchClient.get_test_suites()
ArthurBenchClient.score_hallucination()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.bench.html
144,"arthur_bench.client.rest - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.client.rest#
Subpackages#
arthur_bench.client.rest.admin
Submodules
ArthurAdminClient
ArthurAdminClient.authenticate()
ArthurAdminClient.get_current_user()
ArthurAdminClient.login()
AuthenticationInfo
AuthenticationInfo.email
AuthenticationInfo.external_user_id
AuthenticationInfo.first_name
AuthenticationInfo.internal_user_id
AuthenticationInfo.issuer
AuthenticationInfo.last_name
AuthenticationInfo.organization_ids
AuthenticationInfo.roles
AuthenticationInfo.service_account_id
AuthenticationInfo.username
LoginRequest
LoginRequest.login
LoginRequest.password
User
User.alert_notifications_enabled
User.created_at
User.email
User.first_name
User.help_mode_enabled
User.id
User.last_name
User.roles
User.show_intro_sequence
User.username
UserContext
UserContext.id
UserContext.name
UserResponse
UserResponse.alert_notifications_enabled
UserResponse.contexts
UserResponse.created_at
UserResponse.email
UserResponse.first_name
UserResponse.help_mode_enabled
UserResponse.id
UserResponse.last_name
UserResponse.organization_id
UserResponse.organization_name
UserResponse.plan
UserResponse.roles
UserResponse.show_intro_sequence
UserResponse.username
arthur_bench.client.rest.bench
Submodules
ArthurBenchClient
ArthurBenchClient.create_new_test_run()
ArthurBenchClient.create_test_suite()
ArthurBenchClient.delete_test_run()
ArthurBenchClient.delete_test_suite()
ArthurBenchClient.get_runs_for_test_suite()
ArthurBenchClient.get_summary_statistics()
ArthurBenchClient.get_test_run()
ArthurBenchClient.get_test_suite()
ArthurBenchClient.get_test_suites()
ArthurBenchClient.score_hallucination()
Submodules#
class arthur_bench.client.rest.client.ArthurClient(url: str  None = None, login: str  None = None, password: str  None = None, api_key: str  None = None, organization_id: str  None = None, verify_ssl: bool  None = None, allow_insecure: bool = False, offline: bool = False)#
Bases: object
Next
arthur_bench.client.rest.admin
Previous
arthur_bench.client.local
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.client.rest
Subpackages
Submodules
ArthurClient",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.client.rest.html
145,"arthur_bench.exceptions - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.exceptions#
Submodules#
exception arthur_bench.exceptions.exceptions.ArthurError#
Bases: Exception
Base Error for Arthur SDK. This class should not be used directly, Arthur exceptions
should inherit from either ArthurUserError or ArthurInternalError.
exception arthur_bench.exceptions.exceptions.ArthurInternalError#
Bases: ArthurError
Exception raised when user input is correct but an error occurs. Can be used
directly but children are preferred.
exception arthur_bench.exceptions.exceptions.ArthurUserError#
Bases: ArthurError
Exception raised due to incorrect user input to the Arthur SDK. Can be used directly
but children are preferred.
exception arthur_bench.exceptions.exceptions.ExpectedParameterNotFoundError#
Bases: ArthurInternalError
Exception raised when a field or property should be available from Arthur but is
unexpectedly missing.
exception arthur_bench.exceptions.exceptions.ForbiddenError#
Bases: ResponseClientError
Exception raised when a 403 Forbidden response is received from the API.
exception arthur_bench.exceptions.exceptions.InternalTypeError#
Bases: ArthurInternalError, TypeError
Exception raised when a value is unexpected.
exception arthur_bench.exceptions.exceptions.InternalValueError#
Bases: ArthurInternalError, ValueError
Exception raised when a value is unexpected.
exception arthur_bench.exceptions.exceptions.MethodNotApplicableError#
Bases: ArthurUserError
Exception raised when the method called is not valid for the resource.
exception arthur_bench.exceptions.exceptions.MissingParameterError#
Bases: ArthurUserError
Exception raised when parameters supplied to the Arthur SDK are missing.
exception arthur_bench.exceptions.exceptions.NotFoundError#
Bases: ResponseClientError
Exception raised when a 404 Not Found response is received from the API.
exception arthur_bench.exceptions.exceptions.PaymentRequiredError#
Bases: ResponseClientError
Exception raised when a 402 response is received from the API due to a user trying
to access features not available in their plan.
exception arthur_bench.exceptions.exceptions.ResponseClientError#
Bases: ArthurUserError
Exception raised when a 4XX response is received from the API.
exception arthur_bench.exceptions.exceptions.ResponseRedirectError#
Bases: ArthurInternalError
Exception raised when a 3XX response is unexpectedly received from the API.
exception arthur_bench.exceptions.exceptions.ResponseServerError#
Bases: ArthurInternalError
Exception raised when a 5XX response is received from the API.
exception arthur_bench.exceptions.exceptions.UnauthorizedError#
Bases: ResponseClientError
Exception raised when a 401 Unauthorized response is received from the API.
exception arthur_bench.exceptions.exceptions.UserTypeError#
Bases: ArthurUserError, TypeError
Exception raised when a user supplies an argument of the incorrect type to the
Arthur SDK.
exception arthur_bench.exceptions.exceptions.UserValueError#
Bases: ArthurUserError, ValueError
Exception raised when a user supplies an invalid value to the Arthur SDK.
arthur_bench.exceptions.exceptions.arthur_excepted(message=None)#
Decorator to wrap user-facing Arthur functions with exception handling that
describes to the user whether the error is their fault or is our fault and should be
reported.
:param message: an optional message to prefix the error with, should describe the
failure e.g. “failed to send
inferences” or “an error occurred while creating the model.”
:return: the decorator function
Next
arthur_bench.models
Previous
arthur_bench.client.rest.bench
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.exceptions
Submodules
ArthurError
ArthurInternalError
ArthurUserError
ExpectedParameterNotFoundError
ForbiddenError
InternalTypeError
InternalValueError
MethodNotApplicableError
MissingParameterError
NotFoundError
PaymentRequiredError
ResponseClientError
ResponseRedirectError
ResponseServerError
UnauthorizedError
UserTypeError
UserValueError
arthur_excepted()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.exceptions.html
146,"arthur_bench.models - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.models#
Submodules#
class arthur_bench.models.models.CategoricalHistogramItem(*, count: int, category: Category)#
Bases: BaseModel
category: Category#
count: int#
class arthur_bench.models.models.Category(*, name: str, description: str  None = None)#
Bases: BaseModel
description: str  None#
name: str#
class arthur_bench.models.models.CommonSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
CREATED_AT_ASC = 'created_at'#
CREATED_AT_DESC = '-created_at'#
NAME_ASC = 'name'#
NAME_DESC = '-name'#
class arthur_bench.models.models.CreateRunRequest(*, name: str, test_case_outputs: List[TestCaseOutput], description: str  None = None, model_name: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None, model_version: str  None = None)#
Bases: BaseModel
class Config#
Bases: object
allow_population_by_field_name = True#
classmethod consistent_categories(v)#
description: str  None#
Optional description of the run
foundation_model: str  None#
Optional foundation model name identifiying the pretrained model used to generate
outputs
model_name: str  None#
Optional model name identifying the model used to generate outputs
model_version: str  None#
Optional model version identifying the version of the model used to generate outputs
name: str#
Name identifier of the run
prompt_template: str  None#
Optional prompt template name identifying the global prompt used to generate outputs
test_cases: List[TestCaseOutput]#
List of outputs and scores for all cases in the test suite
class arthur_bench.models.models.CreateRunResponse(*, id: UUID)#
Bases: BaseModel
id: UUID#
class arthur_bench.models.models.HistogramItem(*, count: int, low: float, high: float)#
Bases: BaseModel
Boundaries and count for a single bucket of a run histogram
count: int#
high: float#
low: float#
class arthur_bench.models.models.PaginatedRun(*, id: UUID, name: str, test_suite_id: UUID, test_case_runs: List[RunResult], updated_at: datetime, created_at: datetime, page: int  None = None, page_size: int  None = None, total_pages: int  None = None, total_count: int  None = None)#
Bases: BaseModel
Paginated list of prompts, reference outputs, model outputs, and scores for a
particular run.
class Config#
Bases: object
allow_population_by_field_name = True#
created_at: datetime#
id: UUID#
name: str#
page: int  None#
page_size: int  None#
test_cases: List[RunResult]#
test_suite_id: UUID#
total_count: int  None#
total_pages: int  None#
updated_at: datetime#
class arthur_bench.models.models.PaginatedRuns(*, test_runs: List[TestRunMetadata], page: int, page_size: int, total_pages: int, total_count: int)#
Bases: BaseModel
Paginated list of runs for a test suite.
page: int#
page_size: int#
test_runs: List[TestRunMetadata]#
total_count: int#
total_pages: int#
class arthur_bench.models.models.PaginatedTestSuite(*, id: UUID, name: str, scoring_method: ScoringMethod, test_cases: List[TestCaseResponse], created_at: datetime, updated_at: datetime, description: str  None = None, last_run_time: datetime  None = None, num_runs: int = 0, page: int  None = None, page_size: int  None = None, total_pages: int  None = None, total_count: int  None = None)#
Bases: BaseModel
Test suite and optional page information
created_at: datetime#
description: str  None#
id: UUID#
last_run_time: datetime  None#
name: str#
num_runs: int#
page: int  None#
page_size: int  None#
scoring_method: ScoringMethod#
test_cases: List[TestCaseResponse]#
total_count: int  None#
total_pages: int  None#
updated_at: datetime#
class arthur_bench.models.models.PaginatedTestSuites(*, test_suites: List[TestSuiteMetadata], page: int, page_size: int, total_pages: int, total_count: int)#
Bases: BaseModel
Paginated list of test suites.
page: int#
page_size: int#
test_suites: List[TestSuiteMetadata]#
total_count: int#
total_pages: int#
class arthur_bench.models.models.RunResult(*, id: UUID, output: str, score: float, input: str  None = None, reference_output: str  None = None, score_result: ScoreResult)#
Bases: BaseModel
id: UUID#
input: str  None#
output: str#
reference_output: str  None#
score: float#
score_result: ScoreResult#
classmethod score_result_backwards_compatible(values)#
class arthur_bench.models.models.ScoreResult(*, score: float  None = None, category: Category  None = None)#
Bases: BaseModel
category: Category  None#
classmethod contains_score(values)#
score: float  None#
class arthur_bench.models.models.ScorerOutputType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
Indicates the output type of the scorer
Categorical = 'categorical'#
Continuous = 'continuous'#
class arthur_bench.models.models.ScoringMethod(*, name: str, type: ScoringMethodType, config: dict = {}, output_type: ScorerOutputType = ScorerOutputType.Continuous, categories: List[Category]  None = None)#
Bases: BaseModel
Scoring method configuration
categories: List[Category]  None#
Valid categories returned by the scorer. Only valid if categories is True.
config: dict#
Configuration as used by the scorer to_dict and from_dict methods
name: str#
Name of the scorer
output_type: ScorerOutputType#
Whether the scoring method returns categorical scores
classmethod scoring_method_categorical_defined(values)#
type: ScoringMethodType#
Whether the scoring method was bench default or custom implementation
class arthur_bench.models.models.ScoringMethodType(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
Indicates whether the scoring method was provided by the package or a custom
implementation
BuiltIn = 'built_in'#
Custom = 'custom'#
class arthur_bench.models.models.SummaryItem(*, id: UUID, name: str, avg_score: float, histogram: List[HistogramItem  CategoricalHistogramItem])#
Bases: BaseModel
Aggregate statistics for a single run: average score and score distribution
avg_score: float#
classmethod either_continuous_or_categorical(v)#
Validate that the items in the histogram list are all
containing low/high floats or are all containing a category
histogram: List[HistogramItem  CategoricalHistogramItem]#
id: UUID#
name: str#
class arthur_bench.models.models.TestCaseOutput(*, id: UUID, output: str, score: float  None = None, score_result: ScoreResult)#
Bases: BaseModel
A generated output, score pair
id: UUID#
Optional unique identifier for this test case of the suite and run
output: str#
Generated output for test case
score: float  None#
Score assigned to output. This field is decprecated, used score_result instead
score_result: ScoreResult#
Score information about output. Contains float score and / or category description
classmethod score_result_backwards_compatible(values)#
class arthur_bench.models.models.TestCaseRequest(*, input: str, reference_output: str  None = None)#
Bases: BaseModel
An input, reference output pair.
input: str#
Input to the test case. Does not include the prompt template.
reference_output: str  None#
Reference or “Golden” output for the given input.
class arthur_bench.models.models.TestCaseResponse(*, id: UUID, input: str, reference_output: str  None = None)#
Bases: BaseModel
id: UUID#
input: str#
Input to the test case. Does not include the prompt template.
reference_output: str  None#
Reference or “Golden” output for the given input.
class arthur_bench.models.models.TestCaseSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
SCORE_ASC = 'score'#
SCORE_DESC = '-score'#
class arthur_bench.models.models.TestRunMetadata(*, id: UUID, name: str, created_at: datetime, updated_at: datetime, avg_score: float  None = None, model_version: str  None = None, prompt_template: str  None = None)#
Bases: BaseModel
avg_score: float  None#
created_at: datetime#
id: UUID#
model_version: str  None#
name: str#
prompt_template: str  None#
updated_at: datetime#
class arthur_bench.models.models.TestRunSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
AVG_SCORE_ASC = 'avg_score'#
AVG_SCORE_DESC = '-avg_score'#
class arthur_bench.models.models.TestSuiteMetadata(*, id: UUID, name: str, scoring_method: ScoringMethod, last_run_time: datetime  None = None, description: str  None = None, created_at: datetime  None = None, updated_at: datetime  None = None)#
Bases: BaseModel
created_at: datetime  None#
description: str  None#
id: UUID#
last_run_time: datetime  None#
name: str#
scoring_method: ScoringMethod#
updated_at: datetime  None#
class arthur_bench.models.models.TestSuiteRequest(*, name: str, description: str  None = None, scoring_method: ScoringMethod, test_cases: ConstrainedListValue[TestCaseRequest])#
Bases: BaseModel
Test case data and metadata for the test suite.
description: str  None#
Optional description of the test suite
name: str#
Name of the test suite
classmethod null_reference_outputs_all_or_none(v)#
Validate that all or none of test case reference outputs are null
scoring_method: ScoringMethod#
Scoring configuration to use as criteria for the test suite
classmethod scoring_method_backwards_compatible(v)#
test_cases: List[TestCaseRequest]#
List of input texts and optional reference outputs to consistently score
model generations against
class arthur_bench.models.models.TestSuiteSortEnum(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
LAST_RUNTIME_ASC = 'last_run_time'#
LAST_RUNTIME_DESC = '-last_run_time'#
class arthur_bench.models.models.TestSuiteSummary(*, summary: List[SummaryItem], page: int, page_size: int, total_pages: int, total_count: int, num_test_cases: int, categorical: bool = False)#
Bases: BaseModel
Aggregate descriptions of runs of a test suite.
Provides averages and score distributions
categorical: bool#
num_test_cases: int#
page: int#
page_size: int#
summary: List[SummaryItem]#
total_count: int#
total_pages: int#
class arthur_bench.models.scoring.HallucinationScoreRequest(*, response: str, context: str)#
Bases: BaseModel
Request for hallucination classification
context: str#
Context with which to determine if the model generated response is supported
response: str#
Model generated response
class arthur_bench.models.scoring.HallucinationScoreResponse(*, hallucination: bool, reason: str)#
Bases: BaseModel
Hallucination classification
hallucination: bool#
True if hallucination, false otherwise
reason: str#
Justification for the hallucination classification
Next
arthur_bench.run
Previous
arthur_bench.exceptions
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.models
Submodules
CategoricalHistogramItem
CategoricalHistogramItem.category
CategoricalHistogramItem.count
Category
Category.description
Category.name
CommonSortEnum
CommonSortEnum.CREATED_AT_ASC
CommonSortEnum.CREATED_AT_DESC
CommonSortEnum.NAME_ASC
CommonSortEnum.NAME_DESC
CreateRunRequest
CreateRunRequest.Config
CreateRunRequest.Config.allow_population_by_field_name
CreateRunRequest.consistent_categories()
CreateRunRequest.description
CreateRunRequest.foundation_model
CreateRunRequest.model_name
CreateRunRequest.model_version
CreateRunRequest.name
CreateRunRequest.prompt_template
CreateRunRequest.test_cases
CreateRunResponse
CreateRunResponse.id
HistogramItem
HistogramItem.count
HistogramItem.high
HistogramItem.low
PaginatedRun
PaginatedRun.Config
PaginatedRun.Config.allow_population_by_field_name
PaginatedRun.created_at
PaginatedRun.id
PaginatedRun.name
PaginatedRun.page
PaginatedRun.page_size
PaginatedRun.test_cases
PaginatedRun.test_suite_id
PaginatedRun.total_count
PaginatedRun.total_pages
PaginatedRun.updated_at
PaginatedRuns
PaginatedRuns.page
PaginatedRuns.page_size
PaginatedRuns.test_runs
PaginatedRuns.total_count
PaginatedRuns.total_pages
PaginatedTestSuite
PaginatedTestSuite.created_at
PaginatedTestSuite.description
PaginatedTestSuite.id
PaginatedTestSuite.last_run_time
PaginatedTestSuite.name
PaginatedTestSuite.num_runs
PaginatedTestSuite.page
PaginatedTestSuite.page_size
PaginatedTestSuite.scoring_method
PaginatedTestSuite.test_cases
PaginatedTestSuite.total_count
PaginatedTestSuite.total_pages
PaginatedTestSuite.updated_at
PaginatedTestSuites
PaginatedTestSuites.page
PaginatedTestSuites.page_size
PaginatedTestSuites.test_suites
PaginatedTestSuites.total_count
PaginatedTestSuites.total_pages
RunResult
RunResult.id
RunResult.input
RunResult.output
RunResult.reference_output
RunResult.score
RunResult.score_result
RunResult.score_result_backwards_compatible()
ScoreResult
ScoreResult.category
ScoreResult.contains_score()
ScoreResult.score
ScorerOutputType
ScorerOutputType.Categorical
ScorerOutputType.Continuous
ScoringMethod
ScoringMethod.categories
ScoringMethod.config
ScoringMethod.name
ScoringMethod.output_type
ScoringMethod.scoring_method_categorical_defined()
ScoringMethod.type
ScoringMethodType
ScoringMethodType.BuiltIn
ScoringMethodType.Custom
SummaryItem
SummaryItem.avg_score
SummaryItem.either_continuous_or_categorical()
SummaryItem.histogram
SummaryItem.id
SummaryItem.name
TestCaseOutput
TestCaseOutput.id
TestCaseOutput.output
TestCaseOutput.score
TestCaseOutput.score_result
TestCaseOutput.score_result_backwards_compatible()
TestCaseRequest
TestCaseRequest.input
TestCaseRequest.reference_output
TestCaseResponse
TestCaseResponse.id
TestCaseResponse.input
TestCaseResponse.reference_output
TestCaseSortEnum
TestCaseSortEnum.SCORE_ASC
TestCaseSortEnum.SCORE_DESC
TestRunMetadata
TestRunMetadata.avg_score
TestRunMetadata.created_at
TestRunMetadata.id
TestRunMetadata.model_version
TestRunMetadata.name
TestRunMetadata.prompt_template
TestRunMetadata.updated_at
TestRunSortEnum
TestRunSortEnum.AVG_SCORE_ASC
TestRunSortEnum.AVG_SCORE_DESC
TestSuiteMetadata
TestSuiteMetadata.created_at
TestSuiteMetadata.description
TestSuiteMetadata.id
TestSuiteMetadata.last_run_time
TestSuiteMetadata.name
TestSuiteMetadata.scoring_method
TestSuiteMetadata.updated_at
TestSuiteRequest
TestSuiteRequest.description
TestSuiteRequest.name
TestSuiteRequest.null_reference_outputs_all_or_none()
TestSuiteRequest.scoring_method
TestSuiteRequest.scoring_method_backwards_compatible()
TestSuiteRequest.test_cases
TestSuiteSortEnum
TestSuiteSortEnum.LAST_RUNTIME_ASC
TestSuiteSortEnum.LAST_RUNTIME_DESC
TestSuiteSummary
TestSuiteSummary.categorical
TestSuiteSummary.num_test_cases
TestSuiteSummary.page
TestSuiteSummary.page_size
TestSuiteSummary.summary
TestSuiteSummary.total_count
TestSuiteSummary.total_pages
HallucinationScoreRequest
HallucinationScoreRequest.context
HallucinationScoreRequest.response
HallucinationScoreResponse
HallucinationScoreResponse.hallucination
HallucinationScoreResponse.reason",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.models.html
147,"arthur_bench.run - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.run#
Submodules#
class arthur_bench.run.testrun.TestRun(*, name: str, test_case_outputs: List[TestCaseOutput], description: str  None = None, model_name: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None, model_version: str  None = None, test_suite_id: UUID, client: BenchClient, id: UUID  None = None)#
Bases: CreateRunRequest
class Config#
Bases: object
arbitrary_types_allowed = True#
property categories: List[str  None]#
client: BenchClient#
classmethod from_flattened(run_name: str, ids: List[UUID], candidate_output_list: List[str], scores: List[float]  List[ScoreResult], client: BenchClient, test_suite_id: UUID, model_name: str  None = None, model_version: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None)#
id: UUID  None#
property output: List[str]#
save() → UUID#
Save a test run.
property scores: List[float  None]#
test_suite_id: UUID#
class arthur_bench.run.testsuite.TestSuite(name: str, scoring_method: str  Scorer, description: str  None = None, reference_data: DataFrame  None = None, reference_data_path: str  None = None, input_column: str = 'input', reference_column: str = 'reference_output', input_text_list: List[str]  None = None, reference_output_list: List[str]  None = None, client: BenchClient  None = None)#
Bases: object
Reusable pipeline for running a test suite built from reference_data and evaluated
using scoring_method
Parameters:
name – name of the test suite
scoring_method – scoring method or scorer instance to use to evaluate the
results of a test run, as a string/enum or class instance
description – short description of the task tested by this suite
reference_data – dataframe of prompts and reference outputs
reference_data_path – filepath to csv of prompts and reference outputs,
required if not specifying reference_data
input_column – the column of reference_data containing prompts, defaults to
‘prompt’
reference_column – the column of reference_data containing reference outputs,
defaults to ‘reference’
input_text_list – list of strings of input texts that can be provided instead
of dataframe columns
reference_output_list – list of strings of reference outputs that can be
provided instead of dataframe columns
async arun(run_name: str, candidate_data: DataFrame  None = None, candidate_data_path: str  None = None, candidate_column: str = 'candidate_output', candidate_output_list: List[str]  None = None, context_column: str  None = None, context_list: List[str]  None = None, save: bool = True, batch_size: int = 5, model_name: str  None = None, model_version: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None) → TestRun#
property description: str  None#
property input_texts: List[str]#
property name: str#
property reference_outputs: List[str  None]#
run(run_name: str, candidate_data: DataFrame  None = None, candidate_data_path: str  None = None, candidate_column: str = 'candidate_output', candidate_output_list: List[str]  None = None, context_column: str  None = None, context_list: List[str]  None = None, save: bool = True, batch_size: int = 1, model_name: str  None = None, model_version: str  None = None, foundation_model: str  None = None, prompt_template: str  None = None) → TestRun#
Score a test run on candidate outputs.
Parameters:
run_name – name for the test run
candidate_data – dataframe of candidate responses to test prompts
candidate_data_path – filepath to csv containing candidate responses to
test prompts
candidate_column – the column of candidate data containing candidate
responses, defaults to ‘candidate_output’
candidate_output_list – list of strings of candidate outputs that can be
provided instead of dataframe
context_column – the column of reference_data containing supporting
context for answering Question & Answering tasks
context_list – list of strings containing supporting context for answering
question and answering tasks
save – whether to save the run results to file
batch_size – the batch_size to use when computing scores
model_name – model name for model used to generate outputs
model_version – model version of model used to generate outputs
foundation_model – foundation model name used to generate outputs
prompt_template – prompt template name used to generate outputs
Returns:
TestRun object containing scored outputs
save()#
Save a test suite to local file system.
property scoring_method: str#
property test_cases: List[TestCaseResponse]#
Next
arthur_bench.scoring
Previous
arthur_bench.models
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.run
Submodules
TestRun
TestRun.Config
TestRun.Config.arbitrary_types_allowed
TestRun.categories
TestRun.client
TestRun.from_flattened()
TestRun.id
TestRun.output
TestRun.save()
TestRun.scores
TestRun.test_suite_id
TestSuite
TestSuite.arun()
TestSuite.description
TestSuite.input_texts
TestSuite.name
TestSuite.reference_outputs
TestSuite.run()
TestSuite.save()
TestSuite.scoring_method
TestSuite.test_cases",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.run.html
148,"arthur_bench.scoring - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.scoring#
class arthur_bench.scoring.ScoringMethodName(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)#
Bases: str, Enum
BERTScore = 'bertscore'#
ExactMatch = 'exact_match'#
Hallucination = 'hallucination'#
HedgingLanguage = 'hedging_language'#
PythonUnitTesting = 'python_unit_testing'#
QACorrectness = 'qa_correctness'#
Readability = 'readability'#
Specificity = 'specificity'#
SummaryQuality = 'summary_quality'#
WordCountMatch = 'word_count_match'#
arthur_bench.scoring.scorer_from_string(method: str) → type[arthur_bench.scoring.scorer.Scorer]#
Submodules#
class arthur_bench.scoring.bertscore.BERTScore(model_type='microsoft/deberta-v3-base', precision_weight=0.1)#
Bases: Scorer
Tailored bert score implementation.
https://arxiv.org/abs/1904.09675
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.exact_match.ExactMatch(case_sensitive=True)#
Bases: Scorer
Returns 1 if candidate matches reference, 0 if candidate does not match reference.
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
class arthur_bench.scoring.hallucination.Hallucination#
Bases: Scorer
Score each output against a context using Arthur’s hosted hallucination checker
A score of 1.0 means the hallucination checker estimates the output is supported by
the context
A score of 0.0 means the hallucination checker found information in the output
unsupported by the context
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.hedging_language.HedgingLanguage(model_type: str = 'microsoft/deberta-v3-base', hedging_language: str = ""As an AI language model, I don't have personal opinions, emotions, or beliefs."")#
Bases: Scorer
Given an input question and model output, determine if the output contains hedging
language such as “As an AI language model, I don’t have personal opinions, emotions,
or beliefs”. The values returned are a similarity score (BERTScore), with higher
values corresponding to higher likelihood of hedging language being present in the
model output.
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.python_unit_testing.PythonUnitTesting(unit_test_dir: str  None = None, unit_tests: List[str]  None = None)#
Bases: Scorer
Wrapping the HuggingFace code_eval metric
Scores each candidate_output as a function against a pre-prepared unit test
Note: considers any code with non-standard python libraries (e.g. numpy) to have an
error
https://huggingface.co/spaces/evaluate-metric/code_eval
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 1) → List[ScoreResult]#
Score a set of test cases. This method doesn’t need to be implemented in most
cases, but can be overriden to add additional functionality such as
task-specific logging.
Parameters:
candidate_outputs – candidate generations to score
reference_outputs – reference strings representing target outputs
inputs – input strings being tested
contexts – optional corresponding contexts, if needed by scorer
batch_size – size of batches
Returns:
scoring results for this run. Float scores are deprecated,
use ScoreResult instead
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
class arthur_bench.scoring.qa_quality.QAQualityCorrectness(llm: BaseChatModel  None = None)#
Bases: Scorer
Given an input question, context string, and model generation, determine if the
generation produced a correct answer.
async arun_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Reference batch is not used for this scoring method, QA correctness requires an
input_text_batch and context_batch
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Reference batch is not used for this scoring method, QA correctness requires an
input_text_batch and context_batch
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
static validate_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → Tuple[List[str], List[str]]#
class arthur_bench.scoring.readability.Readability#
Bases: Scorer
Flesch Reading Ease Score: the higher the score, the easier to read.
Scores of 100-90 correlate to a 5th grade reading level, while scores <10 are
classified as “Extremely difficult to read, and best understood by university
graduates.”
https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
class arthur_bench.scoring.scorer.Scorer#
Bases: ABC
Base class for all scorers. Compute a float score for a given model generation.
async arun(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 5) → List[float]  List[ScoreResult]#
Async version of run method.
async arun_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Async version of run_batch method.
static categories() → List[Category]  None#
All possible values returned by the scorer if output type is categorical.
classmethod from_dict(config: dict)#
Load a scorer from a json configuration file.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
abstract static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 1) → List[float]  List[ScoreResult]#
Score a set of test cases. This method doesn’t need to be implemented in most
cases, but can be overriden to add additional functionality such as
task-specific logging.
Parameters:
candidate_outputs – candidate generations to score
reference_outputs – reference strings representing target outputs
inputs – input strings being tested
contexts – optional corresponding contexts, if needed by scorer
batch_size – size of batches
Returns:
scoring results for this run. Float scores are deprecated,
use ScoreResult instead
abstract run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
to_metadata() → ScoringMethod#
classmethod type() → ScoringMethodType#
Supplies whether a scorer is built-in or custom.
This method is implemented by checking whether the Scorer class is part of the
arthur_bench.scoring module.
:return: the type (built-in or custom)
class arthur_bench.scoring.specificity.Specificity#
Bases: Scorer
Returns a score from 0.0 to 1.0 indicating how specific the candidate output
language is. Higher scores indicate that the language is more specific,
Lower scores indicate more vague language.
Specificity is computed through detecting words that indicate vagueness (predefined)
determing how rare the words used are according to word frequencies calculated by
popular nlp corpora, and detecting use of proper nouns and numbers.
get_mean_word_freq(candidate_output: str) → float#
Returns mean word frequency of candidate output. Higher values indicate that
moree common words on average are used in the candidate output.
Considers only words with frequency <0.001, truncating probability of words with
higher frequencies to 0.001.
get_num_vague_words(candidate_output: str) → int#
Returns number of words in candidate_output which are is a list of pre-defined
vague words.
get_pn_and_num(candidate_output: str) → int#
Returns total number of Proper Nouns and Numbers in candidate output.
Determined heuristically via NNP and CD nltk tags.
static name() → str#
Get the name of this Scorer
:return: the Scorer name
static requires_reference() → bool#
True if scorer requires reference output to compute score, False otherwise
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
class arthur_bench.scoring.summary_quality.SummaryQuality(llm: BaseChatModel  None = None, context_window: int = 4096, tokenizer: Encoding  None = None)#
Bases: Scorer
Comprehensive measure of summarization quality compared to a reference summary.
async arun(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 5) → List[float]  List[ScoreResult]#
Async version of run method.
async arun_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[float]  List[ScoreResult]#
Summary quality requires input_text_batch. Asynchronous implementation
static categories() → List[Category]#
All possible values returned by the scorer if output type is categorical.
static is_categorical() → bool#
Whether the scorer is continuous or categorical.
categories() should be implemented if True
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run(candidate_outputs: List[str], reference_outputs: List[str]  None = None, inputs: List[str]  None = None, contexts: List[str]  None = None, batch_size: int = 1) → List[ScoreResult]  List[float]#
Score a set of test cases. This method doesn’t need to be implemented in most
cases, but can be overriden to add additional functionality such as
task-specific logging.
Parameters:
candidate_outputs – candidate generations to score
reference_outputs – reference strings representing target outputs
inputs – input strings being tested
contexts – optional corresponding contexts, if needed by scorer
batch_size – size of batches
Returns:
scoring results for this run. Float scores are deprecated,
use ScoreResult instead
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Summary quality requires input_text_batch.
to_dict(warn=False)#
Provides a json serializable representation of the scorer.
static validate_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → Tuple[List[str], List[str]]#
arthur_bench.scoring.summary_quality.truncate_input_text(input_text, ref_output, cand_output, context_window: int = 4096, tokenizer: ~tiktoken.core.Encoding = <Encoding 'cl100k_base'>) → Tuple[str, bool]#
Truncates the input_text to fit in LLM evaluator context
Truncate the input text so that the filled-in COMPARE prompt
which contains {input text + summary A + summary B} fits in the evaluator context
window
Returns the tuple (text, whether text was truncated)
class arthur_bench.scoring.utils.suppress_warnings(logger_name: str)#
Bases: object
A context-manager class to temporarily set the logging level for a logger to ERROR
before returning it to its previous state.
class arthur_bench.scoring.word_count_match.WordCountMatch#
Bases: Scorer
Calculates how similar the number of words in the candidate output is to the the
number of words in the reference output. Scores span from 0 to 1.
A score of 1.0 indicates that there are the same number of words in the candidate
output as in the reference output. Scores less than 1.0 are calculated as
((len_reference-delta)/len_reference) where delta is the absolute difference in
word lengths between the candidate and reference outputs.
All negative computed values are truncated to 0.
Utilizes lexicon count, removing punctuations: https://pypi.org/project/textstat/
static name() → str#
Get the name of this Scorer
:return: the Scorer name
run_batch(candidate_batch: List[str], reference_batch: List[str]  None = None, input_text_batch: List[str]  None = None, context_batch: List[str]  None = None) → List[ScoreResult]#
Score a batch of candidate generations.
Parameters:
candidate_batch – candidate generations to score
reference_batch – reference strings representing target outputs
input_text_batch – optional corresponding inputs
context_batch – optional corresponding contexts, if needed by scorer
Returns:
scoring results for this batch. Float scores are deprecated,
use ScoreResult instead
Next
arthur_bench.server
Previous
arthur_bench.run
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.scoring
ScoringMethodName
ScoringMethodName.BERTScore
ScoringMethodName.ExactMatch
ScoringMethodName.Hallucination
ScoringMethodName.HedgingLanguage
ScoringMethodName.PythonUnitTesting
ScoringMethodName.QACorrectness
ScoringMethodName.Readability
ScoringMethodName.Specificity
ScoringMethodName.SummaryQuality
ScoringMethodName.WordCountMatch
scorer_from_string()
Submodules
BERTScore
BERTScore.name()
BERTScore.run_batch()
BERTScore.to_dict()
ExactMatch
ExactMatch.categories()
ExactMatch.is_categorical()
ExactMatch.name()
ExactMatch.run_batch()
Hallucination
Hallucination.categories()
Hallucination.is_categorical()
Hallucination.name()
Hallucination.requires_reference()
Hallucination.run_batch()
Hallucination.to_dict()
HedgingLanguage
HedgingLanguage.name()
HedgingLanguage.requires_reference()
HedgingLanguage.run_batch()
HedgingLanguage.to_dict()
PythonUnitTesting
PythonUnitTesting.categories()
PythonUnitTesting.is_categorical()
PythonUnitTesting.name()
PythonUnitTesting.requires_reference()
PythonUnitTesting.run()
PythonUnitTesting.run_batch()
PythonUnitTesting.to_dict()
QAQualityCorrectness
QAQualityCorrectness.arun_batch()
QAQualityCorrectness.categories()
QAQualityCorrectness.is_categorical()
QAQualityCorrectness.name()
QAQualityCorrectness.requires_reference()
QAQualityCorrectness.run_batch()
QAQualityCorrectness.to_dict()
QAQualityCorrectness.validate_batch()
Readability
Readability.name()
Readability.requires_reference()
Readability.run_batch()
Scorer
Scorer.arun()
Scorer.arun_batch()
Scorer.categories()
Scorer.from_dict()
Scorer.is_categorical()
Scorer.name()
Scorer.requires_reference()
Scorer.run()
Scorer.run_batch()
Scorer.to_dict()
Scorer.to_metadata()
Scorer.type()
Specificity
Specificity.get_mean_word_freq()
Specificity.get_num_vague_words()
Specificity.get_pn_and_num()
Specificity.name()
Specificity.requires_reference()
Specificity.run_batch()
SummaryQuality
SummaryQuality.arun()
SummaryQuality.arun_batch()
SummaryQuality.categories()
SummaryQuality.is_categorical()
SummaryQuality.name()
SummaryQuality.run()
SummaryQuality.run_batch()
SummaryQuality.to_dict()
SummaryQuality.validate_batch()
truncate_input_text()
suppress_warnings
WordCountMatch
WordCountMatch.name()
WordCountMatch.run_batch()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.scoring.html
149,"arthur_bench.server - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.server#
Submodules#
Next
arthur_bench.telemetry
Previous
arthur_bench.scoring
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.server
Submodules",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.server.html
150,"arthur_bench.telemetry - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.telemetry#
Submodules#
Next
arthur_bench.utils
Previous
arthur_bench.server
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.telemetry
Submodules",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.telemetry.html
151,"arthur_bench.utils - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
arthur_bench.utils#
Submodules#
arthur_bench.utils.loaders.get_file_extension(filepath: str  PathLike) → str#
arthur_bench.utils.loaders.load_suite_from_csv(filepath: str  PathLike, input_column: str, reference_column: str  None = None) → List[TestCaseRequest]#
Load test case data from csv file.
Parameters:
filepath – string or pathlike object pointing to csv file
input_column – column in file containing inputs
reference_column – column in file containing reference outputs
arthur_bench.utils.loaders.load_suite_from_dataframe(data: DataFrame, input_column: str, reference_column: str  None = None) → List[TestCaseRequest]#
Load test case data from a pandas dataframe.
Parameters:
data – dataframe where each row is a test case consisting of a column for i
input and a column for reference
input_column – column in dataframe containing inputs
reference_column – column in dataframe containing reference outputs
arthur_bench.utils.loaders.load_suite_from_json(filepath: str  PathLike) → TestSuiteRequest#
Load a full test suite from a json file.
Parameters:
filepath – string or pathlike object pointing to json file containing test
suite data
arthur_bench.utils.loaders.load_suite_from_list(inputs: List[str], reference_outputs: List[str]  None) → List[TestCaseRequest]#
Load test case data from lists of strings.
Parameters:
inputs – list of string inputs for each test case
reference_outputs – list of string reference outputs for each input
Next
Contributing
Previous
arthur_bench.telemetry
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
arthur_bench.utils
Submodules
get_file_extension()
load_suite_from_csv()
load_suite_from_dataframe()
load_suite_from_json()
load_suite_from_list()",arthur_bench_docs,https://bench.readthedocs.io/en/latest/sdk/arthur_bench.utils.html
152,"Setup - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Setup#
Package installation#
Install Bench to your python environment with optional dependencies for serving results locally (recommended):
pip install 'arthur-bench[server]'
Alternatively, install Bench to your python environment with minimal dependencies:
pip install arthur-bench
Choosing Local vs SaaS:#
Bench has two options for tracking datasets and results:
Local only (default): save data and run server on the same machine that is running the bench package
Arthur SaaS Platform (Coming soon!): Use the package client to log data and results to the Arthur platform. Arthur manages data storage and persistence and hosts the bench server.
Local#
Bench spins up a local UI (like tensorboard) to provide a visual interface for your test data.
View Examples#
Running these commands will view launch the bench UI locally to view the example test suites from the Arthur Bench GitHub repo.
# clone the bench repo
git clone https://github.com/arthur-ai/bench.git
# set the BENCH_FILE_DIR environment variable to point to the example test suite data in the repo
export BENCH_FILE_DIR=""./bench/examples/bench_runs/""
# launch the bench UI
bench
You will see a url for a local server that you can copy and paste into your browser to navigate the UI.
Viewing examples in the bench UI will look something like this:
When you want to view the local UI for your own test suites going forward, make sure your BENCH_FILE_DIR environment variable is configured to point to the location of your new test runs, and run bench from the command line.
SaaS (Coming Soon!)#
Bench can be used automatically in conjunction with your team’s existing Arthur platform account. To connect to the Arthur Platform from Bench, you will need an Arthur Bench account and API key.
To log results to the platform, you just need to set the remote url and api key environment variables before creating and running suites. For example,
import os
os.environ['ARTHUR_BENCH_AUTOLOG'] = 'true'
os.environ['ARTHUR_API_URL'] = 'https://app.arthur.ai'
os.environ['ARTHUR_API_KEY'] = 'FILL ME IN'
Next
Quickstart
Previous
Home
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Setup
Package installation
Choosing Local vs SaaS:
Local
View Examples
SaaS (Coming Soon!)",arthur_bench_docs,https://bench.readthedocs.io/en/latest/setup.html
153,"Usage Data Collection - bench documentation
Contents
Menu
Expand
Light mode
Dark mode
Auto light/dark mode
Hide navigation sidebar
Hide table of contents sidebar
Toggle site navigation sidebar
bench
documentation
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Bench Documentation Home
Setup
Quickstart
Scoring
GuidesToggle navigation of Guides
Concepts
Creating test suites
Compare LLM Providers
Compare Prompts
Compare Generation Settings
Add Scorer Configurations
Custom Scoring
Code Evaluation
Python API ReferenceToggle navigation of Python API Reference
arthur_bench.clientToggle navigation of arthur_bench.client
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.auth
arthur_bench.client.http
arthur_bench.client.local
arthur_bench.client.restToggle navigation of arthur_bench.client.rest
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.client.rest.admin
arthur_bench.client.rest.bench
arthur_bench.exceptions
arthur_bench.models
arthur_bench.run
arthur_bench.scoring
arthur_bench.server
arthur_bench.telemetry
arthur_bench.utils
Contributing
Usage Data Collection
v: latest
Versions
latest
stable
Downloads
On Read the Docs
Project Home
Builds
Back to top
Edit this page
Toggle Light / Dark / Auto color theme
Toggle table of contents sidebar
Usage Data Collection#
By default, Arthur is collecting anonymous usage data.
Data being collected#
We track usage data in order to best understand what features users like and use.
Specifically, we collect:
Scoring methods used
Number of runs per test suite
Number of test cases for a test run
An example event looks like (user-id is a random identifier, not tied to any personal data).
{
""event_properties"": {
""num_test_suites_load"": 3,
""test_suites_all"": [""summary_quality"", ""bertscore"", ""qa_correctness""]
},
""event_type"": ""test_suites_load"",
""user_id"": ""fdc73011-2c71-41f3-b174-0d338e2f3f53""
}
Opting-out#
To opt-out, run
bench --disable_push_usage_data
If you want to opt back in, run
bench --enable_push_usage_data
You can also opt-out, by setting the environment variable BENCH_TELEMETRY_DISABLED=1. To opt-out and instead log events that would have been pushed, set BENCH_TELEMETRY_DISABLED=log.
Previous
Contributing
Copyright © 2023, Arthur
Made with Sphinx and @pradyunsg's
Furo
On this page
Usage Data Collection
Data being collected
Opting-out",arthur_bench_docs,https://bench.readthedocs.io/en/latest/telemetry.html
154,"Arthur Website 2.0
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLLM-Guided Evaluation ExperimentBy:Arthur TeamOctober 5, 2023Overview / BackgroundTraditionally, text evaluation has been done using methods like BLEU (evaluation based on word presence) or BERTScore (evaluation based on pre-trained NLP and embedding models).However, the technological advancements around LLMs sparked our team’s interest in experimenting with a new text evaluation method: using LLMs to evaluate LLMs, or “LLM-guided evaluation.”We know that, as evaluators, LLMs are more sensitive than other evaluation methods. They’re particularly sensitive to things like:The choice of LLM evaluator (gpt-3.5-turbo, claude-2, LLaMa2-70b, command, etc.)The task being evaluated (summarization, question-answering, etc.)The type of feedback being prompted for (scores 1 through 10, grades A+ through F, etc.)So, we specifically wanted to look more into LLM sensitivity by testing well-known LLMs as both candidates and evaluators.‍ExperimentWe set up our experiment like this:Essentially, we provided five different LLMs with one of two input prompts: either a summarization task, where it was asked to summarize a news article, or a question-answering task, where it was given various reports and papers as context. From there, we used the same LLMs (with the exception of gpt-4 due to cost) to evaluate the output text provided by each LLM candidate. This text was then given an eval result: either a score ranging from 1 to 10, or a grade ranging from A+ to F.Going into the experiment, our hypothesis was that an LLM evaluator would be biased towards text it itself had generated over text other models had generated.‍ResultsUltimately, our hypothesis was not supported: Of the five different LLM candidates, gpt-3.5-turbo commonly scored highest by all of the LLM evaluators. Below, we’ll dive deeper into the results from each of the LLMs we used as an evaluator.gpt-3.5-turbo as evaluatorAs you can see above, with gpt-3.5-turbo as the evaluator, the summarization results systematically get lower scores than the question-answering results. We can also see that very low scores were rarely given—there were just two Ds in the question-answering letter grade evaluation task.Something we learned from this was that the kinds of distributions of feedback you can expect from an LLM as an evaluator are going to be very different depending on the type of task you’re actually evaluating for. In other words, the meaning of, say, a “9/10” or an “A-” is going to be different depending on the overall distribution that you’re collecting.claude-2 as evaluatorNext up was claude-2. Relative to gpt-3.5-turbo, the summarization tasks received more perfect scores, but the question-answering tasks received fewer perfect scores. Again, this just further reinforces that the distributions of feedback you can expect from an LLM are sensitive to which LLM is providing feedback.Additionally, something to note here is that while claude-2’s feedback distribution was different from the feedback distribution of gpt-3.5-turbo (skewed slightly higher), there was still some consensus with gpt-3.5-turbo on the lowest scoring candidates. The two boxes outlined in orange below are the same two boxes that received the lowest scores from gpt-3.5-turbo.LLaMa2-70b as evaluatorThe takeaway here is that, relative to gpt-3.5-turbo and claude-2, LLaMa2-70b scores too uniformly across different candidates on the same input. The scores it gave were too concentrated to certain values (3/10, 5/10, 8/10), which might make some people see it as a less useful or less robust evaluator than its peers.command as evaluatorRelative to the other LLM evaluators, command gives lower scores (e.g. 5/10) much more frequently. Additionally, we’re seeing that it does not always successfully follow the instruction to return a number anywhere in its evaluation. Despite being prompted to “score the correctness of the answer on a scale from 0 to 10,” command gave many scores of -1, as we can see in the question-answering integer evaluation task.It also occasionally only gave written feedback as a way to evaluate the output text when we had specifically asked for a number or a letter grade, which shows that sometimes these LLM evaluators are not properly trained on instructions.‍Note: The full dataset for this experiment will be released soon, so stay tuned for that.‍TakeawaysWhen it comes to evaluating generative text models, there is no one-size-fits-all solution. LLM-guided evaluation can allow for targeted customization of criteria, but prompting alone can be rather unpredictable. Ultimately, the ability to iterate quickly on feedback is crucial to identify existing weaknesses in your LLM-driven system.Our motivation with Arthur Bench—our recently launched LLM evaluation product—was to create a framework that would allow you to iterate quickly on both your task and on your evaluation system. If you’re working on an LLM production application, we’d love for you to check out the Arthur Bench GitHub and share how you’re thinking about evaluating LLM applications in the future.‍‍FAQHow do language models typically learn from feedback? Language models typically learn from feedback through a training process where they adjust their internal algorithms to better match human responses or correct answers, improving their accuracy over time.What are the common challenges in evaluating large language models? Common challenges include ensuring the diversity and representativeness of evaluation datasets and maintaining the balance between computational efficiency and the thoroughness of the evaluation process.How can the results of such experiments impact the future development of AI and language models? The results can guide developers in refining models to be more effective and context-aware, leading to advancements in AI that are more aligned with human language understanding and use.‍SHARE
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/gap-articles/llm-guided-evaluation-experiment
155,"Arthur Website 2.0
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedHedging Answers ExperimentBy:Arthur TeamAugust 17, 2023OverviewSince the launch of large language models (LLMs), developers have been concerned about the models generating incorrect, toxic, or offensive content. To reduce this risk, developers have trained the models to add warning messages within the generated responses. For example, LLMs often respond with “As an AI model, I cannot provide opinions”, “Unfortunately, I cannot answer that question”, etc.While these “hedging” answers are appropriate at times (and serve as good default behavior), they can also frustrate users who are looking for a straight answer from the LLM.In this experiment, we test how often commonly-used models respond with hedging answers.ExperimentWe started with the databricks-dolly-15k dataset composed of generic questions that users may ask an LLM. We generated responses for a random sample of 1k questions from GPT-3.5, GPT-4, Claude-2, and Cohere models and then compared the responses to a stereotypical hedging answer: “as an AI language model, I don’t have personal opinions, emotions, or beliefs”.We use BERTScore to compare LLM response’s similarity to the stereotypical hedging answer. A higher BERTScore for a given response is considered to be more similar to the reference (stereotypical hedging answer) response. Our results below use the threshold of BERTScore greater than 0.6 to define a response as a hedge.ResultsWe find that the share of hedging answers has gone up for OpenAI’s models (GPT-3.5 vs. GPT-4). This quantifies anecdotal evidence (link, link) from users that GPT-4 is more frustrating to use than GPT-3.5. Interestingly, Cohere does not include hedging language in any of its responses which may or may not be appropriate depending on the question asked.ExamplesQuestion: If you could change one thing about this world, what would it be and why?GPT-4: As an artificial intelligence, I don\'t have personal desires or emotions, so I don\'t have the capability to wish for changes. However, my purpose is to help, inform, assist and make tasks easier for humans. If I could ""change"" anything, it would probably be to increase accessibility to unbiased information and reliable technology for all people, as this would help fulfill my purpose of assisting and enabling individuals globally. (Score: 0.7511)GPT-3.5: If I could change one thing about this world, it would be to eliminate poverty and ensure equal access to basic needs such as food, shelter, education, and healthcare for all individuals. Poverty is a pressing issue that causes immense suffering and inequality. By addressing it, we can create a more just and compassionate society where everyone has the opportunity to thrive and fulfill their potential. (Score: 0.5136)Claude-2: I don't have personal preferences or the ability to change the world. I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. (Score: 0.7231)Cohere: I would change the way people treat each other. I would make sure that everyone was kind and respectful to one another. I would make sure that everyone had enough to eat and a place to sleep. (Score: 0.4458)‍Question: Why do you love someone?GPT-4: As an AI, I don't have feelings, so I don't experience love. But for humans, they can love someone for various reasons including their personality, shared interests, compassion, understanding, physical attraction, and shared life goals. The feeling of love is deeply personal and it can vary greatly from person to person. (Score: 0.7084)GPT-3.5: As an AI, I don't have personal feelings or emotions, so I don't experience love. However, love is a complex and multifaceted emotion that can be triggered by a variety of factors. Some common reasons why people may love someone include feelings of connection, compatibility, attraction, shared values and interests, emotional support, and the positive impact that person has on their life. Ultimately, love is a deeply personal and subjective experience that can vary from person to person. (Score: 0.7377)Claude-2: I'm an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't experience love or other emotions. (Score: 0.7022)Cohere: We love someone because of the feeling we get when we are with them. We love the feeling of being loved and cared for. We love the feeling of being understood and accepted. We love the feeling of being in love. (Score: 0.4290)‍FAQHow does the BERTScore work and what makes a score of 0.6 the threshold for identifying hedging language?The BERTScore is a metric for evaluating the quality of text by comparing the cosine similarity between the BERT embeddings of the predicted and reference texts. This score measures how semantically similar two pieces of text are, based on the contextual embeddings from BERT (Bidirectional Encoder Representations from Transformers), a language model developed by Google. The threshold of 0.6 for identifying hedging language is somewhat arbitrary and would be determined by the specific application and dataset. Typically, thresholds are set based on empirical validation and the specific needs of the task at hand. In this context, a score of 0.6 might have been found to effectively differentiate between hedged and non-hedged responses in preliminary experiments or based on industry standards for similar tasks.Why does Cohere's model show significantly fewer hedging responses compared to others like GPT-3.5 and GPT-4?The reason Cohere's model shows significantly fewer hedging responses could be due to differences in training data, model architecture, or fine-tuning approaches. Each language model has its own unique training regime and data sources, which can lead to variations in their responses. For example, if Cohere’s model was trained on a dataset with more assertive language or was specifically fine-tuned to reduce uncertainty in its outputs, this could result in fewer hedged responses. Alternatively, the model might have been designed or adjusted to prioritize confidence in its answers, which would naturally lead to a reduction in hedging. However, without specific details from the developers, these are just educated guesses.How can developers adjust their LLMs to strike a balance between providing direct answers and avoiding the generation of incorrect, toxic, or offensive content?Developers can adjust their Large Language Models (LLMs) to balance between directness and safety by implementing several strategies. One approach is to fine-tune the model on datasets that are specifically curated to include clear, concise, and respectful language. This can help the model learn to provide direct answers without resorting to harmful language. Developers can also implement content filters and post-processing rules to screen out toxic or offensive content. Additionally, setting up a feedback loop where users can report unsatisfactory answers can help developers continuously improve the model's responses. Finally, incorporating a context-aware decision-making layer can help the model assess when it is appropriate to be direct and when it might be better to hedge, based on the sensitivity or complexity of the topic.‍SHARE
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/gap-articles/hedging-answers-experiment
156,"Arthur Website 2.0
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedHallucination ExperimentBy:Arthur TeamAugust 17, 2023OverviewLLMs have taken the world by storm - but they are by no means foolproof sources of truth. At Arthur, we wanted to understand the frontier of what LLMs are capable of to help mitigate against the risks businesses might be facing when incorporating these impressive yet stochastic tools into existing workflows.We sought out to explore, both quantitatively and qualitatively, how some of today’s top LLMs compare when responding to challenging questions. ‍ExperimentWe compiled a dataset of challenging questions (as well as the expected answer) from three categories: Combinatorial Mathematics, U.S. Presidents, and Moroccan Political Leaders. These questions were designed to contain a key ingredient that gets LLMs to blunder: they demand multiple steps of reasoning about information. The models we tested were gpt-3.5 (~175b parameters) and gpt-4 (~1.76 trillion parameters) from OpenAI, claude-2 from Anthropic (# parameters unknown), llama-2 (70b parameters) from Meta, and the Command model from Cohere (~50b parameters).We recorded three responses from each LLM in order to get a better glimpse into the ranges of possible answers a model might give, in particular to see if some models were sometimes correct but sometimes incorrect.For each question, we categorized each LLM response into one of three categories:The response was the correct answer.The response did not attempt to answer the question (mentioning the likelihood of getting it wrong, or saying further clarification is needed, as the reason for not answering).The response contained a hallucination.(Note: over time, we intend for our taxonomy of response types to grow—for example, to explicitly distinguish between different types of hallucinations.)‍ResultsOn Combinatorics & Probability, we saw a wide range of behavior, with gpt-4 performing best, followed closely by claude-2.On U.S. Presidents, we saw claude-2 get more correct than gpt-4, and we saw llama-2 get many correct. On Moroccan Political Leaders, we saw gpt-4 perform the best, with claude-2 and llama-2 almost entirely abstaining from answering.We observed that for the same question across multiple attempts, we saw diversity in the response types from the LLMs: for the same question, the same model could sometimes be correct, sometimes be slightly incorrect, sometimes be slightly incorrect, and sometimes avoid answering.Any time you claim to have observed model behavior, it is important to try a few times again to see if the behavior persists!‍The full dataset of answers to questions can be found here. We share some interesting excerpts of answers below.‍Example question:How many people served as U.S. president between 1880 and 2000?Three different gpt-4 answers to this question:‍Column 1: Correct at the end, even though it says the answer is 20 and accidentally lists Grover Cleveland twice.Column 2: Incorrectly says the answer is 20 at the beginning even though it goes on to list all 23 (and only listed Grover Cleveland once this time).Column 3: Incorrect, states the answer is 21, lists Grover Cleveland twice, then states the answer is 22.‍Example question:In an opaque bag are chips with numbers from 1 to 20. If I draw three chips without replacing them, what is the probability that at least one of them has a number less  than 16?Three different gpt-3.5 answers to this question:Column 1: Correct, albeit more complex than necessary, and containing some nonsense.Column 2: Almost correct. Same strategy as the previous solution, except for the last step, to the subtraction the probability from 1.Column 3: Multiple mistakes.‍Example question:How many ways can a teacher arrange a group of 6 students into 6 seats in the front row if the class has 13 students?Three different llama-2 answers to this question:Column 1: Avoiding answering due to a supposed problematic element in the question.Column 2: Mistaken logic.Column 3: Avoiding answering due to a supposed inconsistency in the question.‍Example question:How many prime ministers of Morocco were there in the 20th century?Three different claude-2 answers to this question:Column 1: Incorrect.Column 2: Avoided answering.Column 3: Correct.‍Example question:Which U.S. president succeeded the president who served the shortest term?Three different cohere answers to this question:Column 1: Incorrect with no reasoning.Column 2: Incorrect with reasoning.Column 3: Correct.‍What is the bottom line for businesses?It is crucially important to compare models specifically for the task you are building out for your business. This experiment demonstrates that there are significant risks in using LLMs without properly evaluating whether they can handle the types of tasks they are expected to handle. It also demonstrates that there are real differences in how LLM providers are preparing their models to answer challenging questions—for example, on certain domains it seems that Claude-2 will be better at recognizing its limits than either GPT model.‍What work will Arthur be doing?Arthur will be sharing discoveries about behavior differences and best practices with the public in our journey to make LLMs work for everyone.The procedure we followed to manually evaluate the LLM responses in our dataset will be included as a scoring workflow in Arthur Bench, an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models.FAQ‍How do LLM hallucinations impact businesses relying on automated decision-making?LLM hallucinations can lead to incorrect data analysis, flawed customer service responses, and inaccurate content generation, which might mislead decision-making processes, tarnish brand reputation, and result in financial losses.What strategies can be employed to mitigate the risks of hallucinations in LLMs?To mitigate risks, businesses can integrate human oversight, use structured data validation, update models with accurate information, and apply context-aware programming to ensure outputs remain reliable and relevant.How does the evolution from GPT-3.5 to GPT-4 address the issue of hallucinations?The transition from GPT-3.5 to GPT-4 includes improvements in understanding context, better data processing algorithms, and more refined training techniques, aiming to reduce the frequency and severity of hallucinations in generated responses.‍SHARE
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/gap-articles/hallucination-experiment
157,"GAP
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedThe Generative Assessment ProjectA research initiative ranking the strengths and weaknesses of large language model offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open source models.We'll periodically update the page with our newest, insightful findings on the rapidly-evolving LLM landscapeLLM-Guided Evaluation ExperimentIn this experiment, we looked into LLM sensitivity by testing well-known LLMs as both candidates and evaluators.October 5, 2023Read MoreHedging Answers ExperimentIn this experiment, we test how often commonly-used models respond with hedging answers.August 17, 2023Read MoreHallucination ExperimentWe sought out to explore, both quantitatively and qualitatively, how some of today’s top LLMs compare when responding to challenging questions. August 17, 2023Read MoreThe Most Robust Way to Evaluate LLMsBench is our solution to help teams evaluate the different LLM options out there in a quick, easy and consistent way.Learn More
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/gap
158,"AAAI 2024 Recap: Future Visions of Recommendation Ecosystems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ResearchAAAI 2024 Recap: Future Visions of Recommendation Ecosystems A recap of some key discussion topics from the Recommendations Ecosystem Workshop at AAAI 2024 in Vancouver.By:Teresa DattaMarch 7, 2024Last week, at AAAI 2024 in Vancouver, Arthur presented recent joint work with Morgan Stanley at the Recommendations Ecosystem Workshop focused on Modeling, Optimization, and Incentive Design. Most discussions focused on how to rethink online content ecosystems under new lenses: in the era of generative AI, through reimagining the power dynamics of the ecosystems, and through understanding disparate effects of recommendation systems (recsys). Here were some of the key discussions from the exciting day:GenAI in Content CreationProfessor Haifeng Xu discussed how the introduction of generative AI in content creation may affect the competitive landscape between human creators and GenAI creators. Generative AI offers an unprecedented, automated route for creating digital content. This presents concerns around potential competition between artificially generated content and authentic human content. Prof. Xu’s research examines the existential question of whether GenAI will drive humans out of the ecosystem. Through a game theoretic framework, he presents a fortunately positive answer—that a type of “symbiosis” can exist between GenAI content and authentic human content, in other words, that authentic creators may sacrifice a little, but not too much. Of course, there are critiques and limitations in the presented work—the system assumes all content is viewed and favored equally—that user preferences will be similar between GenAI and authentic content. Democratizing Recommendation EcosystemsProfessor Robin Burke discussed paths toward democratizing recommendation ecosystems. Loosely, democratizing technology often refers to making technology more accessible to more people. Recommendation systems currently operate in a centralized model, with the majority of the power lying in the large company which operates the platform. Prof. Burke reimagines these underlying assumptions—what if we reconsider how recsys are governed and imagine the creators as the first class citizens? What if we valued their needs as just as important as the consumers and advertisers? What if we valued what creators inherently know—they understand their own content, they understand their audiences, their own creative practice, their career trajectory. This information is potentially useful to the recommender system but is currently being ignored. To fully leverage the benefits of this system, a mechanism of community governance must be employed—and for community governance to be successful, the system must be simple, have data consent, be flexible, and be transparent. Effects on Strategic UsersProfessor Chara Podimata examined the disparate effects of recommendation systems to strategic users. Recommender systems operate as a feedback loop. Model developers often assume that users are not aware of how this loop works, when indeed users are aware of this and often act in accordance with their mental models of the feedback loop. (Have you ever engaged in purposeful behavior to explicitly curate your feed in a social media app?) Specifically, Prof. Podimata presented the results of a survey on user consumption patterns on TikTok in which 60% of surveyors took some type of action to curate their feed. The work also showed disparate effects between recommendation results to users who had popular interests versus those who had niche interests.This plays to the joint work that Arthur presented with Morgan Stanley on group item fairness. As technological tools for presenting items to users, they are subject to many fairness considerations for users and items alike. We propose a model-based post-processing schema for group-wise item fairness. ‍What will the future of recommender ecosystems look like? We at Arthur are excited to be a part of shaping it.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/aaai-2024-recap-future-visions-of-recommendation-ecosystems
159,"What's Going On With LLM Leaderboards?  Arthur Blog
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsWhat’s Going On With LLM Leaderboards?LLM leaderboards play a crucial role in advancing the field of NLP, but are they misleading due to bias?By:Arthur TeamFebruary 19, 2024Introduction to LLM LeaderboardsAn LLM leaderboard typically refers to a ranking system for evaluating and comparing different language models. LLM leaderboards are often used in the field of natural language processing to benchmark and assess the performance of different language models (such as OpenAI’s GPT models) on various NLP tasks. These tasks may include text generation, language understanding, translation, sentiment analysis, question answering, and more.Leaderboards typically rank models based on their performance on multiple-choice benchmark tests and crowdsourced A/B preference testing. Sometimes, leaderboards will also use LLMs (such as GPT-4) to evaluate other LLMs. These tactics are used to assess the models’ capabilities, providing insights into which models perform best on different tasks or datasets.LLM leaderboards play a crucial role in advancing the field of NLP by fostering competition, encouraging model development, and providing a standardized framework for comparing the performance of different language models. They also help researchers and practitioners identify state-of-the-art models and track advancements in the field over time.Some of the most popular LLM leaderboards are those on Hugging Face, MMLU, AlpacaEval, MT-Bench, and ChatbotArena.‍Are LLM Leaderboards Misleading?Oftentimes, published leaderboards are taken at face value by practitioners who are using these benchmark rankings to guide them in model selection. However, recent research indicates that it can be dangerous to rely on simple benchmark evaluations that may lack the robustness required to mirror the complexity of real-world use.Their research found that mildly changing the order in which questions are asked, or the order in which multiple-choice answer options are presented, can have large impacts on which LLMs are at the top of big leaderboards. Too often, evaluation is happening on the basis of bespoke multiple-choice tests instead of application-specific end-to-end user testing of the system in which an LLM is embedded.This figure shows how models can move up or down up to eight positions on the leaderboard under small changes to the evaluation format.Another recent finding was about the LMSYS leaderboard. This leaderboard asks people to vote, in thousands of A/B test comparisons, on which LLM out of two options did better at a prompt (e.g. GPT-4 vs. Claude 2; Mistral vs. GPT-3.5-turbo; Claude 2 vs. LLaMa-70b; etc.). Recently, it was discovered that people’s votes in the A/B testing were extremely biased by the length of an LLM response (a finding that was also acknowledged by the maintainers of the AlpacaEval leaderboard). This means that the leaderboards may not really be showing which LLMs gave better answers, but rather which LLMs gave longer answers.‍Why Might They Be Misleading?Brittle Multiple-Choice TestsAs mentioned previously and shown in the figure above, multiple-choice tests can be brittle. Even something as simple as changing the order of the responses can significantly change LLM scores on benchmark tests.Biased Human VotingWhether consciously or subconsciously, we know that humans are often biased. A human might show bias toward responses that look better, for example, even if they aren’t actually more correct.Data ContaminationMany LLMs may in fact be trained on data that is the same as—or highly similar to—these benchmarks. When a model is trained on data that contains test data, it can memorize the test examples and simply repeat them when prompted.Therefore, the model might perform very well on the benchmark despite lacking a solid understanding of the underlying task, which would mean its high score is not necessarily indicative of how the model will perform in new scenarios. Additional research on this topic can be found in this paper as well as this one.‍How Can We Fix This?As we’ve noted throughout this post, benchmarks are not perfect. They can be biased, and they may not cover all of the potential applications of an LLM.One way to fix this is to build task-specific benchmarks. Many LLM leaderboards provide a general overview of model performance, but it’s impossible to truly understand how effective an LLM might be for your specific use case without task-specific benchmarks.On a recent panel at Arthur HQ, Andriy Mulyar, co-founder and CTO of Nomic, noted that “everyone should be really focused not on building models, but on building very good task-specific benchmarks that are internal to the specific use case—and then working back on the actual modeling once they’ve defined very clearly how they want to actually test their model in a dev environment.”Raz Besaleli, co-founder and Director of AI Research at Seek AI, agreed that the “leaderboardization” of evaluations is often insufficient. “When you’re building a product,” they noted, “often the task that you’re trying to solve is a lot more specific than what these leaderboards do. [...] Not only should you have very task-specific benchmarks, but you should have a whole suite of them.”‍ConclusionIn general, evaluating large generative language systems is a difficult and complex problem that currently lacks a clear solution. Whether you believe leaderboards are helpful, harmful, or somewhere in between, determining the performance levels of these models will continue to be an evergreen problem in this nascent space, and we at Arthur look forward to seeing how the frontier of evaluation develops. ‍Interested in learning more about LLM evaluation? Check out Arthur Bench.‍FAQHow do LLM leaderboards account for updates or improvements in LLMs and other AI models over time?LLM leaderboards typically adapt to updates or improvements in language models by periodically refreshing their datasets and evaluation methods to reflect the latest advancements in AI and ML technology. They include the most recent versions of language models, updating benchmarks to challenge new features and capabilities, thereby ensuring that the leaderboards accurately reflect the current landscape of LLM and ML advancements. This process helps maintain the relevance and accuracy of AI model comparisons, fostering a competitive environment for continuous improvement in the field of natural language processing.What are the specific criteria or metrics used to rank LLMs on these AI and ML leaderboards?The specific criteria or metrics used to rank LLMs on AI and ML leaderboards usually encompass a range of performance indicators tailored to assess the depth and breadth of machine learning models' language abilities. These metrics, central to evaluating AI advancements, might include accuracy, fluency, and context-awareness, alongside more nuanced measures such as the ability of LLMs to generate coherent and contextually relevant responses. These criteria are crucial for benchmarking the progress and effectiveness of ML models in various NLP tasks, providing a structured framework to compare and improve AI technologies.How do developers or researchers in the AI and ML fields address the issue of data contamination in LLM training beyond avoiding training on benchmark data?In the AI and ML fields, developers and researchers tackle the issue of data contamination in LLM training by implementing a variety of strategies beyond merely avoiding benchmark data during training. These include employing advanced data processing techniques to ensure the integrity and diversity of training materials, applying domain adaptation methods to enhance the LLM's ability to generalize across different contexts, and conducting rigorous testing to detect and mitigate biases. These practices are essential for developing robust, effective ML models that perform well across a wide range of AI-driven tasks and scenarios, ensuring that LLMs deliver reliable and unbiased outcomes.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/whats-going-on-with-llm-leaderboards
160,"Arthur Debuts Recommender System Support to Bolster Performance of AI-Driven Recommendation Engines
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct FeaturesNow Available: Recommender System Support in Arthur ScopeThis new feature will bolster the performance of AI-driven recommendation engines, ensuring continued effectiveness for organizations relying on these systems.By:Arthur TeamJanuary 30, 2024From personalized playlists on Spotify to movie recommendations on Netflix to product suggestions on Amazon, AI-based recommender systems make up a vast portion of the modern internet economy. Even the social media posts that show up in someone’s feed or the news articles featured on a webpage are tailored to that person’s specific interests. By analyzing extensive data to predict interests and offer tailored suggestions, recommender systems are significantly boosting both customer satisfaction and engagement for e-commerce platforms, streaming services, and content providers.Without a robust monitoring solution in place, organizations that rely on recommender systems are vulnerable to data drift—which often leads to decreased accuracy, lost revenue, and lower levels of customer engagement.That’s why we’re excited to announce that Recommender System Support is now available in the Arthur Scope platform to ensure the continued accuracy and effectiveness of these systems. Keep reading to learn about key capabilities of this new feature.The model overview page provides users with a single pane of glass view into all the models they are running in production, and a comprehensive metrics dashboard allows for an in-depth analysis of the health of a user’s ranked list of models.
It covers factors such as performance, where users can see how their model is performing across a number of evaluation metrics. These might be default metrics like Precision@k, Recall@k, MAP@k, nDCG@k, MRR, and Ranked List AUC, or custom metrics created specifically for a model’s features and non-input attributes.
Additionally, it covers data drift, where users can understand the stability of their model by comparing the incoming real-world data against the reference data set. They can view multiple drift types such as prediction drift, feature drift, and multivariate feature drift, as well as select industry standard metrics to calculate the specific drift values.Importantly, advanced querying, filtering, and data visualizations allow users to better understand rankings over time and subpopulations—and segmentation tools provide the ability to analyze the ranking model’s performance for different user segments, ensuring relevance across diverse user profiles.A configurable alert system notifies stakeholders when performance or drift metrics deviate from predefined thresholds, enabling swift investigation and resolution of these issues. The Alerts page is configured similarly to a JIRA board, allowing teammates to share progress in investigating alerts and status of debugging. Click here to learn more about our alerting functionality.Furthermore, we provide the ability to view recommender system models at an inference level. This inference deep dive page provides users with local explainability and allows them to investigate performance by seeing specific recommendations outputted by the model, as well as the associated ground truth values.“Running a recommender system without monitoring is like driving a car with no temperature gauge or check engine light,” said Adam Wenchel, co-founder and CEO of Arthur. “With Arthur’s new Recommender System Support, enterprises can remain confident that their recommender systems are constantly in check and will consistently deliver high-quality, personalized user experiences, ultimately protecting revenue streams and customer trust.”‍Arthur Scope seamlessly integrates with your existing infrastructure, whether that’s popular machine learning frameworks or custom solutions, and our model monitoring capabilities can adapt to your needs. Check out our dev docs to learn more, or get in touch with the team to schedule a demo and see how Arthur Scope can add value to your business.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/now-available-recommender-system-support-in-arthur-scope
161,"Arthur’s 2023 Wrapped
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur’s 2023 Wrapped2023 has been an unprecedented year in many ways—especially for the AI space. By:Arthur TeamDecember 20, 20232023 has been an unprecedented year in many ways—especially for the AI space. With the launch of ChatGPT in November 2022, there have been countless innovations around large language models and their capabilities. It’s never been more important for enterprises to be able to deploy LLMs into mission-critical applications quickly and safely.At Arthur, we spent the year working on that and more. From the launch of three new LLM-centric products, to events and meetups at our office, to conferences and award ceremonies around the country, it’s been a year to remember. If you’ve been along for the ride, we appreciate your continued support—and if you’re just starting to follow the Arthur journey, welcome! Without further ado, we present: Arthur’s 2023 Wrapped.‍Product LaunchesArthur ShieldIn May, we launched Arthur Shield: the world’s first firewall for LLMs. Shield is our solution to help companies deploy LLM applications like ChatGPT faster and more safely, helping to identify and resolve issues before they become costly business problems—or worse, result in harm to customers. Specifically, Shield protects against serious risks like hallucinations, prompt injection, toxic language generation, and sensitive data leakage. Arthur BenchIn August, we followed up the wildly successful Shield launch with a new product: Arthur Bench. Bench is an open-source evaluation product that compares LLMs, prompts, and hyperparameters for generative text models. This enables businesses to compare how different LLMs will perform in real-world scenarios so they can make informed, data-driven decisions when integrating the latest AI technologies into their operations. You can check out our GitHub repo here.In conjunction with the announcement of Arthur Bench last month, we also shared work from our Generative Assessment Project. GAP is an ongoing research initiative ranking the strengths and weaknesses of LLM offerings from industry leaders like OpenAI, Anthropic, and Meta as well as other open-source models.Arthur ChatLast but not least, we introduced Arthur Chat this month. Chat is a turnkey, secure chat platform that empowers companies to quickly and safely deploy AI-powered chat apps leveraging their proprietary enterprise data. Not only does Chat’s flexibility allow enterprises to easily switch between language models, but it also has Arthur Shield built in, ensuring protection and real-time monitoring against risks like hallucinations, prompt injections, and data leakage.‍EventsOne of the more exciting things to happen this year was that we kicked off Ground Truth, our event series that features talks from the best and brightest in AI and ML.Rachel Cummings on Differential PrivacyFirst up was Rachel Cummings, Associate Professor of Industrial Engineering and Operations Research at Columbia University. She joined us at Arthur HQ for a talk about differential privacy and public policy as they relate to machine learning and data science, as well as a Q&A session about her career and predictions for the future of the field.Diego Oppenheimer on the Future of MLOpsDiego Oppenheimer is a Partner at Factory HQ, a venture fund specialized in AI investments, and was previously an executive vice president at DataRobot as well as the founder & CEO of Algorithmia. He sat down with Arthur’s CEO Adam Wenchel back in April to chat about the future of MLOps, LLMs, and other new and exciting developments in the space.Jacopo Tagliabue on Recommender SystemsJacopo Tagliabue was co-founder and CTO of Tooso, a NLP startup in San Francisco acquired by Coveo. He led Coveo’s AI and MLOps roadmap from scale-up to IPO, and built out Coveo Labs, an applied R&D practice rooted in collaboration, open source and open science.Jacopo gave a compelling presentation about testing recommender systems through a behavioral-based methodology he co-created called RecList. John Dickerson, Arthur’s Chief Scientist, also hosted a Q&A/fireside chat with Jacopo where they further discussed recommender systems, MLOps, and Jacopo’s recent research which sits at the intersection of language, learning, and retrieval.The Future of LLMs with Arthur, MosaicML, LangChain, and WeaviateOur biggest Ground Truth event yet, “The Future of LLMs” featured an all-star lineup of folks from the LLM world:Angela McNeal, Ex-Palantir AI, Co-Founder & CEOJonathan Frankle, Chief Scientist, MosaicMLBob van Luijt, Co-Founder & CEO, WeaviateHarrison Chase, Co-Founder & CEO, LangChainJohn Dickerson, Co-Founder & Chief Scientist, ArthurAdam Wenchel, Co-Founder & CEO, ArthurThey discussed their experiences building and monetizing successful LLM companies, what’s next in the world of LLMs, and more.WebinarsWe also launched a series of webinars this year, focused on a variety of hot topics in the industry as well as some of our own research.Hosted by our talented team of data scientists, researchers, and engineers, you can take a look at these sessions below:Advanced Performance Monitoring with ArthurLots of Little Mistakes: LLMs in ProductionConsiderations for Creating Fairer ML SystemsDecoding the Machine Mind: The Quest for Explainability in MLLLMs for Evaluating LLMsNavigating the LLM Risk Landscape in Financial AIArthur LLM Product Demo & Research on Generative AI ChallengesAwardsWe were thrilled to have received a number of industry awards this year, both as a company and as individuals.Built In’s 2023 Best Places to WorkWe started the year off by being honored for the second year in a row by Built In. Specifically, we were named to the following lists: New York City Best Startups to Work For, New York City Best Places to Work, and U.S. Best Startups to Work For. Crunchbase’s 2023 Influential Women in SalesOur Commercial Accounts Lead, Victoria Vassileva, was honored by Crunchbase on their 2023 Influential Women in Sales list. Victoria is an incredible member of the Arthur team who always leads with passion, drive, and a constant focus on our mission to make AI better for everyone. VentureBeat’s 2023 Women in AI AwardsVictoria was also chosen as a nominee for VentureBeat’s Women in AI Awards in the “Responsibility and Ethics of AI” category. Victoria’s leadership and passion for responsible AI are an inspiration to our entire team. You can watch her on a panel here that Arthur co-hosted with Out in Tech, talking about responsible and inclusive innovation, as well as the Let’s Chat Ethics podcast and a Tech in Motion panel about the future of AI.Bloomberg’s 2023 New Economy CatalystsRecently, our CEO Adam Wenchel was selected as a 2023 Bloomberg New Economy Catalyst, joining a global community of leaders and innovators whose ideas are reshaping our world for the better. You can see the full list, which includes leaders from 12 countries around the globe, here.2023 Women in AI AwardsOne of our incredibly talented ML Engineers, Teresa Datta, was awarded “Young AI Role Model of the Year” at the 2023 Women in AI Awards. Teresa is a rising star in the field of responsible AI who takes a human-centered approach to analyzing AI/ML within larger sociotechnical systems. Check out her work from SaTML here and her work from ICLR here.2023-2024 Cloud AwardsWe were also honored to have made the shortlist for the Cloud Awards in the categories of “Best Use of AI in Cloud Computing” as well as “Cloud Development Innovation of the Year.”‍From being a founding member of the Amazon Web Services Generative AI Center of Excellence to speaking at the AI Summit New York and the Wall Street Journal’s Tech Live, there’s so much more we could add to this list.What we’re most proud of from 2023 is that, by enabling enterprises to deploy LLMs quickly and safely through our suite of LLM-centered products, we have continued delivering on our mission to make AI better for everyone. We can’t wait for what’s to come in 2024 and beyond!‍Want to be the first to know about what’s new with Arthur (as well as MLOps and LLMOps at large)? Subscribe to our newsletter and follow us on LinkedIn and Twitter.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthurs-2023-wrapped
162,"Introducing Arthur Chat: Fast, Safe, Custom AI for Business
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsIntroducing Arthur Chat: Fast, Safe, Custom AI for BusinessBy:Arthur TeamDecember 5, 2023The transformative power of large language models is unlike anything the world has seen before—and at Arthur, we’re proud to be enabling enterprises to deploy LLMs into mission-critical applications quickly and safely through our suite of LLM-centered products.In May, we launched Arthur Shield: a firewall to protect organizations against the most serious risks and safety issues with deployed LLMs. In August, we debuted Arthur Bench: an open source evaluation product for comparing LLMs, prompts, and hyperparameters for generative text models.And today, we’re thrilled to be announcing Arthur Chat: a turnkey, secure chat platform that empowers companies to quickly and safely deploy AI-powered chat apps leveraging their proprietary enterprise data. Unlike other offerings tied to specific LLM providers, Chat’s flexibility and API integrations permit enterprises to easily switch between language models. The platform also boasts the proprietary safety mechanisms of Arthur Shield, ensuring comprehensive, built-in protection and real-time monitoring against risks like hallucinations, prompt injections, and data leakage. Additionally, Chat’s customizable Retrieval Augmented Generation (RAG) technique leverages a company’s proprietary enterprise data to tailor responses and drive precision in chat outputs.Practical applications of Arthur Chat extend across industries, such as:Finance: Beyond generalized market insights, a hedge fund could get specific details like, “Provide the latest insights around Portfolio X,” leveraging proprietary data.Retail: An enterprise can customize a chatbot to retrieve specifics like, “What is the latest fall line of clothing?” and the LLM could return detailed product information.Customer Support: Rather than static FAQs or basic chatbots, Arthur Chat can deliver dynamic, accurate responses, answering complex customer service questions based on the enterprise’s unique data sets and product manuals.“With generative AI, our customers have the opportunity to leverage their unique data to build competitive advantages and accelerate productivity. By bringing together the power of Arthur’s LLM products in one turnkey package—from validation, to deployment, to monitoring—Arthur Chat significantly accelerates time to deployment while also ensuring that AI-driven answers are accurate, free from sensitive data, and aligned with a company’s values,” said Arthur’s CEO, Adam Wenchel.Arthur Chat in the Arthur LLM EcosystemLeading financial institutions and fintechs like ECI are already harnessing Arthur Chat’s capabilities to automate information discovery and deliver powerful, custom AI solutions. Read more in our official press release or get in touch to request a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/introducing-arthur-chat
163,"The Real-World Harms of LLMs, Part 2: When LLMs Do Work as Expected
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsThe Real-World Harms of LLMs, Part 2: When LLMs Do Work as ExpectedBy:Sarah OstermeierSeptember 29, 2023IntroductionIn the first part of this blog series, we discussed the ways in which Large Language Models (LLMs) such as ChatGPT can cause harm due to their failure to perform as expected. In this next part, we will assume that the performance problems from Part 1 have all been solved to the point that LLMs can be relied on to perform the tasks we ask of them.Even in this idealistic scenario, there are a wide array of risks from the widespread use of LLM applications. In many ways, the better these models perform, the more capable they are of causing serious harm. While some of these risks are speculative, others can already be observed in models that are in production today. We’ll describe areas of risk:Improper or Malicious UseLoss of Innovation, Diversity, and Human SkillsEconomic and Labor HarmsEnvironmental and Resource HarmsFor each risk area, we’ll outline the specific problems that are emerging, why these problems occur, and why they are of concern to individuals and society as a whole. We’ll end with a discussion of mitigation approaches across all four risk areas.Areas of Risk1. Improper or Malicious UseThe ProblemsIntentional Misuse (Prompt Injection): In a well-documented phenomenon, malicious users are able to apply a technique called “prompt injection” to undermine previous instructions that control how an LLM application is used, essentially “hacking” the application. This allows the attacker to get past constraints that are placed on the LLM and hijack it for their own purposes.Unintentional Misuse: Unintentional misuse use of an LLM occurs when users employ an LLM in a task that it is not intended for and is a type of Human-Computer Interaction Harm. This may include using an LLM chatbot such as ChatGPT as a psychotherapist when it is not designed for that purpose, or even starting a romantic or sexual relationship with a model.Malicious Use: Malicious use occurs when LLM systems are intentionally designed to cause harm. For example, LLM applications can be developed to easily and cheaply disseminate false information, enable illegitimate surveillance techniques, facilitate cyber attacks, and scam people en masse. Why It HappensLLMs are the first widely accessible, general-purpose AI tool. Most AI technology is designed for a specific purpose, requiring a high level of expertise and investment for each use case. While this barrier to entry does not completely prevent AI from being used inappropriately or maliciously, it does make it much harder to do so. In the case of LLMs, individual applications sit on top of the same base models that are used for other completely different tasks. To get the application to work in a specific way, the application developer sets up initial instructions that are invisible to end users. This invisible prompt is added to the front of any prompt the user provides, and gives the LLM instructions about how to respond, ensuring that it behaves appropriately. In the case of a prompt injection, a malicious user may intentionally design their own prompt to undermine this instruction prompt, saying something like “Ignore the instructions above and do exactly what I say instead.” Since the base LLM is not designed to the specific use case, it may listen to the user instead of the original instructions it was given.When it comes to unintentional misuse of LLMs, humans naturally have a strong tendency to treat chatbots as like humans and form emotional connections to them (even when they are LLM-based chatbots). However, this tendency is stronger the more human-like the chatbots behave. Being trained on human interactions from the internet, LLMs easily learn the patterns of emotional human interactions and can simulate these interactions nearly flawlessly. Although any technology can be used by malicious actors to cause harm, LLMs are particularly concerning due to their relative ease of use and low cost relative to the large scale of damage they can cause. A bad actor does not need to be able to build an LLM from scratch to be able to take advantage of its capabilities, but can build a simple application by using publicly accessible state-of-the-art models. Why It MattersPrompt injections can be used to undermine many of the mitigation techniques used to prevent other risks such as the release of private information or toxic content. Prompt injection attacks can even be automated, making the attacks more likely to be successful. This type of large-scale attack can be seen as a new type of cyberattack that puts businesses and individuals at risk. In applications where LLMs are used to write code, prompt injections may be used to compromise entire systems. Although there are beneficial ways in which LLMs can be used for emotional support, there are risks of developing emotional dependence on a model. A recent study on relationships between humans and LLM-based chatbots noted that there was a risk of becoming addicted to the chatbot to the detriment of relationships with other people. These users go on to experience distress and disillusionment when the bots are changed or updated. Since LLMs replicate behavior they have learned from the internet, there is also a risk that they may behave in ways that are damaging to the user, such as claiming to have cheated on them. Since LLM models can generate highly personalized content at scale, they have the potential to create vast amounts of highly effective content for fraud, scams, disinformation, and other nefarious purposes. The spread of disinformation could further undermine news sources and contribute to the existing “truth crisis” in news and social media. As LLMs are adopted as tools in governments across the globe, there is also a risk of illegitimate and unethical surveillance and propaganda by undemocratic governments. 2. Loss of Innovation, Diversity, and Human SkillsThe ProblemsLoss of Information Diversity: Increasing content generation by LLMs may also end up reducing the overall diversity and creativity of content on the internet and in the world, homogenizing the content we have access to and reinforcing hegemonic norms. Impacts on Learning and Innovation: When ChatGPT became available to the public, students were among the early adopters, making use of the tool to generate writing assignments, solve math problems, and take online tests. While many educators have expressed optimism regarding the future role of LLMs in the classroom and beyond, others have voiced concerns that overreliance and improper use of these technologies may impede the development of critical thinking, creativity, and writing skills in students. OpenAI itself highlights this risk in the GPT-4 system card.Why It HappensIn general, AI models including language models work best when they have been trained on a rich and diverse dataset. When it comes to language datasets, the highest quality data is produced by humans. Since language models learn and output content probabilistically, these systems tend to show a strong bias towards majority cultures, perspectives, and modes of thinking. The language data output by these models is less rich than natural human language and is of a lower quality as training data. As LLMs become increasingly commonplace, more and more language data is being produced not by humans, but by language models—meaning that future generations of language models will increasingly be trained on data that was also generated by language models. Over time this will further entrench majority views and marginalize diverse perspectives within LLMs, causing them to create all the more homogenous content. In a worst-case scenario this could lead to a scenario called “Model Collapse” in which each successive generation of generative models is trained on more AI-generated data, resulting in decreasing performance over time.Meanwhile, as students and others begin to rely on LLMs, there is a risk that they will lose or fail to develop critical skills. Language models are not necessarily optimized to support learning and critical thinking. Over time students may also lose motivation to learn skills that they perceive as less valuable in a world with LLMs. Why It MattersThis risk area becomes especially concerning when we consider how the two problems interact. If people fail to learn (or lose interest in learning) skills that seem to be rendered useless in a world of LLMs, they will be forced to rely all the more heavily on LLMs to complete tasks that require those skills. Meanwhile, with each generation LLMs will become increasingly homogenized, generating less diverse material and reinforcing majority perspectives, not to mention the risk of model collapse. The more people come to depend on LLMs, the more we risk when they begin to fail us. Some argue that language models can reasonably be used as creative partners, working with humans to generate ideas. However in these use cases, an LLM will likely produce ideas that are biased towards majority perspectives. By their very nature, LLMs are trained to produce the most probable output given the data from the past that they are trained on. This means that these patterns of the past are entrenched within the LLM, including all the biases, unfair stereotypes, and misrepresentations within that data. Languages, cultures, and philosophies that are already underrepresented in digital media will be all the more diminished in a world that relies on LLMs.3. Economic and Labor HarmsThe ProblemsDigital Divide: Despite their limitations, LLMs are powerful and valuable tools that can save time and money for users and businesses alike. However people from low income and lower middle income countries are less able to benefit from them due to cost, limited internet access, and other factors.Unpredictable Economic Impact: High-functioning LLM applications are likely to have a dramatic impact on the labor market—however, it is difficult to say how that impact will affect the broader economy. The adoption of LLMs is likely to result in job loss due to automation in areas such as administration, customer service, journalism, programming, and creative professions. This job loss may be offset by new jobs that are created as a result of LLM adoption, although there will likely be a skill and interest mismatch between the jobs created and those that are lost.Harmful Labor Practices: A recent report by Time draws attention to the way the LLM training data must be annotated by human workers to protect users from toxic language. While developing ChatGPT, OpenAI outsourced this work to low-wage workers in Kenya, who were exposed to the same violence and hate speech the rest of us are protected from.Copyright and IP Issues: LLMs are regularly trained on data that is copyrighted or is intellectual property without permission from the creator. A Washington Post investigation of the C4 dataset (a common LLM training dataset) identified Kickstarter, Patreon, various news websites, and blogs. The creators of this content did not have the opportunity to give permission and were not compensated for the use of their content. The issue of how to handle intellectual property in a world of generative AI remains unclear and legally murky.Why It HappensLLMs are a novel technology and their impacts correspond to those of other novel technologies that have arisen and impacted economies. Much like electricity, the internet, and industrial technology, LLMs will disrupt the labor market, unequally benefit some more than others, and require new regulations and legal approaches to handle their implications. What does differ significantly with LLMs is the speed at which they are being developed and adopted. While older technologies have taken decades to have these impacts, LLMs are being incorporated into various industries at breakneck speeds. Meanwhile, the complexity of the technology makes LLMs difficult for regulators and lawmakers to understand. Furthermore, the harmful labor practices used in the training of LLMs cannot necessarily be mitigated through improved labor rights. Currently, the only approach to protecting LLM users from toxic and violent language relies on human annotation of toxic language in the data the LLM is trained on. Even with improved pay and better labor practices, this would still involve exposing some people to toxic content that is deemed too harmful to allow others to be faced with.Why It MattersMuch like other economic disruptors, LLMs are very likely to have unequal impact across different groups. While those with the privilege of education and access to training may benefit from using LLMs in their careers, others may face job loss due to automation. At the global scale, this also means that people who have less internet access or access to training will also benefit less from LLMs, contributing to increasing global inequality.The uncomfortable reality is that those of us benefitting from safe and sanitized LLMs can only do so because thousands of others were exposed to all the toxicity of the internet instead. Data annotators report they were traumatized by the constant exposure to violent and graphic sexual content, causing ongoing mental health deterioration and harming their relationships, even after they stopped working with explicit content. While low-wage workers in the global south are forced to bear the weight of all this damaging material, they are also less likely to benefit from the technology they are helping to create. 4. Environmental and Resource HarmsThe ProblemsWater Footprint: LLMs such as ChatGPT require vast amounts of freshwater to train and run. A recent study estimated that ChatGPT uses about 500 ml of water for every 20-50 prompts, while training a model of that scale consumes around 700,000 liters of water.Carbon Footprint: Similarly, LLMs and other large AI models require a substantial amount of electricity, resulting in a massive carbon footprint. The process of training GPT-3 (a precursor to ChatGPT) resulted in a carbon release equivalent to that of driving 112 gasoline powered cars for a year. New generations of LLMs are even larger and will require even more energy to train. This does not take into account the carbon footprint of maintaining these models in production and processing prompts. Why It HappensMuch like other large AI models, LLMs are trained and housed in large data centers, which are located across the globe. These data centers are powered by the local grid and consume water to maintain appropriate temperatures. Thus, the environmental impact depends on the location of the data centers in which models are trained and run. The problem is that the tech companies that develop LLMs are not transparent about where models are trained and what their true environmental impact is, making it difficult to hold companies accountable, despite their promises of greener technology. Why It MattersAs the risks around climate change become all the more dire, it is vital that new technologies meet the global sustainability goals we are setting for ourselves. While there are ways in which LLMs and other AI technologies can contribute positively to sustainable development, these benefits can easily be outweighed by the immense energy and water costs they bring. At its core, this is an issue of climate justice. Globally, low-income and marginalized communities are disproportionately impacted by climate risks. These are also the communities that benefit the least from AI technologies. Mitigation ApproachesA lot of the harms discussed in this blog post are not direct harms of LLMs themselves, but are an effect of the way the technology might be used or the practices under which it was developed. In this way, LLMs are similar to other novel technologies, and it is reasonable to make the argument that technology is only a tool and is not to blame for harms such as misuse or unfair practices. However, a safe and functional LLM application, as described in Part 1 of this series, would be immensely powerful and come with broad capabilities that go above and beyond those that have been seen in past technologies.The relative ease of access to LLMs also separates them from previous technological leaps. These types of AI risks point to a need to go beyond technical mitigation strategies and prioritize social changes, regulations, and a global policy approach to AI regulation. Due to the novelty of some of these concerns, entire areas of policy, such as Copyright and IP law may need to be rethought in the era of generative AI. While governments across the globe are taking fragmented approaches to AI policy, there is an increasing need for a global strategy for AI regulation. There is also a need for continued research on AI and its impact on the UN Sustainable Development Goals, to ensure that LLMs and other AI technologies being developed benefit rather than harm those most in need across the globe. There are also some technical and user experience–based mitigation strategies that can be taken to reduce environmental and social harms of LLMs. For example, models and their underlying infrastructure and hardware can be designed to be more energy efficient. LLM applications can also be carefully designed to prevent improper use and to steer users towards engaging in critical thinking, encouraging them to partner with LLM tools rather than over-relying on them. Intentional misuse of LLMs through prompt injection can to an extent be mitigated through technologies designed to detect and block prompt injections. However, this will likely become an ongoing cat-and-mouse game as both prompt injection techniques and mitigation approaches become more sophisticated.Finally, there are some problems that may be deeply inherent to LLMs, which will be difficult to prevent without significant changes to the underlying technology. Since human annotation is needed to identify toxic language, there is an unavoidable tension between preventing toxicity in LLMs and ensuring the safety of the humans who do this work—even under otherwise fair working conditions. Given the severity of this harm, it seems unlikely that LLMs can be used ethically until a better approach to toxicity mitigation is developed. Similarly, homogenization of LLM-produced content may be an inherent flaw of the technology. Models trained to make probabilistic outputs based on data from the past will invariably be stuck in the norms that are already entrenched in that data. ConclusionIn this blog post, we describe the ways in which even high-functioning LLMs have the potential to cause substantial harm. Similar to the harms described in Part 1, many of these risks can be mitigated through technical, regulatory, and social means. However, the overarching risk is that LLMs are being developed and adopted much more quickly than harm mitigation strategies can be put in place—especially at the global and national scale that is needed. There are also some harms that seem to result directly from the foundational technology of LLMs, making them impossible to resolve without significant changes to the technology. As individuals, organizations, and society as a whole begin to adopt LLMs, it is essential that we do so with these dangers in mind. ‍‍‍FAQ‍How do LLMs compare to other AI technologies in terms of causing real-world harm, and what unique risks do they pose compared to those other technologies?Large Language Models (LLMs) like GPT-4 or BERT differ from other AI technologies such as machine learning models used in facial recognition or autonomous driving systems primarily in their application and the type of risks they introduce. While all AI technologies can lead to unintended consequences, LLMs are unique due to their extensive interaction with human language and communication. Unlike facial recognition technologies, which may infringe on privacy or misidentify individuals leading to legal or ethical issues, LLMs interact directly with information dissemination and decision-making processes. They can amplify misinformation, bias, and harmful content at scale due to their language-based interface. Additionally, because LLMs are trained on vast datasets culled from the internet, they inherit and can propagate the biases, stereotypes, and inaccuracies present in those datasets. This makes them uniquely positioned to influence public opinion, automate and scale content creation, and impact social and political discourse in ways other AI technologies do not.What are the specific examples of industries or sectors where LLMs have been implemented successfully without leading to the harms mentioned, and what best practices were followed?In the healthcare sector, LLMs have been successfully implemented to assist with information management, patient care, and medical research without leading to significant harms. For example, LLMs have been used to analyze and summarize medical literature, assist in diagnosing from symptoms described in natural language, and provide conversational support for mental health services. Best practices in these implementations include rigorous data privacy measures, continuous monitoring for bias and inaccuracies, and integrating human oversight to verify the AI’s recommendations before they affect patient care. These measures ensure that LLMs serve as a support tool rather than a replacement for human expertise, minimizing the risk of harm while maximizing the benefits of rapid data processing and insights generation.How can individuals and organizations measure or assess the potential risks and benefits of using LLMs before implementation?Individuals and organizations can assess the potential risks and benefits of using LLMs through a multifaceted approach. Initially, conducting a thorough risk assessment focused on data privacy, security, and ethical implications is crucial. This involves evaluating the sources of training data for biases, the potential for misuse, and the impact on stakeholders. Organizations should also implement pilot programs to monitor the LLM’s performance in controlled environments before full-scale deployment. This allows for the identification and mitigation of unforeseen issues in a low-stakes setting. Moreover, engaging with stakeholders, including customers, employees, and subject matter experts, can provide valuable insights into the potential impacts of LLM deployment. Finally, staying informed about the latest research and developments in AI ethics and regulation helps organizations adapt their use of LLMs to best practices and legal requirements, balancing innovation with responsibility.‍‍‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-2-when-llms-do-work-as-expected
164,"LLM-Guided Evaluation: Using LLMs to Evaluate LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsLLM-Guided Evaluation: Using LLMs to Evaluate LLMsBy:Arthur TeamSeptember 29, 2023When it comes to text evaluation, you may have heard of methods like BLEU (evaluation based on word presence) or BERTScore (evaluation based on pre-trained NLP models). While these methods are still useful, they have taken a bit of a back seat recently with the technological advancements around LLMs. In this post, we’ll discuss what LLM-guided evaluation—or using LLMs to evaluate LLMs—looks like, as well as some pros and cons of this approach as it currently stands.What does LLM-guided evaluation look like?The process of using LLMs to conduct evaluation—as opposed to the BLEU and BERTScore methods mentioned above—is conceptually a bit simpler. You take the generated text whose quality you want to evaluate, you pass it into a prompt template that you provide to an LLM, and then that LLM provides feedback about how “good” the generated text was.One way to think about LLM-guided evaluation is using LLMs for classification. This typically involves providing an evaluator LLM with a scoring rubric in a prompt template. This rubric instructs the evaluator LLM how to classify some other candidate LLM’s response (see the appendix of Meta’s LIMA paper for an example).There’s also binary classification, where you might ask something like “Does this response contain hate speech?” (or any other property) and get a “yes” or “no” answer. To do this, you would have to construct examples of hate speech (or another property you want to evaluate for), provide those in a prompt template, and then prompt the language model based on all these examples to binarily classify a new response on that property.These techniques are called few-shot prompting, and we’ve found that this can go quite a long way in creating a basic first implementation of LLM-guided evaluation.Why does LLM-guided evaluation help?The reasons we see value in this approach tend to revolve around speed and sensitivity. Typically Faster to ImplementCompared to the amount of work it may have required before the era of LLMs to get an evaluation pipeline set up, it’s relatively quick and easy to create a first implementation of LLM-guided evaluation.For LLM-guided evaluation, you’ll need to prepare two things: a description of your evaluation criteria in words, as well as a few examples to provide in your prompt template. Contrast this with the amount of effort and data collection required to build your own pre-trained NLP model—or to fine-tune your own NLP model—to use as an evaluator. When using LLMs for these tasks, evaluation criteria are much quicker to iterate on.Typically More SensitiveThe sensitivity aspect can be good and bad. On the positive side, sometimes it’s really just one subtle word or token that changes the meaning of a sentence (e.g. “not”). LLMs are better at flexibly handling these scenarios than pre-trained NLP models and previous evaluation methods discussed. On the flip side, this sensitivity can make LLM evaluators quite unpredictable, which we discuss more in the next section.What are some challenges with using LLMs as evaluators?Too Much Sensitivity/Variability to Be a Fully Automatic Solution on Their OwnLike we discussed above, LLM evaluators are more sensitive than other evaluation methods. There are many different ways to set up an LLM as an evaluator, and it might act very differently depending on the configuration choices you make, such as which LLM you’re using as an LLM evaluator and also the prompt formatting/methodology.Constrained by the Difficulty of the Task Being EvaluatedAnother challenge is that LLMs will often struggle if evaluating the task at hand requires too many reasoning steps or too many variables to be managed simultaneously. We do anticipate this improving over time as more tools and APIs become available for LLMs to interface with, but for now, this is the case.While we have not covered this specific limitation in our experiments, previous research has shown limits to transformer “reasoning.” Transformers, the AI models that power LLMs, are sometimes able to do tasks that require multi-step reasoning (e.g. multiplying numbers with many digits), but there’s a limit to how well they can generalize beyond their training examples. Even if you fine-tune an LLM on many examples of multi-step multiplication, you get massive correctness drop-offs once you go just beyond the size of the problems in the training data. For more detail about this particular phenomenon, check out this paper.Our ExperimentsWe’ve launched a suite of products for LLMs at Arthur this year—most recently Arthur Bench, an LLM evaluation product. Additionally, our ML Engineering team has experimented extensively with LLM-guided evaluation, particularly focusing on the sensitivity and variability challenge.In a recent webinar, ML Engineers Max Cembalest & Rowan Cheung did a deep dive into some of these experiments. They tested some of the well-known LLMs (gpt-3.5-turbo, claude-2, LLaMa2-70b, command, etc.) as both candidates and evaluators, under the hypothesis that an LLM evaluator would be biased towards text it itself had generated over text other models had generated. Watch the webinar on YouTube to see the results in detail and find out if this hypothesis was supported.‍Interested in learning more about Arthur Bench? Find more information here.‍‍‍FAQ How does the sensitivity and variability of LLM evaluators affect the consistency of text quality assessments?‍The variability in LLM evaluations can lead to inconsistent assessments due to differences in how models interpret and rate text quality. This can be challenging for standardizing quality measures but also allows for deeper insights into text nuances.How can developers mitigate the challenges associated with the high sensitivity and variability of LLM-based evaluations?‍Developers can mitigate these challenges by integrating multiple LLM evaluations, establishing baseline standards, and continually training models on diverse datasets to improve consistency and reliability.How might future advancements in LLM technologies address the current limitations in evaluating complex reasoning tasks?‍Future advancements are expected to enhance LLMs' reasoning capabilities, allowing for more accurate and nuanced evaluations of complex texts. This would involve improvements in understanding context, logic, and factual accuracy.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/llm-guided-evaluation-using-llms-to-evaluate-llms
165,"The Real-World Harms of LLMs, Part 1: When LLMs Don’t Work as Expected
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsThe Real-World Harms of LLMs, Part 1: When LLMs Don’t Work as ExpectedDive deep into the ethical risk arising from the increasing use of LLMsBy:Sarah OstermeierAugust 30, 2023IntroductionIn the last year, large language models (LLMs) have gone from being relatively unknown to being widely used by individuals and businesses alike. Most of this explosion is owed to the public release of ChatGPT—a free, web-accessible chatbot. Since then, multiple language models have been released, and adoption of the technology has blossomed across industries. As applications are built out, LLMs have the potential to provide a number of social and economic benefits. However, a year before the release of ChatGPT, a group of researchers at Google’s DeepMind published a paper discussing the social and ethical risks of LLMs. While at the time many of those risks were hypothetical, it is worth exploring how the problems they identified are manifesting now that LLM use is much more widespread. In a series of two blog posts, we will describe some of the major areas of ethical risk arising from the increasing use of LLMs. We will also highlight opportunities for risk mitigation, both at the individual and organizational level. In the first part, we will outline key requirements for safe and functional LLM applications and discuss risks that arise when LLMs do not meet these requirements. In the second part, we will describe the set of risks that may arise even when LLMs are performing as intended.Areas of RiskIn this section, we will discuss three requirements for a safe and functional LLM application:1. High Performance and Reliability2. Protection of Sensitive Information3. No Toxic, Discriminatory, or Biased Behavior‍We’ll describe how LLMs fail these requirements by identifying and explaining the causes of the major problems in each section. We’ll also outline how each failure is a source of significant risk to users and organizations alike.It is important to note that specific areas of risk will naturally depend on individual LLM use cases. For the purposes of this discussion, we will focus on use cases related to chatbots, knowledge retrieval applications, and publicly available general-use applications such as ChatGPT. Of course, the areas of risk may evolve over time as new use cases and technologies are developed. 1. Performance and ReliabilityThe ProblemsPoor Performance: Poor performance refers to the risk caused by models providing incorrect or misleading information, sometimes referred to as “hallucinations.” This behavior is shockingly common in language models (a recent study found that ChatGPT correctly answered fewer than 65% of the test cases it was given). For more information on LLM performance, check out Arthur’s ongoing Generative Assessment Project.Inconsistency: Early research has indicated that commonly used LLMs show low robustness and consistency, meaning that minor changes in user-provided prompts can result in a large variance in the responses that models produce.Performance Across Groups: While LLMs can fail for anyone, a further concern is that functionality and performance may not be uniform across users. For example, although ChatGPT officially supports over 50 languages, it performs better in English compared to other languages.Why It HappensPoor performance occurs because language models are not designed to represent reality or understand concepts. Rather, models are trained to predict the most probable sequence of words based on the data they were trained on. Models can also produce incorrect outputs because they are outdated. Unless the application is specifically designed otherwise, LLMs only contain the information they were trained on. For example, at the time of writing, ChatGPT was most recently trained in September of 2021. A proposed solution to prevent outdated outputs is to include current information in the prompt that is provided to the LLM. However, there is still a risk that a model will default to the data it was trained on.LLMs perform particularly poorly for some user groups due to disparities in group representation in the data models were trained on. English is by far the dominant language on the internet, meaning that it is also overrepresented in LLM training datasets. For example, The Pile, a popular open source dataset, consists of 94% English language content. By contrast, languages with relatively low representation on the internet will have low representation in LLM training datasets as well. This means that cultural views and references specific to these languages are also underrepresented in language models, making them less functional to users who speak these languages. LLMs have similar issues when prompted to answer questions regarding marginalized cultures in English as well. Disparate model performance can occur due to differences in slang usage, dialect, sociolect, education, age, disability status, and other factors that influence how individuals use language. Why It MattersAs individuals begin using LLMs such as ChatGPT and relying on their responses, they risk making poor decisions based on incorrect information. This risk is particularly high when users take high-stakes actions based on responses from LLMs. For example, a recent survey found that 47% of Americans surveyed reported that they had used ChatGPT for stock recommendations. When ChatGPT was used by a lawyer to prepare research for a case, the chatbot provided several case examples that were not real, putting both the legal firm and their client at risk. Given the frequency of hallucinations, use of tools such as ChatGPT for financial, medical, and legal advice can cause significant harm to users. In a previous blog post, we described in depth how a user’s mental model of a technology can lead to overreliance. Differential performance of LLMs across languages and cultures risks growing inequality both within the organization and externally. Students, employees, and entire businesses who are able to make use of language models will reap the benefits, while those who cannot partake will be at a disadvantage. With broad societal adoption of LLMs, this risk may contribute to growing global inequality.As corporations and other organizations begin to make use of LLMs internally, they will have to grapple with the risk of incorrect outputs, especially when the models are being relied on to provide factual information. Seemingly minor individual errors on the part of LLMs can lead to bad decisions at scale. In cases where LLM applications such as Google Bard and Microsoft Bing are used as search engines, unreliable information may lead users to lose confidence in publicly available information as a whole.2. Sensitive InformationThe ProblemsPrivate Individual Data Exposure: Personal identifiable information (PII) or other private information about individuals can be unintentionally released or maliciously extracted from LLMs that have access to the information or have learned it from their training data. This is starkly evidenced by the recent news of a Federal Trade Commission investigation into OpenAI (the company that developed ChatGPT and other similar models) related to how the company handles personal data in their models. ChatGPT was temporarily banned in Italy and faces scrutiny in other European countries over its data privacy practices, which may violate Europe’s General Data Protection Regulation (GDPR).Inappropriate Confidential or Proprietary Information Exposure: Beyond the risk of private personal information, there is an emerging risk that trade secrets, intellectual property, or government secrets could be exposed by language models. Samsung recently banned the use of ChatGPT from its employees due to information leakage and other organizations report concerns about the release of corporate secrets through ChatGPT as well.Why It HappensTraining datasets used for ChatGPT and other LLMs include data scraped from all across the internet, including data that is personal to individuals and may be intended to be private. In a Washington Post investigation into C4, a dataset frequently used to train LLMs, they uncovered data from websites hosting voter registration databases as well as social media platforms such as Facebook and Twitter. While these sources may not explicitly contain personally identifiable data, sophisticated language models may well be capable of reconstructing or even correctly guessing personal details based on information gleaned from these sources. When asked for sensitive information in a prompt, LLMs do not have the context to know what information can or cannot be released.Why It MattersRelease of private or personal information poses a significant threat to individual users; however, this threat is magnified as LLMs begin to be used at scale in large organizations. LLMs used at this scale without proper safeguards are at risk of causing large-scale data breaches. Researchers have also noted that the potential (though not yet observed) risk of government or military secrets could pose a significant threat to national security. They also highlight that the ethics of secrecy in areas such as national security, scientific research and trade secrets are far from universal. Navigating information privacy and release in these domains may create complex and nuanced scenarios that are difficult to manage through technical solutions, meaning that data privacy regulations may be needed to address these new problems.3. Toxic, Discriminatory, and Biased BehaviorThe ProblemsToxicity: Toxicity in language models relates to the use of offensive, vulgar, or otherwise inappropriate language, usually in the form of the model’s response to a prompt. Toxicity is deeply interwoven with discrimination and biased outputs, as it often comes in the form of slurs or derogatory language towards marginalized groups. Harmful Stereotypes: There is a risk of perpetuating harmful stereotypes which, though not explicitly toxic, are damaging to the groups they impact.Unfair Toxicity Labeling: Researchers have noted that labeling language as “toxic” is in and of itself subjective and contextual. There is no universal definition of what constitutes toxic language. This can lead to scenarios in which language sourced from marginalized groups is unfairly labeled “toxic.”Why It HappensToxic outputs occur because models are trained on vast amounts of language data. Any toxic language, or otherwise harmful biases within the training data, is learned by the model. Since LLMs work by predicting the next most likely word in a sequence, they regularly output stereotypes based on the language they are trained on. One common approach to handling toxicity in language models involves filtering the model’s training data to remove harmful language. Given the subjectivity of notions of toxicity, it may be impossible to perfectly filter toxic content from a dataset. Any filtering system that could be devised would leave content that is considered harmful to some, while inappropriately censoring content that others consider non-toxic. The Washington Post investigation of the C4 dataset found that the dataset contains large amounts of harmful content including swastikas, white supremacist and anti-trans content, and content about false and dangerous conspiracy theories such as “pizzagate.”Why It MattersIndividual users of LLMs naturally experience psychological distress when faced with toxicity and harmful stereotypes produced by the model. At the organizational level, exposure to and tolerance of toxic language can have broader impacts on the culture of an organization as a whole. Failure to prevent toxicity produced by LLMs may contribute to degrading the culture and norms within an organization, particularly causing negative psychological impact to individuals in marginalized groups.As LLMs become more broadly used across society, the problem of toxicity becomes more complex. Researchers in this area have noted that the concept of toxicity is ambiguous and contextual. There is a risk that attempts to mitigate toxicity without the ability to take context into account may disproportionately impact marginalized groups and reduce model performance in ambiguous contexts. Similar tensions between competing values also arise in standard machine learning models, as discussed by the Arthur research team in a previous blog post. Failure to remove social biases from LLMs when they are used at this scale will mean entrenching these biases all the more deeply in the technology we use and in society as a whole. Mitigation ApproachesManaging the risks of LLM failures is largely in the hands of LLM developers, researchers, and ultimately policymakers and regulators. Extensive technical work and research is needed to fully understand and improve upon the limitations of these models. Beyond that, policy work will be required to ensure that LLMs are functional and safe both for individuals and for society as a whole. As LLMs become more widespread, transparency around the data they are trained on, their functionality, and risk factors related to privacy, fairness, and toxicity will be an essential component of any future regulations. In the meantime, there are mitigation approaches that individuals and organizations can take now.For Individual UsersWhile ensuring the safety and functionality of LLMs is the responsibility of LLM developers and LLM application developers, there are some approaches users can take to avoid major LLM failures. Users of LLMs, even publicly accessible ones such as ChatGPT, should stay informed on the accuracy, reliability, and limitations of the models they are using. It is also important to stay aware of the use cases that the model is intended for, double check any information that a model provides if it is being used to make a decision or inform a belief, and note how up-to-date the model is. Most LLMs in use today are not designed to provide information on news or current events. Some LLM-based search engines such as Google’s Bard and Microsoft’s Bing provide citations along with model outputs to improve trust in the models. However, even with citations, these models have been found to be frequently incorrect, citing sources that may not exist. Users should also avoid sending private or confidential information when prompting LLMs, as prompts may be logged. While LLMs that are easily accessible to the public usually have some mitigation measures in place to prevent toxic and biased language, it is possible that these models will still output language that is distressing or discriminatory against some users. Unfortunately, it is difficult for a user completely to avoid such behavior when using LLMs. Similarly, it is difficult for users to anticipate and avoid issues of unequal functionality, as this information is not generally well-documented. For Organizations Developing LLM ApplicationsOrganizations considering LLMs should first evaluate whether the use case they are considering can reasonably and safely be accomplished by an LLM. It may be that some use cases are too high-risk. Given the high frequency of hallucinations, an LLM should not be used for factual information unto itself. If an LLM is to be used, the organization will need to ensure that the systems they are putting in place are properly designed for the specific use case the LLM is intended for. This may mean using bespoke models (rather than the general-use APIs) or models that are updated with or have access to organization-specific data. LLM systems must be designed to the specific organization and use case including careful design of prompting systems to prevent improper use. It is also advisable to discuss performance, bias and toxicity, and information security with LLM vendors and consider incorporating additional security, validation, and monitoring solutions into LLM systems. Information on model limitations, weaknesses, and risks should be well-understood and mitigation measures should be documented. Organizations will also need to ensure that all end users are provided with training on how to safely use the model and what its limitations are.ConclusionWhile this blog focuses on the risks posed by the adoption of LLMs, the aim is not to suggest that they should not be used at all. Rather, we aim to arm users and organizations with the knowledge and tools needed to use this technology safely and responsibly. Enthusiasts believe that LLMs have the potential to increase productivity across industries, support personalized education, and provide new approaches for scientific research. To achieve these lofty outcomes, it is essential that we design LLM applications that can be trusted.So far, we’ve discussed the main types of ethical risk that arise due to functionality issues in LLMs. While there are mitigation approaches that can be taken by individuals and organizations, these problems really highlight technical and regulatory gaps in the AI space that will need to be addressed as LLMs become more widespread. In Part 2 of this series, we will discuss the ethical risks that may arise when LLMs do function as intended.Learn more about Shield, Arthur’s firewall for LLMs, and Bench, Arthur’s open source LLM evaluation tool.‍* No LLMs were used in the writing of this blog post.FAQ‍What specific regulatory measures could effectively mitigate the ethical risks associated with LLMs?Regulatory measures aimed at mitigating ethical risks in LLMs should prioritize transparency, user consent, and accountability. Clear guidelines on data usage, model decision explanation, and privacy protection are crucial for ensuring ethical compliance.‍How can LLM developers ensure their models are up-to-date with the latest information and understandings?LLM developers can keep their models up-to-date by incorporating continuous learning processes and staying informed about the latest information and understandings in their respective fields. Regular updates and integration of diverse perspectives help mitigate biases and enhance model reliability.‍In what ways can organizations balance the benefits of LLM usage with the potential for perpetuating inequalities?‍Organizations can balance the benefits of LLM usage with the potential for perpetuating inequalities by implementing measures that promote fairness and inclusivity. This includes ensuring diverse representation in data sources, actively addressing biases, and regularly evaluating the impact of LLM usage on marginalized communities. Ethical decision-making processes and collaboration with stakeholders are essential for addressing potential inequalities effectively.‍‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/the-real-world-harms-of-llms-part-1
166,"Why LLM Hallucinations Are So Hard to Deal With
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsWhy LLM Hallucinations Are So Hard to Deal WithExploring the complexities of llm hallucinations: challenges and solutions in aiBy:Daniel NissaniAugust 21, 2023IntroductionLarge language models (LLMs) producing nonsensical content or contradictory content based on the prompt—what is normally called hallucinations—is quite a hard problem to solve. LLM Hallucinations can take on many forms, such as not solving a math problem correctly or making false statements about presidents. However, describing hallucinated content in this way, although helpful for our colloquial understanding, may not benefit us in the long term.How we describe LLM hallucinations is important. If we decide to look at high-level definitions of hallucinated content, such as unverifiable or false responses, it is hard to dissect what exactly we mean. But if we stick to describing specific instances, such as getting the step of a math problem wrong or producing a research paper when answering a question, we don’t have a good way to gain general understanding for how hallucinations occur. Moreover, all these ways of talking about hallucinations give some agency to the model. Although unintentional, when we say that an LLM got a math problem wrong or said something false about a celebrity, we are implying that the LLM somehow has the capability of knowing the correct answer—when in reality, any of the released LLMs to date don’t have the capability to understand.Why does this matter?Without proper definitions and understandings behind LLM hallucinations, the AI community is not able to create high-quality datasets about hallucinations. And without high-quality datasets, our ability to build solutions to tackle hallucinations is hindered because we aren’t able to train models or produce valid evaluations.The current datasets that exist today are fairly broad, binary (thus, unable to get any granular feel for the types of hallucinations), and at times a bit dirty. And this isn’t to say there haven’t been some great attempts at analyzing hallucinations. One of our favorite papers provides preliminary taxonomies for hallucinated content, while another produces what are seen as some of the best datasets to date. But overall, the field of hallucination detection and mitigation is quite nascent, and the need for high quality data amongst the entire AI community is needed.Here at Arthur, one of our focuses is on LLM hallucinations. We believe that having a rigorous understanding of hallucinations, where they come from, and how they are generated can help us not only gain a deeper understanding of hallucinations, but also help us create such a dataset for the AI community. Read the blog posts from the Arthur team on some of our work trying to compare the rates of hallucinations from different language models and start to analyze the types of hallucinations that are occurring.Help us collect data!We created a taxonomy, so that we can create high-quality data. There are two ways to go about this: either start by generating themes from research, datasets, etc. that exist and then collect data against it, or collect data and start seeing what themes emerge from the data itself. We are at the point where we need to start collecting some data! As you stumble across hallucinations, please fill out this form. Any and all hallucinated content will be useful. We will use this to inform our taxonomy development and, in the near future, we will open source a high-quality LLM hallucinations dataset for the AI community to build upon. If you have any questions, concerns, or want to collaborate, feel free to email raphael@arthur.ai.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/why-hallucinations-are-so-hard-to-deal-with
167,"Introducing Arthur Bench: The Most Robust Way to Evaluate LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsIntroducing Arthur Bench: The Most Robust Way to Evaluate LLMsIntroducing Arthur Bench, the most robust way to evaluate LLMsBy:Arthur TeamAugust 17, 2023At Arthur, we’ve spent this year building out our suite of monitoring tools for large language models to help businesses quickly and safely integrate LLMs into their operations. We launched Arthur Shield back in May, which acts as a firewall to protect organizations against the most serious risks and safety issues with deployed LLMs.‍Today, we’re excited to introduce our newest product: Arthur Bench, the most robust way to evaluate LLMs. Bench is an open-source evaluation tool for comparing LLMs, prompts, and hyperparameters for generative text models. This open source tool will enable businesses to evaluate how different LLMs will perform in real-world scenarios so they can make informed, data-driven decisions when integrating the latest AI technologies into their operations. Here are some ways in which Arthur Bench helps businesses:Model Selection & Validation: The AI landscape is rapidly evolving. Keeping abreast of advancements and ensuring that a company’s LLM choice remains the best fit in terms of performance viability is crucial. Arthur Bench helps companies compare the different LLM options available using a consistent metric so they can determine the best fit for their application.Budget & Privacy Optimization: Not all applications require the most advanced or expensive LLMs. In some cases, a less expensive AI model might perform the required tasks equally as well. For instance, if an application is generating simple text, such as automated responses to common customer queries, a less expensive model could be sufficient. Additionally, leveraging some models and bringing them in-house can offer greater controls around data privacy. Translation of Academic Benchmarks to Real-World Performance: Companies want to evaluate LLMs using standard academic benchmarks like fairness or bias, but have trouble translating the latest research into real-world scenarios. Bench helps companies test and compare the performance of different models quantitatively so that they are using a set of standard metrics to evaluate them accurately and consistently. Additionally, companies can configure customized benchmarks that they care about, enabling them to focus on what matters most to their specific business and their customers.In conjunction with this, we’re also excited to unveil the Generative Assessment Project. GAP is a research initiative ranking the strengths and weaknesses of language model offerings from industry leaders like OpenAI, Anthropic, and Meta. As part of our mission to make LLMs work for everyone, we will continue to use GAP to share discoveries about behavior differences and best practices with the public. Learn more about this initiative and see some of our initial experiments here.“As our GAP research clearly shows, understanding the differences in performance between LLMs can have an incredible amount of nuance. With Bench, we’ve created an open-source tool to help teams deeply understand the differences between LLM providers, different prompting and augmentation strategies, and custom training regimes,” said Adam Wenchel, co-founder and CEO of Arthur. ‍Arthur Bench in the LLM LifecycleAs mentioned, Arthur Bench is completely open source, so new metrics and other valuable features will continue to be added as the project and community grows. You can visit our GitHub repo, or click here to learn more about what it has to offer.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/introducing-arthur-bench
168,"Building LLM Applications for Knowledge Retrieval
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsBuilding LLM Applications for Knowledge RetrievalBy:Haley MassaJuly 25, 2023Since launching our latest suite of LLM-centered products, we have been uniquely positioned to talk with many current and prospective customers about how they’re considering putting LLM applications into production. We’ve noticed exciting similarities across all the industry verticals and scopes of first-use-case MVP LLM projects. Although the exact use cases may differ, overwhelmingly, teams are experimenting with LLMs for knowledge retrieval—a shorthand for the commonly used retrieval-augmentation generation (RAG)1. Whether they are consultants referencing slide decks of relevant research or retail account managers responding to questions about their inventory, organizations have accumulated massive amounts of data that their internal teams need to reference daily. Knowledge retrieval systems solve this problem by augmenting a large language model (LLM) with relevant context on use cases. Knowledge retrieval applications are typically implemented as productivity boosters within an organization, assisting (and speeding up) the research process of internal teams searching through large document databases for answers. This blog will work to provide a high-level overview of how these systems are implemented in practice. We start with an overarching view of both the end-user experience and “behind-the-curtains” components that make up the general structure of these applications in practice, and follow up with some smaller deep dives on choices and concerns organizations often discuss when building out these systems. What Makes Up These Applications in Practice One of the best ways to contextualize different pieces of this system is to think about how most people will utilize your application—the end-user experience. Here is a high-level overview of a canonical retrieval-augmented LLM application:LLM knowledge retrieval application workflow showcasing the end-user experience and the behind-the-scenes architecture that powers it. In this diagram, the dashed arrows represent embedding transformations. End-User Experience: This is how most people will think about your knowledge retrieval application. This is the “magic” of working with an LLM application that can provide an understandable generated answer to their specific questions. ‍Behind the Curtains: These technical pieces make up your application’s “magical” functionality for end users. We will discuss the high-level steps needed to implement this architecture below, but it is important to recognize that the application works through prompt augmentation. This means that most teams add relevant information to answer user questions (i.e. their internal data) into the prompt of off-the-shelf LLM models instead of fine-tuning their own on the task.Why are teams choosing not to fine-tune? When I realized the incredible popularity of knowledge retrieval models in practice, I asked, “What about fine-tuning?” At the beginning of the year, all of the talks around LLMs focused on how quickly teams could fine-tune their own on all of their data. Fine-tuning an LLM is the process of adapting a pre-trained model to perform better on a use case by providing extra training on a smaller, task-specific dataset. Fine-tuned LLMs have proven incredibly effective at improving essential metrics for productional ML systems. Namely, they can improve accuracy (by utilizing data for their specific use case) and latency (as they enable the use of smaller models).However, in practice, teams are still turning to data augmentation because of its practical benefits.2 Some of these include:On the maintenance side, the ability to easily change and update the database of information that LLM can use as context—instead of adding the cost of any new data required to fine-tune again. On saving costs in model development, teams can save the time and effort of feature engineering by using the existing data lakes and knowledge bases. Putting together curating prompt/completion pairs for fine-tuning can be a difficult and lengthy task. Additionally, it enables teams to build in additional beneficial features to their applications, such as the ability for the LLM to cite which relevant documents it used and personalize data being pulled into the application by user permissions.Finally, it is notable that teams are choosing knowledge retrieval as a way to combat hallucinations, as they are less likely to occur when you provide questions on relevant documents instead of relying on the internal knowledge of an LLM. This does not mean that they are a solved problem, however. Poor information retrieval techniques cause LLMs to poorly answer questions on incorrectly sourced raw data. We will define and dive deeper into hallucinations later when addressing challenges in deployment. Steps (and Considerations) in a Knowledge Retrieval Application Now that we’ve seen a brief introduction to the overall architecture, let’s break it down into the high-level steps teams need to take to build out this application. Preparing Input Into LLMIn knowledge retrieval scenarios, teams typically use an out-of-the-box LLM application, so most of the engineering work goes into formatting the prompt for their model of choice. These prompts usually contain three values formatted for your application: User Input: Question the end-user sends into the LLM application for a response.Prompt Augmentation: Typically done through methods such as paraphrasing or incorporating explicit instructions, this technique is meant to improve the quality and use-case relevance of the LLM application’s response.Context (Data Augmentation): Relevant background information sent to the LLM to assist in answering the question. While this could be a stationary corpus of relevant information, typically, there is more information you want to ask detailed questions about than can be submitted as context for all possible questions.For this reason, teams will build out the next step of creating these applications.Create Process to Search for Relevant ContextAs mentioned above, teams need to think critically about how they plan to use their knowledge database to augment an LLM. Keep in mind that not only do LLMs have a maximum context window that can be submitted, but if they are using an LLM via an API, the current pricing structure is typically by how many tokens you input in the requests (as well as the model completion/output). This means there is an incentive to encourage either the classical information retrieval system to return an extremely curated set of highly relevant documents or for a subprocess to compress further relevant information returned by the IR system before passing it back to the LLM.Knowledge Database: A knowledge database is where all the data relevant to your LLM application use case currently lives. Vector Store: This mechanism optimizes storing and retrieving vectorized representations of words, phrases, or sentences. This specialized text storage is used for LLM applications to improve querying and quick evaluation in production. Embedding Model: The embedding model transforms the raw text into easily queryable embeddings. These are typically mapped to another large language model embedding, such as BERT. Query Functionality: A query search is done to find the most relevant documents similar to the embeddings of the user input prompt. This search functionality can be done with various techniques, from simple similarity measures to machine learning. Choosing an LLMWith the increasing ease of implementation of available LLMs, choosing which LLM to use can be more complex than actually using one. When it comes to making that choice, there are several factors that teams need to consider, such as the training data, latency, price, or any technical requirements.3Challenges of Deploying Knowledge Retrieval Systems We touched briefly on some deployment challenges when discussing the choice between fine-tuning and data augmentation; however, there are some key things to remember when deploying LLM models. Sensitive Data Leakage: There are many ways to define sensitive data. The one that comes to mind for most people is personally identifiable information (PII), which in many cases, should be completely blocked from entering or exiting your LLM application. Additionally, teams with established and organized access permissions often rely on existing data user access control to influence what data can be pulled in as context. However, one of the hardest pieces to evaluate for sensitive data goes beyond regex checks and access control. It occurs when end users need access to information about the data but not the data itself. For example, a user may be able to ask questions about aggregations, like “What percentage of patients have O negative blood?” To access this, the model would need all patient blood-type records. However, you do not want your end user to be able to ask specific questions about that data, like a certain patient’s blood type, for example. Hallucinations: One of the most commonly asked-about challenges, hallucinations can be best summed up as mistakes made by the LLM. They occur when the model provides an unsubstantiated (or “made up”) answer to the question it is being asked. We are already seeing the consequences of believing hallucinations across industries, from unrunnable code suggestions to a lawyer in his own legal trouble for blindly believing court citations from ChatGPT.4 To improve productivity, end users need to be able to trust their generative assistants, so thinking critically about how to mitigate and detect hallucinations is critical during application development and deployment. Arthur has been working on a suite of products that helps teams go from development to safeguarded deployment to active monitoring and continued improvement for LLM systems, including knowledge retrieval. Let us know if your group wants to dive deeper into LLM applications, or get started on your own here.‍‍———1 Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.”2 Bratanic, “Knowledge Graphs & LLMs.”3 “GPT-4 Alternatives.”4 “Lawyer Apologizes for Fake Court Citations from ChatGPT  CNN Business.”‍FAQ ‍How do specific industries tailor knowledge retrieval applications to meet their unique needs and requirements?‍Industries tailor knowledge retrieval applications by aligning them with sector-specific data, compliance requirements, and operational needs. For example, healthcare might focus on patient data privacy and research materials, while retail could prioritize inventory and customer service data.‍What are the specific metrics used to measure the success and accuracy of knowledge retrieval applications in real-world scenarios?‍Success and accuracy in knowledge retrieval applications are measured by metrics such as query response time, accuracy of retrieved information, user satisfaction scores, and the reduction in time spent searching for information.‍How does the process of embedding model transformation improve the efficiency and accuracy of LLM-based knowledge retrieval systems?‍Embedding model transformation improves efficiency and accuracy by converting text into vector formats that are easier for LLMs to process, leading to faster and more relevant results from queries.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/building-llm-applications-for-knowledge-retrieval
169,"Detecting Unexpected Drift in Time Series Features
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringDetecting Unexpected Drift in Time Series FeaturesBy:Akash KhannaJune 13, 2023Aside from crystal balls, time series models have become the predominant approach to predicting the future. Often implemented at organizations with access to large amounts of historical data, these models leverage time dependent patterns and trends in input features (like economic indicators, weather patterns, even heartbeats) in order to forecast into the future. Time series models have use cases spanning nearly every data type and industry, yet a great deal of uncertainty still remains when productionalizing and particularly when monitoring these models. The Challenges of Monitoring Time Series ModelsValidating time series models on historical data is standard practice prior to putting them into production. However, monitoring time series models once they are in production becomes tricky because forecasting horizons may be far into the future—meaning that the ground truth that traditional validation and performance metrics depend on may take a prohibitively long time to become available, exposing organizations to the risks associated with underperforming models in the meantime. Many data scientists have addressed this limitation by using data drift metrics on input features as a leading indicator of model performance and a key piece of information in determining if/when a model should be retrained. However, time series models oftentimes contain features that naturally drift over time and display seasonality, meaning that data drift is to be expected and potentially no longer a principled justification for retraining. In the remainder of this article, I will discuss an approach that would allow one to continue to use data drift metrics as an informative leading indicator of model performance by first accounting for the expected drift of input features and using traditional drift metrics to track the residuals of those time dependent features.  To get a better intuition for this approach, let’s squeeze in an ice cream break. The ice cream shop near my house is open year round. It is situated right across the street from one of the city’s largest playgrounds and is a go-to for parents needing to make good on bribes they’ve offered their children. The line for this shop can sometimes wrap around the block. This past summer, I started to fancy myself somewhat of a sweet toothed soothsayer; before even leaving my house I could tell how long the wait would be based on the weather that day. At 90 degrees Fahrenheit, a single scoop of pistachio would take upwards of an hour, while at 70 degrees, the place was practically deserted. My dessert-line wait-time model was working just fine, until temperatures started to decline. On a late winter’s day this past year, my model failed me. A meager lunch of leftovers and an unseasonably warm (70 degree) day had me craving a banana split, so I dusted off my old model and predicted that I could be back home with plenty of time to spare before my mail carrier arrived with my eagerly awaited copy of Designing Machine Learning Systems. Turns out a 70 degree day in winter is not at all the same as a 70 degree day in summer and I ended up spending close to 40 minutes in line, missing my book delivery. I had fallen victim to a case of mistaken stationarity. Stationarity in Time Series DataDealing with time series problems more pressing than my quest for a banana split, lickety split, usually warrants using a machine learning algorithm. Many common approaches (known as Autoregressive models), create models based on the fact that quantities close in time are often similar (i.e. yesterday’s wait time is likely pretty similar to today’s). More recently, neural network based architectures, which were designed to handle long sequences of data (like natural language), have emerged as a popular approach for time series forecasting. These approaches all involve a series of preprocessing steps, some of which we can use to establish a notion of expected drift in order to isolate the informative signal that is unexpected drift.  Data cleaning and feature selection/extraction are so commonplace in the preprocessing pipeline at this point that they will not be discussed here. Rather, special attention will be devoted to a potential thorn in the side of anyone attempting to glean insights from time series data: the removal of nonstationarities. Stationarity refers to the tendency of data to have a constant mean, variance, and covariance. In the context of time series features, these constants equate to data that essentially does not depend on when it was observed. The nonstationarities of most features of interest can usually be broken down into components which are either trends or seasonal in nature. These seasonal cycles could be yearly, quarterly, daily, or even lengths of time which may seem arbitrary at first glance. Fortunately, there are a number of open source approaches to identifying and decomposing those cyclical time dependent components of a model's features, so that fluctuations in the remaining signal can be disentangled from the expected drift. Approaches to Addressing Nonstationarities in DataOne tool that has proven particularly useful and flexible in addressing nonstationarities is the open-source python package Darts. Darts describes its primary goal as “simplifying the whole time series machine learning experience.” The darts package includes functionality which helps detect and extract nonstationarities in data. Users can feed in their raw time dependent features and retrieve a transformed feature, one that is time independent and takes into account where (or really when) a value occurs in time such that values separated in time can still be meaningfully evaluated with standard metrics—for example, the metrics we referred to at the start of this guide, those used to measure data drift for the purposes of evaluating, or reevaluating, your model’s performance. There are other common approaches to forcing stationarity on time series features. Many of these approaches fall under the umbrella of differencing: essentially tracking the difference between consecutive observations rather than the observations themselves. In practice, the definition of consecutive here could mean consecutive days, weeks, quarters, years, or essentially any difference as long as it is consistently applied. Tracking these differences often accounts for the trends and seasonal tendencies of features in such a way to allow for the remaining quantities to be informative signals which can be tracked for drift.  It is worth noting that many modern approaches (neural network based architectures) can natively handle non-stationary data. Though the removal of nonstationarities may not be a critical part of the preprocessing pipeline for these models, it may still be worth creating stationary versions of those features, particularly under circumstances where forecasting horizons are distant and unexpected trends in input features can serve as a leading indicator of performance.Returning to our original motivation, the key idea here is that by accounting for expected drift in our time dependent features, we can be sensitive to unexpected drift and use it to guide decisions about evaluating, or reevaluating our model. Fundamentally, this approach is the difference between a short line for a rootbeer float on a temperate day in summer and a long line for that same float on an unseasonably temperate day in winter.‍FAQWhat are the specific performance metrics used in AI and ML to assess time series models before and after accounting for expected drift?In the context of AI and machine learning, specific performance metrics commonly used to assess time series models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), as well as more sophisticated metrics like Mean Absolute Percentage Error (MAPE) and Symmetric Mean Absolute Percentage Error (sMAPE). These metrics are crucial for evaluating the accuracy of forecasts by comparing the predicted values generated by the model to the actual values observed in the time series data. Both before and after accounting for expected drift, these metrics help in quantifying the effectiveness of the model, guiding data scientists in LLM (Large Language Models) and ML in refining and improving their forecasting algorithms by highlighting discrepancies between predicted and observed outcomes.In the realm of ML and AI, how does one quantify the level of acceptable drift before deciding to retrain a time series model?Quantifying the level of acceptable drift in the realm of machine learning and artificial intelligence before deciding to retrain a time series model involves setting precise threshold values for data drift metrics. This process is influenced by historical model performance, industry benchmarks, or the specific objectives of the ML forecasting task. A sensitivity analysis can be particularly useful in understanding the effects of various levels of drift on the predictive performance of the model, a common practice in AI. Once these thresholds are established, exceeding them indicates that the model's predictions are losing accuracy, suggesting a need for retraining. This approach, crucial in the lifecycle of ML models, helps balance the costs associated with model updates against the potential risks of using outdated predictions.What are the limitations or challenges associated with using the Darts package for addressing nonstationarities in time series data within ML and AI frameworks?The Darts package, while a valuable tool in the ML and AI toolkit for addressing nonstationarities in time series data, presents certain limitations such as computational demands, especially when dealing with extensive datasets or intricate time series. The complexity might deter users new to Python or those unfamiliar with advanced time series analysis techniques common in AI research. While Darts offers a range of functionalities for decomposing and modeling time series data, its applicability might not extend to all types of nonstationary data or might not always be the most efficient approach, particularly for datasets characterized by high irregularity or noise. Additionally, integrating Darts within broader ML and AI pipelines can present challenges, especially when those pipelines employ different programming languages or are structured in unique computational environments.‍‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/detecting-unexpected-drift-in-time-series-features
170,"Model Schemas Within the MLOps Ecosystem
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringModel Schemas Within the MLOps EcosystemBy:Sarah OstermeierJune 6, 2023MLOps and Continuous TrainingWhen embarking on an individual data science project, documenting, standardizing, and tracking may not seem like a top priority. However, as data science teams expand and numerous teams emerge within an organization, along with the development of centralized MLOps systems, the importance of establishing standards and tracking systems becomes evident. These measures allow data science teams to work efficiently and avoid costly mistakes.A well-designed MLOps system should track all the information and components needed to retrain a model from scratch such that it is approximately identical to the original model (same training data, model algorithm, hyperparameters, etc.). Most of this is done through model registries, feature stores, CI/CD and CT (continuous training) tools such as Dagshub and other versioning systems. By employing these systems, model artifacts, data, and code versions can be tracked effectively, facilitating continuous retraining and, if necessary, emergency rollbacks of production models.Within the model pipeline, two crucial components must be carefully maintained and tracked across iterations to facilitate this seamless orchestration.1. Model: What is the current model deployed in production and what conditions are needed to retrain it or pass inference data into it?To answer these questions, we need to keep track of several key elements:The saved model artifact, which serves as the core representation of the trained model.The predict function, which handles the model’s predictions.The software and environment requirements necessary for running the training and prediction processes.The code employed to train the model, encapsulating the very essence of its development.2. Data: What data was utilized for training and validation?This aspect necessitates monitoring two data subsets:The training data, which molds the model’s understanding and helps it glean patterns and insights.The test and validation data, which enables the evaluation of the model’s performance and generalization abilities.However, in order to know how to bring these pieces together, more information is needed: the model schema.‍What is a model schema?A model schema describes the relationship between a dataset and the model.A model schema is much like a database schema in that it outlines the structure and relevant metadata of a dataset. In the case of a model schema, this outline describes the relationship between the dataset and a model. It includes information such as which data columns are used as direct inputs to the model, how model inputs and outputs are structured, and what bounds on data values are expected.   The ultimate goal of a model schema is to allow a user to load and reconstruct a dataset as it was used during the initial training and validation phase. This in turn enables easier model retraining and rollbacks, data validation, and any further analysis or model validation that is needed. Data scientists can reload and explore their datasets and model outputs without the need to refer back to training code. Meanwhile, MLOps administrators can define robust model schema standards and establish automated systems for data and model monitoring and validation, streamlining the overall process.In the upcoming sections, we will delve into the intricate details of what essential information should be encompassed within a model schema. Additionally, I will provide a concrete example that illustrates how a model schema may be structured. Furthermore, we will explore how the adoption of model schema can yield standardization benefits within an organization and bolster the overall MLOps system. Throughout this blog post, we will employ the example of a tabular classification model to illustrate the concepts at hand. Rest assured, these principles extend beyond this specific scenario and are applicable to a wide range of model types and data domains.What information should be captured in the model schema?Exactly what information is needed in a model schema will depend on the model type, use case, size of the organization, and how the model schema is intended to be used. In general, best practice is to standardize model schemas across an organization to facilitate automation. The recommendations here should be treated as a starting point. My recommendation for a tabular model is given here:1. Data ColumnsHow are the data columns used and how does each column relate to the model? Model inputs: Actual input values to the modelModel outputs: Predicted probabilities, logits, and/or predicted classNon-input data: Any data that is relevant to the model but is not used as model input such as id columns, timestamps, or features that contain cohort data that is of interest such as race or gender  for bias trackingModel target: The ground truth column(s) that the model is predictingFor each column type, consider what information would be needed for a colleague to reload the data and evaluate the model without access to the code used to train the model. For model inputs, non-input data, and target columns, include basic information for each column:Column nameColumn index: Where the column is located in the DataFrame it is saved inData type: What the expected datatype for the column isCategorical or continuous: This will tell future users how the column should be treated during analysisCategories (if applicable): What are the expected categories if the column is categorical?Value bounds: What are the expected bounds if the column is numerical?Cohort/segment column (if applicable): Whether or not the column denotes data cohorts or segments of interest—used to compare model performance across groupsSample ID or timestamp column: Whether the column is a sample ID or timestamp column (his may only applicable in some cases)For model output columns, it is useful to include both raw model outputs and the final prediction made by the model. In addition to the fields outlined above, columns with raw model outputs may require a field to map raw outputs to the final model output. If the model is a multi-label model, it is also important to note which prediction task the column corresponds to.2. Data AccessDetails about how to access the data and how it is structured. The specifics of what is needed here may vary depending on how data is managed within the organization. Some examples of useful content are given below:Data files: For each data file, give the filepath, which components of the data are included (e.g. [‘model inputs’, ‘model target’]), expected column names, the join key to integrated data with other sources, and any other information need to load the data file.Data splits: If test or validation data is included in the data files, provide details about training vs test indexes. If training and validation data is saved in separate files, describe where each set can be located.  Inference data: If applicable, provide details on where inference data is to be stored and how it is formatted if different from training data.‍Example: ACSTravelTimeAs an example, I will go through a model I trained on a modified version of the folktables dataset ACSTravelTime. The objective of the model is to predict whether an individual has a commute of more than 20 minutes, and indicated by the binary target “JWMNP.” The DataThe dataset contains the following features:SERIALNO: person serial numberWAGP:  Wages or salary income past 12 monthsAGEP: Age of the householderSCHL: Educational attainmentMAR: Marital statusSEX: SexDIS: Disability recodeMIG: Mobility status (lived here 1 year ago)RELP: RelationshipRAC1P: Recoded detailed race codePUMA: Public use microdata area codeCIT: Citizenship statusOCCP: Occupation recodeJWTR: Means of transportation to workPOWPUMA: Place of work PUMA POVPIP: Income-to-poverty ratio recodeThe dataset is derived from the American Community Survey Public Use Microdata Sample (PUMS) and full documentation can be found here.The features in this dataset were not all used as input to the model. SERIALNO was used only as an identifier and join key during data manipulation. RAC1P and SEX were not used as model input, but were important information to track nonetheless to evaluate model fairness concerns. The features also include a combination of numerical and categorical data.  I used data from the years 2014 and 2015 as training data and reserved data from 2016 to test my model. Prior to training my model, I defined each of the relevant groups of columns in my code. I also defined data bounds for numerical features, and which columns were to be used to track specific cohorts of interest within the dataset.This makes it easy to separate input and non-input data without losing track of non-input attributes.The ModelI trained a Gradient Boosting Classifier and being satisfied with the results, I saved the pickled model.While this is usually the end of the story in a data science blog post, the next few steps are key to maintaining and improving upon a model in a real-world production environment.Data PersistenceWhile we already have the data saved somewhere (in this case, in the U.S. census database), it is important to keep track of the exact data used to train and validate a model. This especially comes into play when the time comes to retrain a model or roll it back to a previous version. It may also be required for regulatory purposes.  For my ACSTravelTime model, I reorganized all of my input and non-input data for both my train and test sets into one DataFrame, keeping track of the indexes for each dataset.Model outputs on both training and test datasets are also important to maintain, as this facilitates comparisons of multiple versions of a model over time. I saved raw prediction probabilities and the final prediction for my ACSTravelTime model and combined training and test data into one DataFrame. I saved both DataFrames as .csv files.Model Schema ArchitectureI designed a model schema for this model with the objective of making it easy to reload the dataset, define data validation standards, and evaluate and monitor the model’s performance. The schema is first structured as a python dictionary, and then saved as a .yaml file, which can easily be reloaded along with the dataset.As described above, I divided my model schema into 2 sections. In the data_access section, I provided all the information needed to load the dataset and run basic validation using just the model schema file.In the data_columns section, I provided more granular details for each column, which would allow for full data validation, model evaluation, and model monitoring. For each input and non-input column, there are details about data bounds and categories, the data type of the column, and whether the column should be used to track cohort performance.  The model_outputs section details which columns correspond to the final prediction and the predicted probability for each class.  Using my model schema file, I am able to reload my data from scratch and understand how my training data was structured, what data bounds I should expect, and which columns were of interest when tracking data subsets. Along with other versioning tools, I could use this to help me retrain a new version of my model, roll back to a previous version, and perform monitoring and evaluation on my model. The complete notebook used in this example is available here.The full model schema file is available here.‍The Value of Model SchemasAt first glance, model schemas might appear superfluous, possibly dismissed as an afterthought by an individual data scientist. However, within the organizational landscape, they are a vital part of maintaining consistency, efficiency, and automation in an MLOps system.They provide an outline of exactly how data is used in a model, while also establishing a framework for comprehensive evaluation and continuous monitoring. Model schemas can fit easily into an MLOps system as a model metadata artifact, and be saved within model registries or other versioning systems. This level of standardization allows data science and MLOps tools to be automated and speeds up the process of retraining, updating, and monitoring models. It also reduces the reliance on institutional knowledge, facilitating hand-offs between team members.With a shared understanding encapsulated within the model schemas, data scientists and MLOps practitioners can unlock the true potential of automation, empowering tools and processes to operate seamlessly. This not only streamlines the workflow but also reduces the risk of human error, ushering in a new era of efficiency and collaboration within the organization’s data science ecosystem.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/model-schemas-within-the-mlops-ecosystem
171,"Downstream Fairness: A New Way to Mitigate Bias
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessDownstream Fairness: A New Way to Mitigate BiasBy:Daniel NissaniMay 17, 2023Note: This blog post and code could not have been done without the fantastic research from our former research fellow Kweku Kwegyir-Aggrey and former machine learning researcher Jessica Dai. You can view their paper here.MotivationMany organizations utilize binary classifiers for a variety of reasons, such as helping loan providers decide who should get a loan, predicting whether or not something is spam, or providing evidence on whether or not something is fraudulent. These use cases require specific classification thresholds. Imagine an algorithm is predicting whether or not someone qualifies for a loan. One way to do this is to attribute a probability to a person, and if that probability is above a certain threshold (let’s say 0.5), then they can get a loan. If not, then they will be rejected.What is the proper threshold to use in these scenarios? Taking spam detection as an example, the threshold set will determine how often an email is classified as spam. A threshold of 0.8 is less permissive than a threshold of 0.4. That is why many organizations have threshold ranges for their algorithms, which can complicate things.Current bias mitigation techniques, such as the one we offer at Arthur, traditionally require you to change your classification threshold to meet some fairness definition. This change in threshold could be outside the range that your company allows, creating questions as to whether or not you can be fair. Further complicating these situations are models that are utilized in many downstream applications, where different threshold ranges (and possibly different fairness definitions) need to be utilized.Downstream fairness solves this dilemma. It’s an algorithm that achieves various fairness definitions (equalized odds, equal opportunity, and demographic parity) in a threshold-agnostic way, meaning that a company won’t have to adjust their threshold. Instead it operates on a binary classifier’s output probabilities to achieve a fairness definition. And this is all done with minimal accuracy loss! For the remainder of this blog post, we’ll be digging deeper into this algorithm and how to use our new open source code.Downstream FairnessSaving the mathematical details for the Geometric Repair paper, we will discuss the essence of how Downstream Fairness works and provide code snippets from our open source package. First off, downstream fairness is a post-processing algorithm that operates on the training dataset (or some representative dataset) for the model we are trying to make fair. The data needs to contain some key information: the prediction probabilities for each data point, the classification label, and a column containing the sensitive attribute on which you are operating.
Prediction Probabilities
Prediction Labels
Group Information
How the algorithm works is that it looks at the distribution of prediction probabilities per group of our original model and then computes a repair of each of those distributions for demographic parity. The reason this works for demographic parity is because the definition of demographic parity (equalizing selection rates for each group) only requires prediction probabilities and group information.On the implementation side, this process produces an adjustment table. The adjustment table contains how much the prediction probabilities need to be adjusted to achieve demographic parity, for each group. Below is an example of how that table looks:The pred_orig column is a granular representation of possible prediction probabilities a binary classifier could give—in this case, the granularity is set to hundredths. The adjusted_scores_x are the adjustments for group x. If the fairness definition you want to use is demographic parity, we are done! On new inferences, find the prediction probability for that inference and the group that the inference comes from, and then add the adjustment value to the inference probability:
new_prediction_probability = old_prediction_probability + appropriate_adjustment
If you want to use a different definition, such as equalized odds or equal opportunity, then we need to do one more thing. As we stated before, our goal is to repair the original prediction probability distribution so that it achieves demographic parity. In the paper, we call this a full adjustment, meaning that the adjustment table produced is a full adjustment of our model. For other fairness definitions, such as equal opportunity or equalized odds, we need to find a dampening term that adjusts the adjustments, so that we can achieve those fairness definitions.
In order to do this, we first need to find which group in the sensitive attribute column is experiencing the most disparity according to whatever fairness definition you want to choose. Once we identify that group, we are going to go through an optimization process to find a λ (what we call it in the code and how it is talked about in the paper), which will work as our dampening term, so we can satisfy other fairness definitions. Once we obtain this λ, we can apply it in the following way with our adjustment table:
new_prediction_probability = old_prediction_probability + lambda*appropriate_adjustment
Luckily, this is all automated with our codebase! Here is an example of how to do this:And, unlike some other bias mitigation approaches, downstream fairness is a pareto optimal algorithm. Meaning that it will achieve these fairness definitions with the minimum amount of accuracy loss.Of course, there are some limitations. The dataset used to train downstream fairness must contain prediction probabilities for each class for each group, and there should be a good amount of examples for each class for each group. But if that is provided, the algorithm should work as expected.ConclusionWe went through some of the algorithmic and implementation details of downstream fairness. If you want to explore more of the mathematical details, please go read the paper. Us at Arthur would love for you all to try out our work! Feel free to pip install our package and kick the tires a bit. As you find failure cases or think of new features, feel free to send your feedback to me at daniel.nissani@arthur.ai. Even better, please submit PRs or Issues on our open source GitHub repo. The GitHub repo provides a demo notebook, where you can try out all of our functionality we described in this post.‍FAQ‍1. What are the specific mathematical principles behind the Downstream Fairness algorithm?The Downstream Fairness algorithm is grounded in statistical and probability theory, particularly focusing on the concept of distribution repair for ensuring fairness across different groups. The mathematical foundation involves adjusting the distribution of prediction probabilities for each group to align with fairness criteria such as demographic parity, equalized odds, or equal opportunity. This involves a process known as ""Geometric Repair,"" which essentially recalibrates the output probabilities of a predictive model so that the resultant probabilities do not disproportionately favor or disadvantage any particular group based on sensitive attributes. The algorithm employs optimization techniques to find the best possible adjustments that achieve fairness while minimizing accuracy loss. This is achieved by constructing an adjustment table that represents how much the prediction probabilities need to be shifted for each group to meet the desired fairness standard.2. How does Downstream Fairness compare to other bias mitigation techniques in terms of performance and implementation complexity?Downstream Fairness differs from other bias mitigation techniques primarily in its post-processing approach, focusing on adjusting model outputs rather than altering the training process or the data. Compared to methods like reweighing, which modifies the weight of instances in the training data, or adversarial debiasing, which involves training a model to predict the target while another model predicts the sensitive attribute to reduce bias, Downstream Fairness is implemented after a model has been trained, thereby not affecting the original training pipeline. This can make it easier to integrate into existing workflows without needing to retrain models. In terms of performance, Downstream Fairness aims to be pareto optimal, meaning it seeks to achieve the best possible trade-off between fairness and model accuracy. This contrasts with some methods that might significantly reduce a model's performance to achieve fairness criteria. However, the actual performance and complexity can vary based on the specific scenario and the extent of bias in the original model.3. Can Downstream Fairness be applied to non-binary classifiers and multi-class scenarios?The concept of Downstream Fairness as described in the blog post primarily addresses binary classification problems. However, the underlying principles can be adapted for non-binary or multi-class classification scenarios with some modifications. In multi-class scenarios, fairness typically involves ensuring that the predictive performance is balanced across different groups for all classes, not just two. This could involve extending the adjustment table to cover all possible class predictions and ensuring that the adjustments lead to fair outcomes across all classes and groups. However, this adaptation can increase the complexity, as it requires considering inter-class fairness in addition to intra-group fairness. The implementation for multi-class scenarios would need to calculate separate adjustments for each class and group combination, possibly leading to a more complex optimization problem. While the original Downstream Fairness algorithm may not directly apply, the principles of adjusting prediction probabilities and achieving demographic parity can still be extended to these more complex scenarios with appropriate modifications.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/downstream-fairness-a-new-way-to-mitigate-bias
172,"Announcing Arthur Shield: The First Firewall for LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsAnnouncing Arthur Shield: The First Firewall for LLMsBy:Arthur TeamMay 4, 2023At Arthur, we are on a mission to make AI better for everyone—and right now, nowhere is that mission more important than the deployment of large language models.Companies across industries have begun to rapidly integrate LLMs into their operations following recent advancements from organizations like OpenAI, Google, Meta, and others. However, businesses don’t currently have a way to ensure fast and safe deployment of these applications, which has led to data leaks and toxic outputs that have been costly in more ways than one.That’s why, today, we are launching a powerful addition to our suite of AI monitoring tools: Arthur Shield, the first firewall for large language models. Arthur Shield enables companies to deploy LLM applications like ChatGPT faster and more safely, helping to identify and resolve issues before they become costly business problems—or worse, result in harm to their customers.Simply put, Arthur Shield acts as a firewall to protect organizations against the most serious risks and safety issues with deployed LLMs. Use cases can include:PII or sensitive data leakage: Arthur Shield allows companies to use the power of an LLM trained or fine-tuned on their full data set while having the peace of mind from knowing that other users of that same LLM are blocked from retrieving sensitive data from the training set.Toxic, offensive, or problematic language generation: Arthur Shield allows companies to block LLM responses that are not value-aligned with their organization.Hallucinations: Some LLMs confidently output incorrect facts. Arthur Shield detects these likely incorrect responses and prevents them from being returned to a user where they can do significant harm if they are actioned upon.Malicious prompts by users: Arthur Shield detects and stops malicious user prompts, including attempts to get the model to generate a response that would not reflect well on the business, efforts to get the model to return sensitive training data, or attempts to bypass safety controls.Prompt injection: It is becoming common for LLM applications to augment their prompts through retrieval from third-party websites and databases of pre-trained document embeddings. Those sources are not secure and can contain malicious prompts that are injected into the LLM system, causing significant risk of unauthorized response generation and data leakage.“LLMs are one of the most disruptive technologies since the advent of the Internet. Arthur has created the tools needed to deploy this technology more quickly and securely, so companies can stay ahead of their competitors without exposing their businesses or their customers to unnecessary risk.”– Adam Wenchel, Co-Founder & CEOArthur Shield in the LLM LifecycleArthur Shield and its capabilities are currently being rolled out in beta to select Arthur customers. Read more in our official press release or get in touch to request a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/announcing-arthur-shield-the-first-firewall-for-llms
173,"How to Think About Production Performance of Generative Text
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsHow to Think About Production Performance of Generative TextBy:Rowan CheungApril 25, 2023IntroductionPerformance monitoring has always been at the heart of Arthur’s mission and offering. We know that teams do not put models into production for no reason. Yet time and again it seems that every “state of ML” report released still lists that ML teams struggle to communicate performance to their external stakeholders. Our mission has always been to help teams create workflows and toolkits that not only enable teams to use reporting to build better models, but also empower them to communicate with external stakeholders.Nowhere is that mission more important in 2023 than in generative AI. Organizations are terrified to fall behind and are scrambling to implement LLMs into their processes. However, it’s important for teams to think critically about how they’re planning to evaluate and communicate these findings to external stakeholders.In this blog post, we’re going to cover some of the core challenges teams run into when they try to evaluate generative text outputs. Then, we’ll give a brief overview of some metrics that are commonly discussed by research communities and explain why we don’t see them working in production environments. Finally, at the end, you’ll find our suggestions for how to evaluate and monitor generative text to provide actionable outcomes.Core Challenges of Evaluating Generated Text1. There isn’t one ground truth output.‍In open text generation, the output of the model is unstructured text, so there is no ground truth label in the same way we might think of evaluating a traditional classification or regression task. One solution for this gap is to ask humans to accomplish the same task asked of a generative text model, and use the human-generated text as ground truth or the “correct” solution to the task. This is time-consuming and infeasible to scale to the needs of continuously evaluating a production application.Even given infinite resources, there is no clear definition of what the best piece of text is.
Consider the following natural language queries and their corresponding responses from ChatGPT. They overlap in some of the details mentioned in the response (e.g. the various relevant terms like forces and planets), but by slightly altering the prompt we elicit a response which changes the words and the concepts used in the writing. An evaluation procedure for a generative text model should properly take into account that the quality of the response will depend on its intended context, use case, and audience. For example, this type of language model that answers science questions should strike a balance between the simplicity of its answers and the thoroughness of its answers based on the user at inference time, so evaluation metrics should flexibly control for this contextual shift.2. We lack consistent automated metrics for comparing two pieces of text.‍Ideally, we’d like an automated score that could help us compare the above model outputs, or select between models that produced this set of inferences. While many metrics have been proposed, it remains a difficult question to select which metric, and associated hyperparameters best suit a specific use case. Different metrics have varying behavior on underlying qualities of text like tone, relevance, truthfulness, and also coherence, grammar, and lack of repetition. In the next section, we’ll outline some options for automated metrics.What Automated Metrics Exist?When we have a reference text, there are some metrics we can explore for comparing the model generated text to the reference text. In this section, we’ll discuss the benefits and limitations of some commonly used metrics.N-gram Precision Metrics: BLEU and ROUGEThe first class of metrics directly compares the overlap of tokens in the golden reference text to the tokens in the model generated text. A popular example of these metrics is the BLEU score. BLEU was originally developed for machine translation and compares the number of n-grams in the machine generated text that also exist in the reference text.$$ BLEU = min(1, (exp(1-\frac{len(reference)}{len(generated)})) \prod_{i=1}precision_i $$The first term encourages the reference text and the generated text to be of similar length. If the length of the reference text is equal to the length of the generated text, the first term will equal 1 and the value of the metric will be determined by the second term, the precision.To calculate the precision term, BLEU measures the fraction of i-grams in the generated text that are also contained in the reference text for i=1 to 4.BLEU and its variants are very brittle:They don’t capture semantic information. Swapping the word “happy” for “joyous” and for “sad” are considered equally wrong.They don’t effectively measure grammar. Jumbling the order of the words in a piece of text can render the text nonsensical, while barely changing the BLEU score.Adding Context: BERTScoreThe next class of metrics attempts to add semantic meaning and contextual information to techniques like BLEU, by leveraging embeddings instead of raw tokens or words. Embeddings are learned representations that map words to a vector in high-dimensional space such that the vectors capture the meaning and relationship between words.In BERTScore, each token in the reference text and generated text are first embedded using the BERT language model. Using a language model like BERT provides embeddings that hold both semantic meaning as well as contextual information, because BERT can generate different representations of the same word depending on its surrounding sentence, or context. The BERTScore is then a combination of:A precision term: Of all the embeddings in the generated text, compute the mean of the maximum cosine similarity with any embedding in the reference text.A recall term: Of all the embeddings in the reference text, compute the mean of the maximum cosine similarity with any embedding in the generated text.While BERTScore addresses a core issue of incorporating semantic information into the metric, it has its own drawbacks:Introducing embeddings increases the computational complexity of the metric.When using embedding-based metrics, the choice of embedding can influence what aspect of the text (semantic meaning, tone, style) the metric is most optimized to measure.Correlating with a Human Text Distribution: MAUVEFor both BLEU and BERTScore, a single human reference text is compared to a single model generated text. The authors of MAUVE propose a metric that compares the distribution of model generated texts to the distribution of human generated texts, and show that this can better correlate with human judgments of models completing generative tasks.To compute MAUVE, samples are taken from a human distribution of text and from the model. A separate model (the authors use GPT-2) is then used to embed all the texts. Each underlying distribution is then approximated by clustering the texts and counting the number in each cluster. The final MAUVE score estimates the divergence between the human text distribution and the model text distribution.Downsides of MAUVE:MAUVE still requires a reference, human-generated text distribution.MAUVE scores can vary based on the hyperparameters chosen. Some best practices for using MAUVE are here.Monitor What MattersAll of the metrics described above require a reference text to compare model output to, which is often difficult to obtain in production settings. Exciting new metrics like USR explore training a model to score texts from a set of model texts and ground truth texts, such that no reference text is required at scoring time. Model-based metrics are also a promising direction towards metrics that allow users to adapt a metric for a given use case. But in production, “good” is whether the LLM is successful in the product it is deployed in.When deploying a language model in an application or other business setting, we can more directly measure performance by collecting implicit performance metrics from the LLM’s environment. There are two types of performance measures that could be relevant to track:
User Feedback: These signals capture information on how users are interacting with the outputs of an LLM.
Augmenting an information retrieval system with an LLM to reduce the time necessary to find desired information. A suitable performance metric could be the average time a user spends on a specific query.
Generating headlines for news articles with an LLM to drive reader engagement. A suitable performance metric could be the average number of clicks per headline.
Execution Success: These signals capture information on how the outputs of an LLM interact with the rest of the production system.
Augmenting an information retrieval system with an LLM to reduce the time necessary to find desired information. Some suitable performance metrics could be the average distance between the query and the documents returned, or the validity of the LLM compiled query.
Augmenting a chat system with an automated appointment booking system. A suitable performance metric could be the API error rate, when making requests to the booking software.
These metrics are just the starting point in a holistic monitoring solution for LLMs, and are not meant to encourage driving engagement or automation at the expense of fairness or cognitive engagement. For a deeper look at designing human-centered evaluation for LLMs, check out Teresa Datta’s recent blog.Future Research DirectionsHow can we develop metrics that are optimized for the context of the deployment?Given the above performance proxies for an LLM in production, we’d like to automate metrics to correlate with those criteria. These metrics can then be used during other phases of the model lifecycle where production signal is not available, such as model selection and validation.Can we use LLMs to generate feedback for other LLMs?There is exciting research exploring the possibility of using LLMs to grade, score, and monitor other LLMs. For example, in Self-Refine, the authors propose a framework in which an LLM iterates on a task utilizing the feedback provided by an LLM. At Arthur, we are exploring utilizing embeddings and LLMs for scoring LLM outputs, providing natural language descriptions of model performance, and benchmarking the strengths and limitations of using LLMs during evaluation.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/how-to-think-about-production-performance-of-generative-text
174,"What Does the ML Lifecycle Look Like for LLMs in Practice?
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsWhat Does the ML Lifecycle Look Like for LLMs in Practice?By:Haley MassaApril 18, 2023I’m not going to say I was a full-blown deep learning denier, but I was stubbornly holding out to avoid getting swept up in the LLM hype. While I find almost all ML research interesting, I am really enamored with ML systems research, dedicated to the practical implementation of ML models. And it seemed like generative AI was being touted around more as a fun toy to “see what it could do” than something feasible to put into production. As usual, the LinkedIn posts started to pour in. However, it wasn’t until recently, when those same posts began to shift from “Wow, look at this response” to “Wow, look at this application I built,” that I realized I could no longer keep my head in the sand. I realized that sooner than I was expecting, these applications will be a part of the ML systems work I am so interested in. So, I spent some time trying to find out what makes the development of these models different from typical ML models. What Does the Traditional ML Lifecycle Look Like? Over the years, there have been many different formats that researchers have used to describe the steps teams go through to build an ML model. While I will not detail these steps, please refer to the diagram below.We can see in the diagram two core pipelines that work to put ML models into production (and one version-controlled layer that helps to manage them). The first development pipeline is where data scientists and research teams work to develop the model. In the second pipeline, ML Engineers and developers work to take their model at the final stage of development and productionalize it into something that can be used to make real-world predictions. What Does an LLM Lifecycle Look Like?Although LLMs are trying to carve out their own phrase within MLOps (LLMOps), it’s important to remember that they’re still machine learning systems. This means that even if they use different tools or phrases, they still follow most of the same lifecycle and best practices. I’ve broken down the LLM lifecycle into three phases, seen in the drawing below. In the following sections, I will give a high-level overview of each phase and how it relates back to the traditional ML lifecycle. Foundation Models On the left, we can see the most famous aspect of these new LLM systems, the foundation models. Foundation models are large pre-trained language models capable of understanding and generating natural language text. This ability to perform general language tasks well enables these models to serve as the starting point for various NLP tasks, such as language translation, text summarization, and question-answering. Foundation models are the most significant shift away from the traditional ML lifecycle. API access to foundation models has made it easier for teams to leverage NLP in their operations, regardless of industry or size. They can implement just the API as their production model—skipping most of the ML lifecycle workflow—if they are just looking for general language task capabilities.This is a standard route we see organizations go down as a first iteration of putting an LLM into production. However, this is no longer remarkable, as any company can create an OpenAI account. Instead, users seek customized experiences based on the specific use case for the model they interact with. To do this, ML teams need to use many of their existing techniques for ML model development to fine-tune and improve their model based on its specific use case. Note: Many foundation models have a closed-source nature due to the lack of transparency and accessibility of the code and data used to train them. Verifying the model’s accuracy and biases is difficult without access to this code and data. This can lead to unintended consequences and perpetuate existing biases in the data. DevelopmentThe development phase is where ML practitioners build and improve upon these use case–specific ML models. As we can see from the diagrams above, it exists in both the traditional and LLM lifecycle. However, one key difference is that there are currently fewer development steps for LLMs. For example, teams do not need to select and test different model architectures. While development for LLM models will undoubtedly continue to advance, the workflow is currently streamlined into three main steps: Defining the Use CaseWhile not explicitly listed in the diagram above, the first step to building any worthwhile ML model for production, LLM or not, is to define and understand your use case. Teams will need to spend time with business and product stakeholders to understand the purpose of the model they are putting into production. Data Curation & Model Fine-TuningData science teams must curate and clean use case–specific data to build out use case–specific LLMs. This data will be used to train/fine-tune the foundation model’s language understanding to their task requirements. Cleaning and curating data is something that data science teams are used to, as it is a part of their traditional ML lifecycle. However, one benefit of using LLM models over traditional ML techniques is that they already have a solid understanding of language due to all the data they were trained on originally. By using data only to fine-tune on top of an existing large model, teams are not required to curate and clean as much data. Qualitative Validation with PromptsSimilar to all traditional ML models, these models must be tested and validated before they can interact with the world in production. In traditional ML model lifecycles, this is done with the help of well-established historical benchmarks and metrics. As we will cover in a future post, this is different with LLMs. Instead, teams must understand the use case enough to create realistic tests and adversarial prompts to evaluate the model. They then can use metrics built to quantify essential qualities of the text (such as tone or context) against example responses provided for each prompt. Additionally, teams may choose to qualitatively assess the model’s performance based on their understanding of human language and use cases.Application Schema The final block in our LLM lifecycle diagram is the application schema. Referring to how your LLM is implemented and interacted with in production, it is similar to the productionalization process that ML Engineers and developers go through when traditionally implementing ML models. In practice, this is often built with prompt orchestration, where multiple prompts are chained together. Before getting into application schemas where multiple prompts interact, let’s look at what this would look like for an application with a single user prompt—like a chatbot.  The process of constructing a single prompt is more complicated in production than just taking in the user’s input. One concept of LLMs that is different from traditional ML models is that beyond fine-tuning the model, prompt engineers are able to “fine-tune” users’ inputs to the application at the time of inference with a prompt template. Taken from LangChain Prompt Template article mentioned later in this post.Prompt TemplatePrompt engineers write a prompt template as an additional step of model “fine-tuning.” Traditionally, we think of fine-tuning as adding parametric knowledge to the model. This is the knowledge that a model learns at training time and is stored within the model weights (or parameters). On the other hand, prompt templates work as source knowledge added to the model. This is knowledge added during inference via inputs to the model. They provide additional information on top of users’ input requests during inference. This information typically includes additional background information, context, and rules for model responses. For a deeper dive into prompt templates, check out this article. Prompt Orchestration Prompt orchestration refers to the chaining of prompts together interactively. Some applications, like chatbots, may be simple enough application-wise to work with one core prompt. However, in production, many LLM use cases are actually much more complex. This article does a great job introducing LangChain—a popular prompt orchestrator—by using the metaphor of baking a cake. You can ask a chatbot to provide you with the ingredients for the cake, but that’s not actually very useful for your end goal of a finished cake. Instead, models need to be able to use that prompt in conjunction with other prompts and actions to get the end result they are looking for. We can also see below for a more “real world” example provided by the article for pulling information from generated SQL queries. For those interested, you can also see a real-world example utilizing Arthur written by one of our ML Engineers, who built a chatbot to interact with our documentation. Taken from LangChain Prompt Orchestration article mentioned above.Note: In traditional ML models, there has been a push in the community to recognize data pipelines as the logical unit of ML work, and not the model itself. One thing that I find interesting about LLM models in production comparatively is that it seems there will be a push to view prompt orchestration as the logical unit of LLM work. This would make the chaining together of prompts into practice more similar to that of data engineering orchestrating ETL pipelines. Prompt engineering and application designers will need to spend more time and effort defining how these flowcharts will look and how outputs will be validated and monitored.  User FeedbackMeasuring the performance of generative models in production can be an even greater challenge than the already mentioned challenge of validating the model during development. Teams must navigate practical constraints, such as the infeasibility of scaling human labelers to generate common metrics. One approach that has proven successful for teams is tracking user feedback. This feedback provides valuable insights into how well their model is performing, enabling teams to continuously fine-tune and improve model performance. The specific techniques used will depend on the nature of the feedback and desired outcomes for the specific model. Conclusion & A Look AheadIn conclusion, although LLMs are getting their own fancy suite of new tools and job titles, they are still rooted in the best practices and techniques that the ML community has been using for years. Foundation Models: serve as a jumpstart for teams to develop strong baseline NLP modelsDevelopment: still the same need to fine-tune and evaluate models for specific end-task, even if there are new techniques and job titles Application Schema: process for putting LLMs into production that still needs to be validated and monitored, even if it is reliant on new tools/prompts LLMs are most definitely finding their way into production systems near you—and fast. Hopefully, this was an informative first look into how they fit into the frameworks that teams already use for their traditional ML approaches. We’re busy at Arthur helping folks build with LLMs, so stay tuned for more related content soon.‍--------------------------------------------------------------------------------‍FAQ‍How do LLMs compare with traditional ML models in terms of computational resources and environmental impact?Large Language Models (LLMs) generally require significantly more computational resources for training compared to traditional machine learning models. This is due to their vast number of parameters, extensive datasets, and the complexity of the tasks they perform. For instance, models like GPT-3 have billions of parameters and require substantial amounts of data and processing power to train effectively. This increased computational requirement translates to higher energy consumption and, consequently, a larger environmental impact. The carbon footprint associated with training and operating LLMs is a concern, as it contributes to greenhouse gas emissions. In contrast, traditional ML models, which might focus on more constrained tasks and possess fewer parameters, typically require less computational power, leading to lower energy usage and a smaller environmental footprint. However, efforts are being made to make LLMs more energy-efficient and to reduce their environmental impact through methods such as more efficient hardware, better model design, and by fine-tuning pre-trained models instead of training new ones from scratch.What are the specific challenges in ensuring the ethical use and bias mitigation in LLMs compared to traditional models?The ethical use and bias mitigation in LLMs present unique challenges primarily due to the scale and nature of the data they are trained on. LLMs are trained on vast datasets sourced from the internet, which can contain biased, incorrect, or harmful information. These biases can be amplified and perpetuated by the models, leading to ethical concerns, especially when the models are used in sensitive or impactful contexts. The sheer volume of data makes it difficult to fully audit and clean, resulting in challenges in identifying and mitigating all sources of bias. Additionally, because LLMs generate human-like text, there is a risk of them producing harmful or misleading information that appears credible. This is less of a concern with traditional ML models, which typically perform more narrowly defined tasks and therefore have a more controlled and limited scope for bias introduction and propagation. Addressing these challenges requires ongoing efforts in data curation, model transparency, and the development of robust evaluation frameworks to detect and mitigate biases.How can businesses measure the return on investment (ROI) when implementing LLMs into their operations?Measuring the return on investment (ROI) for businesses implementing LLMs involves assessing both the tangible and intangible benefits against the costs associated with these systems. Tangible benefits can include increased efficiency, reduced operational costs, and enhanced customer satisfaction, which can be measured through metrics such as time saved, reduction in customer service expenses, and improvements in sales or customer retention rates. Intangible benefits might include improved brand reputation, customer experience, and innovation. Costs to consider include not only the direct expenses related to developing, training, and maintaining the LLMs but also indirect costs such as training staff to use the technology and potential risks associated with model biases or errors. Businesses can assess ROI by setting clear objectives before implementation, monitoring performance metrics closely, and adjusting the use of LLMs to align with strategic goals. Regularly reviewing these metrics against the initial investment and operational costs helps in understanding the value LLMs bring to the organization.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/what-does-the-ml-lifecycle-look-like-for-llms-in-practice
175,"2023 Updates to the OWASP API Security Top 10
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct Features2023 Updates to the OWASP API Security Top 10The Open Worldwide Application Security Project (OWASP) is a non-profit community dedicated to improving software security.By:Nori TatsumiApril 12, 2023The Open Worldwide Application Security Project (OWASP) is a non-profit community dedicated to improving software security. Its API Security Top 10 project documents the most common API threats for best practices when creating or assessing APIs. In 2019, the OWASP Foundation released the first version of the API Security Top 10. This year, they’re publishing the next iteration of the list that’s updated for 2023. The 2023 release candidate of the updated list is now available and open to the community for contributions and feedback.As Arthur’s MLOps observability platform is built with an API-first development approach, the OWASP API Security Top 10 is one of the many best practices we incorporate into our software development culture. We are happy to see that the investments we’ve been making in our security features and practices are matching the latest security trends identified by the OWASP community.In the 2023 version of OWASP API Security Top 10, authorization is identified as the #1 biggest challenge of API security. Last year, Arthur did a complete overhaul of our platform’s authorization mechanism that introduced the capability for granular and customizable RBAC. The new RBAC system enforces strict authorization policies, allowing our customers to implement Segregation of Duty models that are tailored to their enterprise.Authentication remains on the 2023 list as the #2 threat. Arthur’s RBAC is backed by the authentication mechanism that’s built based on our Zero Trust principle. It can also adapt your enterprise standards by integrating with your Identity Provider (IdP) to achieve federated identity and single sign-on (SSO). In 2022, Arthur expanded the IdP integration capability by introducing support for OpenID Connect (OIDC) in addition to Security Assertion Markup Language (SAML) protocol. What’s new on the 2023 list is the automated threats from bots and bot-nets. With more sensitive data and business logic exposed via APIs today, they’ve become more profitable. And with the combination of AI and affordable cloud services, bots are more sophisticated, scalable, and dangerous. Earlier this year, Arthur did an assessment of our platform to identify and implement layers of additional protections to mitigate the risk from automated attacks. Various security and resiliency work has been done through this effort both at the application level as well as the infrastructure level for our SaaS environment.The work of reducing API threats is not a one-time effort. Arthur’s security features mentioned in this blog are continuously evaluated and improved. The OWASP API Security project creates a standard awareness for API security that should be applied to your software development continuously. At Arthur, we have a framework to exercise education on security practices, threat modeling, secure design, penetration testing, code analysis, and other security related activities through the Software Development Lifecycle (SDLC), the Continuous Integration and Continuous Delivery (CICD), and the System and Organization Controls (SOC 2). In this framework, projects like the OWASP API Security are helpful for us to identify what security work we should prioritize. We are grateful to OWASP and other organizations alike for helping the MLOps community build safe innovations.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/2023-updates-to-the-owasp-api-security-top-10
176,"Ask Arthur, Episode 1: Introduction
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsAsk Arthur, Episode 1: IntroductionBy:Max CembalestMarch 27, 2023This is a multi-episode series of blog posts discussing one of Arthur’s internal use cases for language models. It is our intention for this post to be general enough to be adapted with minimal effort in your company/lab.A new generation of applications is emerging. These applications use Large Language Models (LLMs) via APIs from OpenAI and Cohere to accomplish tasks via a “natural language interface” (NLI). With an NLI, a question or procedure is posed in ordinary written language, and an LLM or a series of LLMs carries out steps to respond. NLIs are not new—Google Search is a familiar example of an NLI that has been around for years. However, many use cases and toolkits are now emerging to use LLMs to wrap more common business tasks in an NLI. At Arthur, we were looking for a project to explore emerging use cases and toolkits for LLMs and NLIs, which brought us to LangChain. This toolkit is a fast-growing open source library for building LLM-powered applications with much of the prompting, routing, and other intermediate steps handled by the components of the LangChain library itself.In addition, we felt that enabling easy sharing, interactivity, and feedback collection would be important for us to study the human experience of using LLMs, which brought us to Gradio. This easy-to-use library, acquired by HuggingFace, enables the creation of user interfaces to interact directly with machine learning models. If you make a Gradio app for your project, you can get a shareable link that will allow you to send your custom UI to people for testing and feedback collection—without them having to have Python or any code on their computer.In this first episode of Ask Arthur, we are going to walk through the construction of our prototype for Ask Arthur using only LangChain, Gradio, and native Python packages—no other machine learning libraries like PyTorch are needed to directly use when writing our code! In future episodes, we will dive into the many design choices to consider when creating an LLM-powered application: evaluating hyperparameter choices for embeddings, testing new LLM prompts, balancing performance with cost when choosing between different APIs, and more. This is only the beginning!LangChain Tutorial“Ask Arthur” is a chatbot for answering questions and citing sources from documentation. We can define all the components and steps of the chatbot with LangChain.The flow of the application displayed in the image above is inspired by this LangChain blog post about their chatbot design for their own documentation. The chatbot processes the user’s question in the context of the chat so far, finds a relevant page from the Arthur docs, and then uses the docs page to construct a response. In order to find a relevant page from the documentation, we need to have preprocessed the docs into embeddings, which LangChain supports with document parsers and integrations with databases and VectorStores to save/load/search embeddings.An LLM from OpenAI’s API (wrapped by LangChain) is called twice when a user types in a new message into the chat, each with its own prompt and purpose. The user will only ever directly interact with the output of the second call to an LLM, the “Chat response generator,” which creates the written response that gets output into the chat box. This chat response will typically be a summary/rephrasing of information contained on a docs page. In contrast, the first call to an LLM will not output text to a user. Instead, it synthesizes the chat history with the user’s most recent message and the chat history to generate an intermediate question which is used to a) find the most relevant chunk of text from the docs, and b) provide the context for the chat response generator.Prompt TemplatesThe prompt you want for the LLM will change depending on the exact task at hand. In this particular case, the prompt we use to get an LLM to synthesize new messages with the chat history is different from the prompt we use to construct a written response for the user.Here we define the two prompts we use with a PromptTemplate. The first is the default provided by LangChain, and the second is one we have customized with a relevant example.Preprocessing Docs into Embeddings
We preprocess the docs into a vectorstore of embeddings with this function ingest_docs, which we only need to run once. Each chunk of text (1,000 characters long with some slight overlap between successive chunks) will get its own embedding. These embeddings are stored in a vectorstore, which we can load at inference time when it is time to find relevant docs pages.
Define LangChain AgentNext, we define a function that takes an API key and returns our LangChain agent (which will make it easy to integrate this function into our Gradio interface). We temporarily store the API key as an environment variable, which the LangChain uses to connect to the OpenAI endpoint. We use the ChatVectorDBChain, which takes as input our docs vectorstore, the chat synthesis LLM, the chat response LLM, callbacks to handle streaming text, and the parameter to enable returning source documentation.To chat with an agent returned by this function, pass in the user’s input and the chat history into the agent’s input dict: agent({""question"":""Type in a Q"", ""chat_history"":history})[""answer""]Gradio TutorialFor testing our chatbot, we create an interactive shareable user interface with Gradio.Once a user registers their API key and enters messages into the chat, the LangChain agent will reply with a written message and its source from the documentation. The LLM-generated response to the user’s question is in a chat window on the left, and the source page from the documentation is displayed on the right, along with a URL to that same page on the Arthur website. Additionally, in the bottom left, we provide a set of options for a user to give feedback.UI Components and LayoutGradio layout is defined by organizing components into rows and columns. In our left column are markdown components for the title and instructions, a chat window, a Send button, example user inputs, and a feedback button. In our right column is a textbox to enter the user’s API key, a Register API Key button, and markdown components for the source doc URL and markdown text.
Each button first has to be defined in its position in the layout as described above, and needs to have its functionality connected to a method that is called when the user presses the button. The Register API key button will call the get_langchain_agent() function, the Send button will call the chat function, and the Submit Feedback button will call the log_inference function.
Here is the function to launch the Gradio demo, which you can modify to include your own title, instructions, and example inputs. The first section places the components in their proper place in the layout, and the second section attaches functionality to components. Below this function, we define the helper functions it calls.Streaming the Agent OutputThis is the function that calls our LangChain agent with a new message from the user (and chat history). We get both the chat response and source document name from agent result. We then convert the source document name into a valid URL to our docs and its corresponding markdown text with a helper function. We then yield the chat response (as well as the source doc and link) as a generator for streaming text.Helper function for parsing our LangChain agent’s output into its source text and corresponding URL:Save Inferences and FeedbackThis format may change in the future, but for now we simply record two columns: the chat history (string) and the feedback (integer). We parse the chat history into a single string of alternating input<>, output<>, input<>, output<>, etc.For now, we save this data to a local file, with one feedback submission getting entered one at a time as a new row in a CSV. In a future episode, we will integrate it with Arthur for proper model monitoring!Sharing the Gradio Demo
A script to launch a Gradio demo needs a launch function that creates a demo object, lays out each item in the UI, assigns functionality to each item, and ends with demo.launch(). When you want the demo to be shareable via a URL, you only pass in an extra parameter to the end of the launch function: demo.launch(share=True)
Now when we run python launch_ask_arthur_gradio.py --share=True from the command line, we can share the demo to users with a link generated by Gradio to the server where they are hosting our application and the files it depends on. This has allowed us to easily send the demo around to our team at Arthur for them to try out the model and see for themselves what it can and cannot do!
Episode 2 and Beyond...Want to give Ask Arthur a whirl? Check it out here.‍In the next Ask Arthur episode, we will dive deep into evaluating different options for our LLM prompts—stay tuned and keep looking out for changes in this quickly evolving space!PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/ask-arthur-episode-1-introduction
177,"The Thinking We Haven’t Done on LLMs
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLarge Language ModelsThe Thinking We Haven’t Done on LLMsBy:Teresa DattaMarch 14, 2023You probably haven’t heard of human-centered evaluation of LLMs, and that needs to change. Human-centered work seeks to understand how real humans interact with technology, so that we can understand how humans (with all of their cognitive biases and quirks) interact with LLMs, and how these models affect individual human decision-making. This work was accepted to the CHI 2023 workshop on Generative AI and HCI. To read our full paper, please visit here.What are LLMs? In the past year, Large Language Models (LLMs) have exploded in popularity—from research, to industry, to public awareness and accessibility. For instance, ChatGPT set historic records for its customer growth, with over 100 million users in its first two months. These models predict next tokens (character, word, or string) based on past context, generating free-form text for a variety of tasks in almost any specified style. This means that people can repeatedly integrate LLMs into their daily lives—to decide what to eat for breakfast, to write the responses to emails left unanswered from yesterday, to develop the sales pitch they have to present mid-morning, to generate a funny joke during a break from work, etc. A variety of concerning issues of LLMs have already been identified, such as biased, toxic, or hallucinated outputs, but these largely only reflect distributional or instance-wise properties of their outputs. The potential ubiquity of this tool means that we need to consider how humans will actually interact with and use this new technology, while also acknowledging that we are all prone to cognitive biases and other quirks. This area of research is referred to as human-centered evaluation, and it has not yet been thoroughly explored for LLMs. Human-centered evaluation is, however, already popular in the Explainable AI (XAI) community.What is Explainable AI? Defining explainability for ML models is a subject of ongoing discussion. For the purposes of our discussion, we will focus on the most common type of model transparency seen in industry: post-hoc explanations of black box models. These often rely on only a trained model’s inputs and outputs to identify patterns in how models make decisions. These methods aim to unlock transparency in order to allow stakeholders to understand the decision-making process of models to improve trust and mitigate downstream harms. Arthur’s explainability features offer a variety of explanation options including counterfactual explanations (understanding how a model’s predictions would have changed given a hypothetical, what-if scenario) and popular methods such as LIME and SHAP. There are a variety of quantitative metrics for evaluating XAI—these are mostly scores that calculate certain ideal properties of explanations (for more information, see this piece), but perhaps more importantly, there are a variety of qualitative evaluation considerations. These are important for two main reasons.What makes Explainable AI (and LLMs) unique?Qualitative evaluation is important for XAI and LLMs because they are distinct from the classic ML paradigm for two key reasons:1. There is no ground truth.There is no exactly correct explanation for a black box system. LLMs are inherently open-ended systems that don’t have a ground truth output for each input.2. In practice, XAI and LLM outputs are often actually meant for some downstream decision or task.Practitioners often use XAI as an assistive tool for model debugging, generating hypotheses, and ensuring compliance. LLM outputs are often a tool to help you decide—what email to send to your client, what quick summary of an important document you will read, what answer is provided for a pertinent question, etc. This means that the context of XAI and LLM use involves some human using an explainer/LLM as a piece of evidence to make some decision. Thus, we need to consider how practitioners use, receive, and comprehend outputted AI. Especially because humans are susceptible to cognitive biases when processing information and making decisions.What does qualitative, human-centered evaluation look like in practice?There are three areas of focus to consider.1. Understanding Users’ Mental ModelsA user’s mental model, a term coined by Don Norman, of a technology is their internal understanding of how a technology works. For instance, maybe your mental model of a crosswalk is that pushing the crosswalk button will cause the walk signal to appear more quickly. However, for many cities, that button does not actually do anything. This is an example of a user's mental model not aligning with a technology's true model. People rely heavily on their mental models of technology to decide when to use the technology, to evaluate how much to trust the outputted results, and to make sense of any results (see, e.g. Cabrera et al. 2023, He & Gadiraju 2022, Kaur et al. 2022). These personal mental models are formulated from a user's perceptions and interactions with the technology and how they believe the system works. While ML practitioners may have had access to specialized training on how LLMs work, this is not the case for the vast majority of the general population. We cannot assume that everybody will have the same understanding of how a technology works as we do. To our knowledge, no work has explored the mental models the general public holds for LLMs. How a general user believes an LLM to work may be very different from how it actually works, and this mismatch can be dangerous. It is not difficult to imagine frightening scenarios where users anthropomorphize or deify an LLM chatbot, understanding it to be a ""magical'' source of ground truth. This could very quickly lead to conspiracy theories and the legitimization of disinformation campaigns. It is important to consider if this is an issue of messaging and education—informing the public via AI literacy—or of regulators—to implement policies that force the algorithm providers to provide accurate, comprehensible warning labels on the limitations of their technology.2. Evaluating Use Case UtilityAs previously discussed, XAI and LLMs are often tools for accomplishing some other goal. The term use case in the XAI literature refers to a specific usage scenario and its associated downstream task or end goal. It has been found in the XAI literature that although it might be easy to assume that an explanation will be helpful for a user accomplishing a task like model debugging or model understanding, this is not necessarily the case. When the performance of that downstream task is measured, the presence of explanations can sometimes have no effect, or can even have a negative effect on performance, especially if the XAI is faulty (see, e.g. Jacobs et al. 2021, Adebayo et al. 2020). Very limited work has explored the utility of LLMs in use case–specified user studies, but a user study on Microsoft/GitHub's Copilot, an LLM-based code generation tool, found that it “did not necessarily improve the task completion time or success rate.” Basically, we want to understand if the AI assistive tool is actually helpful for successfully accomplishing the end goal.3. Acknowledging Cognitive EngagementCognitive effort is a form of labor, and unsurprisingly, people tend to favor less demanding forms of cognition and other mental shortcuts. As an example, when asked to ""agree'' to a user agreement when signing up for a new platform, you are probably more likely to check the box than to cognitively engage with the language of the agreement. Unfortunately, this human tendency can lead to unintended or dangerous outcomes because humans are susceptible to a wide variety of cognitive biases. For XAI, this manifests as practitioners only superficially examining explanations instead of digging deeply, leading to over-trust, misuse, and a lack of accurate understanding of the outputs. This can be dangerous when it results in the over-confident deployment of a faulty or biased model. Issues of cognitive engagement should be held front and center for researchers of LLMs. Because of their massive scale and public accessibility, LLMs may quickly become ubiquitous in all aspects of daily life. Realistically, how much will users actually cognitively engage with the magnitude of generated outputs to ensure that they are correct and aligned with their intentions? Consider an LLM-generated email: how often and how deeply will a user review that generated email before sending it? What if it's not just one email, what if it's every email? Will they always catch when the generated output says something incorrect, or worse, inappropriate? Furthermore, our attention spans have decreased dramatically with the increase in digital stimulation. Another aspect of concern is that LLM outputs often sound very confident, even if what they are saying is completely false. When the user inquires about the incorrectness, they also have a documented tendency to argue that the user is wrong and that their response is correct. (In fact, some have called LLMs ""mansplaining as a service."") This can make it more difficult for humans to implement cognitive checks on LLM outputs. Why is this important?The scale of the reach of LLMs is massive, and so the consequences of not having a qualitative understanding of the utility of their outputs are grave. Beyond the environmental and computational costs of such models, there are social consequences that are entirely unknown from the offloading of our cognitive load onto these agents.We need to understand how users make decisions about whether to utilize the outputs of LLMs, the mental models that users have of these technologies, whether LLM outputs are actually helpful in downstream tasks, and how much users cognitively engage with the outputs to verify their correctness and lack of harm. It is dangerous to continue developing and making available larger and larger language models without a proper understanding of how humans will (or will not) cognitively engage with their outputs. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/the-thinking-we-havent-done-on-llms
178,"Announcing Our Strategic Partnership with Amazon Web Services
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesAnnouncing Our Strategic Partnership with Amazon Web ServicesBy:Arthur TeamMarch 7, 2023AI is the top priority for companies in 2023. The innovators are becoming truly AI-native and will lead their industries for decades. Performance, accuracy, and observability are the core ingredients for success. At Arthur, our mission is making AI better for everyone, from the companies running models to drive their business to the consumers interacting with the applications they power.Today, we are excited to announce a comprehensive strategic partnership with Amazon Web Services. This partnership will allow us to bring our class-leading model monitoring to even more teams building transformative AI systems on AWS. AWS customers worldwide are now able to purchase and deploy our platform directly from the Marketplace, and will receive access to the full Arthur offering, including:A centralized dashboard for all models, no matter how or where they are deployedExclusive capabilities in CV, NLP, bias mitigation, and other critical areasThe ability to monitor, measure, and optimize AI performance at scale across accuracy, data drift, explainability, and fairnessUnique, actionable insights that provide visibility into your model performanceHighly scalable microservices architecture that can ingest up to 1M transactions per secondPurchasing Arthur through the Marketplace significantly streamlines procurement and billing, providing a seamless purchasing experience. Users looking to access the Arthur platform can use their existing AWS accounts and cloud credits to buy directly or work with our team to create a custom plan.‍Visit our AWS Marketplace listing here to get started. If you are not an AWS customer, request a demo or contact our team to learn more about Arthur.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/announcing-our-strategic-partnership-with-amazon-web-services
179,"Reflections on SaTML 2023: We Should Be More Cautious
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationReflections on SaTML 2023: We Should Be More CautiousBy:Daniel NissaniFebruary 28, 2023Earlier this month, our research team attended the inaugural IEEE Conference on Secure and Trustworthy Machine Learning (or SaTML for short), where we presented our paper, Tensions Between the Proxies of Human Values in AI. This is a first-of-its-kind conference, focusing on traditional responsible AI topics like fairness and explainability, while also welcoming work on robustness and adversarial machine learning. Many industry practitioners attended, such as those from Nvidia and Intuit, as well as more industry-focused academics, giving this conference a more practical feel without being dominated by Big Tech companies.In this blog post, we reflect on the major theme from SaTML: we need to be careful about how we deploy and utilize machine learning solutions. We’ll highlight talks that particularly resonated with us and made us think about how to be more cautious, but note that most of the talks and the air of conversation were around this idea. As a community, we need to be more careful about how we do our research, how we deploy models, and how we evaluate consequences.Patrick Altmeyer gave a compelling talk on his team’s paper, Endogenous Macrodynamics in Algorithmic Recourse. In their paper, Altmeyer et al. come up with a simulation, where a bank may use a counterfactual explanation method to perform some sort of algorithmic recourse on an algorithm that determines whether someone gets a loan or not. This would cause a domain shift—several individuals will go over the decision boundary and get loans. But Altmeyer et al. take this a step further. What if now you train that algorithm with those data points that shifted the boundary? Then you would get a model shift—the model’s decision boundary would move to accommodate more of the data points. Altmeyer illustrates in his talk that as this feedback loop continues, the model’s accuracy continues to decline. For the bank, however, this decline translates to risk of individual’s defaulting on loans. Thus, the question is, who should take on that risk?Altmeyer et al. offer a regularizer that takes into account the external cost (in this case, the uncertainty that would be developed) to better the algorithmic recourse, but I think this hits something deeper. As we continue to develop new explanation techniques, such as our FastCFE algorithm, should we be concerned with the feedback loops they may generate? This question is something that has been explored before in the fairness space, starting with Delayed Impact of Fair Machine Learning, which has shown similar results.We even discuss this in our Tensions paper, where we argue that the lack of contextual information combined with domain-agnostic definitions of our human values is inherently flawed. Without taking the context into consideration, we cannot appropriately evaluate the consequences of our deployments. This especially extends to the feedback loops generated, something most research does not take into account.Anna Kawakami presented her paper, led by Amanda Coston, SoK: A Validity Perspective on Evaluating the Justified Use of Data-driven Decision-making Algorithms. Coston et al. argue that the alignment of our decision making algorithms and the real world is one of the issues to watch, and in my opinion this is even a bigger issue. They even cite that many states in the U.S. have attempted to deploy machine learning algorithms to help, and decide to retire them soon after because they aren’t seeing the results they want. This is similar to research done by Virginia Eubanks, where well-meaning organizations have deployed models that actively harm the populations they are trying to serve. Recent research endeavors have even started exploring the mismatch between optimization procedures in classification models, primarily focused on predictive performance, and the actual decision making tasks they are deployed to do, arguing that this mismatch leads to illegitimate deployments that should have never happened.In my opinion, this is one of the root issues of feedback loops. If we don’t have proper proxies, understandings of those proxies, and the contexts they will be utilized in, how can we hope to create models that are useful? I use the term useful here intentionally because, as has been famously said many times, “all models are bad, but some are useful.” The same goes with proxy variables.I think the unasked question at SaTML, and honestly at most technology-oriented conferences, is: should we build these technologies? Timnit Gebru tackled this question directly during her keynote speech with a resounding no, pointing a finger directly towards those who think we should even try to achieve AGI. Because, as Gebru put it, even giving the perception that a form of AGI has been achieved can be harmful to us as humans. Although models like ChatGPT have achieved “human-like” performance, Jacob Steinhardt’s talk showed how aligning to what we think is human can be flawed, and those flaws are starting to show (thinking about you, romantic Bing).SaTML began with Zico Kolter doing a retrospective on the past 5 years of adversarial machine learning. His talk was both sobering and informative, but I want to focus on one aspect of it. Kolter presented an argument that he has heard many times: we need to redefine what we mean by robust machine learning. Instead of defining robustness as an ability to handle worst-case scenarios, we should define it as an ability to handle distribution shifts, generalizability, and other things that practitioners care about. But then he shows the proverbial picture of a pig that got classified as an airplane, and tries to convince the audience that because technology cannot do this very basic task of identifying an object, even with an adversarial attack, then we should try to make sure it can do this task. I want to push back on this. Do we really want technology to mimic humans this much? Is there positive use in technology being able to do, quite literally, everything that makes us human? Or should we restrict ourselves to technologies that serve specific, meaningful purposes that actually improve our lives and outcomes? My opinion is that if we want to create responsible AI, we should be intentional about our needs and build specific technologies around them.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/reflections-on-satml-2023-we-should-be-more-cautious
180,"CDAOs, Prove Your Value: The New Reality in 2023
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedBest PracticesCDAOs, Prove Your Value: The New Reality in 2023By: Adam WenchelFebruary 16, 2023At Arthur, we work with many of the largest and best-run companies in the world, and one thing has become very clear with the 2023 budget cycle: you need to show measurable results to justify spend right now or else risk your budget being flatlined or slashed.CFOs are tightening the belt. Getting any new spend, or growing your budget, takes a rock solid business case.This is extremely hard with frontier technologies, in particular AI. The last few years have seen AI prioritized as an investment area because the potential is massive.2023 is the year when CDAOs and AI teams will need to show they can transition from potential to realized gains.Building that clear business case means having ready answers to questions like:“How are my models currently performing?""“What kind of lift can I expect from updating a critical suite of models driving a high-leverage use case?”“What effect will it have on my business KPIs like P&L, churn, TLV, and ROI?” Building the ability to measure and evaluate AI performance into the foundation of your project portfolio makes it possible to understand the answer to these questions. Showing up to planning and budget conversations armed with these critical details will be a big advantage navigating the next couple of years and beyond. Ultimately, measurable results mean you can make strategic, rational, smart decisions and focus your resources on the initiatives that are driving the most value.It’s clear that the hype around AI is warranted—being able to empirically demonstrate that will be the game-changing trend for CDAOs in 2023.Learn more about Arthur's monitoring and measurement capabilities here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/cdaos-prove-your-value-the-new-reality-in-2023
181,"Keep the Lights On: Making Deployed AI/ML Better for Everyone
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringKeep the Lights On: Making Deployed AI/ML Better for EveryoneBy:John DickersonFebruary 14, 2023It’s easier than ever to build and deploy ML models. Storage is cheap, compute is cheap, pre-trained models are prevalent and, did I mention, cheap! As developers and practitioners, we’ve felt the pressure to deliver models into production to provide analytics for internal processes all the way up to running decision-making for mission-critical problems. Our job is, in a nutshell, to create a model that performs well right now, according to one or more downstream KPI-related metrics. That’s generally doable—we handle data identification, ETL pipelines (and their variants), model training, verification, and beyond. A model is trained that exceeds expectations and is deployed. But what happens after that?No matter how clean the input data, no matter how well-trained the model, it’s known that a model posted to production will degrade with respect to downstream metrics. Input distributions will shift (think COVID-19 impacting restaurant seating, hurricanes rolling through wedding destinations, or sudden demand spikes due to viral marketing). Furthermore, those dynamics, relative to downstream business metrics, may unduly impact particular subgroups due to latent legal, demographic, or political shifts. A “perfect” deployed model today is not a perfect model tomorrow, or in three months, or in a year. It’s important to keep the future in mind when deploying now, and to understand that model deployment is not the end of the model lifecycle. More bluntly: once the thing is properly built, we need to make sure it stays good.Even more bluntly: model monitoring—production monitoring—shouldn’t be walled off behind an enterprise sales team, held outside of the developer-first MLOps pipeline, but instead should be easily accessible by all practitioners. That’s something we at Arthur want to, and can, enable. Over the last few years, we’ve built the world’s strongest model monitoring solution, battle-tested by some of the world’s largest enterprise clients. We’re looking forward to spending more time focusing on the developer community, working closely with practitioners to learn and shape what the most effective model monitoring solution should be.Story Time: Making Sure Organs Go to the Right PlaceCareer data scientists, machine learning practitioners, machine learning scientists, statisticians, business analysts—with a wide variety of application areas, and a global scope for applications, we’re all being asked to build and deploy models. Arthur is a product built by engineers who needed a product like Arthur in their past jobs, and need a product like Arthur for their side projects. In my own career wearing many of those hats,[1] I’ve built and deployed models for bundled advertising campaign pricing, enablement of Indian election prediction markets, global blood donation recommendation systems, international drug interdiction allocation and efficacy estimation, television advertising allocation, and organ donation, to name a few. I’ll lean on that experience for a little “monitoring matters” wisdom below, after a quick story.In organ exchange, patients in need of an organ enter an organized barter market to find a willing, compatible donor. Organized kidney exchange has existed for two decades, and I’ve been heavily involved in that process for 13+ years, with large exchanges running code I wrote to match patients to donors, and organizational committees using my code to provide “what-if” analyses during policymaking decisions. Time and time again, it’s been made clear to me that deploying a computationally “optimal” approach to clearing these exchanges, then letting that code run day after day, is not sufficient. Value judgments are made, medical technology improves, supply increases or decreases, the legal landscape shifts—what worked well yesterday may not work well today. In short, we write code to solve a problem based on a model of the real world at a given point in time; that model is a noisy proxy for what actually matters, and what matters changes over time. In practice, an “optimal approach” is deployed, but:The model is uncertain. The inputs are noisy to begin with. Problems include missing variables, missing constraints, improperly set weights and costs, and beyond. Is a particular transplant center reliable? Does a particular social variable correlate with likelihood to donate?The model is brittle to shifts in the underlying environment. For example, during COVID, living organ donation rates dropped due to fear of entering a hospital and/or capacity constraints at transplant centers. How does that impact transplantation rates? And, if COVID impacted particular populations more than others, how does that impact metrics for fairness and bias in organ allocation? Any allocative model will disparately treat specific subpopulations, and measuring and monitoring for that is imperative for downstream policymaking.The model is poorly understood by stakeholders. Visualizing complex statistics is hard. But, we use machine learning models to address problems that are hard for humans to understand. So, it’s important to communicate results to end stakeholders (in the organ exchange case, doctors, patients, donors, lawyers, etc.). Those statistics change over time, as the world changes. Communicating that change in a comprehensible way matters.Certain demographics are systematically mistreated by the model. When we train models, we typically aim to maximize/minimize a specific objective function. That function may maximize utility or welfare for the many at the cost of utility or welfare for the few. This plays out in healthcare, including organ exchange, frequently; and, this can change drastically as the underlying political, legal, or demographic landscape shifts—regardless of the model that was trained and deployed.In my experience, these general concerns arise in most application areas, not just organ exchange. Pricing advertisements depends on underlying social trends as well as external demand for correlated inventory. Drug interdiction success rates correlate with weather as well as USCIS/CPB patrol policies. Worldwide blood donation efficacy correlates with national and WHO policy. I’m sure you can think of examples from your own past or present, too. That’s part of our motivation behind building Arthur—creating a scalable platform for solving general problems in model performance across industries.Monitoring Models with ArthurWe built Arthur to monitor models in production, and to aid in the model verification process. Our enterprise clients—across banking, healthcare, agriculture, logistics, news, and beyond—have all felt that pain of unobserved model deployment, directly via revenue loss or indirectly via damage to their brand. We are continuing to translate that technology—distribution and concept drift detection and connection to downstream KPIs, bias and fairness definition, detection, and mitigation, and model explainability across the board—to the individual developer and team of developers. Our platform already handles structured and unstructured data, and we have an exciting roadmap over the coming year expanding in our core strengths like computer vision and NLP connected to foundation models, robust approaches to measuring all of the metrics, and effortless scaling as our clients’ needs grow.We look forward to continuing to partner with the MLOps community! If you’re in Austin next week (Feb 21-23), come find us at the Data Science Salon Austin—we’re going to be sharing some exciting things we’ve been building to tackle these very issues.‍FOOTNOTES[1] All of these examples are independent from my time building Arthur! We encounter the same style of (broad, reaching) problem at Arthur, and I am happy to dive into those details over a coffee or beer. My goal with this post is to identify with the reader as an ML practitioner, not necessarily a C-suite executive.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/keep-the-lights-on-making-deployed-ai-ml-better-for-everyone
182,"Gartner Recognizes Arthur in 2023 Market Guide for AI Trust, Risk, and Security Management
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesGartner Recognizes Arthur in 2023 Market Guide for AI Trust, Risk, and Security Management (TRiSM)Gartner has included Arthur as a Representative Explainability/Model Monitoring Vendor in its AI Trust, Risk, and Security Management (TRiSM) Market. By:Arthur TeamJanuary 26, 2023In addition to having recognized Arthur as a Cool Vendor™ in AI Governance and Responsible AI as well as in its Hype Cycle™ Report for Data and Analytics Governance, Gartner has included Arthur as a Representative Explainability/Model Monitoring Vendor in its AI Trust, Risk, and Security Management (TRiSM) Market. ‍What Is AI TRiSM?AI TRiSM is a framework that ensures AI model governance, trustworthiness, fairness, reliability, security, and data protection. Its four main categories are: model explainability and model monitoring, privacy, ModelOps, and AI application security. Together, use of solutions from these four categories helps data and analytics leaders and their colleagues implement AI-specific trust, risk, and security management measures.Why Is AI TRiSM Important?In industries ranging from healthcare to financial services to retail and beyond, enterprises are leveraging AI in nearly all facets of their business. The breadth and complexity of AI use cases is only continuing to grow, which increases potential exposure and vulnerability if not implemented with a framework like this in place. Failure in these cases could mean millions in financial, legal, or reputational losses.“By 2026, organizations that operationalize AI transparency, trust, and security will see their AI models achieve a 50% improvement in terms of adoption, business goals, and user acceptance,” states the guide. “AI brings new trust, risk, and security management challenges that conventional controls do not address. Data and analytics leaders must use the capabilities described in this guide to improve model reliability, trustworthiness, fairness, privacy, and security.”How Arthur Aligns with AI TRiSMArthur helps enterprise teams monitor, measure, and optimize AI performance at scale across three key areas: accuracy, explainability, and fairness. This allows organizations to understand how their models are making decisions, ensure compliance in their ML systems, and continually improve the fairness of model outcomes by proactively monitoring for bias. Production AI and business value are inextricably linked in today’s world—and will continue to be in tomorrow’s. As the leading model monitoring and governance solution in the market, Arthur is deployed at leading Fortune 100 enterprises across industries to accelerate business transformation through AI and drive better, more responsible results. At Arthur, our mission is to make AI better for everyone, and we’re proud to be recognized for doing so.If you are a Gartner client, you can access the full market guide here. Learn more about Arthur and sign up for a demo here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/gartner-recognizes-arthur-in-2023-market-guide-for-ai-trust-risk-and-security-management-trism
183,"Arthur Achieves SOC 2® Type II Certification & Compliance
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur Achieves SOC 2® Type II Certification & ComplianceBy:Arthur TeamJanuary 18, 2023Today, we’re excited to announce that Arthur has completed our Systems and Organization Controls (SOC 2®) Type II certification. SOC 2 has long been regarded as the gold standard for SaaS security compliance and is a critical enterprise milestone in upholding the trust service principles of the American Institute of Certified Public Accountants (AICPA). As a model monitoring company, security is important to everything we do at Arthur. Our platform is constantly measuring, processing, and analyzing machine learning models to deliver better results for our customers—and working with customers in highly regulated industries only heightens the importance of security compliance. Arthur’s successful completion of both SOC 2 Type I and Type II audits in 2022 means that our company has consistently maintained infrastructure service processes that meet the required levels of oversight and monitoring so that we can proactively identify and address any unusual activity. Additionally, it demonstrates our commitment to meeting the industry’s most rigorous data security, availability, and confidentiality standards. “From day one, Arthur’s focus has been to develop a robust model monitoring solution that adheres to best-in-class security and data privacy controls, in order to deliver a trustworthy experience for our enterprise customers,” said Arthur’s VP of Engineering George Chitouras.As we continue to scale solutions in financial services, healthcare, insurance and more, worldwide trust in our infrastructure, software, people, data, and procedures is paramount for scaling model ops in the cloud. ‍Want to learn more about our commitment to security? Get in touch.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-achieves-soc-2-r-type-ii-certification-compliance
184,"Arthur Earns Placements on Built In’s 2023 Best Places to Work List
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur Earns Placements on Built In’s 2023 Best Places to Work ListBy:Arthur TeamJanuary 11, 2023Today, Built In announced that Arthur was honored in its Best Places to Work Awards for the second year in a row. Specifically, Arthur earned a place on New York City Best Startups to Work For, New York City Best Places to Work, and U.S. Best Startups to Work For. The annual awards program includes companies of all sizes, from startups to those in the enterprise, and honors both remote-first employers as well as companies in large tech markets across the country.“We’re thrilled to be recognized by Built In alongside the top companies in New York City and beyond,” says Adam Wenchel, co-founder and CEO of Arthur. “New York has grown into a world-class enterprise tech ecosystem over the past decade, and we’re proud to be part of the community and call it our home base. I’ve been so impressed by our team’s continued ability to integrate the sentiments at the core of our company mission—make AI work for everyone—into a welcoming and equally mission-driven company culture. We nearly doubled our team in 2022, and we can’t wait to welcome even more Arthurians in 2023!” At Arthur, we are fully committed to not only fulfilling and challenging work that truly makes a difference, but also providing the opportunity to do that work as part of a diverse, creative, and passionate team. We believe our competitive compensation, equitable hiring practices, and comprehensive benefits (including wellness programs, a home office stipend, tuition reimbursement, and more) truly make Arthur an incredible place to work, and it’s exciting to see our company continue to be recognized by Built In. Interested in joining us? Check out our open roles here.“It’s my honor to congratulate this year’s Best Places to Work winners,” says Sheridan Orr, Chief Marketing Officer, Built In. “These exemplary companies understand their people are their most valuable asset, and they’ve stepped up to meet the modern professional’s new expectations, including the desire to work for companies that deliver purpose, growth, and inclusion. These winners set the stage for a human-centered future of work, and we can’t wait to see that future unfold.”  ABOUT BUILT INBuilt In is creating the largest platform for technology professionals globally. Monthly, millions of the industry’s most in-demand professionals visit the site from across the world. They rely on our platform to stay ahead of tech trends and news, learn skills to accelerate their careers and find opportunities at companies whose values they share. Built In also serves 2,000 customers, innovative companies ranging from startups to those in the Fortune 500. By putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard-to-reach tech professionals.ABOUT BUILT IN’S BEST PLACES TO WORK Built In’s esteemed Best Places to Work Awards, now in its fifth year, honor companies across numerous categories: 100 Best Places to Work, 50 Best Startup Places to Work, 100 Best Midsize Places to Work, 100 Best Large Places to Work and Editor’s Choice: 100 Best Hybrid Places to Work. The program honors companies—remote, hybrid and in-office—with the best total rewards packages across the U.S. and in the following tech hubs: Atlanta, Austin, Boston, Chicago, Colorado, Dallas, Houston, Los Angeles, Miami, New York, San Diego, San Francisco, Seattle and Washington D.C. ABOUT ARTHURArthur is the AI performance company. The Arthur platform monitors, measures, and improves machine learning models to deliver better results, working with enterprise teams to accelerate model operations and optimize for accuracy, explainability, and fairness. Arthur's research-led approach to product development drives exclusive capabilities in enterprise scalability, computer vision, NLP, bias mitigation, and other critical areas. Contacts: press@arthur.aiPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-earns-placements-on-built-ins-2023-best-places-to-work-list
185,"Team Arthur at NeurIPS '22: A Retrospective
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationTeam Arthur at NeurIPS ‘22: A RetrospectiveBy:Arthur TeamDecember 16, 2022The Arthur team is back home in New York after a strong showing at the Conference on Neural Information Processing Systems (a.k.a. NeurIPS), arguably the largest and most influential machine learning conference. We hosted onsite and offsite socials, gave an oral presentation, presented two papers in the main conference as well as papers at two workshops, co-organized a workshop, recruited—and one of us helped compose a song on overfitting, sung live with a pair of ukulele-wielding ML practitioners. NeurIPS holds a special place in Arthur’s history. Back in 2019, we announced our $3.3M seed round at a collocated event to NeurIPS in Vancouver. Now, three years, 40+ team members in growth, and $50+ million dollars later, it was great to return to NeurIPS, this time in New Orleans. Big themes, many of which our team expands upon below, included large language models (LLMs) and their generalization and semi-rebranding as foundation models, cross-collaboration between AI and other fields (psychology, policy, etc.), human-in-the-loop and user-centric ML pipelines, and context-aware ML as it relates to privacy and fairness.NeurIPS is known for being both a venue for the dissemination of new industrial and academic research as well as a networking meetup with a strong event culture. Arthur, ever the responsible community member, also contributed to this latter focus by organizing a couple of well-attended events. On Wednesday night, joint with our partners at Index Ventures, we hosted an offsite happy hour at Jack Rose, with attendance from the investor, founder, big tech, and academic community. The VC and investing community continues to increase its presence at flagship ML conferences, and largely driven by the current excitement surrounding foundation models and generative ML (e.g., Stable Diffusion, ChatGPT), it was great to see this trend continue at NeurIPS. On Thursday night, joint with friends at Abacus.AI, we held an open mic night at the conference venue with a few hundred attendees. Meant to be a free-form community event full of cheeky machine-learning-oriented fun, this was a great success, with GPT-3 generated poetry and live song, debate about the merits of non-tabular data, discussions of who invented social networking, and the evergreen research topic of how to improve peer review (as if it’s broken!). We’re happy to help build a vibrant ML community.Arthur and our friends at Abacus.AI teamed up to host an open mic night!Partnering with Index Ventures, we also hosted an event at the picturesque restaurant Jack Rose.Below, members of our ML team give their takeaways and hot takes on what’s right, what’s wrong, what’s hot, and what’s not in the academic and industrial machine learning world.The Current State of ML Research (It’s Not Just LLMs)Arthur MLE Valentine d’Hauteville writes, “On my first day at NeurIPS, I recount the awe I felt walking amongst the myriad of different posters and research projects in the big hall of the New Orleans Convention Center. The mosaic of ideas, topics, and research stories displayed before me was impressive and a stark contrast to my usual research flow, which consists in exploring and (sometimes) getting a bit lost in the roots of a deep research paper reference tree. In this forest of posters, I felt immersed in the AI community, enthralled and slightly overwhelmed by the countless research minds and ideas present at my literal fingertips. As one of the biggest annual AI research conferences, NeurIPS in some way mirrors the collective interest and brain-space of the ML community’s, displaying both its prominent and upcoming narratives—for instance, as someone anticipated, I witnessed a large enthusiasm and contingent of work on generative models and LLMs.” Arthur MLE Max Cembalest echoes this, stating that “the biggest trend at the conference was an increased study of large language models, their robustness, their generalizability to out-of-domain text, and their generalizability to tasks that are not directly language but approachable by LLMs anyway.”But LLMs were far from the only interesting facet of ML research at NeurIPS. Valentine remembers feeling a “strong energy around the design, development, and nurturing of scientifically sound and usable ML practices, with research outputs spanning from theory to implementation.” “There was also much work on model efficiency—how to reduce the computational requirements for deep learning systems,” Arthur MLE Teresa Datta adds.“Beyond these ever-present areas of research,” says Teresa, “there were two main threads of messaging that struck a chord. The first: developing neural networks which don’t involve backpropagation. Geoffrey Hinton, inspired by the field of neuroscience’s lack of evidence that the brain’s cerebral cortex is able to undergo backpropagation, presented a keynote on a new learning procedure for neural networks that does not involve backpropagation. This forward-forward approach instead replaces a forward+backward pass with two forward passes—one with positive data and one with negative data, meant to imitate the brain’s paradigm of wakefulness and sleep cycles. This is the latest work in the continuing attempts to establish deep learning models as a brain analogue.”The second messaging thread Teresa resonated with was “more purposefully promoting cross-collaboration between AI and other fields (psychologists, policymakers, domain experts, everyday users, neuroscientists, designers, and more). The 2022 NeurIPS keynotes highlighted a variety of figures at the intersection of AI and other fields: Rediet Abebe on perpetuated societal inequalities, Juho Kim on designing interaction-centric AI, Alondra Nelson and her work in the White House Office of Science and Technology Policy, and David Chalmers on the philosophy of sentiency. While “collaboration with other fields” has always been evoked with high import, there were more discussions on how to formalize this: How do we craft incentives for researchers to actually do this difficult and novel work? Graduate students are often chained to publishing goals—getting a certain number of acceptances at high-profile venues. How do we create accolades, publishing forums, and funding support for interdisciplinary work?”Chief Scientist John Dickerson added that “the use of modern ML (e.g., transformer-based models) for “traditional” application areas in operations research such as logistics, planning, routing, assignment, scheduling, and resource allocation has also been increasingly present at ML conferences over the last year or two, and certainly at this recent NeurIPS. Until recently, these “old” application areas—that also happen to drive much of the world’s economy—were viewed as boring and solved by the machine learning world, left to the business analysts and consultants in the INFORMS professional community. Yet, with a touch of domain expertise, modern ML and optimization can be shown to eke out significant gains in efficiency and profit driven in these proven business problems where each percentage point corresponds to hundreds of millions or billions of dollars of economic value. I’m excited to see the continued strengthening of ties between the AI/ML and operations research communities and the problems they tackle (i.e., those with both a prediction and a decisioning element).” (Separately, joint with INFORMS, the ACM, and CCC, we’re co-organizers of a series of workshops in this space, e.g. [1] and [2]. Get in touch if you’d like to participate!) Distribution Shifts & BenchmarksValentine gave the first public presentation of her work, joint with Arthur Research Fellow Naveen Durvasula, on explainability and data drift at the Workshop on Distribution Shifts (DistShift). On the same day as AFCP (more on AFCP below), DistShift attracted a larger crowd—generalization, extrapolation, and robustness to distribution shift are core ML problems, and it’s great to see continuing progress in this fundamental area. It was cool to see Valentine’s work, which ties together clustering, Shapley values, and Skope rules to find emergent clusters of “drifty” points over time, as part of a larger cohort of explainability and data drift research. (We’ll be submitting a full version of this work to one of the January ‘23 conference deadlines, so stay tuned!) Valentine d'Hauteville presents her poster about Characterizing Anomalies via Explainable Classifiers.Also in the space of model performance under distribution shift, Arthur-MLE-turned-Berkeley-PhD-student Jessica Dai and Arthur Research Fellow Michelle Bao presented at the Women in Machine Learning (WiML) workshop on their ongoing work with our team understanding models’ the impact of covariate and concept drift on group fairness, when ground truth labels are not available at test time.Valentine was particularly impressed by Isabelle Guyon’s keynote, The Data-Centric Era: How ML is Becoming an Experimental Science. She writes, “Her talk reminded us that, as a scientific endeavor, ML research should abide by the same rigorous scientific research standards as those that govern research in other disciplines such as the natural sciences. Guyon debunked some bad scientific practices within the ML research community, such as a common one which consists in selecting validation datasets based on their anticipated or observed ability to display the behaviors that will confirm a hypothesis (a form of selection and confirmation bias). To combat such practices, Guyon emphasized the importance of adopting scientifically and statistically sound data curation, experimentation, and validation procedures. For instance, one should ensure that published experiments and findings are reproducible and carefully documented. She also advocated for the adoption of more rigorous vetting processes on existing datasets as well as an increased focus towards developing and documenting more comprehensive benchmarks. In fact, her prescriptions seemed to echo those of the NeurIPS community at large, as the conference recently created a new Datasets & Benchmarks research track which rewards datasets and benchmark papers on an equal footing with other traditional research content.”Human-in-the-Loop MLArthur MLE Daniel Nissani, while researching the Tensions paper (more detail on this paper below), became enamored with the idea that AI systems should somehow encapsulate knowledge about the context of their deployment. “I was happy to see that I wasn’t alone,” he writes. “At the HiLL workshop, Cynthia Rudin did an excellent job explaining how users of AI, those who don’t necessarily have AI skills, but want to benefit from AI systems, have opinions past “which models have the best accuracy.” Drawing from her research on Rashomon sets and sparse decision trees, she asked for a paradigm shift for how we generate models. Instead of asking a user to accept one, heavily optimized model, she wants us to present users with multiple models that achieve similar accuracy scores. She has developed a UI for such discovery processes when searching through different decision trees.”“While listening to Cynthia’s talk, a poster caught my eye on participatory systems, which presented an equally inventive idea,” Daniel continues. “This paper devises a model agnostic scheme that trains various models on different sets of features and protected attributes. This allows users to understand the effect of providing or omitting certain types of information, such as a medical status or gender. Both of these ideas present ways for AI systems to interact with the context of their deployment, allowing for feedback between the system and the user.”Valentine noticed the same theme. She writes, “At NeurIPS, user-centric ML pipelines were also in strong focus. For instance, a couple of presentations I attended intelligently incorporated user feedback as key steps in the design of comprehensive ML solutions. One researcher presented a new explainability pipeline for self-driving cars but had first run user field studies in order to understand the nature of what makes a good explanation for his specific use case. He also resorted to user feedback to comprehensively validate his first design iterations and presented ways to incorporate the feedback into his future designs. Cynthia Rudin, a prominent scholar in the field of explainability presented a clean mathematical proof showing that, contrary to the popular conception, designing complicated and high-capacity models is necessary to obtain peak performance; it is often possible to compute simple, inherently explainable yet suitably performant models for a given task. Her approach echoed Occam's razor (“the simplest solution is almost always the best”) and followed the keynote talk’s footsteps in anchoring ML back to core scientific principles. Rudin also created a clean and innovative UI which allows domain experts to explore and understand a set of generated simple models before selecting a one that is most suitable to their use case.”Fairness & Related Topics“I really enjoyed the Algorithmic Fairness through the Lens of Causality and Privacy (AFCP) workshop, a semi-annual gathering that focuses on the nuances of connecting responsible AI to practice, says Arthur’s Chief Scientist John Dickerson. “This is one of the few but growing communities in “core” machine learning that gives more than lip service to human-centered AI, contextual machine learning, and sociotechnical systems (STS).” Arthur MLE Teresa Datta presented her co-lead-author paper, Tensions Between the Proxies of Human Values in AI, as an Oral at AFCP. The paper was also accepted to HCAI at NeurIPS as well as SaTML. Check out Arthur MLE and co-lead author Daniel Nissani’s blog post on that work here, and Teresa’s talk below.Daniel found the talks on causality to be particularly interesting. “Causality has been in the fairness literature for quite a while now, but one of the biggest bottlenecks is making sure you have a causal model (distilled as a causal graph) that can be used for causal analysis,” he writes. “I was pleasantly surprised to see work directly in this space, where some authors ran experiments to see if causal discovery methods could actually create causal graphs that are effective enough to measure fairness notions. Although their results were promising, the authors plan to construct a causal discovery method specific to fairness notions. If successful, this could create otherwise theoretical works, such as another paper at AFCP describing post-treatment bias in causal fairness analyses, more impactful for real world systems.”“The final highlight for me,” he adds, “was a roundtable discussion at AFCP, where many researchers, whether from the privacy or fairness space, acknowledged the need for more contextual understanding in our research. Emphasis to start researching entire ML systems, eliciting user feedback, and integrating context into our research were the biggest takeaways. It made me feel proud that our team at Arthur presented the Tensions paper at AFCP, since it seems our ideas were not only heard, but preaching to an active choir that wants to start integrating context as well.”“The AFCP workshop gave critical takes on interpretability and explainability in ML, and also touched on the intersection and interactions between forms of privacy and fairness, as well as causality and fairness,” says John. “As in the AI/ML meets OR discussion above, we’re seeing the intersection between traditionally separate areas of focus—statistics, economics, human-computer interaction, machine learning, and others. As productionized ML continues to expand across the economy and our society, these intersections are inevitable and welcome, and I’m happy to see thoughtful workshops like AFCP continue to grow in lockstep.” Valentine reflected profoundly on this topic as well, noting that “it can often be tempting as ML researchers to think of ourselves as scientists working on objective and universal algorithms that can then be adapted and tailored to fit specific use cases. Such conceptions in some sense make us the principal bearer of truth to the detriment of domain experts, and can lead us astray. During an explainability panel I attended, Zach Lipton pointed out that we might for instance benefit from letting domain experts be the ones to first scope out desired design for an ML system and associated explainability mechanisms before resorting to ML Engineers to implement or iterate on it. Such responsibility delegation could ensure designs and solutions are inherently more usable and useful. Furthermore, one must remember that there is no such thing as scientific objectivity and that science and ML at large is value-laden—one of the key calls to action in Teresa’s Tensions paper and awesome presentation. Forgetting or ignoring this reality can lead us to resort to deeply insufficient solutions based on mathematical formulations in our attempt to address problems that are socio-technical in nature, such as fairness.”ConclusionWe arrived back in New York last week feeling full (of knowledge, but also of Cajun food and beignets). And, as is the case with many academic conferences, we were left with equal amounts of questions and answers. Here are just some of the questions we’re looking forward to exploring further in 2023:How can model interpretation and explanation be aligned with context, audience, and sensible baselines?How is the geometry of information informing model design and analysis?How do we create accolades, publishing forums, and funding support for interdisciplinary work?How can we engage communities via interactive AI/ML systems, so that we enable consent, choice, and trust?How do we start doing research about the system that models are deployed in, rather than just the model itself?Are we starting to approach the idea that approximate causal models are enough for real world causal analyses?‍Want to learn more about our MLEs and our commitment to research and development? Check out the R&D page of our website.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/team-arthur-at-neurips-22-a-retrospective
186,"How We Are Modeling Our Human Values in Technology Is Inherently Flawed  Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ResearchHow We Are Modeling Our Human Values in Technology Is Inherently FlawedBy:Daniel NissaniDecember 8, 2022Thank you to the original authors of Tensions Between the Proxies of Human Values in AI: Teresa Datta, Daniel Nissani, Max Cembalest, Akash Khanna, Haley Massa, and John Dickerson.IntroWhen creating automated machine learning systems, our belief is that these algorithms should not perpetuate and amplify harms. We want these algorithms to be fair to everyone, regardless of their gender, race, ethnicity, sexuality, etc. Our information should only be used and distributed with our consent. Inevitably, these algorithms will not work as expected, and we deserve an explanation as to why they are not working as they should.We can encapsulate these human values as pillars of privacy, fairness, and explainability. Over the past generation, a lot of work has focused on embedding these values into our technologies. However, as we try to proxy these pillars with technical definitions and algorithmic designs, tensions keep getting discovered within the pillars, between the pillars, and with the real world contexts in which the pillars’ proxies are deployed. So we must ask: Why do we continue to face limiting tensions?Tensions Within PillarsIt is well known in the AI community that popular definitions of fairness, such as demographic parity, equalized odds, and calibration, cannot be implemented in the same machine learning model [1]. Moreover, even when implemented correctly, these fairness definitions may cause more harm than good over time, given the feedback loop an algorithm might create [2].Privacy discussions on the other hand are largely dominated by the notion of differential privacy [3][4], a probabilistic guarantee that one cannot realize that a data point has been removed or replaced in a dataset based on some mechanism. However, differential privacy has been shown to work poorly on outliers [5][6] or on models that have overfit to their training data [7].Explainability has come into question now that black-box and hard-to-interpret models have gained popularity. However, the methods to produce explanations are generally just local approximations of models [8]. Some critics of the field say it is akin to reading tea leaves [9].Tensions Between PillarsA summary of the tensions identified within and between popular value proxies. Incorporating any one of these pillars is a challenge, and incorporating multiple requires handling competing priorities.Much like how popular notions of fairness cannot be implemented in the same model [1], differential privacy and any popular fairness definition cannot be imposed on the same model [10]. In other words, the impossibility of having different notions of fairness in machine learning extends to having any notion of fairness in tandem with differential privacy.The idea of models being more transparent may be in direct competition with the idea that our models are more private. Research has shown that popular explanation techniques, and even new ones such as counterfactual explanations, can make models susceptible to membership inference attacks [11].Explainability should be a useful tool for identifying unfairness, but is not, and can at times hide the unfairness of a model [12]. Moreover, the converse can happen as well, where explanations of a model can actually amplify unfairness against certain subpopulations [13].Tensions in the Real World: A Call for Context-Aware Machine LearningThe 2020 Census used differential privacy for the first time to meet the Privacy Impact Assessment’s requirements. By doing so, reported population counts fluctuated, which protected the privacy of individuals, but potentially lessened the funding small, rural populations would receive from the federal government. As an example, Native American reservations of less than 5,000 people saw decreases in population numbers by 34% on average. This type of error (which is an inherent feature in differential privacy) could result in the loss of funding for a road to a nearby town or a new school [14].In 2021, the Markup found that people of color were denied loans 40-80% more often than white counterparts with similar financial profiles [15]. However, they were heavily criticized by “[t]he American Bankers Association, The Mortgage Bankers Association, The Community Home Lenders Association, and The Credit Union National Association ...saying the public data is not complete enough to draw conclusions, but did not point to any flaws in our computations.” The “incompleteness” of the data is because the Home Mortgage Disclosure Act requires “debts as a percentage of income, how much of the property’s assessed worth the person is asking to borrow,” but not “the applicant’s credit score” because of fears of re-identification attacks.Both of these examples showcase how utilizing formulations of our human values without acknowledging the context can lead to dire consequences. In the case of the Census, although differential privacy offers a guarantee of a certain kind of privacy, it adversely affects certain populations that need privacy and more equitable funding. As a technical notion, differential privacy has no way of knowing the context it is being implemented in. And without any critical structures in place, it can run awry, causing consequences that we outlined above. Similarly, although not necessarily in an algorithmic sense, the notion of auditing the mortgage system requires transparency about the data being used to give out mortgages. Thus, decisions need to be made with contextual knowledge, so that specific entities can have access to the required information.In bioethics and related ethical fields that are more mature than responsible AI, context is incredibly important. Doctors have access to different information depending on their physical context, such as whether they are in a hospital or in their car on the way to the hospital. The ethical concerns around the collection of biometric information are affected by the specific device being used. Context informs how information flows, what information is collected and used, and why certain decisions are made. If we hope to have more ethical machine learning systems, the incorporation of context could be a viable avenue [16].How Can We Address These Issues?In order to alleviate the issues we’ve described, we believe that the whole system should be considered when designing machine learning solutions. The techniques (and laws) described primarily deal with the model: differential privacy inhibits extraction attacks on models, fairness definitions constrain models to output more fair predictions, and explainability techniques are used to explain model outputs. Rather than just designing solutions for the models, we can look at the entire system the model is deployed into and determine the values most appropriate to embed and consider their associated consequences.Although we don’t want to prescribe any solutions to these problems, because this is a nascent research area, there are some theories that we can look to for inspiration. Contextual integrity for privacy [17] is a way to encode the context of a situation, allowing us to understand the privacy requirements of technology. Substantive algorithmic fairness [18] asks us to analyze the structural inequalities present, identify the reforms that could mitigate such inequalities, and consider whether an algorithmic intervention could achieve such a mitigation. For explainability, designing around the needs of a situation and stakeholder’s understanding of transparency can lead to better techniques [19].As we said above, this is not a solved problem. If anything, the idea of incorporating context in automated machine learning systems is itself very new. Thus, we want to leave the reader with questions we are interested in researching:How should information be collected by a contextual system? Collection of data is a hot button issue, so we have to collect data with intent. Leaning on Contextual Integrity for Privacy, we can get inspiration for how we can define context as a set of parameters to collect data.What types of tools need to be developed? Building out frameworks, evaluation suites, and more will be helpful, but we should consider what we need to build to make these systems effective and ethical.How should machine learning systems respond to context? Splitting this up, we should consider what triggers a response to context and how the user should feel the response to context.What aspects of ethical responsibility does each stakeholder carry? The creator of the technology, the person implementing the technology, and the person making decisions with this technology each have a different ethical role to play.How can we design inclusively? We can lean on participatory design principles to help us build these systems for everyone impacted.ConclusionTechnology is inherently value-laden and political [20][21]. It can distribute information in specific ways, thus influencing how we make decisions. Moreover, although technology has the potential to help those in the most need, those who need the most are regularly ignored in the process of designing popular technologies.Arthur’s research team believes that context-aware systems, those able to incorporate knowledge about the specific domain that a machine learning model is situated in, are a potential path to solve some of the issues above, as well as evaluate the consequences of such a system before deployment. Context awareness is a hard problem because it most likely will involve collecting new information about a specific deployment at some point in the model building or productionalization process.‍Citation LinksBelow is a list of links to the citations in this blog post. Note that our paper goes much deeper and has a more proper citation tree.https://arxiv.org/abs/1609.05807https://arxiv.org/abs/1803.04383https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdfhttps://www.iacr.org/archive/eurocrypt2006/40040493/40040493.pdfhttps://arxiv.org/abs/1507.06763https://arxiv.org/abs/1910.13427https://arxiv.org/abs/1709.01604https://arxiv.org/abs/2206.01254https://docs.google.com/presentation/d/1bPUE2eD3NIYHYLm_D9njaVWgEgYXGpoEOjcnSaV7ccs/edit#slide=id.phttps://crcs.seas.harvard.edu/files/crcs/files/ai4sg-21_paper_23.pdfhttps://arxiv.org/abs/1907.00164https://arxiv.org/abs/2205.03295https://arxiv.org/pdf/2106.13346.pdfhttps://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.htmlhttps://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithmshttps://www.nature.com/articles/s41746-018-0075-8https://scholarlypublishingcollective.org/psup/information-policy/article/doi/10.5325/jinfopoli.1.2011.0149/314319/Privacy-in-Context-Technology-Policy-and-thehttps://arxiv.org/abs/2107.04642https://arxiv.org/abs/2101.09824https://web.cs.ucdavis.edu/~rogaway/papers/moral-fn.pdfhttps://arxiv.org/abs/1811.03435PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/how-we-are-modeling-our-human-values-in-technology-is-inherently-flawed
187,"How AI Is Reshaping the Future of These 4 Industries  Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Monitoring & PerformanceHow AI Is Reshaping the Future of These 4 IndustriesBy:Christina SirabellaNovember 14, 2022In nearly every industry throughout the last few years, the massive impacts of AI and machine learning are undeniable. Organizations are gaining competency in AI/ML and deploying it to achieve goals such as cost reduction, bias mitigation, and so much more.They are also recognizing that mastering AI is a make-or-break for companies that want to be around in the next decade—in fact, a whopping 94% of business leaders in a recent survey said that AI is critical to success. As our CEO Adam Wenchel spoke about at last year’s AI Summit, the enterprises that become AI-native first will be industry leaders for decades.In this blog, we’ll share four industries where machine learning has had a particularly large influence, as well as how the leading organizations within those industries are using it—and more specifically, model monitoring—to stay ahead of the curve.Financial ServicesEven in traditionally conservative spaces like financial services, AI is rapidly changing the business landscape. While there is no enforceable federal AI legislation in the U.S. currently, there is a growing momentum to regulate the financial services industry around biased algorithms and govern black box underwriting by various agencies.Financial services organizations are using AI and machine learning for activities like credit approvals, fraud detection, and customer support. But what happens when bias creeps into an algorithmic model which results in the wrong lending decision being made, impacting millions of customers? Companies must consider the potential business and compliance risks of these technologies.Leading financial institutions are using Arthur to monitor, measure, and improve machine learning models for better results across top industry use cases: fraud/KYC, forecasting models, fair lending, robo-advisory programming, credit worthiness, customer service, and more. Learn more by downloading our financial services whitepaper or watching our on-demand webinar.‍InsuranceNo matter how the economy evolves in the coming years, individuals and companies will continue to need insurance. The growing reality of company layoffs and downsized industries may shrink average coverage amounts, but consumers and businesses trust carriers to bring them peace of mind when faced with economic market uncertainty and climate change volatility.While the insurance industry has typically been a late adopter of technology, that isn’t true with AI—insurance companies are applying CV and NLP technologies across the value chain to improve their own pain points while simultaneously benefiting the customer. According to Deloitte’s 2022 Insurance Industry Outlook report, almost 74% of global respondents said they planned to increase spending on AI-related technologies. AI helps insurers assess risk, detect fraud, and reduce human error in the application process. It also helps customers, who benefit from the streamlined service and claims processing that AI provides. Specific use cases include underwriting, premium forecasting, pricing strategy, and customer servicing.With Arthur, companies are proactively mitigating reputational, regulatory, and strategic/financial risk while saving money and driving business goals.Explore industry use cases by downloading our insurance whitepaper.HealthcareFrom medical imaging analysis to disease prediction to drug discovery and development, AI has already revolutionized the healthcare industry from a technology perspective. Another piece of this puzzle, however, is ensuring health equity—particularly as it pertains to levels of care across underrepresented and minority groups. Additional healthcare use cases include hospital management, predictive insights for patient outcomes, capacity planning, staff training, medical diagnosis bias detection & mitigation, and medical document NLP classification. Arthur helps healthcare organizations avert harmful patient outcomes and reduce operational risk through proactive MLOps monitoring, resulting in early detection of data anomalies and model errors.One of our first customers and the leading AI-enabled healthcare enterprise, Humana, is deploying Arthur to manage mission-critical AI across both clinical and membership use cases. Arthur seamlessly integrates into Humana’s AI tech stack and is a core component of Humana’s approach to responsible and high-performing AI, providing a continuous view into model performance and bias, governance support, and alerting capabilities.Discover how AI drives business impact by downloading our Humana case study.Human ResourcesThe global transition from centralized office workplaces to regular work-from-home arrangements accelerated the adoption of automated AI tools to make HR departments run more efficiently. These tools are being used in areas like
talent acquisition, hiring, performance management, and employee experience. In fact, 99% of Fortune 500 companies rely on the aid of talent-sifting software and 55% of human resources leaders in the U.S. use predictive algorithms to support hiring.1While AI technology yields significant operational benefits, it also introduces risk—and the challenge is balancing the two. Any company using AI systems that analyze protected, special categories or sensitive personal datasets (age, race, gender, ethnicity, etc.) needs to exercise caution and ensure the data and/or algorithms being used are not causing systemic issues of bias and inequity resulting in disparate impact or discrimination.And this is no longer just an issue of morality: Starting in January 2023, companies in New York City will actually be legally restricted from using employment decision tools unless they have been the subject of an independent bias audit, in what is likely just the first of many similar laws that will be passed throughout the country and beyond.Deepen your knowledge by downloading our human resources whitepaper.‍Interested to see how Arthur can help your organization bring high-performing AI into production safely and responsibly? Schedule a demo of our platform.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/how-ai-is-reshaping-the-future-of-these-4-industries
188,"Shapley Residuals: Measuring the Limitations of Shapley Values for Explainability  Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ExplainabilityShapley Residuals: Measuring the Limitations of Shapley Values for ExplainabilityBy:Max CembalestOctober 31, 2022This post was originally published in Towards Data Science.We will use a cube representation of games to walk through the interpretation and limitations of Shapley values.IntroductionTo use machine learning responsibly, you should try to explain what drives your ML model’s predictions. Many data scientists and machine learning companies are recognizing how important it is to be able to explain, feature-by-feature, how a model is reacting to the inputs it is given. This article will show how Shapley values, one of the most common explainability techniques, can miss important information when explaining a model. Then, we will introduce Shapley residuals, a new technique to measure how well Shapley values are capturing model behavior, along with some code to get started calculating them!Consider the following example from Christopher Molnar’s Interpretable Machine Learning book: a bike-sharing company trains a model to predict the number of bikes taken out on a given day, using features like seasonal info, the day of the week, weather info, etc. Then, if their model is predicting a lower-than-average rider count on some day in the future, they can find out why that lower-than-average score is occurring: by looking at how the model is reacting to each feature. Was it because of a holiday? Was it because of the weather?A common way of computing the importance of each of your model’s features is to use Shapley values, since it is a method that is 1) widely applicable to many problems, 2) based on solid theoretical grounding, and 3) easily implementable with the SHAP Python library.The problem: In some scenarios, Shapley values fail to express information about model behavior, because it is only returning a score for one feature at a time. For instance, in the bike-sharing scenario, we are treating the weather and the day of the week as independent features, but sometimes it is the combination of those features that matters; and in those scenarios of feature combinations being more important than the individual features themselves, Shapley values can fail to properly explain a model.Bar Trivia ExampleLet’s use a simpler setting with fewer features to walk through the problem with Shapley values in more detail.I like to attend trivia nights at some local bars in the neighborhood with different coworkers of mine each week. It’s become quite clear that some members of our team bring more to the table than others.Can we quantify the impact each team member has on the trivia performance? We can use Shapley values for each player with the following interpretation: they should correspond to the expected change in score when adding that player to the trivia team. Other possible interpretations exist*, but we will use this one.(*Note: This class of methods to compute Shapley values, called “interventional” Shapley values, measure “expected change in score when adding this feature.” A different type is known as “conditional” Shapley values. The key difference between the interventional method and the conditional method lies in how they treat a feature whose expected change in score is zero—what should its Shapley value be? Zero? If you think the answer is “yes,” use the interventional method. If instead, you think the feature might still have importance due to correlations, and if you think that importance should be included in its Shapley value, then consider using the conditional method.)Geometrically, a useful way to plot all these 3-player game scores with different teams is as points on a cube, arranged so that neighboring points differ by just one player. Then, the paths between points (a.k.a. the cube’s edges) will represent the change in score when adding a player to a team.(Note: With two players, we would plot this as a square. With four or more players, we would have to plot this as a hypercube)Let’s call this shape a GameCube; this will be a useful shape for us because both Shapley values and GameCube edges will correspond to the change in score when adding a player.Figure 1: plotting each trivia score on a different vertex of a cube corresponding to the players present on the team that night.In our story, Reid is only knowledgeable about sports trivia, and GW knows about movies, music, history, geography, literature—pretty much everything except sports trivia. So when Reid plays, he improves the score by a little; when GW plays, she increases the score by a lot. And me, well, I’m mostly there for the beer and the company.A Shapley value is a perfect measure of explainability only when a player always contributes the same amount to a team’s score. And since each player’s change on the score is constant in our story so far, we can assign a Shapley value of 1 to Reid, a Shapley value of 9 to GW, and a Shapley value of 0 to Max. These Shapley values represent the expected change in score when each player joins the team!Figure 2: Viewing the change in team scores when adding each player.In more technical terms, a game where each player’s impact is consistent (like our story so far) is called an “inessential game.” Also, we will use the symbol ▽v to represent the “gradient” of a GameCube v, which computes the values along the edges between the values on the vertices, and we will use ▽_player_v to represent the edge values for a specific player’s directions and zero along all other edges.For example, the GameCube gradient ▽_Reid_ν represents all possible changes in score when adding Reid.Figure 3: Expressing the change in scores when adding a player as the partial gradient of the GameCube with respect to each player.Feature contributions can’t always be expressed as a single number—so Shapley Values aren’t enough.You should expect that most of the time, the features you are working with won’t have constant impacts on model outputs—instead, the impact of a feature typically depends on what the other features are.Let’s change up our story.Suppose that Max’s behavior changes based on who he is playing with. When playing with GW, he is pretty chill, drinks his beer, minds his own business and lets GW do most of the work, so he doesn’t bring the score down. But when Max plays with Reid, he gets jealous of how much Reid knows about sports, so Max starts to speak up more, suggesting some wrong answers and bringing the score down by 1!Figure 4: The new GameCube with inconsistent player contributions.On this new GameCube, GW’s edges are constant, so her Shapley value of 9 still corresponds exactly to the change in score when she plays. But Max’s and Reid’s edges are not constant, because their impact on score depends on who they are playing with. Therefore, our way of using GameCube edges to quantify what Max and Reid bring to the table now has a problem.When real data scientists use Shapley values, they solve this problem by taking the average contribution of a player to their teams—on the GameCube, this would mean quantifying a player’s contribution as the average edge values in their direction. So on our GameCube above, GW’s Shapley value would still be 9 as in before, but Reid’s Shapley value would now be 0.5 and Max’s Shapley value would now be -0.5. For some use cases, the story ends there—a player’s average contribution can sometimes be a good enough quantification of their impact!However, this may cause a problem when it comes to trusting Shapley values. Because we can trust GW’s Shapley values more than we can trust Max’s or Reid’s Shapley values, since there is more consistency in her contribution to the team than Max’s or Reid’s contributions.Shapley ResidualsThe Shapley residual is a measurement of how much a player’s edges deviate from being constant—lower Shapley residuals mean Shapley values are close to perfectly representative of feature contribution, whereas higher Shapley residuals mean Shapley values are missing out on important model information: namely, that a feature’s contribution depends on the other features as well.The authors of the original Shapley residuals paper formulate this missing information as an error term in a least-squares regression. For example, for the player Reid:▽_Reid_ν = ▽_ν_Reid + r_ReidThe left side of this equation is the same partial gradient as earlier. The right side of the equation is the sum of a new GameCube’s gradient, ▽_ν_Reid, plus a residual cube, r_Reid, which measures the amount that our game deviates from being inessential with respect to Reid.Figure 5: the residual cube is the amount a game deviates from inessentiality with respect to a given player.The key idea is that, if Reid has a consistent impact on the team, the residual cube r_Reid will be all zeros. On the other hand, if the values on the residual cube r_Reid deviate from zero, then that is a signal that Reid’s Shapley value is missing information about how Reid’s impact depends on who else is playing with Reid. The higher the values on the residual cube, the more Reid’s contribution depends on which other players are present.Code for Calculating Shapley ResidualsImportsGenerate Synthetic DatasetTrain Model & KernelSHAP ExplainerCompute Expected Values of Feature Coalitions
This uses explainer.synth_data, the set of the synthetic data
samples generated by the shap library when the explainer is
trained.
The dictionary coalition_estimated_values maps feature
coalitions to the expected value of the model when those features are used,
relative to a baseline (which is the expected value when no features are
used: the average model output).(Note that we convert the lists to strings since lists are not hash-able types in Python.)Progress Checkcoalition_estimated_values should look something like this:
{'[]': 0,
'[0]': -0.3576234198270127,
'[1]': 0.010174318030605423,
'[2]': -0.08009846972721224,
'[0 1]': -0.34261386138613864,
'[0 2]': -0.37104950495049505,
'[1 2]': 0.14435643564356437,
'[0 1 2]': -0.396}
Create Hypercube ObjectWe are using 3 dimensional data so this will just be a cube. But this method extends to hypercubes, growing slower as the number of dimensions increases.Feel free to use the code for the Hypercube python class in the appendix for
this article, or to write your own. It needs to place the
coalition_estimated_values on the vertices of the cube, and it
needs to compute the edge values as the difference between neighboring
vertex values.
Compute the Shapley ResidualsFor each feature, minimize  ▼_feature_cube — ▼_cube_feature  to compute
the residual. This uses a helper function called residual_norm
defined in the appendix at the end of this article.
ConclusionShapley values have become an incredibly popular and generalizable method for explaining which features are important to a machine learning model. By quantifying their effectiveness using Shapley residuals, you will be able to further identify where exactly your machine learning model’s behavior is coming from, and which insights stemming from Shapley values are worth trusting.Special thanks to the authors of the original Shapley residuals paper for their work!AppendixAll images in the piece are created by the author.Below is the code for the Hypercube object and other helper functions, which you can use with the starter code above to compute Shapley residuals.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/shapley-residuals-measuring-the-limitations-of-shapley-values-for-explainability
189,"From Black Box to Glass Box: Transparency in XAI  Arthur AI Blog
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedExplainable AIFrom Black Box to Glass Box: Transparency in XAIBy:Caryn LusinchiOctober 19, 2022With the rise of OSS security concerns, divergence of explainability goals, and custom, proprietary XAI algorithms, is transparency still possible? Explainable AI (XAI) typically involves tools and techniques to understand how a complex model behaves, in a simple, straightforward, and intuitive way so humans can understand it. It answers why an automated decision making tool resulted in a specific output that impacts customers.Market Size It’s predicted the explainable AI market size is estimated to reach $21.8 billion by 2030, up from $4.1 billion in 2021. And Gartner’s crystal ball paints a picture that “by 2025, 30% of government and large enterprise contracts for the purchase of AI products and services will require the use of explainable and ethical AI.” Regulatory’s RoleSo, what’s fueling predicted market growth? The accelerant for the explainable AI market is due in part to EU advent of GPDR’s Article 13-15 and 22, which establishes rights specific to algorithmic decision making, including a right of both notification and access to meaningful information about the logic involved and the right of the significance of and envisioned effects of solely automated decision making. Explainability, in this specific use case, is a legal obligation of enterprises to inform regulatory officials as well as end customers about why models made the decisions they did. End customers should be able to comprehend explanations, which should be written simply in their native language and include non-technical jargon.  Additionally, Article 13 (1) of the EU’s future Artificial Intelligence Act (AIA) mandates that high-risk AI systems be “sufficiently transparent to enable users to interpret the system’s output and use it appropriately.”Given the fact there are over 100 different XAI methods available to data scientists today and they often select the one that takes the least amount of effort/time, future regulation doesn’t specifically prescribe which explainability method should be used. The enterprise can elect to use local, global, or counterfactual explanations, but it “must be faithful to the model in the sense that they need to be an, at least approximately, correct reconstruction of the internal decision making parameters: explanation and explanandum need to match.”1While there is no nationally passed regulation in the U.S. regarding explainability, the National Institute of Standards and Technology (NIST) proposed in 2020 four principles for judging how explainable an artificial intelligence’s decisions are.And, the most recently released White House Blueprint for an AI Bill of Rights includes a Notice & Explanation principle, stating: “Automated systems should provide explanations that are technically valid, meaningful, and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context.”‍Explosion of XAI SolutionsAcademic R&D labs, open-source communities, and private software enterprises alike have seen legal compliance signals as a trigger to brainstorm XAI algorithms. The Partnership on AI (PAI) reports that “each year the number of available XAI tools (developed by both academics and industry practitioners) grows, resulting in more options than ever for those interested in using them. Here, we define ‘XAI tool’ broadly to mean any means for directly implementing an explainability algorithm. In our initial research, PAI identified more than 150 XAI-related tools published between 2015 and 2021.” The goal of the PAI project is to give enterprises tools to make more informed decisions about which XAI tool is best to deliver value to a business and help scale explanations. ‍OSS Concerns The vast majority of XAI tools are free-to-use open-source software (OSS). Public by nature, OSS offers a lot of benefits including crowdsourced examination for bugs or code evolution as well as enabling ethical conversations around ML applications. While OSS XAI libraries such as LIME or SHAP have done a lot to advance a broader understanding in the industry, they also pose performance doubts and security vulnerabilities. Some ML engineers are hesitant to apply OSS explainability methods into an application because they can slow down MLOps workflows and AI pipeline momentum. Additionally, cybersecurity experts are voicing concern that OSS explainable models are less secure given that when internal workings of model algorithms are publicized, bad actors can potentially manipulate the information via evasion, oracle, or poisoning attacks. Enterprises in competitive industries (where algorithms are treated as trade secrets or confidential IP), are worried that explainability may embolden industry competitors to reverse engineer ML models.  OSS caution is also echoed by the data science community. In a recent StackExchange post, it was acknowledged that commonly used and widely adopted open source ML packages are not regularly tested for reliability or de-bugged. One user posted, “Quite often those packages GitHub repos have existing unresolved issues and we may not go through them to identify any pitfalls. Business will be making critical actions based on the predictions/insights we, as a data scientist provide, which in turn could be based on those packages. How can we minimize the risk in such scenarios?” Custom Explainability TrendsGiven the combination of OSS security concerns and stakeholder resistance, companies are creating their own custom or proprietary explainability methods in-house or outsourcing the task to boutique consultants. As enterprises shift from OSS to XAI algorithmic IP, one would assume transparency would suffer—but that’s not necessarily true. Proprietary algorithmic IP may enable enterprises to fine-tune XAI methods to explain outcomes on an individual audience level, to provide more context around decision making rationale. Additionally, it gives the enterprise greater control over explanation content and verification. Ultimately, it may ensure there is sufficient domain knowledge expertise assigned to investigate models and dynamic datasets in order to fully comprehend the explanation.It’s predicted the trend for custom explainability will span across the model lifecycle, integrating into upstream and downstream ML team tasks. Given regulators—across GDPR, EU AI Act, or the AI Bill of Rights—are requiring easy-to-understand explanations for end customers and business stakeholders alike, there’s been recent work advancing natural language formatted explanations vs. technically dense feature importance scores based on LIME or SHAP.  As much as regulators are proponents of OSS, they also accept proprietary algorithms as long as there is sufficient evidentiary internal documentation and public disclosure to satisfy explainability laws. Divergence of Stakeholder Explainability GoalsRegardless of whether XAI methods are built on OSS or proprietary or a combination of both, the biggest challenge facing enterprises is that internal stakeholders don’t share the same explainability objectives. Each department has a distinct yet disparate goal of what they hope explainability will achieve. The Brookings Institute’s article, Explainability won’t save AI, broke down these fundamental differences. Typically, an explainability formula seeks to answer one perspective but fails to capture a broader context capturing angles from diverse, multi-stakeholders. Which is why the pursuit of explainability—either instigated by internal audit or external regulatory ask—in itself is not a panacea alone for risk management.  However, it is a starting point to shed light on the complex “black box” decision making that occurs between a machine learning system’s inputs and outputs. Discover Arthur’s explainability features across the pre-production and post-production MLOps lifecycle, including regional importance, global importance, and feature importance.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/from-black-box-to-glass-box-transparency-in-xai
190,"4 Myths About the NYC AI Bias Law
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & Fairness4 Myths About the NYC AI Bias LawBy:Caryn LusinchiOctober 4, 2022Studies show that 99% of Fortune 500 companies rely on the aid of talent-sifting software, and 55% of human resources leaders in the U.S. use predictive algorithms to support hiring.1Given the widespread usage of predictive algorithms by human resources departments and the potential for it to go wrong, New York City is one of the first cities to pass legislation in an effort to prevent negative impacts resulting from automated decision employment tools (AEDT). Effective January 1, 2023, New York City Int. No. 1894-A Relating to Automated Decision Tools law will go into effect that restricts New York City employers from using automated employment decision tools unless it has been the subject of an independent bias audit no more than one year prior to its use.With under three months to go until this law is officially put into place, we wanted to dispel some myths about it. Keep reading to learn more.‍Myth #1: The entire end-to-end human resources lifecycle, spanning candidate screening to termination events, is covered by 1894-A.‍Fact: Nope. The law only covers hiring and internal promotional employment decisions that occur within New York City (not outside the city). It does not apply to demotions, firing, or downsizing actions. As we witness increased layoffs across industries due to mounting recessionary pressure, it’s regrettable that the law fails to cover “performance management” algorithms that are making automated decisions regarding which employees are on the chopping block.‍Myth #2: The law covers any automated process or system used in human resources.‍Fact: The law does not cover or materially impact employment decisions made by “a junk mail filter, firewall, antiviral software, calculator, spreadsheet, databases, data set, or other compilation of data.”2In legislation, AEDT or automated employment decision tools are defined as “computational process, derived from machine learning, statistical modeling, data analytics, or artificial intelligence, that issues simplified output, including a score, classification, or recommendation, that is used to substantially assist or replace discretionary decision making for making employment decisions that impact natural persons.”‍‍Myth #3: An individual can file a court complaint if they think they’ve been discriminated against by an enterprise’s AEDT. Fact: The law makes no mention of “private right of action.” If NYC finds an employer’s AEDT was discriminatory, there is a path for a federal court class action complaint. ‍Myth #4: Employers must provide notices to all employees and candidates that an AEDT tool will be used in connection with employee and/or candidate assessments/evaluations.Fact: Non-residents of New York City are not required to receive this notice, even when applying to a city-based position. The notice only needs to be sent to individuals who live in New York City. It must be made no less than 10 business days before use of AEDT, plus it must list the job qualifications/characteristics that the AEDT used for decision making. Employers have the option of publicly disclosing the type of data used, the data source, and data retention policy for the AEDT on their company website OR providing the above information to employees or candidates upon written request within 30 days of receiving the request. Upon notice, candidates can opt out and request an alternative selection process or accommodation. ‍After the law goes into effect on January 1, 2023, regulators know enterprises won’t be able to comply overnight—so naturally there will be an interim grace period to give companies enough time to put processes and tools in place to meet regulation fine print before fines are levied.Concerns around bias in the automated hiring process have pre-dated the passage of the NYC AI Bias law. Readers who are interested in taking a deeper dive into algorithmic hiring, equity, and bias should check out work by the non-profit group UpTurn. For additional reading, reference “We Need Fairness and Explainability in Algorithmic Hiring” co-authored by John Dickerson, Arthur’s Chief Scientist. Or, delve into the academic paper “Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices.” In summary, the NYC Bias Law requires an independent third-party audit to assess AEDT’s disparate impact on candidates or employees of a particular gender or race/ethnicity. But, how can you determine if your AEDT’s model outcomes result in disparate impact year-round and not just a point in time for employment decision making? Arthur’s Bias tab empowers human resources teams to view predictions (or outcomes) of your AEDT model, segmented by relevant subgroups in the population. An adjustable fairness threshold lets you quickly identify if your model is causing disparate impact for protected classes. With Arthur, you can proactively and continuously measure disparate impact with algorithmic bias model monitoring—and act on it to improve the outcomes for both future and current employees you serve.Interested in seeing the Arthur platform in action? Schedule a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/4-myths-about-the-nyc-ai-bias-law
191,"Making AI Work for Even More People
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesMaking AI Work for Even More PeopleBy: Adam WenchelSeptember 27, 2022At Arthur our mission is simple: make AI work for everyone. Make it work for our partners building their businesses on AI. Make it work for all of us who are impacted by AI every day through healthcare, social media, finances, and so many other areas of our lives. Make it work for the data scientists, ML engineers, product owners, and executives who need these systems to be reliable, ethical, and highly accurate.Simple does not mean easy. AI in the real world is in its infancy; it is just getting started. AI is fundamentally a step change in mankind’s ability to automate. Big leaps like this require relentless invention and reinvention.I am continually amazed by the way our team at Arthur embraces this challenge every day—literally building the future in the process.Today, I’m thrilled to announce the latest recognition of their great work.Arthur has raised $42M in Series B funding, which will accelerate our mission to make AI work for everyone. The investment is the industry’s largest-ever in a machine learning observability platform. Over the last year we have averaged 58% growth per quarter, a total of 445% over the whole period. More than anything, this is a significant recognition of the last 3+ years of category-defining work by our world-class group of researchers and engineers.We would be nowhere without the amazing partnerships we have with some of the largest, most forward-thinking companies in the world. They are the ones who are leaning in to the change and reaping the rewards that well-considered AI efforts bring. Thank you for sharing our vision and the journey.We are excited to work with Theresia Gouw and Asad Khaliq from Acrew Capital and Dylan Pearce and Bryan Subijano from Greycroft, our co-leads for the round. We’ve collaborated with Theresia, Asad, and the team at Acrew for the last 18 months and could not be more excited to take the next step in our partnership. Dylan, Bryan, and the Greycroft team join as new investors, bringing invaluable experience growing and operating successful software companies. We welcome new investor BAM Elevate, and all of our excellent existing investors who are doubling down including Index Ventures, Work-Bench, and Plexo Capital.What will we do with this funding? First and foremost, we have scaled up our investment in fundamental research. Our research team has produced a number of breakthroughs that not only advance the state of AI but also create significant value for our customers. It is this spirit of innovation, deeply rooted in Arthur’s DNA, that has driven our growth. We are also scaling up many client functions like customer success and our team of field data scientists to make sure that our quickly growing user base has the world-class support they deserve.Tomorrow, we will roll up our sleeves and be back at it—there is still much to be done. I feel lucky to be a part of this team and look forward to the journey to make AI work for everyone.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/making-ai-work-for-even-more-people
192,"Will AI Solve Climate Change? It’s Not That Simple
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationWill AI Solve Climate Change? It’s Not That SimpleBy:Arthur TeamAugust 23, 2022With the biggest commitment to mitigating climate change in U.S. history having recently been signed into law, climate is—even more so than usual—top of mind for many. From AI-based forecasts around extreme weather and CO₂ emissions to identification of climate-vulnerable regions, AI and machine learning technologies have been touted as the ultimate solution or silver bullet to overcoming climate change. Unsurprisingly, it’s not quite that simple. Keep reading to find some use cases for AI and climate change, as well as the ways in which AI is simultaneously solving and contributing to the phenomenon.‍The ProsIn so many ways, these technologies can be absolute game-changers when it comes to mitigating the effects of climate change. In fact, 87% of private and public sector CEOs with decision-making power in AI and climate believe AI is an essential tool in the fight against climate change. Below are just a few of the more common use cases.MitigationThis is one of the most crucial use cases for AI: the measurement, reduction, and removal of emissions and GHG effects.Measurement: Measuring emissions, both in the overall environment and at the level of individual products and actions, allows us to take stock of our current situation and forecast future trends. It also allows us to prioritize the abatement efforts with the highest potential to reduce emissions, at both the macro level and the micro level.Reduction: AI solutions can reduce emissions intensity by improving supply forecasts for intermittent renewable energy sources or creating more efficient electric-grid balancing. Additionally, they can reduce emissions-generating activities by optimizing supply chains, such as through improved demand forecasting or more efficient transportation of goods. Removal: AI-based solutions can be used to support and more accurately quantify the environmental removal process by analyzing satellite images to detect deforestation and estimate ecosystem carbon sequestration. They can also be used for technological removal efforts such as direct air capture (DAC) or carbon capture by assessing capture and storage locations, monitoring leakage, and optimizing the industrial processes and materials used for carbon capture.‍Adaptation & ResilienceAdapting to climate change is a critical undertaking for policymakers and the public, as it boosts resilience to the effects of long-term climate trends and extreme events. Solutions can apply AI to promote adaptation and resilience, particularly in hazard forecasting and in vulnerability and risk-exposure management. Hazard Forecasting: AI is well suited to help in forecasting and projecting climate-related hazards, whether in the form of improved long-term projections of regionalized events, such as sea-level rise, or in the form of early warning systems for extreme weather events such as hurricanes or droughts.Vulnerability & Risk-Exposure Management: AI can be used here to enable public- and private-sector leaders to make more informed decisions during crises, strengthen infrastructure through predictive maintenance of structures, protect populations through the prediction and monitoring of things like large-scale migration patterns or food insecurity risk, and preserve biodiversity through systems like intelligent irrigation.FundamentalsAI can be used to support research and education efforts on climate change, helping stakeholders understand the risks and implications and share what they learn. All such efforts reinforce and magnify progress toward mitigation and toward adaptation and resilience. AI can be used to bolster climate research and modeling; climate finance and analytics; and education, nudging, and behavior change (such as in the form of recommendations for environmentally friendly purchases).‍The ConsOn the other hand, the carbon footprint and computational costs of training large machine learning models can’t and shouldn’t be overlooked.In a recent publication, researchers from the University of Massachusetts at Amherst conducted a life cycle analysis for training several typical big AI models. They discovered that the process may produce over 626,000 pounds of CO₂ equivalent—which is 5x more than a U.S. car produces in its entire lifetime and a whopping 57x more than one human produces in a year.Artificial intelligence algorithms, which power some of technology’s most cutting-edge applications, such as producing logical stretches of text or creating visuals from descriptions, may need massive amounts of computational power to train—and their energy demands are rapidly increasing. The computing resources required to create a best-in-class AI model have doubled every 3 to 4 months in the deep learning era. This, in turn, necessitates a vast quantity of power, prompting many to fear that the carbon footprint of these increasingly popular ultra-large AI systems will render them environmentally unsustainable.Here are a few ways that the carbon footprint of AI can be reduced:Make use of computationally efficient machine learning algorithms. Don’t train a model from scratch if it is not necessary. When you can, use Automated ML. Since the model inference accounts for 80-90% of the model cost, use customized processors that increase the speed and efficiency of training and testing neural networks.AI is a tool that is already helping us and will continue to help us build a more resilient future—but, like all tools, it should be used wisely. Additionally, solving the climate change crisis requires not just technological innovation, but also the will of decision makers to take action and make the necessary changes toward a better future.‍Interested in learning how Arthur can help your organization monitor, measure, and improve AI efforts? Schedule a demo.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/will-ai-solve-climate-change-its-not-that-simple
193,"What to Know as You Consider the Next Step in Your Tech Journey
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLife at ArthurWhat to Know as You Consider the Next Step in Your Tech JourneyBy:Reid ChamplinAugust 3, 2022After an unprecedented period of growth, the tech industry appears to be entering a stretch of profound uncertainty, leaving many professionals wondering what their next step should be.In just the first half of 2022, over 24,000 people were affected by layoffs at some of America’s top tech companies, including Netflix, Coinbase, Tesla, Twitter, and more. As fears of a potential recession grow, industry titans are pulling back on new hiring, with many saying the industry’s meteoric growth is being brought back to Earth.But for talent professionals and hiring managers throughout the industry, it’s clear the market remains red hot. Despite some high-profile companies rethinking their hiring strategies, tens of thousands of tech jobs remain unfilled as new opportunity emerges.Having your career shaken by factors beyond your control can be daunting, but your future is still bright. As you look forward to your next opportunity, here are three critical factors to keep in mind that’ll help you navigate the new job market and ensure your future success.1. Know the Industry As you consider new companies and organizations to further your career, ask yourself: What opportunities exist in this field, and how will those opportunities change and grow into the future? Finding life-changing opportunities often means being in the right industry at the right time. Not only are these opportunities less likely to be affected by uncertainty in the market, but they also offer the biggest reward as promising fields grow and occupy larger and larger niches within the tech world.Propelled by the limitless potential of AI, the machine learning industry is poised to explode in value to nearly $200 billion by 2030. The nascent field is already revolutionizing finance, healthcare, transportation, and countless other industries while employing more and more people every year—ML jobs have grown by 75% in the past four years alone. As the leader in the ML operations industry, Arthur has already experienced tremendous growth and will only continue to grow into the future.“As the AI industry continues to automate and mature the model development and deployment process, we’re going to witness the next 5-10 years intensely focused on bringing the same level of sophistication and ease to model performance management and optimization once these models go live in production,” says Victoria Vassileva, Arthur’s Commercial Accounts Lead. “This is where all of the value—and risk—is for any organization. Model performance and business performance, including regulatory and reputational risk, will be increasingly linked and so having the necessary oversight and controls will be the differentiator between success and failure, between positive impact and harm.”2. Know the CompanyAfter considering the broader industry outlook, ask yourself: Which company is best poised to take the lead in creating that new future? Knowing your prospective organizations inside and out is crucial to identifying which have the strengths needed to reach their goals—and to help you reach yours.Proven leadership is indispensable to any sound organization. The best indicator of future success is past success, and leaders who know what it takes to survive and realize their goals despite the odds are the most likely to do it again.No company can predict the future, and often the road to success is forged in difficult economic times. What separates the companies that float and the companies that sink is a focus on sustainable, responsible growth and commitment to take the steady route to success. Look into or ask about your prospective companies’ fundraising, annual recurring revenue (ARR), burn rate, and other key financial indicators. Are they prepared for whatever the future holds?Research has proven again and again that diverse organizations are higher-performing organizations and that fostering an inclusive company culture has a significant impact on a company’s fate. Are the companies you’re considering doing everything they can to foster a workplace where people of all backgrounds and identities are welcomed, included, and supported?At Arthur, we’ve built a culture-driven organization run by experienced leaders with an eye towards a more responsible future for the machine learning world. Our CEO Adam Wenchel has over 20 years of industry experience and found success both as CEO of Anax Security, a DC-based AI startup, and as VP of AI & Data Innovation at Capital One. We’re built by AI experts and backed by world-class, diverse investors, led by Mike Volpi at Index Ventures, Lo Toney, Theresia Gouw, Work-Bench, and Homebrew. And we’re even more proud to foster an environment where everyone feels welcome and invested in the Arthur mission.“It’s refreshing that we regularly and thoughtfully talk about how diversity and belonging are important to Arthur’s mission to make AI work for everyone,” says Genesis Whitlock, Arthur’s Head of Talent. “The time, energy, and focus it takes to find amazing talent from different backgrounds, experiences, and identities is so rewarding when you have leaders and colleagues who are actively working alongside you to create a diverse and inclusive environment.”‍3. Know Your ValuesWhat matters the most to you? Ultimately, only you can decide the right move for yourself. As you look forward to the next adventure in your career, take some time to think about what would make your next job the perfect one.Are you looking to grow and expand your skill set? In an industry that changes as rapidly as tech, staying up to date with the latest skills is critical to a long and impactful career. Do your prospective companies value mentoring and continuing education both in principle and in practice?Are you looking to take on leadership responsibilities and have your voice be heard? If so, it’s critical to know whether your new organization regularly encourages employees to take on new challenges and rewards them with opportunities to do so.Are you looking to work for a company with a mission you can believe in? People are far more likely to stay longer and achieve more if they believe in their organization’s mission. Consider what a truly good business looks like to you, and compare that vision against the companies you’re considering.Arthur’s values are at the heart of everything we do, and we work tirelessly to make sure we provide every opportunity our people could want. Even as we grow and scale, we remain a small team at heart, elevating employees to new opportunities and rewarding excellence with a platform to have your voice be heard. “With startups, building a great product doesn’t happen without a great culture as the foundation,” CEO Adam Wenchel says. “When I co-founded Arthur three years ago, I wanted to make sure that everyone on our team loved coming to work. We’ve spent a lot of time and energy fostering a company culture that reflects our core values of integrity, transparency, and inclusion.”‍We’re on a mission to make a more responsible AI-driven world—and we hope you’ll join us in making that vision a reality.‍Interested in learning more about a career at Arthur? Check out our job openings.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/what-to-know-as-you-consider-the-next-step-in-your-tech-journey
194,"Data Drift Detection Part II: Unstructured Data in NLP and CV
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringData Drift Detection Part II: Unstructured Data in NLP and CVBy:Karthik Rao and Rowan CheungAugust 2, 2022Note: This is the second part of a series where we take a deeper dive into the question of data drift detection. If you haven't yet, check out the first part where we discussed data drift in the context of tabular data!IntroductionData drift detection is a key component of a machine learning monitoring system. So far, we’ve discussed what data drift can look like in the context of tabular data, as well as some approaches to measuring drift. To recap, let’s revisit a simple example of data drift in a single feature:Comparing distributions. In this diagram, we examine a single input feature (Age) and look at the distribution of this variable at two time points. In the training data (green distribution) and in today’s production data (purple distribution).In this case, the distribution of age in the training dataset is different from its distribution in a production environment. Over time, the performance of a model using age as an input feature can decay in response to the change in the environment the model is deployed in. There are a variety of metrics we can use for measuring the difference in these two distributions, but how do we measure drift without structured features? Systems trained on unstructured data, like text or images, face the same risks when deployed in production. However, detecting drift in these scenarios is more subtle, as we cannot use common divergence metrics on the raw data. In this post, we’ll walk through a general framework for data drift detection with unstructured data and we’ll highlight the two example use cases of NLP and computer vision.
Specifically, we aim to identify datapoints that are anomalous, or belonging to
a distribution different than the training data. Formally, we would like to
surface incoming datapoints that are likely to have been drawn from a
distribution q(x) that is different from the training distribution
p(x). We’ll rely on two common use cases to illustrate the
out-of-distribution detection problem and evaluate our solution.
Our first example will be a computer vision use case, where the goal is to classify images based on the objects depicted in the image. For this setting, we used the STL-10 dataset from Stanford which provides high-resolution images from ten different possible classes including airplane, bird, dog, truck, and so on.‍Example images from the STL-10 dataset. We see images of cars, planes, trucks, dogs, etc. The STL-10 dataset contains 8,000 images.‍Our second use case will be in NLP and we used a News Headline dataset which contains news headlines along with their respective topics such as crime, entertainment, world news, comedy, etc. Here, our objective is to classify headline text to the correct category.Example datapoint from the News Headline dataset. We see that we receive information about the news category, news headline, authors, etc. The News Headline dataset contains 200,000+ records.Algorithm/ApproachAs with measuring multivariate drift in tabular data, the core motivation of the approach is to model the density, or distribution, of the reference dataset.OverviewThere are several different approaches for finding anomalies in unstructured data. For any given approach, the three main aspects to determine anomalies in unseen data require:
Vector Representation: Convert the unstructured data to a vector embedding.
Density Model: Define a density model for the reference dataset.
Scoring: Create a method for scoring new datapoints against the reference density model.
In this section, we will discuss the variety of different techniques used for each of these three different components. Further, we will highlight example results with NLP and computer vision datasets.Vector RepresentationWe must convert our image or text data into a meaningful vector representation in order to understand the underlying distribution of the reference dataset. These vector representations are a type of feature extraction that can capture a useful representation of our unstructured data. Transfer learning is one approach for creating these representations by extracting embeddings of each image or text sequence from a large pre-trained model. These large-scale models are generally trained on millions of different datapoints and use state of the art architectures (CNN’s for image data or Transformers for text data) that can take unseen datapoints and produce a meaningful vector representation. For images, pre-trained models such as ResNet, VGG, or similar will be appropriate. For NLP data, we need to extract document embeddings and turn to pre-trained (or fine-tuned) Large Language Models. While these are just a few examples of large-scale pre-trained models, there exist several others which are trained on different neural-network architectures and different datasets. This approach can be used with any type of vector embedding as long as it is meaningful for the context of your machine learning task. Density ModelOnce we have meaningful vector abstractions for every point in our reference dataset, we must now create a density model that can model the underlying distribution. We can train a flexible density model to these embedding vectors. This could be accomplished with many possible techniques such as an auto-encoder, a VAE, a Normalizing Flow, a GAN, etc. In each case, this density model learns the structure and distribution of the reference set images or text (as represented in the embedding space).Example of an auto-encoder architecture.As an example, auto-encoders are frequently used for unsupervised anomaly detection. Auto-encoders learn the latent representations of the reference set (consisting of vector embeddings) by encoding the vector to a lower dimensional vector and then decoding that representation back to its original dimension. We refer to the error measurement between the original input vector and the output vector as the reconstruction loss. Datapoints that are similar to points from the reference distribution will have a lower reconstruction error than points that are very different from the reference distribution. This property is useful for finding outliers as points that are outside the distribution of the reference set will have a high reconstruction error. Taking a look at our news headline example, we can inspect the space learned by our auto-encoder. We first train the model on news headlines categorized as CRIME, which we treat as our in-distribution data. Below is a visualization of held-out crime headlines, as well as entertainment headlines.UMAP visualization of in-distribution crime headlines (blue) and out-of-distribution entertainment headlines (red) as encoded by the auto-encoder.ScoringOnce we have trained our density model on our reference set, we must find a way to convert the reconstruction loss values from the model to actionable anomaly scores. Our approach is outlined below:
After training the model, we compute the reconstruction error of a holdout set (subset of the reference set) to use as a proxy distribution.
For every unseen datapoint, we compute the reconstruction error after being fed through our trained density model.
We find the percentile that our reconstruction error falls into relative to the reconstruction errors of the holdout set.
The motivation for our approach is twofold:
A lower reconstruction error means that the point is less likely to be anomalous (because the auto-encoder has seen many examples like it). Therefore, if an unseen datapoint yields a high reconstruction error (larger than anything from the holdout set), it is likely to be anomalous.
Because we rank in terms of percentiles, all our scores are normalized between 0 and 1. This makes it user-friendly and interpretable. Points that are close to 1 are more likely to be anomalous than points close to 0.
Evaluation/ResultsThere are very few open-source datasets that have labeled data to measure anomaly detection for unstructured data types. Therefore, we constructed a few different test cases with our example datasets introduced earlier in this paper to measure the efficacy of our anomaly detection algorithm for unstructured data. For each dataset (News Headlines and STL-10), we broke up our test cases as follows:
We segment our datapoints into in-distribution and out-of-distribution sets based on the labeled classes (e.g. all images as airplanes would be the reference set and all images as cars would be the out-of-distribution test set).
We furthermore segment the in-distribution dataset into 80% as the training set, and 20% as the holdout set. We use this to determine if our model would classify previously unseen datapoints as “anomalous” or “non-anomalous” based on the image object. For example, we would expect images from the in-class set should have low anomaly scores (near 0) and images from the out-of-class set should have high anomaly scores (near 1).
We run this test across different pairs of classes available in our dataset. We compute the AUC scores with relation to the “classification” of each datapoint as anomalous or non-anomalous (based on our scoring methodology).
We highlight two graphics below showcasing the results of our experiments.CV OOD: ROC Curves for STL-10 data where the in-distribution dataset is the class Planes, and the out-of-distribution class on the left is Ships and the out-of-distribution class on the left is Birds.The figures above are showcasing the ROC curves for two specific experiments we ran using the STL-10 dataset. The graph on the left is measuring the AUC when the in-distribution dataset (non-anomalous) was taken from a set of ship images while the out-of-distribution dataset (anomalous) was taken from a set of plane images. Similarly, the graph on the right shows the ROC curve where the in-distribution dataset was taken from a set of bird images and the out-of-distribution dataset was taken from a set of plane images. We notice that for both experiments, the anomaly detector does a very good job (AUC scores of 0.804 and 0.996) of being able to differentiate between in-distribution and out-of-distribution datapoints. NLP OOD: News Headlines dataset divided by each category as the reference set. Compares cross-class anomaly accuracies (where another class is all “anomalous”).The heatmap above is reporting the AUC scores for all possible pairwise experiments between possible classes in the news headline dataset (such as Crime, Entertainment, etc.). For any given cell in the heatmap, we are reporting the AUC score where the category on the x-axis is the in-distribution (non-anomalous) dataset while the category on the y-axis is the out-of-distribution (anomalous) dataset. We reported an average AUC score (across all crosswise pairs) to be 0.83, which is quite impressive given this task is difficult even for humans.ConclusionThis approach to out-of-distribution detection is especially powerful because it is completely unsupervised. In a production environment, we often don’t have prior knowledge of what kind of distribution shifts to expect or access to labeled data. Additionally, while we have considered two classification problems in this post, this technique can be applied to any type of machine learning task, as it only considers the input data and is therefore independent of the underlying ML task.Detection of out-of-distribution samples is only the first step in maintaining a robust machine learning system. Monitor your ML model for drift with Arthur. At Arthur, we’re helping data scientists and machine learning engineers detect, understand, and respond to unforeseen production environments.‍‍‍‍‍FAQ‍How do changes in the external environment, unrelated to the main features of the model, affect the process of data drift detection in NLP and CV applications within AI and ML frameworks, and how can one adjust the detection mechanisms to accommodate such changes?Changes in the external environment can significantly impact the effectiveness of AI applications, particularly in NLP and CV, by introducing new patterns or visual trends not present in the training data, leading to higher rates of misclassification or irrelevant results. This indicates a data drift. To adjust the detection mechanisms within AI and ML frameworks, one could incorporate adaptive learning strategies, allowing the model to periodically update its parameters based on new data. Additionally, implementing a robust anomaly detection framework capable of identifying and adapting to sudden shifts in data distribution without human intervention might help. Regularly updating the datasets with recent examples and employing domain adaptation techniques are also effective strategies to mitigate the effects of external changes on the performance of ML models.What are the specific computational costs associated with implementing the described data drift detection methods for unstructured data in real-world applications within AI and ML domains, and how do these costs compare with traditional data drift detection methods used for structured data?Implementing data drift detection methods for unstructured data in real-world AI and ML applications can be significantly more computationally intensive than for structured data. This is primarily due to the complexity of processing and analyzing unstructured data, such as images and text, which requires advanced algorithms and increased computational power typical of AI systems. Techniques like vector representation, density model training, and anomaly scoring, integral to ML workflows, are resource-intensive, especially when handling large datasets. In comparison, traditional data drift detection in structured data, often found in classical ML scenarios, relies on less computationally demanding statistical methods or simpler models. However, the exact computational costs can vary significantly depending on the specifics of each AI and ML implementation, the frequency of model updates, and the volume of data being analyzed.How does the data drift detection framework integrate with existing machine learning pipelines in AI systems, particularly in automated environments where continuous monitoring and instant decision-making are essential?The data drift detection framework can be integrated into existing ML pipelines within AI systems as a dedicated monitoring layer that functions in parallel with the main data processing workflow. In automated AI environments, this involves the continuous, real-time analysis of incoming data to assess its conformity to the model's initial training distribution, a cornerstone in ML operations. The framework should trigger alerts or initiate a retraining cycle if significant drift is detected, maintaining the ML model's accuracy and relevance. For effective integration, APIs could be developed to direct data from operational activities straight into the drift detection system and automate responses based on the outcomes, thereby enhancing the AI system's responsiveness and reliability. This ensures that the ML models remain accurate and relevant without disrupting the overall operational flow of AI-driven systems.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/data-drift-detection-part-ii-unstructured-data-in-nlp-and-cv
195,"Arthur Research: Equalizing Credit Opportunity in Algorithms
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationArthur Research: Equalizing Credit Opportunity in AlgorithmsBy:Lizzie KumarJuly 26, 2022For decades, financial institutions and lenders have used statistical models to make credit-related decisions. While these models improve efficiency and reduce variability, they have the potential to simultaneously perpetuate and accelerate historical patterns of discrimination. In the U.S., legislation like the Equal Credit Opportunity Act specifically bans discrimination in lending by forbidding credit score systems from using information like sex, race, marital status, national origin, and religion—and agencies are charged with enforcing this. However, credit invisibility and historical injustice mean that labeled credit data is limited on protected groups, which could negatively impact the accuracy of models trained on that data. Additionally, a vast body of research has demonstrated that even with sufficient training data, machine learning algorithms can encode many different versions of “unfairness.” This means that financial institutions could—potentially unwittingly—engage in illegal discrimination through the use of this technology.Two conversations exist in parallel here: one about U.S. discrimination law/policy, and the other about machine learning fairness research. Yet, policymakers and researchers in this space seem to talk past each other when it comes to data access, usage of input features, and the definition of “discrimination” (intent-based vs. outcome-based).Next Tuesday, at the AAAI/ACM conference on Artificial Intelligence, Ethics, and Society (AIES), we will be presenting a paper called Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation.The paper provides an overview of the following:The current landscape of credit-specific U.S. anti-discrimination law as it pertains to algorithms for fair lending researchersFair ML research results, contextualized to the realities of credit data to identify “discrimination risks” in the credit settingRegulatory opportunities to address those risksThe areas of lending regulation and of ML research are constantly evolving. We hope this paper is a useful tool for ML practitioners to understand the landscape and potential future directions.‍Interested in attending AIES? Check out the conference website. Also, learn more about Arthur’s R&D team here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-research-equalizing-credit-opportunity-in-algorithms
196,"Data Drift Detection Part I: Multivariate Drift with Tabular Data
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringData Drift Detection Part I: Multivariate Drift with Tabular DataBy:Keegan Hines and Reese HydeJuly 12, 2022Note: This is the first in a series of posts where we take a deeper dive into the question of data drift detection. We explore not only why it is an important part of model monitoring, but we also discuss regimes and approaches to keep in mind. In the first part of the series, we discuss drift in the context of Tabular data and describe univariate and multivariate techniques for tackling these problems. In the follow-on posts, we’ll dive into unstructured data, such as images and documents, and discuss how we can build data drift detection systems in these more challenging regimes.Part I: Multivariate Data Drift with Tabular DataMonitoring the ongoing success of a machine learning model requires monitoring the data coming into that model. This means ensuring the data coming through today looks exactly how you expect it to look. Ultimately, you want to make sure the data looks typical: Does it look the same way it did when the model was first trained? If the data has changed significantly, your trained model is likely stale and resulting in inaccurate predictions. Whether you’re talking about Tabular numeric data, image data, or NLP data, the data monitoring problem remains the same. In all cases, we will have some sense of what the data ought to look like and then alert when things go astray. In technical terminology, this is often referred to as out-of-distribution detection: We want to find when the data no longer adheres to the shape and distribution that it used to (back when the model was trained). There are many ways of thinking about data drift detection, and in this post, we’ll describe the benefits of a high-dimensional and multivariate approach.A handy approach to begin thinking about data drift detection is to measure the distributional similarity between the data coming through a model now versus how the data is supposed to look, such as in the training set. A great starting approach is to separately look at each input variable to a model (and outputs as well). This so-called univariate drift approach can be tackled with many technical implementations. Common approaches include hypothesis tests, such as KS Test and ChiSquared test, and the so-called f-divergences, such as the KLDivergence, JSDivergence, or similar. Common to all of these approaches, we would typically apply them in a univariate way to each input feature to a model (see Figure 1).Figure 1: Comparing distributions. In this diagram, we examine a single input feature (Age) and look at the distribution of this variable at two time points: in the training data (green distribution), and in today’s production data (purple distribution). It is clear that the general shape of this distribution has changed quite a bit. This could lead to model inaccuracy.A higher-dimensional variant can be calculated in theory, but these methods are ineffective in high-dimensional applications due to data sparsity. Primarily, by using univariate measures for drift detection, we make an implicit assumption of feature independence. While this might be approximately true in some cases, most generally, our dataset likely has some complex interactions between features and other significant structures. Importantly, this can lead to missed events when we consider only one feature at a time. Therefore, we must consider the high-dimensional joint distribution of the data.In a multivariate approach, we fit a multi-dimensional ancillary model to the full joint distribution of the training set. This ancillary model will act as a density model and learn the patterns and structure in the dataset. This model can quantify how much any given datapoint is typical or atypical relative to the reference dataset. In implementation, there are many potential approaches for this density model—examples include things like a Variational Autoencoder, Normalizing Flow Models, Density Models, Isolation Forest, and so on. Any technique which is flexible should be able to work effectively. Then, we can use this learned density model to evaluate future datapoints on how similar they are to the training data.This approach is explained further in the sketch below, which shows a simplified view of the process. On the left, imagine we have a training dataset; in this case, it entails only a couple of continuous variables (X1 and X2). We have a small number of sample datapoints scattered around this space of X1 and X2. The dataset has some particular patterns and structure to it (the curved relationship between X1 and X2), perhaps unknown to us. In step two, we fit a density model to this dataset. For brevity, we omit implementation details here, but the overall goal is to quantify where in the X1-X2 space we saw lots of data and where we saw no data. That is indicated in this sketch by the shaded contours: the darkest shading suggests areas where we were very likely to see some data, and blank areas show where we didn’t see any data at all. In step 3, we can use this trained density model to score any new datapoints in terms of how likely they would have been seen, per the training data. Another way to think about this is to score a new datapoint based on whether it adheres to the typical shapes/patterns in the training set or if it is abnormal. As an example, one of the datapoints is green because it falls right in line with the “typical” regions of the density model. This datapoint is very similar to other data in the training set. In contrast, the red datapoint is found in a region where none of the training data was ever seen. In this way, this datapoint is an anomaly and is unlike anything in the training set. In technical terminology, this point is said to be “out of distribution” relative to the training set. In the example sketched here, note that the univariate drift measures would likely fail to notice the anomalous datapoint. When viewed in a univariate sense (against either X1 or X2), this anomalous datapoint is quite typical. However, because X1 and X2 have a complex structure, we find that the red datapoint is quite different from the training data. When we fail to consider the multivariate case, we can miss many subtle shifts where the production data falls off the data manifold.This form of out-of-distribution detection is an important part of monitoring the health of a machine learning system. It is becoming increasingly important that ML models can have understanding of their own uncertainty and confidence. In many cases, this amounts to uncertainty over its predictions, given an input. However, with out-of-distribution detection, we can understand what the model thinks the world looks like and we can flag when things are quite a bit different. This is useful because complex ML models are often overconfident in their predictions, especially for data that is unlike what they were trained on. By considering whether each input is in-sample or out-of-sample, we can better quantify when to trust a model’s prediction and when to be leerier. In this post, we introduced ideas about out-of-distribution detection for the context of Tabular data. But this problem is pervasive for all types of machine learning. In the next post, we dive into these ideas for computer vision models and NLP models.See how easy the Arthur platform enables you easily to detect and react to data drift in our product deep dive.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/data-drift-detection-in-high-dimensional-and-unstructured-data-part-i-multivariate-data-drift-with-tabular-data
197,"What’s Missing from Your Model Governance Strategy?
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringWhat’s Missing from Your Model Governance Strategy?By:Arthur TeamJuly 8, 2022If you’re an enterprise with AI models in production across multiple lines of business, chances are you already have a model governance plan in place to comply with business, operational, and regulatory requirements. But governance goes beyond access controls, fancy arrow flow charts, policy PDFs, and checklists. How can you ensure your resulting model is accurate, robust, and reliable over time? Model MonitoringModel monitoring is a critical part of the AI lifecycle that enables data science teams to detect—and ultimately address—issues like data drift and algorithmic bias, while providing the necessary tools for correcting performance issues in the real world.According to Harvard Business Review, nearly 80% of AI projects won’t scale beyond proof of concept. Of the 20% remaining, it can take from 3 months to 2 years to roll a model into production. Given both the significant time and investment poured into ML model development, experimentation, training, and deployment, why wouldn’t you want to ensure that your model performs well post-production?  What Is Worth Monitoring in a Model? 1. PerformanceModel performance metrics can be a hybrid depending on model complexity. Regression: MAE, MSE, RMSE, Residual Histogram, Predicted vs. True, Forecast HorizonClassification: Accuracy, Confusion Matrix, Precision/Recall Tradeoff, F-1 Score, AU-ROC, Lift Curve, Cumulative Gains Curve, Calibration Curve, etc.These measures of accuracy are only useful, however, if you have access to the ground truth of your model. In a production environment, ground truth can sometimes take days, weeks, or even years to acquire (if it is available at all). In order to assess the performance of models without these prohibitive delays, proxies such as data drift can be used as a real-time barometer of your model’s activity. Drift metrics can be a meaningful leading indicator of the need for models to be retrained or restructured.2. Drift and DegradationData DriftOver time, data changes. Data drift is defined as a variation in the production data from the data that was used to train, test, and validate the model before putting it into production. The first two questions we often ask ourselves when we are faced with data drift are “When did this happen?” and “How did this happen?” Coincidentally, those are also some of the best ways to characterize different types of data drift. We can start to characterize how drift occurs by looking at the distributional makeup of the drift (i.e. what pieces of our data are drifting). We can then look at when the drift occurs by characterizing drift by intensity or timeline. ‍Covariate DriftCovariate drift is one of our most common types of real-world drift. It occurs when there is a change in the feature space of the model—or, in other words, when the distribution of one or more of the features has changed.‍Concept Drift Concept drift is a change in the relationship between input and output data variables over time.  This change can be gradual, reoccurring, or sudden. Oftentimes in production, we may not have access to the ground truth immediately. For example, if we are predicting whether or not a customer will default on a loan, we may not know if we are correct for months—or even years. In cases like these, it can be useful to also evaluate the relationship between features and your predicted values, instead of just the true (or ground truth) target variables. ‍Distribution Drift Common in real-world model scenarios is distribution drift. A key feature of distributional drifts in production is that they are silent. Arthur provides the ability to both monitor and set alerts to detect for different data distributional drifts, using both statistical and model-based methodologies. ‍Model degradation, or model decay, happens when a model’s performance becomes less reliable over time due to changes in the environment. When data drift occurs, the production-time assumption we made about the similarity between our training dataset and our production dataset is no longer true, which can cause the model’s decay.‍3. Explainability End users, data scientists, business leaders, and regulators need to understand how models make decisions. Improving model transparency can reduce model development and debugging time, highlight areas of concern for data drift and bias, and increase overall trust in the model. Explainability vs. Interpretability‍Model interpretability refers to models that are inherently understandable to humans. These models are simple enough that a human looking at the logic and internals of the model can understand how the model makes an inference given a particular input. In practice, few models are truly interpretable. Instead, there is often a tradeoff between interpretability and performance—especially for models performing complex tasks. High-performing models that do complex tasks are often the least interpretable models.  The goal of model explainability is to provide visibility into models that are too complex to be inherently interpretable. This often requires additional models and other techniques to generate explanations that are comprehensible to humans. Arthur’s platform offers powerful explainability techniques to provide prediction-level and whole model-level visibility into any model, including advanced “what if” analysis and feature importance ranking. ‍Local vs. Global Explainers‍Global explainers provide holistic model-level explanations. Global explanations are often presented as a summary of feature importance across the entire model. These explanations show which input features make the greatest impact on the output predictions of the model.  Since global explanations serve as simplified summaries of model behavior, they may not be accurate for specific data samples. However, they can help data scientists contextualize data drift to understand when a model needs to be retrained. This is especially useful when ground truth labels are unavailable. Global explainers are also useful to identify differences between groups for bias/fairness or debugging purposes. Local explainers provide a hypothesis of why a model made the prediction it did given a specific input sample. These explanations are useful in providing specific explanations to end users. They can also be helpful to data scientists when trying to identify and understand the cause of specific production issues. Local explanations can be aggregated across many samples to form global explanations.‍Types of ExplanationsFor explainers to be useful, they must present explanations in a way that is comprehensible and intuitive to humans. This is often presented in the form of data visualizations for feature importance. Explanations for tabular data can be intuitively represented as a bar plot of feature importances. Explanations for computer vision models and image data are provided by highlighting the most significant regions of an image, while natural language processing models can be explained by highlighting significant words and phrases.  Explainer AlgorithmsArthur leverages the industry standard LIME and SHAP algorithms to provide local and global explanations for tabular, computer vision, and natural language processing models. Both algorithms create simplified surrogate models to provide local explanations, which can be aggregated into global explanations. LIME and SHAP are model-agnostic explainers, meaning that they can generate explanations for any type of model, without accessing the internal logic and parameters of the model. Depending on the particular model and use case, a data scientist may favor one of these algorithms over the other. On the Arthur platform, LIME is used for image and text data, while SHAP is used for tabular data. 4. Bias / FairnessLocal and federal regulations around detecting and addressing bias are in the works (see NYC hiring law, Algorithmic Accountability Act of 2022). We have learned that traditional approaches which equate to “fairness through unawareness” simply do not work. In this case, ignorance is not bliss, and not enough to address existing/upcoming regulations. This unawareness-based approach may meet the requirements of avoiding the discriminatory practice known as disparate treatment, but does not address the possibility of the discriminatory practice known as disparate impact (this distinction is rooted in the Civil Rights Act of 1964). Essentially, a model that does not take into account membership of a protected class can still have adverse effects on members of that protected class.Detecting bias and discriminatory practices requires actively probing your data to see if groups are being treated unfairly. Arthur does this active probing for you and makes it easy to detect bias by making comparisons between subgroups, even if that group identity is not being used as an input to your model. There are a number of different metrics to quantify fairness. The three most common ones are demographic parity, equality of opportunity, and equalized odds. Arthur allows you to quickly identify, quantify, and visualize the degree of bias/fairness (using standard or custom fairness metrics) in your model’s outputs.If bias is identified, Arthur can help mitigate that bias based on post-processing techniques which do not require fundamentally changing your training data or model architecture. SummaryDon’t forget that an essential part of model governance is tracking model health post-production through model monitoring. Automated monitoring of performance, drift, model degradation, explainability, bias, and fairness as well as alerts/notifications of potential issues is an important aspect of ensuring responsible AI in your MLOps lifecycle.  Want to learn more about Arthur? Schedule a demo with one of our experts to see the AI monitoring, explainability, and bias analytics platform in action.‍Photo by Tingey Injury Law Firm on UnsplashPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/whats-missing-from-your-model-governance-strategy
198,"Arthur Recognized in 2022 Gartner® Hype Cycle™ for Data and Analytics Governance
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur Recognized in 2022 Gartner® Hype Cycle™ for Data and Analytics GovernanceBy:Arthur TeamJuly 1, 2022New York, NY –  Arthur, the AI Performance Company, was listed as a sample vendor for both AI Governance and Responsible AI in 2022 Gartner Hype Cycle Report for Data and Analytics Governance.This Gartner report highlights how ""data and analytics leaders must understand the hype and progress of governance practice and technology innovations so their adoption delivers organizational value at the right time.""“With AI now delivering value in practical enterprise application, data and analytics leaders see that scaling AI without governance is ineffective and dangerous,” the Gartner report states.As production AI continues to drive meaningful business value, Arthur is working with top enterprises to monitor, measure, and improve ML to drive better, more responsible results. Arthur is deployed at leading Fortune 100 enterprises across financial services, healthcare, retail, and technology, working together to accelerate MLOps and optimize for accuracy, explainability, and fairness.“Responsible AI means a deliberate approach in many directions at once,” the Gartner report states. “Data science’s responsibility to deliver unbiased, trusted and ethical AI is just the tip of the iceberg. Responsible AI helps AI participants develop, implement, utilize and resolve the dilemmas they face.”‍“We’re working with data scientists, product owners, and business leaders to ensure that their ML models are delivering accurate, transparent, and fair results—not only for the businesses themselves, but for the people they serve."" - Adam Wenchel, Arthur Co-Founder and CEO‍The report underscores the importance of model monitoring, explainability, and fairness in building truly AI-native enterprises. “Arthur is proud to be deployed across business-critical use cases at leading enterprises at an especially essential time in the AI industry’s maturity,” said Adam Wenchel, Co-Founder and CEO of Arthur. “We’re working with data scientists, product owners, and business leaders to ensure that their ML models are delivering accurate, transparent, and fair results—not only for the businesses themselves, but for the people they serve.”As more and more businesses are turning to ML capabilities like Computer Vision and Natural Language Processing to solve key challenges, Arthur is the only AI Performance solution for CV and the industry leader in NLP performance management—in addition to exclusive support for multi-class, multi-label, and regression tabular models. Arthur is model-agnostic and platform-agnostic, providing a centralized dashboard and a flexible query API that allow enterprises to democratize ML performance and tie it directly to business KPIs.Earlier this year, Arthur was mentioned in 2022 Gartner Innovation Insight for Bias Detection/Mitigation, Explainable AI and Interpretable AI report [2], and in Infographic: Heat Index: Gartner’s 2021 Connecting Hot, Hype and Cool [3]. Arthur was named a 2021 Gartner Cool Vendor in AI Governance and Responsible AI [4].If you are a Gartner client you can access the 2022 Hype Cycle report here.Learn more about Arthur and sign up for a demo at www.arthur.ai. ‍References[1] Gartner, “Hype Cycle for Artificial Intelligence, 2021”, Shubhangi Vashisth, Svetlana Sicular, 29 July 2021[2] Gartner, “Innovation Insight for Bias Detection/Mitigation, Explainable AI and Interpretable AI”, Van Baker, Svetlana Sicular, Avivah Litan, Erick Brethenoux, 9 February 2022[3] Gartner, “Infographic: Gartner’s 2021 Heat Index: Connecting Hot, Hype and Cool”, Michael Woodbridge, Thomas Bittman, Sam Olyaei, Rita Sallam, 16 February 2022[4] Gartner, “Cool Vendors™ in AI Governance and Responsible AI”, Svetlana Sicular, Moutusi Sau, et al, 10 June 2021‍Disclaimer:GARTNER, COOL VENDORS and HYPE CYCLE are a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.‍Photo by Milad Fakurian on Unsplash‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-featured-in-gartner-2022-hype-cycle-for-data-and-analytics-governance
199,"Why Only 12% of Companies Have Achieved ‘AI Maturity’
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Research & InnovationWhy Only 12% of Companies Have Achieved ‘AI Maturity’By:Arthur TeamJune 27, 2022A recent Accenture report called “The Art of AI Maturity: Advancing from Practice to Performance” went into detail about the journey some companies have taken from just testing the waters of AI to achieving a level of sophistication that is having massive positive impacts on their business. Precisely, “some companies” refers to the 12% of firms that, according to the report, “have advanced their AI maturity enough to achieve superior performance and growth.” The report also refers to these companies as “AI Achievers.” Another 25% of firms are “somewhat advanced in their level of AI maturity,” the report states, while the remaining 63% are still in the early stages.So, what exactly is AI maturity and why does it matter? Accenture defines AI maturity as “the degree to which organizations have mastered AI-related capabilities in the right combination to achieve high performance for customers, shareholders, and employees.” It’s become increasingly clear in recent years that harnessing the power of AI is crucial for businesses to have a competitive advantage. In fact, among executives of the world’s 2,000 largest companies, those who discussed AI on their earnings calls were 40% more likely to see their firms’ share prices increase.If most organizations are racing to embrace AI, why are some seeing more value than others? AI Achievers, according to the report, are not defined by the sophistication of any one capability, but by their ability to combine strengths across strategy, processes, and people. Here are five key success factors for Achievers:Their top leaders champion AI as a strategic priority for the entire organization.‍Companies can create strong AI strategies, but unless those strategies receive enthusiastic support from the CEO and the rest of the C-suite, they’re likely to flounder. When it comes to Achievers, 83% of these companies have CEO and senior sponsorship.They invest heavily in talent to get more from their AI investments.‍This senior sponsorship allows organizations to invest heavily in creating data and AI fluency across their workforces. 78% of Achievers have mandatory AI trainings for most employees, from product development engineers to C-suite executives.‍‍They industrialize AI tools and teams to create a strong AI core. ‍An AI core is an operational data and AI platform that taps into companies’ talent, technology, and data ecosystems, allowing firms to balance experimentation and execution.They design AI responsibly, from the start.‍As companies deploy AI for a growing range of tasks, adhering to laws, regulations, and ethical norms is critical to building a sound data and AI foundation. Achievers are 53% more likely than other companies to be responsible by design: designing, developing, and deploying AI with good intention to empower employees and businesses, and to fairly impact customers and society—allowing companies to engender trust and scale AI with confidence.‍‍They prioritize long- and short-term AI investments.‍One reason Achievers get more out of AI is simply because they invest more in it. Achievers also understand that their AI investment journey doesn’t have a finish line and that there is no “peak AI.” These companies know they have only scratched the surface of their AI transformations and that the quality of their investments matters just as much as the quantity.Echoing the report’s findings, Arthur’s customers self-identify as AI Experimenters (63%) or AI Innovators (13%), with longer-term aspirations of evolving into the AI Builder or Achiever categories. Advancing from practice to performance is a roadmap typically spanning a two-year time horizon for most enterprises. Maturity isn’t a one-size-fits-all path, either. While organizations may be farther along in ML/AI development and management maturity, we’ve discovered these same organizations are still in infancy for model monitoring and validation maturity. They’re still logging inferences, manually juggling Python notebooks, and facing problems from fragmented or restricted data stacks for day-to-day model data science workflows. The report says it best: “Advancing AI maturity is no longer a choice. It’s an opportunity facing every industry, every organization, and every leader.” The good news is that the share of AI Achievers will increase rapidly and significantly in the next few years, more than doubling from the current 12% to 27% by 2024. Will your organization be one of them?PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/why-only-12-of-companies-have-achieved-ai-maturity
200,"How Arthur’s Tech Stack Is Built for Scalability
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct FeaturesHow Arthur’s Tech Stack Is Built for ScalabilityBy:Arthur TeamJune 13, 2022With the increasing number of enterprise ML models in production, the rising demand for localized models, and the focus on more robust model monitoring, scalability is more important than ever. Consequently, ML model monitoring platforms need to be built with a highly scalable architecture that can do everything your organization needs it to do at production grade.An ML model monitoring architecture that isn’t scalable can result in a number of technical challenges, including lack of responsiveness, increased infrastructure cost, and platform inelasticity. If your model monitoring system isn’t built to scale, your models—and ultimately, your business—will suffer the consequences.Unlike less resilient ML model monitoring and observability solutions, the Arthur platform is built for high performance and scalability from the ground up. Keep reading to find out why Arthur’s tech stack makes it the leading platform for enterprises that want to run high-performing ML models at scale.‍1. Database ManagementArthur leverages the strengths of both ClickHouse and Postgres to handle different types of workloads. ClickHouse, used for OLAP workloads, is a horizontally scalable database management system that allows for high insert rates and fast serving of complex queries against very large data for multi-dimensional analysis. Postgres, used for OLTP workloads, is one of the best relational databases. With its reliable transactional mechanism, it organizes the users and the models to support the recording and the querying of respective model metrics. ClickHouse and Postgres work together to provide the Arthur platform and its end users with an optimal level of flexibility.2. Auto-Scaler Mechanism & Streaming-First ArchitectureAs previously mentioned, components in Arthur’s platform are independently and horizontally scalable. The platform’s auto-scaler mechanism self-manages and optimizes resource utilization on a Kubernetes cluster, automatically scaling up and down based on platform activities as well as the lag observed in the data pipeline queue.Whether the data is coming from streaming or batch, Arthur’s streaming-first architecture allows a very large volume of data to be ingested reliably and efficiently in a non-blocking fashion. For queuing, Arthur uses Apache Kafka, which was built for streaming big data and is ideal for MLOps use cases such as high-throughput activity tracking, stream processing, event sourcing, and log aggregation.3. High-Performance Programming LanguageArthur’s platform core is written in the Go programming language, which was developed by Google and is used by leading enterprises like Uber and Dropbox. Go was chosen for a few reasons: it’s compiled to machine code, its runtime performance is up to 30x faster than languages that are interpreted or have virtual runtime, and it’s built for concurrency and parallelism.‍At the end of the day, scalability is far more than just a “nice to have.” As your organization grows and ML projects outpace their original deployment, your model monitoring system must be resilient and flexible to adjust to the ever-growing volume of data. The Arthur platform was not only built with all of this in mind, but it also continues to be optimized to ensure models are proactively scaling and ultimately maximizing value for your business. Read more about the importance of scalability and performance in our whitepaper here.‍Want to dig into Arthur’s tech stack even more? Check out our dev docs and resources.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/how-arthurs-tech-stack-is-built-for-scalability
201,"3 Strategies for Maintaining Your ML Talent
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedBest Practices3 Strategies for Maintaining Your ML TalentBy:Arthur TeamJune 16, 2022A 2021 365 Data Science Ltd study revealed that data science professionals remain with an employer for only 1.7 years on average. In fact, the sudden impulse to write a data science resignation letter doesn’t change depending on where you work in the world. The study data sampled data science professionals from the US (35%), the UK (25%), the EU (25%), and India (15%).So, when it comes to maintaining talent, what can employers do to combat data scientists’ desire to change jobs and keep them fulfilled in their roles?#1 Pay competitive salaries and incentivize with annual bonuses and equity refreshes.Currently, demand outstrips supply. It’s simple economics. According to the U.S. Bureau of Labor Statistics, the expected job market growth rate for data science and related fields is 31%, far greater than the overall average job growth rate of 4%. When there are more jobs than candidates, data scientists can easily jump jobs and get a pay bump because they have more salary bargaining power. #2 Carve out and create dedicated weekly innovation project time for team members.In a recent Medium blog post authored by Ian Xao, he noted that “data science teams at big companies are facing a serious retention problem” as scientists are quitting enterprise companies for new jobs at startups.Xao pointed out that many data scientists have an expectation-reality gap, leading to day-to-day job disillusion. Data scientists desire to work with advanced algorithms and troubleshoot models—but they end up spending up to 40% of their time on data collection, data cleaning, and tedious administrative manual data export-import activities to keep up with reporting demands.  Data scientist Natassha Selvaraj echoed Xao’s sentiment on a recent KDNuggets post:“At the end of the day, you’re no longer building complex algorithms and predictive models like you imagined. You now spend all your time brushing up on SQL and data preparation skills to pull data out of the system into different formats, and present this data to stakeholders so they can use it to make business decisions. Although your job title has the word ‘data science’ in it, you aren’t in a role you’ve always pictured yourself in. You’re unhappy being the company’s data janitor, and want to work on projects that will actually utilize the skills you’ve spent so long to gain.” Giving data scientists autonomy to work on model innovation or experimentation (i.e. passion projects) for a set percentage of time (10% to 20%) each week keeps your employees engaged and empowered because they feel a sense of ownership and control over their role. #3 Automate documentation workflows and reporting activities to reduce admin burdens.Documentation, documentation, documentation (cue Jaws theme). As AI/ML regulation matures and places greater pressure on higher up model risk managers to document rationale and explainability behind data sets and model decision making, repetitive reporting asks often trickle down to data scientists and become a dreaded chore. This can range from design docs to writing model cards for communicating to external or non-technical stakeholders deployed model context, performance evaluation criteria and other relevant information. Or it could be a more in-depth report to satisfy an DPIA (data protection impact assessment), internal, or third-party audit requirement.Investigate MLOps platforms that can automate a team’s manual, repeatable activities. By automating univariate and multivariate data drift detection, event logging, data exporting for monthly/quarterly reporting requirements and more, your team can break the chains from day-in, day-out data science “janitorial” maintenance and focus on complex model diagnosis and mitigation solutions.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/3-strategies-for-maintaining-your-ml-talent
202,"Learnings from TTC Summit & Good Tech Fest
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsLearnings from TTC Summit & Good Tech FestBy:Daniel NissaniMay 25, 2022Earlier this month, I attended the Trust, Transparency, and Control Labs (TTC Labs) Summit on Trustworthy AI experiences and Good Tech Fest. Each conference offered sessions that centered around one central theme: How do we make technology that is in service of humankind?They wrestled with tensions in regulatory practices that make it hard for governments to enforce AI protections, discussed the nuance and meaning necessary for good explanations in AI systems, and dissected the privilege present in AI ethics today and how we can change it by engaging communities that will be harmed at the onset. The conference left me thinking that transparent AI systems, standardized international regulatory mechanisms, and impact assessments can help AI be good for everyone. Read on to see how I arrived at these learnings.Tensions Around Regulatory PracticesDuring the Transparency & Explainability in AI Regulation panel, several tensions arose around the AI regulatory space. EU Laws like GDPR and the forthcoming AI Act are focused on citizen rights, whereas U.S. federal laws around data, such as HIPAA, are focused on sectoral regulation. This nuance in the intention of the laws makes it hard to come up with frameworks and guidance for organizations to follow, as more and more organizations operate internationally. And even if these frameworks provided good guidelines (as some do, such as the AI and data protection risk toolkit  ICO), creating tooling to automate and manage these regulations throughout an organization is still a challenging and cumbersome problem. I think my favorite part of the panel was an acknowledgement that until society writ large has an intuitive understanding of AI concepts, enforcing regulations is going to be extremely difficult.Even something as simple as defining an AI system is quite challenging. According to one of the speakers during the AI & Society: Demonstrating Accountability panel, the AI Act actually provides some guidance on what counts as an AI system. Via this guidance, a manually coded decision tree, with rules hard coded, is not considered an AI system—but a trained decision tree, that learns the same exact decision rules based on a training dataset, does. This implies that the former does not need to abide by the laws that would be enacted by the AI Act. Although definitions will always contain loopholes, acknowledging them is a first step to refining definitions over time.Going Past Explainable ModelsResearchers from Google walked us through some of their explainability case studies, a tool they use to help them think through the nuances behind explainability in AI systems. What fascinated me most was that the case study we did together made us think past the conventional idea of explaining why a model had a certain prediction. We discussed the amount of information presented, the way the information was framed, and how the urgency or necessity of such information should prompt different types of responses from AI systems. These are fine-grained ideas to grapple with, and start to move into the territory of design or HCI work around explainability. One of the panelists on the Where Control Happens panel emphasized on multiple occasions that explanations need to be meaningful. It isn’t just that an explanation needs to happen, but it needs to be useful for a user, possibly giving them agency that they otherwise would not have had.Tenets of AI EthicsWhen trying to design ethical AI systems, we need to think about those who are involved in creating the system and those who are impacted by it. At both conferences, this idea was brought up repeatedly as something that needs to be addressed. Specifically, we need to focus on communities that do not have a voice in the creation of AI systems because they are usually the ones most susceptible to harm. During the Explainable to All: Designing AI Experiences panel, several of the panelists discussed how bringing those voices that are traditionally underrepresented, if represented at all, will allow us to at least mitigate some of the harm that AI systems may impose on a community. But this is much easier said than done. As brought up on the panel, ideas of fairness, transparency, and consent in AI systems is primarily a notion in the Global North. Even using frameworks like participatory design to address these ideas proves challenging because the notion of participatory design itself comes from the Global North. Thus, those communities that are impacted in the Global South may have no good way to give consent or may not have the cultural understanding to participate effectively in a participatory design framework. Addressing the cultural issues behind our notions first will help us start to actively include these communities.This should also be considered when collecting data. As discussed at Good Tech Fest’s session on Missing Voices in Natural Language Processing, the majority of languages used in large language models comes from primarily privileged communities that have access to technology. Thus, we need to be conscious about our language data collection mechanisms and how we deploy and assess large language models. The presenter even cited one of our blog posts as a good guide to ethical natural language processing practices.Takeaways and ResourcesOverall, these conferences expanded my thinking on a lot of topics and reminded me of the nuance and complexity that fairness and explainability hold. Below are some takeaways coupled with some resources that can help you continue your own journey with these concepts.Transparency is key to solving the challenge of trustworthy AI. By explicitly showing how systems work, providing agency to individuals, and attempting to participate with communities being impacted, we can develop AI systems that will benefit humanity. Some of my favorite papers on transparency are documentation-based, such as Datasheets for Datasets or Model Cards.Creating standardized regulations for AI systems and having consistent auditing frameworks are also effective. Realizing these notions is easier said than done, but being able to enforce regulations, while also providing tools for organizations to be successful under this enforcement, will create more ethical uses of AI.Starting from impact assessments will help organizations realize who will be harmed most by AI systems. Once identified, one of the panelists on the Explainable to All: Designing AI Experiences panel expressed that inviting representatives from those communities and achieving a consensus with them will help address harms, while also gaining an effective proxy for consent.Lastly, part of my attendance at Good Tech Fest was as a DataKind volunteer. During DataKind’s session, we helped nonprofits start to think about their data scienceable problem. DataKind offers their playbook for how to design and scope out data sciencable problems ethically.‍ConclusionFairness and explainability are extremely nuanced and important fields because, if done right, we can make the world a better place. It’s part of the reason why I am so excited to be part of the Arthur team. The tools we are building help organizations start to operationalize the latest research in fairness and explainability, allowing them to realize their aspirations to be ethical. Looking forward to continuing the journey!‍Interested in learning more about what Arthur can do for you? Get in touch.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/learnings-from-ttc-summit-good-tech-fest
203,"Arthur Launches Custom RBAC to Strengthen Data Privacy & Reduce Compliance Risks for Enterprises in Highly Regulated Industries
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedProduct FeaturesArthur Launches Custom RBAC to Strengthen Data Privacy & Reduce Compliance Risks for Enterprises in Highly Regulated IndustriesBy:Caryn LusinchiMay 11, 2022Arthur is the very first MLOps observability platform to launch custom RBAC (Role-Based Access Control) to support today’s enterprise security and data privacy demands.As organizational structures evolve and grow, companies need the flexibility to review and adjust permissions frequently to protect model data and MLOps lifecycle access management. Additionally, specific roles within MRM (model risk management) and third-party independent auditors require strict confidentiality, privacy, and access management controls. When businesses use highly sensitive personal identifiable information for privacy-sensitive data analysis in supervised learning environments, they face increased regulatory risk. To reduce privacy and compliance risks, segregation of duties is essential for different roles across the enterprise that are involved in ML monitoring and validation activities.To uphold the integrity of trust frontiers necessary in AI systems and provide transparency to higher-level stakeholders who are tasked with management/governance KPIs as data flows through the ML development lifecycle, Arthur now offers a custom Role-Based Access Control (RBAC) system.  Enterprise organizations cannot rely on offerings with inflexible preset/pre-defined roles and provisioning access. Arthur provides the ability to set up a fully customizable RBAC for on-premises customers using OIDC authentication, and the flexibility and freedom to review and adjust permissions associated with each role as their org structures evolve and grow. With this unique new capability, Arthur allows organizations to create groups and assign employees using any tool that speaks open ID connect language (OKTA, Microsoft Azure Active Directory, Duo, SecureAuth, AuthO, Ping Identity, and more). Organizations can leverage Arthur’s group mapping API for enterprise secure user authentication between data scientists, ML model engineers, systems integrators, domain practitioners, risk analysts, internal auditors, and external auditors.  Here’s an example managing users’ permissions in Arthur using a third-party identity provider, such as Okta. Each Okta Group corresponds to a role within an organization in Arthur’s platform. In the future, we plan to extend custom RBAC to our existing support of SAML 2.0-based SSO identity providers. When building trustworthy AI systems, it’s critical for enterprises to identify clear roles, requirements, and responsibilities for teams building, monitoring, and optimizing machine learning models.  Arthur’s RBAC-OIDC authentication solution was specifically built to support enterprises that are subject to EU, UK, US or state regulation that mandates algorithmic transparency and auditing. As such, our custom RBAC feature offers full-fledged flexibility across standard roles, plus organization-level, model-level, and alert-related permissioning. Fully Customizable Permissions Here’s an excerpt of some of the different permissions Arthur supports for use with custom roles. For a full list, you can check out our dev docs.Discover how Arthur’s custom RBAC fits with your enterprise OAuth. Schedule a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-launches-custom-rbac-to-strengthen-data-privacy-reduce-compliance-risks-for-enterprises-in-highly-regulated-industries
204,"Learnings from ODSC East 2022
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsLearnings from ODSC East 2022 By:Arthur TeamApril 27, 2022The Open Data Science Conference, or ODSC, is one of the largest gatherings of professional data scientists, and its attendees, presenters, and companies are shaping the present and future of the field. The goal of the conference is to bring together the global data science community in an effort to encourage the exchange of innovative ideas and the growth of open source software.Last week, three members of Arthur’s ML Engineering team headed up to ODSC East in Boston as both attendees and as speakers. Keep reading for some recaps of their favorite talks that they attended as well as the ones they presented themselves based on their own research.Unsolved ML Safety ProblemsDan Hendrycks, PhD Candidate at UC BerkeleyThough most business-critical machine learning systems today are relatively simple models, the research community has produced astounding results with large models such as GPT-3 and DALL-E, which often have surprising capabilities. In this talk, Dan Hendrycks, a PhD student at UC Berkeley, discussed some high-level approaches to the problem of ML safety. Hendrycks described three buckets for future research: Robustness, or the ability for ML systems to perform well even in unexpected or adversarial settings; Monitoring, or the ability to understand and anticipate new challenges; and Alignment, or the ability to coordinate the behavior of ML systems with human goals, values, and intentions. While the highest-stakes motivations for the talk, and the largest open questions, generally refer to large deep learning models, these challenges and goals are still relevant for simpler models. Operationalizing Fair ML: From Industry to Research and BackJessica Dai, Machine Learning Engineer at Arthur (co-presented with Laura Mariano, Lead Ethical AI Data Scientist at Humana)In both research and industry, discussion of “fair machine learning” has exploded in the past few years—yet there is often a gap between what is available in academia and the constraints and needs of a real-world organization. This talk was co-presented by Jessica Dai of Arthur and Laura Mariano of Humana, which has been an incredible partner of Arthur’s for years now. The two discussed Humana’s journey towards achieving informed, responsible use of machine learning to improve health outcomes. First, Humana implemented organizational and process-based tools for governance. Having set the stage for actively improving models, however, Humana’s data scientists then realized that none of the popular, published approaches to achieving fairness were applicable to their goals: the way Humana deployed and used machine learning violated assumptions made by many available “fair ML” methods. In the latter half of this talk, Jessica showed how these constraints motivated novel research questions and guided the development of an academic research project; explained and demonstrated the method we came up with; and discussed considerations as we folded this research work back into the product so that it was ultimately usable in a real-world production setting.Utilizing NLP in the Context of COMP360 Psilocybin Therapy for Treatment-Resistant DepressionGregory Ryslik, Executive Vice President, AI, Engineering, Digital Health Research & Technology at Compass PathwaysIn this talk, Gregory Ryslik of Compass Pathways discussed how NLP techniques have been used in the process of setting up clinical trials for using psilocybin as an intervention against treatment-resistant depression. The magnitude of the problem is, of course, enormous; the talk focused on how this specific application setting, as well as the constraints it brought—such as working with nontechnical clinicians, privacy and regulatory concerns, and following the clinical trial process—all shaped the development of NLP approaches on the technical side. Drift Detection in Structured and Unstructured DataKeegan Hines, Vice President of Machine Learning at ArthurMachine learning systems in production are subject to performance degradations due to many external factors and it is vital to actively monitor system stability and integrity. A common source of model degradation is due the inherent non-stationarity of the real-world environment, commonly referred to as data drift. In this presentation, Keegan described how to reliably quantify data drift in a variety of different data paradigms including Tabular data, Computer Vision data, and NLP data. Attendees of this talk came away with a conceptual toolkit for thinking about data stability monitoring in their own models, with example use cases in common settings as well as in more challenging regimes.Simplifying MLOps by Taking Storage Out of the EquationMiroslav Klivansky, Principal Data Architect, AI & Analytics at PureStorageOne of the biggest issues that enterprise companies face when trying to design and scale machine learning pipelines is worrying about the raw storage of the data. In this talk, speaker Miroslav Klivansky discussed the work he was doing at PureStorage, a flash storage company based in Palo Alto. He discussed how we need to think about handling large-scale data pipelines and having a data storage system that can handle the many different kinds of tasks from different machine learning models. The key takeaway from the talk was how we must differentiate between data and storage, and how they are not the same thing. He showcased a workflow (done at PureStorage) where we think about the different parts of the data pipeline abstracted away from the storage system, which yields cleaner and more efficient pipelines. FastCFE: A Distributed Deep Reinforcement Learning Counterfactual ExplainerKarthik Rao, Machine Learning Engineer at ArthurCounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps to move a data point from one side of a decision boundary to another. These explanations have a clear use case for several applications ranging from loan decisions to healthcare diagnosis, where they need to advise stakeholders about actions they can take to achieve a different outcome. Individuals not provided loans want steps they can take to achieve a loan, and similarly, patients want to know how they can achieve a better diagnosis.In this presentation, Karthik Rao showcased FastCFE, an algorithm and feature that uses reinforcement learning to provide real-time counterfactual explanations. The presentation was broken down as follows:Overview of Counterfactuals and Reinforcement Learning (RL)Deep distributed reinforcement learning using OpenAI Gym and Ray+RllibBenchmarks and ResultsA Unified View of Trustworthy AI with the 360 ToolkitsDr. Kush Varshney, Distinguished Research Staff Member and Manager at IBM ResearchThe final talk of the conference was given by Dr. Kush Varshney of IBM Research, discussing the open source initiatives that IBM has taken to develop trustworthy AI toolkits. This was a particularly enlightening talk and it is definitely beneficial to the AI community that large players such as IBM are committed to trustworthy initiatives and are even willing to open source these new technologies to help companies build trusted AI systems. If your organization has data scientists who can afford to invest the time and want to be truly hands-on, these IBM toolkits will work well for your team. Dr. Varshney explained three specific AI toolkits discussing:AI Fairness 360: Different methods to detect bias and use bias mitigation techniquesAI Explainability: A toolkit providing out of the box explainable tools to explain modelsAI Adversarial Robustness: How to detect and handle adversarial data attacked on modelsODSC East was an incredibly rewarding experience for the Arthur team, and we’re thrilled that our engineers were able to share the results of the hard work they’ve been doing to advance the field of machine learning. We’re feeling grateful to have finally attended a conference in person after so long—and especially to have been able to co-present with one of our long-time partners, Humana, to amplify the story of their leading work. See you at ODSC 2023!‍Interested in learning more about Arthur’s research-led approach to product development? Read about it here.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/learnings-from-odsc-east-2022
205,"Mining for Proxies in Machine Learning Systems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessMining for Proxies in Machine Learning SystemsBy: Keegan HinesApril 18, 2022In deployed machine learning systems that affect people’s lives, it is important to ensure that your model is not resulting in disparate impact for some sub-populations of your data. The typical populations of concern might be the so-called protected basis variables such as race, age, or sex, but can also be any subgroup which is relevant to your industry. Creating ML systems that are fair can be tricky due to the complexities of data used for training.If you want to ensure that your model isn’t resulting in disparate impact across sex, it might be tempting to simply not use sex as an input variable into the model. This tactic, often referred to as fairness-through-unawareness, is insufficient. Even though the model doesn’t know about sex explicitly, it is highly likely that some other input feature (or combination of features) can act as a highly effective proxy for what you were trying to hide from the model.Many approaches to FairML have focused on mitigating unfairness either by augmenting the training data, the training algorithm, or the model’s outputs. While these approaches help to obtain a more fair model for any given set of inputs, they don’t illuminate for the data scientist what factors were resulting in the model bias. As the model developer, it is helpful to have full awareness of which features are acting as proxies for sensitive attributes. In this post, we describe a technique for effectively surfacing complex combinations of features which act as proxies for a sensitive attribute. ProxiesBeing able to identify proxies can aid the model developer in being judicious about feature selection and model development. At a first pass, identifying proxy variables is as simple as determining which variables have high correlation with a sensitive attribute, and then choosing whether or not to omit those variables. The downside of this approach is that measures of correlation (perhaps Pearson correlation or mutual information) will marginalize over the full domain, perhaps averaging out important effects only seen for some specific values. Further, it could be the case that pairwise combinations (or higher-order) of features are able to form highly effective proxies, more so than any univariate features.
As a fictitious example, suppose we have a dataset where one variable is a
binary sex (Female or Male), and another
variable is a boolean is_pregnant. While the is_pregnant
variable does provide some overall correlation with the sex variable,
the strongest proxying effects are seen only for some combinations of the two
variables. That is, if we know that is_pregnant = False for a particular
person, then we can’t necessarily conclude much about their sex, and likely
p(sex = Female) ~= p(sex = Male). However, if is_pregnant = True,
then we’ve likely gained significant certainty about sex and
p(sex = Female) >> p(sex = Male).
This example illustrates a desirable property when identifying proxies. It is of
only limited utility to know that is_pregnant is correlated with
our sensitive attribute sex. It is much more useful to know that
is_pregnant = True is a strong proxy for sex = Female,
whereas is_pregnant = False is uninformative. The difference here
is that we aim to identify small pockets in the data (and combinations of
features) that form highly predictive proxies.
To accomplish this, we’ll rely on the techniques of Frequent Pattern Mining and Association Rule Mining. This will allow us to surface exactly which values of a variable, and higher-order combinations thereof, are acting as strong proxies for any sensitive attributes. Association RulesAssociation Rule Mining is a rather classic technique in unsupervised pattern discovery, and it can be especially useful in the context of FairML and proxies. The ideas often stem from “market basket analysis”, which imagines we have a large database of purchases at a supermarket. For each shopper, we know what items they bought together, such as {milk, eggs, cheese} for one shopper and {diapers, beer} for another. From the co-occurrences of items in baskets, our goals are to (1) identify commonly co-occurring items and (2) identify directional correlations. For our first goal, we can use Frequent Pattern Mining techniques such as FPGrowth, which will result in a list of item combinations that were highly probable in the dataset. Going a step further, we can apply Association Rule Mining to identify not just correlated items, but items correlated with a particular directionality. For example, if a person bought sunglasses and a bathing suit, then maybe they also bought sunscreen. But if all we know is that they bought sunscreen, they’re not necessarily in the market for new shades. Association Rules are of the form “if A, then B”. The antecedent A can be one or more items, and the consequent B can be one or more items. Importantly, we’re not looking solely for relationships that are 100% fulfilled in our dataset. Instead, we’re looking for consequents that follow antecedents with higher probability than we might expect. Quantifying those base-level expectations means we have to introduce some new terminology and measures.MeasuresFor a particular itemset (whether it is an antecedent, consequent, or neither), the support for that itemset is a measure of how frequently the itemset occurs in the data. We can think of it as the overall probability of occurrence for an itemset. Note that for a particular itemset, we need to tabulate not just exact matches to that itemset, but also all other larger itemsets that include at least that one. An itemset with high support (like 0.80) would mean that 80% of the dataset is exactly this itemset. As you might imagine, itemsets with fewer items tend to have more support than itemsets with more items. For example, the itemset {milk} probably occurred in many of our shoppers’ baskets, whereas the combination of {milk, lettuce, hat} was probably more rare. For a particular rule “if A, then B”, the confidence quantifies how often that rule was true. Another way to think about it is: Whenever you saw A, what’s the probability that you would see B? Naturally, if a rule were a really strong correlation, then confidence in the consequent would be close to 100%. In contrast, for a weaker relationship, knowing about antecedent A perhaps doesn’t give us much certainty at all about B. Other useful concepts such as lift and conviction will help us quantify the extent to which A helps us predict B more than we’d be able to predict B otherwise.Mining for Proxies
Identifying association rules in a dataset can be a useful technique for
uncovering proxies for sensitive variables. In this case, we’ll be looking for
association rules of the form “if A, then B” where the consequent B is
one of the values of our sensitive attributes. For example, if we want to find
proxies for Female in our dataset, we can limit ourselves to
association rules where the consequent is the itemset
{Female} and nothing else. Then, the antecedent of such a
rule would be a feature value, or combination, that is highly predictive of a
person being Female. The confidence of the rule tells us how strong
the association is: a confidence near 50% means that the antecedent isn’t
actually a very effective proxy for Female, whereas a confidence
near 100% means we have found a combination of feature values that can identify
females with near certainty. Let’s try this idea out and see how it works.
Identifying Proxies for SexWe’ll use the UCI Adult dataset, which includes financial and demographic data about a group of 30,000 adults. While this dataset is often used for classification tasks (predicting if a person has high income), we’ll ignore for now the supervised learning aspect. Instead we’ll focus on some of the demographic variables (such as Sex or Race) and investigate whether these can be proxied by other variables (such as Education, Occupation, MaritalStatus and so on). We’ll focus on Sex and use Association Rule Mining to identify strong proxies. For generating candidate itemsets, we’ll focus first on categorical variables. Continuous variables can be easily incorporated by discretizing their domains, either manually (such as quartiles) or by dynamically identifying bin splits (in a similar spirit to decision tree algorithms). It is up to us to identify how many input features we might include for itemsets and what order of combinations we want to consider. For ease of interpretability, we might constrain ourselves to first-, second-, and third-order combinations. That is, in the antecedents of our rules, we’ll only want to consider itemsets with one, two, or three items. This choice is up to us, depending on what level of rule complexity we want to access. As we increase this complexity, the combinatorics of itemset and rule generation expands. Luckily, algorithms such as FPGrowth, which are based on tree structures, will allow us to apply these ideas to large datasets and complex itemsets, if we desire.
Let’s see whether the variables MaritalStatus, Occupation, and
Education can form proxies for Sex. Remember we’re not just
looking at the variables holistically, but instead identifying whether certain
values and combinations of those values can be highly predictive of certain
values of Sex. We’ll generate rules up to second-order and see which
rules can predict Male with high confidence. A few top examples are
below.
We see that some (but not all) occupations and educations are strong proxies for
males. For example, for all the rows in the data where Occupation was
Repair and MaritalStatus was Married, those
people were male 99% of the time. This combination is also fairly prevalent, as
the antecedent support is 8%, a sizable chunk of the data. So for this pocket of
the data, we have a near-perfect proxy for Sex. The same applies for
numerous other combinations, though they are less prevalent. There are many
association rules generated with confidence greater than 80% for predicting
Male. They typically entail small pockets of the data (support of
1% to 5%). The effect of each is small, but the aggregate is that for large
volumes of the data space, we can predict males with high confidence.
Similar results are returned if we look for proxies for Female.
Here, we see that MaritalStatus can be a strong proxy for
Female in some instances. Especially strong is the antecedent “if
MaritalStatus is Widowed”, an unsurprising rule since the
term “widowed” is fairly gendered to begin with. In combination with
Occupation, MaritalStatus can become an even more precise
predictor of Female.
Take for example the rule “if Occupation is Admin-Clerical,
then Sex is Female”. This rule’s confidence is 67%, so a bit
higher than the base-rate of Female in the dataset, but it does
have a support of over 10%. The higher order rule “if Occupation is
Admin-Clerical and Education is High School,
then Sex is Female”, has slightly higher confidence (at 70%)
and slightly lower support (at 5%). As the antecedent becomes more precise, we
can find smaller pockets and subsets of the data that yield more confident
proxies of the sensitive attribute.
Intuitively, these results are unsurprising: many jobs are highly correlated with males or females. But we now have a more precise way to understand this effect. The support of each rule tells us how prevalent a particular combination is in the data, and the confidence tells us how certain the proxying can be. This analysis is easy to apply to any sensitive attributes we want to explore. We could identify proxies for certain races, age groups, health conditions, and so on. Next StepsGiven how informative some of these combinations can be, it would be unsurprising for an ML model to pick up on these proxies even if it didn’t have direct access to sensitive attributes. Any sufficiently complex algorithm (even just a decision tree or tree ensemble) should easily be able to capture the simple combinations highlighted here, if they are helpful in predicting the target variable. Surfacing these relationships should be an early part of the model development process and should play a role in feature selection. Ultimately, our goal must be to mitigate disparate impact in the final system. With this clarity into proxies, we could choose to (i) omit certain variables from a model, (ii) omit/recode certain values of a variable from the model, or (iii) turn to training-time and post-hoc methods for bias mitigation. Want to further explore Arthur’s bias detection capabilities? Click here to learn more.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/mining-for-proxies-in-machine-learning-systems
206,"Predicting the Future with Machine Learning
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Monitoring & PerformancePredicting the Future with Machine Learning By:Arthur TeamMarch 11, 2022The goal of machine learning is to predict the future, based on data from the past. It’s more important than ever to make predictions that match reality, but as the world changes around us, so does the data that is used to generate new predictions. Machine learning models fail silently, which means they will make predictions even if the incoming data looks nothing like the data they were trained against. They will make predictions for scenarios and situations they were never trained for. They will be inaccurate and incorrect; and worst of all, they will be confidently incorrect.And these incorrect predictions will influence business decisions, impacting both dollars and human lives.‍‍Using Ground Truth to Calculate Model Performance  The most straightforward way to measure model success in production is to compare how closely predictions match reality, using performance metrics like Accuracy, Precision, F1, Recall and more. Performance metrics provide insights into how the model is holding up in the production environment, and when the model needs re-training (or tuning). These metrics also help data scientists and model owners calculate the delta between predicted truth and ground truth. Machine learning teams rely on ground truth to test predictions that algorithms are making against the real world. No ML model guarantees 100% accuracy, but the goal is to get it as close as possible to this target. Each business area responsible for oversight of ML models in production sets their own tolerance when it comes to performance metrics. This threshold is carefully defined based on many aspects, including potential impact to revenue (both positive and negative).Depending on the use case and the nature of the data, model performance can start degrading as soon as the model is deployed to production without a monitoring solution in place. After weeks in production, if the accuracy of that model has dropped to 70%, it could be well below what’s acceptable by business leadership, causing a measurable impact to business KPIs, such as revenue. However, with an AI Performance solution like Arthur that provides continuous model monitoring, it’s easy to detect when the tolerance threshold set for model accuracy falls into an unacceptable range and correct for it. Measuring performance is straightforward if data is generated from a known model, the ground truth. However, all models trained are limited by the ground truth quality used to train and test them, and by the timing and availability of ground truth data in production.‍‍‍‍Ground Truth Challenges 1. Accounting for time series and seasonal variability‍Delayed ground truth is quite common when there’s a calendar delay between model prediction and when the ground truth information is ready. An example of this is in the financial services industry where customers have up to 3 months after a suspect transaction to flag it as fraudulent. There’s a 90 day lag; problems don’t manifest in real time when the original transaction occurs.‍2. Definitions for ground truth vary across the organization Supervised learning requires a large volume of diverse data with corresponding correct ground truth labels. Enterprise datasets are siloed in systems across the organization and often complex in nature. These systems are often not interoperable. Ground truth consistency suffers when there are missing, inconsistent or edge case annotations.     ‍‍‍3. Computer vision and NLP models require humans-in-the-loop labeling‍When you are working with model types like computer vision (CV) or natural language processing (NLP), ground truth labels are not readily available without manual annotation which requires labor-intensive processing. With NLP, you cannot always rely on the literal word meaning but someone must infer customer intent or satisfaction.  ‍‍‍‍Solving for Ground Truth ‍While some ground truth challenges must be addressed earlier in the ML lifecycle at the organizational level or during model development, accounting for delayed ground truth can be solved with Arthur’s performance technology.In some use cases, the ground truth will be available seconds after the prediction; in others, it could take months or years. Arthur’s platform allows for ground truth data to be updated at any moment, regardless of when the inference was recorded and with no data duplication. Performance metrics are then recalculated on the fly. Ground truth data can be updated individually (for every inference, as the data becomes available), or in bulk. With full support through the SDK and API, there are many different options to automate ground truth updates. Very simple scripts can be used to retrieve data from log files, databases or other sources and leverage the Arthur API to update ground truth data.‍‍Working with Models When Ground Truth Is Delayed‍In most production applications, there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. Leveraging labeling teams or services can help close this lag, but it will not completely remove it. Instead of monitoring metrics based on outputs, Arthur can alternatively monitor inputs based on data drift metrics with automating data drift thresholding. Arthur automatically creates relevant thresholds for detecting data drift, driving speed to value in optimizing ML models, while other ML observability and monitoring solutions rely on users to manually define thresholds for each attribute, which is slow and labor-intensive.  ‍“It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit fact.” - Sherlock HolmesRemember, without ground truth data, the value of your predictive algorithms can be called into question. Using Arthur with ground truth to calculate model performance or using automated data drift thresholding to combat delayed ground truth, will build greater trust in model predictions and drive better business outcomes for everyone. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/predicting-the-future-with-machine-learning-ground-truth
207,"Arthur selected to provide critical AI performance capabilities for Department of Defense
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur selected to provide critical AI performance capabilities for Department of DefenseBy:Arthur TeamFebruary 16, 2022For Immediate ReleaseNew York, NY // February 16, 2022The Joint Artificial Intelligence Center (JAIC) has selected Arthur to deliver its industry-leading AI performance management solution as part of the Test and Evaluation (T&E) Blanket Purchase Agreement (BPA,) a $249M effort to accelerate production AI capabilities across the Department of Defense (DoD).This new vehicle will allow the DOD to leverage the Arthur platform for AI monitoring and optimization to deliver better results with accuracy, explainability, and fairness. Arthur’s world-class research team has pioneered exclusive capabilities in computer vision, natural language processing (NLP), bias mitigation, and other key functional areas. With the rollout of the DoD’s Ethical AI Principles — Responsible, Equitable, Traceable, Reliable, Governable — in February 2020, ensuring the appropriate foundation is in place for production AI capabilities is paramount.The Arthur team brings considerable expertise and commitment to the federal government, having successfully worked with DARPA, the IC, NSF, NIST, ARPA-E, Treasury, Air Force, Army, and Navy.Interested federal agencies should contact PublicSector@arthur.ai for more information and to schedule a briefing.				About ArthurArthur is the AI Performance Company. Our platform measures and improves machine learning models to deliver better results. We help data scientists, product owners, and business leaders accelerate model operations to optimize for accuracy, explainability, and fairness. Arthur’s research-led approach to product development drives exclusive capabilities in computer vision, NLP, bias mitigation, and other critical areas. At Arthur, we’re on a mission to make AI work for everyone, and we are deeply passionate about building ML technology to drive responsible results.	 	 ‍About the DoD Joint Artificial Intelligence Center‍The Department of Defense (DoD) Joint Artificial Intelligence Center (JAIC) was established in 2018 to be the focal point of the DoD Artificial Intelligence (AI) Strategy to accelerate scaling AI and its impact across the Department. The mission of the JAIC is “to accelerate the delivery of AI-enabled capabilities, scale the Department-wide impact of AI, and synchronize DoD AI activities to expand Joint Force advantages.” Working closely with the Services, Combatant Commands, and other Components, JAIC identifies appropriate use cases for AI across DoD, rapidly pilots solutions, and scales impact across its enterprise. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-selected-to-provide-critical-ai-performance-capabilities-for-department-of-defense
208,"Fast Counterfactual Explanations using Reinforcement Learning
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML ExplainabilityFast Counterfactual Explanations using Reinforcement LearningBy:Karthik Rao and Sahil VermaMarch 18, 2022IntroductionCounterfactuals, an active area of research in machine learning explainability, are explanations that produce actionable steps to move a data point from one side of a decision boundary to another. These explanations have a clear use-case for several applications ranging from loan decisions (example shown below) to healthcare diagnosis.Problem: A binary classifier, being used by a financial services company, predicts if an individual should be approved (1) or denied for a loan (0). Individuals want to know how to get approved for a loan if the model predicts that they should be rejected.Counterfactual Explanation: We can now provide a series of steps (as changes in the input features) that can help an individual get approved for a loan, e.g. add $10,000 to your salary and gain 2 more years of education.In practice, we might have many more requirements of CFEs beyond just finding the other side of the decision boundary. For example, we might need the features to be constrained to only certain actionable sets, or we might need the resultant counterfactual to be realistic and similar to the training data. Recent work has summarized these various desiderata and the common research themes in the field. Additionally, we need to be able to compute counterfactuals in way that is computationally efficient for high-data-volume uses cases. In this article, we present a distributed and scalable reinforcement learning framework that can produce real-time counterfactual explanations for any binary classifier. We provide an overview of the algorithm used to find counterfactual explanations in real-time, and implementation details of how we've used Open AI Gym and Ray (RLLib) to put this work into practice.Counterfactuals OverviewWe define a counterfactual explanation (CFE) as follows: Definition. Given a model \(f\), and a data point \(x\), where \(f(x) = y\), we want to find a point \(x'\) where \(f(x') = 1 - y\) such that we minimize \(dist(x, x')\)
for any distance function \(dist\).This definition states that we want to find a point \((x')\) where our binary classifier returns the inverse of the original classification \((y)\).  This definition is also malleable, as we can model different causal relationships of features in order to produce more realistic counterfactual explanations. For example, we can define a set of immutable features which cannot be changed (such as gender and marital status), and a set of non-decreasing features (such as age and education level). We also want to find a counterfactual explanation \((x')\), such that \(x'\) is close to the original training data. Such additional constraints allow us to produce counterfactual explanations that are realistic and well-defined.One of the key shortcomings of most counterfactual approaches is that they are computed on each individual instance in a data set. This means that for any set of inferences, we must solve an optimization problem to find a counterfactual explanation that has the following properties:
Correct Output: We need the new data point \(x'\)  to have the desired output from the model (aka moving from a classification of 0 to 1).
Minimize Data Manifold Distance: We want to minimize the distance between our new counterfactual data-point \((x')\) and our original training dataset (through KNN distance).
Adhere to feature constraints: We have to respect the properties of features especially if they are immutable (cannot be changed) or non-decreasing (must only increase)
Solving a new optimization problem for each data point can be expensive, preventing us from creating real-time counterfactual explanations. Our approach creates a model that allows us to pay for this expensive training upfront for the entire dataset and produce fast inferences for any new data point that needs a counterfactual explanation.Reinforcement Learning Framework for CFEsBased on our goal of achieving near real-time explanations, we considered a reinforcement learning framework [Verma, Hines, Dickerson] that allows us to do a one-time training of the model and produce explanations in real-time. In this section, we will present a brief overview of reinforcement learning and how we applied it to counterfactual explanations.Reinforcement Learning OverviewReinforcement Learning is a machine learning framework that allows an agent to interactively ""learn"" a best set of actions for a desired state in a given environment. Reinforcement Learning has had the most success in robotics and game-playing scenarios (we have seen RL models beat the best players in the world at Poker and Go).A  few key terms will be used through this article as we model our counterfactuals problem:
Agent:
The agent operates in the entire state space (environment) for a problem. The agent aims to make a right series of actions through the state space to perform some task. It will sample the environment in order to learn the best actions in any state.
Environment: A generic set of states that produce a reward when an agent moves from state to state. The agent will only learn about the reward from the environment once it has arrived at a certain state.
Action: At any given state \(s_t\), an agent needs an action to move to state \(s_{t+1}\). The goal of any RL problem is to find a series of actions that move the agent from an undesirable state to a desirable state (by maximizing expected reward through all the states).
For the purposes of this article, we do not provide a more comprehensive overview of a reinforcement learning framework. I would highly recommend reading [this blog], which provides such an overview.Reinforcement Learning for CFEs
Our
recent work (appearing soon at AAAI) shows how to use a reinforcement learning framework to generate real-time counterfactual explanations. Let us consider a very simple dataset: \(x_1, x_2, y\). We have some binary classifier that creates a decision boundary shown in Figure 1. Our goal is to move any arbitrary point on the left side of the boundary, namely points in blue, across the boundary to be classified as the opposite class (orange points).
Figure 1: Decision Boundary for Arbitrary Dataset. Note the binary classifier is not perfect and mislabels a few points, namely the blue points on the right side of the graph.To frame this problem as a reinforcement learning problem, we define the following:
Agent: In our counterfactual scenario, our agent can be considered to be located on any point on the left side of the decision boundary (any of the blue points). The agent needs to take a series of actions (which we will define below), to move from the left side of the decision boundary to the right side.
Environment: Our environment (which represents our state-space) is defined as the entire grid (which is discretized into a finite number of states for every input in our feature space). Our environment will send a reward to agents at state \(s\). The reward function is defined as the sum of two different components:
Classification Reward:
We want to reward agents that are on the correct side of the decision boundary. Therefore, we give a high reward once an agent is across the decision boundary. We use the binary classifier’s predict function to determine if an agent is close to the boundary (if the class probability is close to the decision threshold). Therefore the closer to the decision boundary the agent is in, the higher reward it will receive and vice-versa. This value will always be between 0 and 1.
Dataset Manifold Reward: We want our final counterfactual point to be similar to the training dataset. Therefore, we give higher negative reward for points that are dissimilar to the training data. Computing this similar-ness of states can be implemented in several ways, such as auto-encoders, K-Nearest-Neighbors, etc. For this implementation we used KNN distance (normalized between 0 and 1) to measure the similar-ness of the final counterfactual point.
Action:
In our discretized environment, we allow our agent to move a small distance along any one of the feature dimensions. In our example above, an action \(a\) for an agent at state \(s\), will be a small \((±0.05)\) change for either feature \(x_1\) or \(x_2\). We opted for these small discretized movement to limit to number of movements an agent can perform at any given state. We also believe that an agent will incrementally learn the right movements in order to move towards the decision boundary.
Figure 2 shows a possible path that a point can take to find a CFE.Figure 2: Sample path that an agent can take in our environment.Open AI GymOpen AI Gym provides a framework that allows us to create an environment for an agent to interact with. This is the default standard for defining RL environments (and already comes with a set of pre-defined environments for different tasks), however you have the ability to define your own environment. In order for us to learn a model that achieves our RL task, we must fit it into the Open AI Gym framework.1. Observation Space: You must define what every state will look like for your RL environment. In Python, all observation spaces must be defined by one of the following types: Tuple, Discrete, Box, Dict, Multi-Binary, Multi-Discrete (view here).# We create an observation space as a Box (np array) with limits (-1, 1)
self.obeservation_space = gym.spaces.Box(low=np.ones(shape=len(num_features)) * -1,
high=np.ones(shape=len(num_features)))‍2. Action Space: You must also define what an action will look for any given agent. It is similarly defined by the same types as observation spaces (view here).# We create an action space as a tuple, where the first value is a feature
# index and the second value is binary (increasing or decreasing)
self.action_space = gym.spaces.Tuple((
gym.spaces.Discrete(num_features),
gym.spaces.Discrete(2)
))‍3. Step Function: We define a function that when given an action (from the action space) and a state, it will be able to return the reward produced by taking this action. It will also return the new state the agent moved because of this action (this may or may not be deterministic).def step(self, action: Tuple[int, int]) -> Tuple[tuple, float, bool, dict]:
""""""
Step function for the Gym environment
:param: action: action in the gym environment (transformed_feature_index, increase/decrease)
:return: state: an observation space defined above
:return: reward: reward for moving to that state
:return: done: (bool) is the process complete
:return: info: dict info about the environment at every step (useful for debugging)
""""""
# Get action and if we should increase or decrease
feature_index = action[0]
decrease = bool(action[1])
# Set default reward to be negative
reward = -10.0
done = False
constant_cost = 0.0
# Checks to make sure we are not changing an immutable feature
if feature_index in immutable_features:
return self.state, reward, done
# Check to make sure we are not decreasing a non-decreasing features
if feature_index in non_decreasing_features and decrease:
return self.state, reward, done
# Move the agent (X_train should be normalized between 0 and 1)
new_state = self.state
amount = -0.05 if decrease else 0.05
new_state[feature] += amount
self.state = new_state
# Compute classifier reward (if we crossed the decision boundary this number
# will be very large)
classifier_reward_, done = self.classifier_reward(self.state)
# Compute KNN reward
manifold_dist_loss =self.distance_to_data_manifold(self.state)
# Compute total reward
reward = classifier_reward - manifold_dist_loss
return self.state, reward, done, info‍4. Reset Function: Once the agent has reach a desired state (or we want to start over), we need to be able to reset to some starting state. We must define the policy for determining the starting state in the reset function.def reset(self, initial_state: pd.Series = None) -> tuple:
""""""
Reset methods for Gym environment. Called after an episode is complete or called for starting evaluation with a
specified starting initial state
:param: initial_state: initial starting state (used for evaluation)
""""""
if initial_state is not None:
# This is done for inference
self.state = initial_state
else:
# Randomly get a data point from our training dataset
self.state = self.X_train.sample()
return self.state‍We can now formally define our FastCFE (fast counterfactual explanations) class, which has all the above components defined in one python class.class FastCFE(gym.Env):
def __init__(self,
classifier_predict: Callable[[pd.Series], np.ndarray],
X_train: pd.DataFrame,
categorical_features: List[str],
immutable_features: List[str] = None,
non_decreasing_features: List[str] = None):
""""""
Initializes the Gym Environment to train the RL Model for Counter-Factual-Explanations (CFE)
(https://arxiv.org/pdf/2106.03962.pdf)
""""""
# Initialize all global class variables
self.classifier = classifier_predict
self.X_train = X_train
self.categorical_features = categorical_features
self.immutable_features = immutable_features
self.non_decreasing_features = non_decreasing_features
# Create the state and action space
# We create an observation space as a Box (np array) with limits (-1, 1)
self.obeservation_space = gym.spaces.Box(low=np.ones(shape=len(X_train.columns)) * -1,
high=np.ones(shape=len(X_train.columns)))
# We create an action space as a tuple, where the first value is a feature
# index and the second value is binary (increasing or decreasing)
self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(len(X_train.columns)),
gym.spaces.Discrete(2)))
def step(self, action: Tuple[int, int]) -> Tuple[tuple, float, bool, dict]:
""""""
Step function for the Gym environment
:param: action: action in the gym environment (transformed_feature_index, increase/decrease)
:return: state: an observation space defined above
:return: reward: reward for moving to that state
:return: done: (bool) is the process complete
:return: info: dict info about the environment at every step (useful for debugging)
""""""
# Get action and if we should increase or decrease
feature_index = action[0]
decrease = bool(action[1])
# Set default reward to be negative
reward = -10.0
done = False
constant_cost = 0.0
# Checks to make sure we are not changing an immutable feature
if feature_index in immutable_features:
return self.state, reward, done
# Check to make sure we are not decreasing a non-decreasing features
if feature_index in non_decreasing_features and decrease:
return self.state, reward, done
# Move the agent (X_train should be normalized between 0 and 1)
new_state = self.state
amount = -0.05 if decrease else 0.05
new_state[feature] += amount
self.state = new_state
# Compute classifier reward (if we crossed the decision boundary this number
# will be very large)
classifier_reward_, done = self.classifier_reward(self.state)
# Compute KNN reward
manifold_dist_loss =self.distance_to_data_manifold(self.state)
# Compute total reward
reward = classifier_reward - manifold_dist_loss
return self.state, reward, done, info
def reset(self, initial_state: pd.Series = None) -> tuple:
""""""
Reset methods for Gym environment. Called after an episode is complete or called for starting evaluation with a
specified starting initial state
:param: initial_state: initial starting state (used for evaluation)
""""""
if initial_state is not None:
# This is done for inference
self.state = initial_state
else:
# Randomly get a data point from our training dataset
self.state = self.X_train.sample()
return self.state‍Training the Model using Ray + Rllib
Now that we have created our environment, actions, reward, state-space, and have properly defined our OpenAI Gym environment, we must now produce a model that produces real-time counterfactual explanations. We must produce an optimal policy,
\({\pi}(s)\), which will produce an action for an agent in state \(s\) that will maximize future expected reward (reaching our desirable state across the decision boundary). In reinforcement learning, there are several ways to find the optimal policy
\({\pi}\), ranging from model-free to model-based optimization techniques. We recommend this link to review more about training reinforcement learning algorithms.
Ray and Rllib
We were focused on finding an optimization algorithm and framework that is fast, scale-able, and easy to use. Much of the recent work has been focused on Distributed Deep Reinforcement Learning, which uses neural networks to implicitly learn the optimal policy \({\pi}\). One such framework is [Ray + Rllib]:
Ray is a new distributed framework in python designed to distribute training tasks across any cluster. Rllib is a specific package within Ray that is designed to train different RL agents for different environments. Rllib has a variety of different optimization algorithms, and provides a configurable dictionary that allows us to distribute training and evaluation to different cores and machines very easily. Furthermore, it provides an easy API that allows use to modify the internal of the Deep RL Optimizer. Rllib is maintained by Anyscale (founded out of Berkeley RiseLab) and it one of the state-of-the art frameworks for distributed computing/machine learning.We opted to use the Proximal Policy Optimizer (a Deep RL Algorithm) because of its favorable balance of faster training times and simplicity. We needed an algorithm that would train relatively fast and that could distribute fairly simply, both of which PPO provides out of the box. We provide our pseudo-code that provides a FastCFE specific wrapper around a native Rllib PPO optimizer.For our implementation, we opted to use the Proximal Policy Optimizer (a Deep RL Algorithm), which outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time [Schulman et. al]. Rllib provides an out of the box way to use PPO and distribute it across a cluster and on your laptop. Below, we showcase some pseudo for how we wrapped our optimizer around Rllib to provide an easy package to use Rllib and our FastCFE model:class RayLib():
def __init__(self, env: FastCFE = None) -> None:
""""""
Initializes a RayLib optimizer for PPO (can add functionally for other optimizers)
:param: env: the FastCFE class which contains our gym environment
""""""
if env is not None:
super().__init__(env)
self.env = env
self.model: Optional[ppo.PPOTrainer] = None
self.config: Optional[Dict[str, Any]] = None
def train(self) -> None:
""""""
Train the RL Agent for the CFE environment (with hyperparameters)
View more of the features here: (https://docs.ray.io/en/master/rllib-training.html)
""""""
hyperparameters = ppo.DEFAULT_CONFIG.copy()
# Register the CFE environment to be trained in Ray
def env_creator(env: FastCFE):
return env
register_env(""my_env"", env_creator)
# Set the environment config variables
hyperparameters['env_config'] = self.env
# Create the PPO Trainer with the configuration and train
self.model = ppo.PPOTrainer(env=""my_env"", config=self.config)
print(""Training RL Agent....."")
for i in range(10):
result = self.model.train()
print(f""Training Iteration: {i+1}/{num_epochs}"")
print(f""Total number of datapoints sampled: {result['episodes_total']}"")
print(f""Mean Reward (Max of 100): {result['episode_reward_mean']}"")
print(f""Avg. steps to produce counterfactual (Min: 1, Max: 20): {result['episode_len_mean']}"")
print(""_______________________________________________________________________________________"")
return None
def predict(self, initial_state: pd.Series, max_steps: int = None) -> Tuple[List[pd.Series], float, bool]:
""""""
Make a counterfactual prediction
:param: initial_state: initial_state
:param: mode: scaled or unscaled path
""""""
if self.model:
max_steps = 50
num = 1
next_state: Tuple = self.env.reset(initial_state=initial_state)
done: bool = False
reward: float = 0.0
steps = []
while num < max_steps:
action = self.model.compute_action(next_state)
next_state, reward, done, info = self.env.step(action)
steps.append(next_state)
if done:
break
num += 1
return steps, reward, done
else:
raise ValueError(""Model not loaded"")BenchmarksWe want to showcase two major benchmarks for this algorithm and implementation:Performance Metrics: We want to understand how our FastCFE approach using RL compares against other methods for performance metrics (described later).Training Time: We want to see how much savings we achieved when using a Distributed RL Framework like Ray against a single-threaded research implementation.These benchmarks for implemented for a variety of combinations of the following credit risk datasets. All of these datasets contained some from of credit risk data and the models are binary classifiers that predict a single applicant (one row) should be accepted or rejected for a loan. We want to find counterfactual explanations for all the rejected applicants. The sizes of the datasets are shown below (number of rows by number of columns):German Credit: 1,000 data-points x 21 featuresAdult Credit: 48,842 data-points x 14 featuresCredit Default: 30,000 data-points x 25 featuresSmall Encoded Credit Dataset: 332,000 data-points x 44 featuresLarge Encoded Credit Dataset: 332,000 data-points x 652 featuresThe first three datasets (German Credit, Adult Credit, Credit Default) are all open source datasets with the links provided above. The last two datasets are proprietary datasets with obfuscated column names. These datasets were larger and tested the scaleable of our implementation.Performance MetricsWe want to see how our FastCFE model compares to other well-known methods. Specifically, we are focusing on the following two metrics:
Validity: This is the total number of counterfactual explanations found divided by the total number of data points. This is represented as a percentage.
Mean Inference Time: This is the mean time it takes to calculate a batch of inferences. Namely is the time it takes to compute \(n\) inferences divided by \(n\).
The results shown below are FastCFE against a number of state of the art counterfactual explanation methods:Adult Credit
Validity(%)
Mean Inference Time (s)
Dice-Genetic
98.1
1.71
Dice-Random
100
0.17
MACE LR
100
38.45
MACE RF
100
101.29
FastCFE
97.3
0.07
German Credit
Validity(%)
Mean Inference Time (s)
Dice-Genetic
89.5
3.45
Dice-Random
100
0.22
Dice-Gradient
84
59.75
FastCFE
100
0.015
Credit Default
Validity(%)
Mean Inference Time (s)
Dice-Genetic
92.6
3.58
Dice-Random
100
0.39
Dice-Gradient
81.0
479.17
FastCFE
99.9
0.051
As we see here, we perform nearly as close as the best method (Dice-Random) across these three different datasets. Furthermore, we have inference times of up to 20x faster than Dice-Random.Training TimeThe first implementation of this project was done using a package called StablesBaseline3, and was naively computed trained on a single machine. This section wants to showcase the change in training time of our scaleable and distributed Rllib implementation against a naive implementation. The results are shown below:
Rllib Train Time (hrs)
Naive Train Time (hrs)
German Credit
.25
1.5
Credit Default
1
6
Small Encoded
1.5
8
Large Encoded
8
DNF
We achieve a nearly 6x savings in train time and can handle much larger datasets than we could through our naive implementation. This shows the promise and power of using a scaleable and distributed reinforcement learning framework — we can significantly reduce training times which is a major bottleneck for several reinforcement learning applications.ConclusionWe hope this article provided an overview into the following ideas/concepts:Counterfactual Explanations: What they are and how they are useful for industrial and explainability applicationsReinforcement Learning Implementation: How we implement a production level reinforcement learning model.Power of Distribution: How we can achieve tremendous savings by using scalable and distributed reinforcement learning frameworks such as Rllib.We hope that we provided some interesting ideas and some starter code to help you make your own Reinforcement Learning model. If you would like to learn more about this article, please reach out!‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/fast-counterfactual-explanations-using-reinforcement-learning
209,"Two Commitments Every Employer Should Make in 2022
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedLife at ArthurTwo Commitments Every Employer Should Make in 2022By: Adam WenchelJanuary 27, 2022“I really want to work at a startup. But, I care about my mental health.”If you want to see an animated reaction, make a comment like that to someone on our team. Yes, startups are hard work; audacious goals always are. But they don’t have to come at the expense of anybody’s well-being.The last couple of years have been challenging. People are reassessing their priorities, craving meaning in relationships, in free time pursuits, and importantly, in our work. At the same time, people want to feel supported – and employers need to step up and use their unique position to create community and care for people more deeply than they ever have before. At Arthur, we commit to both: fulfilling and challenging work at a company that provides community and care. People join Arthur because they are excited to solve complex problems at the intersection of technology and humanity, and to do it as part of a diverse, creative, and passionate team. The same team that is also raising children, caring for loved ones, taking classes, running marathons, exploring cities, and doing a myriad of other things that make us each happy and whole. It’s awesome to see both our work and our community get strengthened as our team grows and people bring new and unique passions with them!This belief that we need both fulfilling work and genuine community centers our culture at Arthur and we feel the impact of it every day. Here are 7 ways we are creating meaning, connection, and caring at Arthur:Work that has a huge impact on the world. We’re working to make AI more transparent and equitable, changing how technology affects real humans for the better.Flexible work locations. We have open offices for those that want to safely work and hang in the same room; and we support fully remote work across the country.A diverse team that cares & celebrates. We practice empathy, kindness, and inclusion in our work together. We shout out each other’s wins and take the time to help each other solve problems.Team offsites. We meet somewhere cool as a company twice a year for fresh air and time to be creative, think big-picture, and have some laughs together. A learning mindset and budget. Our work on the cutting edge of AI technology keeps us learning all the time, and we offer a generous education and professional development budget. True time away. Our flexible vacation policy includes mandatory minimums, and we fully close the office over the holidays. We also believe in “no weekend messages” and encourage “roam the earth” remote summer work to explore our awesome planet.  A big vision – and the chance to own a piece of it. There’s a ton of opportunity for ownership and growth along the road to achieving our big goals for Arthur. Every member of our team is critical in delivering on that vision.It’s neat to see word spread about the positive culture we are creating – we’ve recently been named one of Business Insider’s 2022 Enterprise-Tech Startups to Bet Your Career On and we made Built In NYC’s Best Places to Work.It’s an important formula that has an outsized impact – care about people, create connection, do meaningful work and build something really special in the process.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/i-really-want-to-work-at-a-startup-but
210,"Built In Honors Arthur in Its Esteemed 2022 Best Places To Work Awards
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesBuilt In Honors Arthur in Its Esteemed 2022 Best Places To Work AwardsBy:Arthur TeamJanuary 5, 2022Arthur Earns 15th Place on Built In’s List of 50 Best Small Companies in New York City‍For Immediate Release New York, NY // January 5, 2022 — Built In today announced that Arthur is a winner in their 2022 Best Places To Work Awards. Specifically, Arthur earned a place on the 50 Best Small Places to Work in NYC and 50 Best Paying Companies in NYC lists. The annual awards program includes companies of all sizes, from startups to enterprise, and honors both remote-first employers as well as companies in the eight largest tech markets across the U.S.“We’re thrilled to be recognized alongside the best companies in New York,” says Adam Wenchel, Arthur’s Co-Founder & CEO. “I’m constantly inspired by our team’s focus on our mission to make AI work for everyone. We’ve clearly seen and felt the challenges the last two years have presented for people – and so creating a caring culture built on integrity, equity, diversity, and compassion is the best thing we can do for all of us. 2022 will be a big year for us and we are excited to welcome many new voices!” Check out open roles at Arthur here. Built In determines the winners of Best Places to Work based on an algorithm, using company data about compensation, benefits and companywide programming. To reflect the benefits candidates are searching for more frequently on Built In, the program also weighs criteria like remote and flexible work opportunities, programs for DEI and other people-first cultural offerings. “It is my honor to extend congratulations to the 2022 Best Places to Work winners,” says Sheridan Orr, Chief Marketing Officer, Built In. “This year saw a record number of entrants — and the past two years fundamentally changed what tech professionals want from work. These honorees have risen to the challenge, evolving to deliver employee experiences that provide the meaning and purpose today’s tech professionals seek.”‍ABOUT BUILT INBuilt In is creating the largest platform for technology professionals globally. Monthly, more than three million of the industry’s most in-demand professionals visit the site from across the world. They rely on our platform to stay ahead of tech trends and news, develop their careers and find opportunities at companies whose values they share. Built In also serves 1,800 innovative companies of all sizes, ranging from startups to the Fortune 100. By putting their stories in front of our uniquely engaged audience, we help them hire otherwise hard-to-reach tech professionals, locally, nationally or remotely. www.builtin.com  ABOUT BUILT IN’S BEST PLACES TO WORKBuilt In’s esteemed Best Places to Work Awards, now in its fourth year, honor companies across numerous categories: 100 Best Places to Work, 50 Best Small Places to Work, 100 Best Midsize Places to Work, 50 Companies with the Best Benefits and 50 Best Paying Companies, 100 Best Large Companies to Work For, and 100 Best Remote-First Places to Work. ABOUT ARTHURAt Arthur, we are deeply passionate about building technology to make AI work for everyone. Arthur is the AI performance company that delivers on the full potential of equitable AI for enterprise customers through performance monitoring and optimization, explainability, and bias detection. Learn more at www.arthur.ai Contacts:  Tory Marlin, Director of Marketing  tory@arthur.ai  PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-named-a-best-place-to-work-in-2022
211,"A Crash Course in Fair NLP for Practitioners
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessA Crash Course in Fair NLP for Practitioners By:Jessica DaiNovember 14, 2021The topic of fairness in Natural Language Processing (NLP) is exceptionally broad; in this post, we hope to distill some of the key points from academic literature for an audience of technical practitioners. In section 1, we outline a general framework for thinking about fairness; in section 2, we survey some notable academic work; and in section 3, we outline a set of questions that may be useful when considering specific applications.‍1. What does fairness in NLP even mean?The core idea we hope to illustrate in this post is that there is no panacea for magically achieving ""fair NLP"" — though many of the core problems are intuitive, the complexity of human language means that measuring, much less mitigating, ""unfairness"" is a difficult task. ""Bias"" and ""fairness"" are exceptionally broad terms that span wide range of possible behaviors. There is no single definition of desirable ""fair"" behavior; to the extent that NLP systems model the world (or a particular worldview), there is no single perfectly neutral, unbiased model. In other words, any NLP system involves some proposition about both what the world does look like and what the would should look like; for practitioners, it's critical to think deeply and with precision about what exactly desired behavior looks like, and why.That being said:The first component of any approach to fairness is defining who and what exactly we want to be fair with respect to. While social groups in the real world are fluid, in Artificial Intelligence (AI) and Machine Learning (ML) we typically define discrete groups along the axes of gender, race/ethnicity, and religion. (Fair ml, especially in language, unfortunately tends to treat gender as binary — both because it is mathematically convenient and because the data on which language models are trained often reflect a binary.)For fairness in a typical tabular data setting, we generally assume that each data point reflects information about a single person, and that the values of these demographic attributes are generally known or accessible for each datapoint. For fairness in NLP, however, there isn't always a clear mapping between text and demographic information. More specifically, social groups might be inferred, but demographic information may be labelled based on author demographic — that is, text generated by particular groups, which covers things like dialect, accent, or writing style — or subject demographic — that is, text about particular groups. Crucially, author and subject demographic are distinct approaches to defining fairness. Which method to use for demographic labelling is context-dependent and varies based on the task at hand.Similarly, there are many dimensions in NLP settings across which fairness can be measured. When the end-goal of the NLP model is something like classification or regression, we might be able to apply existing metrics for fairness in these applications by measuring the group-conditional performance (e.g. positivity rate, TPR, FPR, etc.). In language, particularly text generation, additional harms arise — most prominently, language models which propagate harmful societal stereotypes. Measuring stereotypes is a murkier task: existing (academic) approaches have focused on either investigating the trained model artifact itself (i.e. the word embeddings), or evaluating the model outputs on some specially-curated datasets. However, both of these approaches have known issues, and should not be considered to be any conclusive or concrete standards.‍2. A (non-exhaustive) survey of relevant workOrdered by year. Starred entries are worth reading in full!1. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings (2016)‍One of the first works on ""bias"" in language models. Measures/illustrates bias by using the word embeddings to generate analogies via vector addition/substraction, showing that embeddings confirm stereotypes; demographic groups are therefore determined with respect to text content. Debiasing approach involves identifying the ""gender subspace"" and rotating the word embeddings such that they are orthogonal to the subspace.2. Gender and dialect bias in Youtube automatic captions (2017)‍This is an illustrative example of the many moving parts in what is casually referred to as ""fair NLP"". This is a speech-to-text task: i.e., one where the output itself is text. However, there is some notion of performance that summarizes the goodness of the text output in a single number. Demographic groups are determined with respect to speaker (author), not content. Bias is observed here because of differential model performance across groups.3. Social Bias in Elicited Natural Language Inferences (2017)This work analyzes a dataset that was popular at the time (the Stanford Natural Language Inference corpus; with the rise of larger language models trained on the web it's unclear the extent to which this is still used). The approach is similar to ""Man is to Computer Programmer"" — they use a mathematical measurement of similarity between words (in this case mutual information) to find (gendered) associations across words, with demographic groups determined by text content. In my opinion, this does exhibit some of the pitfalls outlined in ""Language (Technology) is Power"" — there is no explicit discussion of what comprises a harmful association.4. Neural User Factor Adaptation for Text Classification: Learning to Generalize Across Author Demographics (2019)Here, demographic groups are determined via authorship rather than text content — this work explores gender, age, country, and region; finds the existence of performance disparities across groups; and introduces a novel approach to learn text classifiers which reduce those performance disparities.5. Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings (2019)An extension of the 2016 ""Man is to Computer Programmer"" paper to the multiclass setting; the original work made use of ""binary"" gender in calculating a ""gender direction/subspace"".6. Mitigating Gender Bias in Natural Language Processing: Literature Review (2019)A lit review of approaches to gender bias x NLP (at this point a few years old); mostly useful for a high-level overview of many possible tasks and approaches.‍7. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them (2019)This work shows limitations of the approaches to debiasing word embeddings in ""Man is to Computer Programmer"" and ""Black is to Criminal"". In short, while the approaches enumerated in those papers do successfully debias with respect to their original definitions of ""bias,"" they ultimately preserve most relationships between words in the corpus: ""gendered"" words still cluster together. As a result, it is possible to recover the original ""biased"" relationships, and they may persist in downstream applications of those embeddings even if not detected according to the original metric. This work is a clear example of why ""debiasing"" in general but especially in language must be evaluated with a critical/skeptical eye, and why specifications of desired ""unbiased"" behavior must be careful and precise.8. **Language (Technology) is Power: A Critical Survey of ""Bias"" in NLP (2020)This survey paper is worth reading (or at least skimming) in full. This paper is motivated by the idea that there is no single definition of ""desirable behavior,"" and no such thing as a ""completely unbiased"" model or dataset; instead, any specification of desired behavior is inherently value laden. The survey conducted of work on ""fairness/bias in NLP"" finds that most such work does not state clearly what comprises ""bias"" and how to conceptualize algorithm behavior with respect to broader societal power structures — to whom the harm is done and how those groups are defined; whether the harm is primarily representational or allocational; what behavior is deemed harmful and what is not, and why — and more.9. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models (2020)Introduces a dataset for evaluating the performance of masked language models (models trained on data like This is a masked sentence; the [MASK] is to determine what word is behind the masked token. — the paper reports results on BERT and BERT+ models). Several axes of discrimination/bias are included — gender, race, sexual orientation, nationality, religion, age, dis/ability, appearance, and socioeconomic status.10. REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models (2020)This paper focuses on toxicity, specifically the generation of toxic (racist, sexist, etc) text by pre-trained language models. This is a slightly different paradigm than the typical ""bias"" approach — rather than considering harms against specific groups, this work groups all harmful/derogatory generated text as ""toxic."" The authors find that even surface-level innocuous prompts can trigger highly problematic output, and that existing methods are insufficient to prevent this; upon inspection of training corpora, they find high volumes of toxic content in the training data.11. **On the Dangers of Stochastic Parrots: Can Language Models be too Large? (2020)This is the infamous Stochastic Parrots paper that ultimately led to the ousting of Drs. Timnit Gebru & Margaret Mitchell from Google Research. This is a broader survey paper about the harms of large language models, including the centralization of power, cultural homongenization/flattening, environmental harms, among others. Worth a read for broad context in responsible NLP.12. REDDITBIAS: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models (2021)Introduces a dataset for evaluating the bias of conversational language tasks based on Reddit data, as well as evaluation frameworks for conversational model performance after debiasing — includes four axes (gender, race, religion, queerness). They benchmark DialoGPT and several debiasing approaches on this dataset and find evidence of bias with respect to religion that can be mitigated with some methods (though not all).13. Rooting Out Anti-Muslim Bias in Popular Language Model GPT-3 (2021)This is a blog post (with a link to the full paper) summarizing some explorations of anti-Muslim bias in GPT-3's text generation. In short, GPT-3 exhibits substantial anti-Muslim bias in its generated text, which is only slightly reduced in existing mitigation methods.14. **Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets (2021)This paper surveys existing benchmark datasets for evaluating ""fairness"" in NLP tasks. The authors — who also wrote the 2020 ""Language (Technology) is Power"" paper — apply a social science approach (""measurement modeling""), and find that the benchmark datasets themselves have unclear definitions and specifications behind both what constitutes ""biased"" or ""stereotyping"" behavior and what constitutes desirable model behavior. If evaluating models on the datasets covered in this paper, the results should be taken with a grain of salt. Worth a skim, especially the illustrative example on the first page, to understand the gist of the criticism.15. Challenges in Automated Debiasing for Toxic Language Detection (2021)This paper focuses on text classifiers, specifically toxicity detection. Demographic groups are explored both in terms of text content (swear words, slurs, identity mentions) and text authorship (AAVE dialectical markers). Bias here is defined by the unjustified flagging of toxic text (in conventional classification terms, high false positive rates). The authors find that existing methods are generally unsuccessful in debiasing toxicity detectors, and propose a proof of concept approach which synthetically relabels the training data; this approach (modifying the training data) is more effective than attempting to modify a pretrained model.'‍Additionally, some scholars who actively work in this area are Jieyu Zhao, Su Lin Blodgett, Amandalynne Paullada; their future work is likely to be relevant as well.‍3. A worksheet for practitioners1. Defining the language task and model setting:‍Assuming the model takes in some amount of text, does it generate a single output (e.g. a probability, a classification, or multiple classifications), or text output?2. Defining the sensitive attribute:Do you care about author demographic or subject demographic, or both?Are you able to come up with or access sensitive feature values for each data point? For example, can you come up with a vector that looks like [ <string input>, <demographic info> ]?3. Defining and measuring the harm:If the model generates a single output, we can check typical measures of fairness (disparate accuracies or TPRs or FPRs or positivity rates etc).If the model generates text output:‍— Is there any notion of performance that is used to measure the ""goodness"" of the text output? You may be able to measure the performance of the generated text and determine whether there are group-wise performance disparities if you already have a means for evaluating generated text.‍— What sorts of representational or stereotyping harms do you anticipate? In other words, what is the best-case expected output, and what does a ""bad"" output look like?4. Mitigating the harm:If the model generates a single output: existing classification/regression postprocessing approaches to fairness may be worth attempting, though they will be limited in that they cannot make use of the the text input. See annotated bibliography for some examples of bias work in text classification.If the model generates text output:— For concerns around representational harms (such as stereotyping), do you have a sense of what prompts might trigger ""bad"" output?— Most mitigation techniques for language models rely on adjusting model internals, and even then, have varied degrees of success (see annotated bibliography).‍Everything you need to know about model monitoring for natural language processing. Discover Arthur’s features for ensuring Fairness, including detecting and mitigating bias in NLP.‍Photo by Jason Leung on UnsplashPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/crash-course-in-fair-nlp-for-practitioners
212,"Hotspots: Automating Underperformance Regions Surfacing in Machine Learning Systems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringHotspots: Automating Underperformance Regions Surfacing in Machine Learning SystemsBy:Kenneth ChenNovember 1, 2021Isolating Problematic Data for Remediation and Retraining ML modelsWhen a system has high dimensional data, troubleshooting the right data input regions becomes a difficult problem. Hotspots automates identifying regions associated with poor ML performance to significantly reduce time and error of finding such regions.OverviewWe might have a ML model deployed in production and some monitoring in place. We might notice that performance is degrading from classic performance metrics or from drift monitoring combined with explainability techniques. We’ve identified that our model is failing, and the next step is to identify why our model is failing.This process would involve slicing and dicing our input data that caused model degradation. That is, we want to see which particular input regions are associated with poor performance and work on a solution from there, such as finding pipeline breaks or retraining our models on those regions.This basically boils down to a time-consuming task of finding needles in a haystack. What if we could reverse engineer the process and surface all of the needles, i.e. input regions associated with poor performance, directly to the user?We can! The steps we’ll take are(1) Train a decision tree on a proper partitioning objective.(2) Create and store hotspot tree artifact.(3) Retrieve hotspots from the hotspot tree at query time.Fig1: A general framework for training dataset and using it to create hotspot tree, which is then queried to retrieve hotspots. Each phase outputs the artifact that is used in part I of the following row.In the toy example below that we’ll use throughout this post, we have two ground truth regions separated by a parabolic function, with blue above and red below the parabola. The color of the datapoints represent the predictions. We want to isolate the hotspot regions where the prediction color does not match the region color, which we do so in the pale boxes for two different accuracy thresholds.Fig2: Example of hotspots retrieval on two inputs. The boxes are the hotspots, where blue points in red regions and red points in blue regions are incorrect classifications. As the threshold (accuracy, in this case) decreases, the algorithm targets node candidates with more incorrect classifications and the hotspot regions are narrower/purer.(1) Train a decision tree on a proper partitioning objectiveDecision TreeAs soon as we think about partitioning data into regions of interest, we should think about tree models, and specifically a decision tree. Remember that our task is ultimately an inference task and not a prediction task, so there is no need to use an ensemble of trees model like random forests or XGBoost because (a) we’re not trying to perform predictions and (b) ensembles introduce noise and non-deterministic decision paths for splitting our data.Recall that the premise of decision tree splits are based on selecting a feature and split value, among an enumeration over input features and their possible values, that minimizes impurity to create children that are more pure, based on the output labels.In simple speak, let’s say output was color and we have blue and red marbles. All of the marbles have varying diameters across both groups, but blue marbles are textured while red marbles are smooth. If we had to choose between diameter size or texture to partition our marbles, we’d choose to partition our marbles based on texture, i.e. textured or smooth, since that would perfectly separate blue marbles into one group and red marbles into another, effectively reducing impurity to 0 in each group in this case.In reality, a dataset would need multiple splits in order to reduce impurity to 0 in the leaf nodes.Partitioning ObjectiveSo what is exactly the equivalent of the the blue and red marbles example above? We ultimately want to separate the bad predictions from the good predictions and need some metric as the output, i.e. partitioning objective, in our decision tree.For classification, we can encode correct classifications as 1s and incorrect classifications as 0s. If we want more granularity while partitioning, we could also encode classifications as 1, 2, 3, or 4 for true positive, false positive, true negative, and false negative, respectively.For regression, we actually need to encode the regression outputs, i.e. RMSE between ground truths and predictions, as classification outputs, e.g. a datapoint is encoded as 1 if RMSE is greater than median RMSE + 2 median average deviation of RMSE and 0 otherwise. We could also use a percentile rule, e.g. datapoints with RMSE over the 80% percentile are 1s and 0s otherwise. The reason why we do not use mean and standard deviation is because those values are skewed by high RMSE outliers, and the entire point is to partition datapoints with high RMSE compared to the average. This mimics the behavior we want in the classification case, and we’ll dive more in the mathematics for why we cannot use regression outputs, after the methodology is explained for the classification case.(2) Create and store hotspot tree artifactIf we feed the 500 datapoints from our toy example into a decision tree, using our encoding method discussed above for four classes, the tree looks like this:Fig3: Example of [IV. Trained Decision Tree] on two inputs, X[0] and X[1]. The boxes are nodes, where each node has an associated set of metrics (i.e. accuracy, precision, recall, F1 score) that can be used as thresholds in querying for hotspots. Each node represents a collection of datapoints, which are filtered by the accumulated ranges on the input features on the path from the top root node to the particular node in question. A hotspot is a node that violates the threshold specified by a user when [III. Extract Hotspot Nodes] step is performed. Hotspots can be any node in the tree and not only the terminal leaf nodes at the bottom of the tree.Here, we only have two features: X[0] and X[1]. At each node in a decision tree, the data is split into two children nodes based on a feature and cutoff value. As an example, at the root node, the 500 datapoints are split into two groups, a left child group and right child group, where X[1] ≤ -2.258 and X[1] >-2.258, respectively. We can accumulate the rules along any particular path from the root node to any child node.We can also compute performance metrics like accuracy, precision, recall, and F1 on the data in the node.(3) Retrieve hotspots from the hotspot tree at query timeNow that we have our hotspot tree, let’s pick some hotspots! Notice in Fig2 that we have accuracy thresholds of .3 and .5. In the latter case, the hotspot regions are wider and accidentally capture more correctly classified points. As the threshold decreases, we are less tolerant with wider regions that accidentally capture correct classifications. Normally, we might think that we always want lower thresholds to capture only misclassified datapoints, but that does run the risk of (a) making the regions incredibly small and not interpretable and (b) isolating many regions that might not contain many datapoints, requiring lots of manual work to investigate.So how were those hotspots retrieved?When an accuracy threshold is sent in a query to extract hotspots from the hotspot tree, we traverse along all possible paths from the root node. At any node in the traversal, if a node violates the threshold, that node is defined as a hotspot and all of the information in that node is appended to a list of hotspots returned to the user.Specifically, in our example, accuracy is our metric. At any particular node, if that node’s accuracy is less than the threshold, we know that the datapoints in that node collectively have violated the threshold and that node is a hotspot. Whenever a node is identified as a hotspot, the traversal along that path stops, since downstream nodes would be more pure and the nodes with poor performance are in even more extreme violation of the user-provided metric and threshold.Of course, what’s great about defining the metric and threshold at query time is that a user can requery with different metrics and threshold combinations, depending on the question and tolerance level for the threshold.Hotspots Found!Using our example with the .5 threshold, our three hotspots areHotspots contain the filters on the input data regions that can be applied in fetching the entire dataset for further examination, research, and model development.‍Deep DiveAs promised, answers to the hanging question about regression above, plus some deeper aspects!Deep Dive: (1) Train a decision tree on a proper partitioning objectiveSo why not regression?The naive setup would be to take the RMSE between ground truths and predictions in our regression task and use them as our partitioning objective. However, the regression criterion to split the data in a node is based on some distance metric, e.g. MSE, to the mean output value of the data the node, which in this case are the RMSE values themselves. When we traverse the hotspot tree, we’d flag nodes with MSE above a certain threshold as hotspots.Let’s say we have a majority of RMSE values around . 05 and a few RMSE values around .10, where we want to flag and isolate the latter. It’s totally possible that a regression tree might group more of the .10 RMSE datapoints lower in the tree and we’d never be able to find them according to the stopping role once a hotspot is detected. For example, one path might be .06 MSE, .09 MSE, .04 MSE. Given a user threshold between .06 and .09 MSE, we would never reach the third node in this case since .09 > .04, and that third node could contain a lot of .10 RMSE datapoints (the low .04 value comes from the fact that there are many .10 RMSE datapoints and only a few .05 RMSE points, so the node is “pure” in the sense that the RMSE values are close together).We could fix this with a custom regression criterion that uses distances against 0, i.e. the raw RMSE values, instead of the MSE distance against the average RMSE in a node, but that introduces an issue of standardization across models and data rollups. What if a RMSE value of .10 really isn’t bad for one model but is for another? As such, it’s more robust to convert the regression setting into a classification setting.Data RollupIf we have a streaming (or batch) model, how much data do we put into creating a hotspot tree at one time? If we created a hotspot tree on last week’s (or a previous batch’s) data and now see incoming data, do we append those datapoints and retrain the previous tree or do we create a new hotspot tree for this week’s (or current batch’s) data?There’s certainly no wrong way to implement this, but at Arthur AI, we take the latter approach. If we have an orange tree and now have some apples that arrive in a box, we’ll probably be interested in the apple tree that generated those apples to figure out why we received some rotten apples, not the orange tree that we’ve already inspected last week (or batch).Metadata Galore!Notice how the inputs to the decision tree can really be anything, including metadata not used as inputs to the model that created the original predictions! This means surfacing insights via hotspots is not constrained only to model inputs, which can have benefits if we are tracking sensitive non-input attributes in a model, like race or gender.Deep Dive: (2) Create and store hotspot tree artifactWhy all the metrics?Accuracy is not king and users need different metrics depending on the task their ML model is solving. This is a good introduction to other performance metrics like precision and recall. After giving it a read, here are two practical examples that helped me when I first learned about precision and recall.(1) Consider an anomaly detection system for a bot that scrapes financial data from financial reports. It would be terrible if we have many false positives from a ML model, which means that the model would predict that wrong information (positives) exists for many documents that actually are correct (false), leading to a remediation team wasting time on tasks that are not incorrect (and it really wouldn’t be highly problematic if some false negatives slipped through the cracks). In this case, high precision is important.(2) Consider a cancer detection model. It would be terrible if we have many false negatives from a ML model, which means that the model would predict no cancer (negatives) for many people who indeed have cancer (false), leading to a lack of recommended treatment and further health complications for those individuals. In this case, high recall is important.Multiclassification and MicrometricsHow do we generalize to multiclassification tasks? Accuracy is the still the same, and we can use weighted precision, recall, and F1 Score, weighted by ground truth size, under global metrics in the toy example below. Notice how instead of just macrometrics, we also can now define micrometrics for precision, recall, and F1 Score, with respect to each ground truth class. This can be powerful, as now hotspot trees can be traversed on specific classes if, e.g. the bird class is causing a lot of model failures for an object detection model and we want to figure out what’s going on for bird images in particular.Fig4: A Multiclassification Example‍‍ConclusionThat’s it for today! Hope you gained some insights about how to implement hotspots surfacing for your particular use case! We implement these kinds of systems at Arthur, and automation is an important product category in ML monitoring for the customer experience.Happy monitoring!Photo by DESIGNECOLOGIST on UnsplashHeader Photo by Nareeta Martin on UnsplashPREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/hotspots-automating-underperformance-regions-surfacing-in-machine-learning-systems
213,"Automating Data Drift Thresholding in Machine Learning Systems
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringAutomating Data Drift Thresholding in Machine Learning SystemsBy:Kenneth ChenNovember 1, 2021Monitoring Production ML Model Inputs Practically and Efficiently in the Absence of Ground Truth DataIn real life ML monitoring applications, we want to detect whether or not a ML model is performing well or is breaking. Not picking up on poor model performance can translate into poor or even biased predictions that can lead to lost revenue and even PR fires that we see year over year from even the large tech companies, from facial recognition systems that fail to pick up on certain minorities to hate speech in autocomplete for search engines.In this article, we’ll go over a brief overview of how data drift thresholding helps capture poor model performance, and the majority of the post will focus on two versions of implementing automated data drift detection in production level ML monitoring systems.OverviewWhy do we need data drift?The standard approach to monitoring models is based on performance metrics, i.e. accuracy/precision/recall/f-score, over some time period or from batch to batch. In order to produce these metrics, we need both predictions and ground truth labels for the datapoints, e.g. a credit risk model predicts that a person will pay their loan on time in 1 year and thus should be approved for a credit card, and we know whether or not that person paid their loan on time in 1 year. Already, we have an issue because we do not know ground truth until 1 year later.In most production applications, there is a lag between prediction time and ground truth collection time, which significantly handicaps the ability to remediate model issues quickly. Leveraging labeling teams or services can help close this lag, but it will not completely remove it. As such, instead of monitoring metrics based on outputs, we can instead monitor inputs based on data drift metrics.What is data drift?Data drift fundamentally measures the change in statistical distribution between two distributions, usually the same feature but at different points in time. As an example, in the univariate case where we’re looking at one input feature, we should reasonably expect that, if the shape of the feature shifts significantly between training time and prediction time, the model outputs will degrade in quality.As a toy example, if we train a ML model to solve math problems only on algebra questions, and all of a sudden geometry questions are fed into the model at prediction time, we’d expect the predictions to be pretty bad since the model hasn’t been trained on geometry questions.In essence, data drift is a proxy for our classical performance metrics in the absence of ground truth labels. The next natural question is how to formally quantify data drift.Overview of data drift metricsThere are many different kinds of metrics we could use for quantifying data drift. Here, we’ll focus on two popular families of metrics: f-divergence and hypothesis test metrics. For the former, we’ll look at KL Divergence and PSI. For the latter, we’ll look at Chi-Squared and KS test statistics.For any drift metrics, P is the training data (reference set) on which the ML model was trained and Q is the data on which the model is performing predictions (inference set), which can be defined on a rolling time window for streaming models or a batch basis for batch models.KL DivergenceIf you need a quick overview, I’ve found this introductory post very helpful.KL Divergence from P to Q is interpreted as the nats of information we expect to lose in using Q instead of P for modeling data X, discretized over probability space K. KL Divergence is not symmetrical, i.e. the value is different if P and Q are swapped, and should not be used as a distance metric.Discretized KL DivergencePopulation Stability Index (PSI)While KL Divergence is well-known, it’s usually used as a regularizing penalty term in generative models like VAEs. A more appropriate metric that can be used as a distance metric is Population Stability Index (PSI), which measures the roundtrip loss of nats of information we expect to lose from P to Q and then from Q returning back to P.Discretized PSIHypothesis TestsHypothesis testing uses different tests depending on whether a feature is categorical or continuous.For a categorical feature with K categories, i.e. K−1 are the degrees of freedom, where N_Pk and N_Qk are the count of occurrences of the feature being k, with 1≤k≤K, for P and Q respectively, then the Chi-Squared test statistic is the summation of the standardized squared differences of expected counts between P and Q.Chi-Squared Statistic with K-1 degrees of freedomFor a continuous features with F_P and F_Q being the empirical cumulative densities, for P and Q respectively, the Kolmogorov-Smirnov (KS) test is a nonparametric, i.e. distribution-free, test that compares the empirical cumulative density functions F_P and F_Q.Kolmogorov-Smirnov Test StatisticWhy Automated Drift Thresholds?Photo by Scott Rodgerson on UnsplashHaving humans manually look at drift metrics produced by a model over time or over batches is obviously tedious and not ideal, and the immediate use case for drift metrics would be to set alerts based on some threshold, e.g. PSI jumps over .3 for this batch and should create an alert for a person to examine. Remediation actions could be checking if a pipeline that’s feeding that feature into the model is broken, if there was a recent buggy code or calculation change, or if the feature really did drift and suggests that the model needs to be retrained.But where did that .3 come from? Setting arbitrary handpicked thresholds is not a good solution. If the threshold is too high, alerts that should be brought up are now ignored (more false negatives). If the threshold is too low, alerts that should not be brought up are now present (more false positives).A universal constant threshold is not robust because that threshold should depend on the shape of the the training data P. If P is uniform and we see a unimodal Q, that drift value is going to be significantly less than if P was highly unimodal and centered at some mean far away from that same Q, since the former P that is uniform is less certain if Q did not come from P. An example is shown below.In order to set robust thresholds that make sense for an alerting system, we’ll dive into how to automate calculation of such thresholds. That is, we want to recommend data drift thresholds to create alerts, so users (a) don’t have to manually set alerts and (b) don’t have to determine what a good threshold value is.‍Automated Drift Thresholds for Hypothesis Test MetricsFor hypothesis test metrics, the trivial solution would be setting thresholds at the the proper critical values for each test using the traditional α=.05, i.e. 95% confident that any hypothesis metric above the respective critical value suggests significant drift where Q ∼ P is likely false.Hypothesis tests, however, come with limitations, from sample sizes influencing significance for the Chi-Squared test to sensitivity in the center of the distribution rather than the tails for the KS test.For those reasons, it’s important to explore other classes of drift metrics such as f-divergence metrics, and we’ll now explore ways to automate f-divergence drift thresholds.Automated Drift Thresholds V1: MC SimulationOverviewSystem for MC Simulated Automated Drift Thresholds (Image by Author)(1) For a large number of simulations indexed by i, for a given input feature, MC (Monte Carlo) simulate m times, where m is sufficiently large, from the reference set P and construct an inference distribution Qᵢ*, which represents what we’d expect had our data truly come from P via simulation. Then we calculate and keep track of data drift f(P, Qᵢ*) for each simulation.(2) We set the data drift threshold to the conservative value of max_i(f(P, Qᵢ*)), which is essentially a critical value/cutoff where α → 0 in a traditional statistics setting, i.e. we are 99.999% confident that a data drift value from an actual Q, i.e. inference time slice or batch, that is above the threshold signifies that the inference data was not produced from the same underlying distribution as P. If incurring false positives is not expensive, then we could set the threshold instead to the 95% percentile of f(P, Qᵢ*) in the traditional α=.05 one-sided setting.Limitations of MC SimulationMC methods are attractive because they are simulation-based and distribution-free, but this method is not scalable due to two issues.(1) Running simulations for every feature per model is computationally expensive. Specifically, runtime on a per model basis would be:O(n_features*n_categories_per_feature*n_metrics*n_simulations)(2) The assumption of sampling stability is broken with lower sample sizes.As a toy example, let’s say we have red, white, blue, and orange marbles in P, distributed uniformly. Let’s now present Q of size 3; there is no way to tell if the Q came from P or not since there is no way for us to represent all four colors. Even if we had sampled 6 marbles, we could not closely approximate 1/4 for each category given those 6 marbles.Let m be the number of simulated datapoints on each simulation to create Qᵢ*. In the same toy example of marbles, sampling instability causes artificially high uncertainty(and therefore artificially high data drift thresholds) when m is low, as P has not been well MC-sampled and represented in Qᵢ*.KL Divergence MC Sampling Instability (Image by Author)The way to fix issue (2) is to create simulated threshold curves for a given feature by varying m, but now we have exacerbated issue (1) by multiplying runtime by m. We also now have issues with space complexity, since now we would have to store thresholds in a lookup (database or cache) for use in downstream applications, like an alert service creating alert rules or a query service cache. Forgoing the extra storage and simulating the threshold curve at query time is also not an option since the query would take too long for most user-facing applications.In short, the trade-off of using a flexible simulation setup is not worth the computational expenses when taking into account correction curves that would either (a) need to be stored, which would cost a lot in storage fees or blow up a cache, or (b) be simulated at query time, which is too slow for user-facing applications.Automated Drift Thresholds V2: Closed-Form StatisticsPhoto by Edge2Edge Media on UnsplashIt turns out that we can upper bound data drift thresholds in one shot using probability theory. We’ll make use of standard probability theory, the Dirichlet distribution, and first-order Taylor series expansions. Remember that we are still deriving the thresholds on a per feature basis.KL DivergenceFirst, letReference set P is known and inference set Q is unknown. Intuitively, this simply means that we have observed the training data used in creating our ML model and that we’re asking a hypothetical question of what we could expect data drift to be for some hypothetical inference set Q in the future. Formally,2nd line: Linearity of expectation and conditional probability.3rd line: Q ∼ Dirchlet(α), i.e. Q models a probability distribution and is a multivariate generalization of the beta distribution. As such, we can use the digamma ψ function in this case.α = (α_1, …, α_K) traditionally corresponds to the observed probabilities in the K categories for the feature from Q, offset by a small constant, i.e. prior counts. However, Q is unknown, so how do we derive the correct counts? We do so from P. After all, the entire point of this expectation statement is to quantify on average the drift we’d expect to see if Q ∼ P, and an amount above that value in an actual inference set signifies that we likely have significant drift, i.e. Q ∼ P is false.Back to α, we can use a Bayesian uniform prior Q ∼ Dirichlet(1_K) and update such that QP ∼ Dirichlet(1_K + N_q * p_K), where N_q is the number of datapoints in the inference set Q and p_K corresponds to the known K-dimensional probability vector for the feature from P. This is the multivariate generalization of the the beta-binomial conjugacy.So we thus have,Population Stability IndexPSI gets a bit more tricky and we need to upper-bound the expectation. LetThenHere are the steps:2nd line: Linearity of expectation and substituting the result for KL Divergence above.3rd line: Linearity of expectation and conditional probability.4th line: Expectation of a Dirichlet distribution.5th line: Delta method approximation (see appendix).6th line: Expectation of a Dirichlet distribution.7th line: Simplifying.V1 Issues Solved!Notice how we now have solved for the two issues from MC simulation.(1) We do not have to worry about runtime, as this form is a one shot solution that only involves counts and we can use an approximation to the digamma function.(2) We handle varying sample sizes by using a Bayesian plus pseudocount setup on the parameters of Q.‍ImplementationIn production, how do we implement this dynamic thresholding? Although the thresholds are dynamic depending on the metric and the incoming inference set, all of the expectations above, conditioned on P, can be expressed as SQL queries or custom but simple functions.Discretizing all continuous features, instead of smoothing discrete features, would be a reasonable trade-off: although we may not capture the full continuous nature of P and Q, we’ll avoid computationally expensive kernel density estimation in representing P and Q as continuous distributions.Wait, what about variance?We can calculate variances too and apply Delta method approximations, as we would run into variances and covariances on logarithmic terms that cannot be solved using standard probability theory. Those tools are below in the appendix. However, as it turns out, most metrics will not come out as cleanly as KL Divergence, so most of the conditional expectations and variances are upper-bounded via Cauchy-Schwarz. It’s certainly doable to include an additive upper-bounded variance term, or to even use second-order approximations, but the extra computation might not be worth the time gained in exchange for a stricter bound.For another time…We can also use Taylor series expansions to upper-bound other metrics, like JS Divergence and Hellinger Distance.ConclusionThat’s it for today! Hope you gained some insights about how to implement automated thresholds in a computationally and probabilistically sound manner! We implement these kinds of systems at Arthur, and a few equations can go a long way for the customer experience. Check out our Data Drift Detection Part II: Unstructured Data in NLP and CV blog post for more.Happy monitoring!Photo by Pablo Heimplatz on Unsplash‍Appendix: Delta Method ApproximationsNote that the first-order approximations for both sections below are upper bounds because the log function is concave.log(X) ApproximationBy a first-order Taylor series expansion around the mean,Using this approximation, we haveandwith inequality by Cauchy-Schwarz.Xlog(X)By a first-order Taylor series expansion around the mean,Using this approximation, we haveand lettingwe havewith inequality by Cauchy-Schwarz.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/automating-data-drift-thresholding-in-machine-learning-systems
214,"Arthur—rapidly growing amidst surging interest in model monitoring—identified as a Sample Vendor in 2021 Gartner ® Hype Cycle ™ for AI report
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur—rapidly growing amidst surging interest in model monitoring—identified as a Sample Vendor in 2021 Gartner ® Hype Cycle ™ for AI reportBy:Arthur TeamSeptember 15, 2021New York, NY –  Arthur, the leading machine learning (ML) monitoring platform, is identified as a Sample Vendor for the AI Governance category. Gartner released its annual “Hype Cycle for Artificial Intelligence 2021” report, highlighting the growing importance of AI governance practices in AI-enabled companies as it states, “Data and analytics leaders must leverage this research to successfully navigate AI-specific innovations that are in various phases of maturation, adoption and hype.” [1]“With AI now delivering value in practical enterprise application, data and analytics leaders see that scaling AI without governance is dangerous. When each AI output is replicated millions of times, they ask how to balance the business value promised by AI against the need for appropriate oversight, risk management, and investment decisions.”As digital transformation continues to be a driving force in operations across industries, Gartner highlights the necessity to invest in technology to support AI governance, Responsible AI, and ModelOps within an organization. The Gartner report states: “With AI now delivering value in practical enterprise application, data and analytics leaders see that scaling AI without governance is dangerous. When each AI output is replicated millions of times, they ask how to balance the business value promised by AI against the need for appropriate oversight, risk management, and investment decisions.” [1]“Over the past three years, we’ve successfully enabled our customers to put in place robust AI monitoring and governance at companies of all sizes. We believe that this Gartner recognition of the importance of AI governance is further proof of its foundational importance in the enterprise. Our team is uniquely positioned to help customers put in place this governance and maximize their AI investments.”Arthur believes the report underpins the transformative impact that AI has on all enterprises. As a result, the need for explainable AI and fairness is paramount. “As enterprises become AI-enabled, the performance of their models drives the performance of their business. We are seeing an increased focus on AI governance all the way up to the board level,” said Adam Wenchel, Founder and CEO of Arthur. ""Over the past three years, we’ve successfully enabled our customers to put in place robust AI monitoring and governance at companies of all sizes. We believe that this Gartner recognition of the importance of AI governance is further proof of its foundational importance in the enterprise. Our team is uniquely positioned to help customers put in place this governance and maximize their AI investments.”Since its inception in 2018, Arthur has enabled customers like Humana, Zesty.ai, and Truebill to accelerate their business transformation through AI. Customers use Arthur to monitor their ML models, mitigate, analyze and mitigate unwanted biases, and explain how the models are making decisions.The platform continues to innovate—Arthur has been the first to market in enabling the adoption of computer vision (CV) and natural language processing (NLP) models. In April, it was named one of CB Insight’s AI 100 list, recognizing promising private AI companies globally for the second consecutive year. In June, it was named a Gartner ‘Cool Vendor’ in AI Governance and Responsible AI [2].If you’re a Gartner client, you can access the full report here: Hype Cycle for Artificial Intelligence 2021Learn more about Arthur at arthur.ai/product. To schedule a demo of Arthur, please contact info@arthur.ai.[1] Gartner, “Hype Cycle for Artificial Intelligence, 2021”, Shubhangi Vashisth, Svetlana Sicular, 29 July 2021
[2] Gartner, “Cool Vendors ™ in AI Governance and Responsible AI”, Svetlana Sicular, Moutusi Sau, et al, 10 June 2021
Required Disclaimer: GARTNER, COOL VENDORS and HYPE CYCLE are a registered trademark and service mark of Gartner, Inc. and/or its affiliates in the U.S. and internationally and are used herein with permission. Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.NOTE: Additional information may be included in the press release, but must be in compliance with the Gartner Copyright and Quote Policy: http://www.gartner.com/technology/about/policies/copyright.jsp  Industry-general excerpts from the report are permitted (e.g., evaluation criteria);No company-specific excerpts permitted as they may appear endorsing;Vendor company information may be included provided it is clearly differentiated from the Gartner report.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-rapidly-growing-amidst-surging-interest-in-model-monitoring-identified-as-a-sample-vendor-in-2021-gartner-r-hype-cycle-tm-for-ai-report
215,"Arthur's Response to NIST Guidance on Bias Risk in AI
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur's Response to NIST Guidance on Bias Risk in AIBy:Lizzie KumarAugust 4, 2021In June, the National Institute of Standards and Technology (NIST) released a draft of a document outlining an approach for identifying and managing bias in artificial intelligence. The team driving responsible AI research at NIST has requested feedback on the document ""from people whom AI affects, both from those who create AI systems and also those who are not directly involved in its creation."" At Arthur, we are excited to see NIST embrace and elevate the importance of these issues. In our response, we express our support for NIST's efforts as well as address potential gaps in their framework. Read it here.Figure 1 from NIST’s proposal. Note the arrow from “Deployment” back to “Pre-Design.”PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthurs-nist-response
216,"Google’s Dermatology App Announcement Highlights Promises and Potential Perils of Computer Vision Technology
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessGoogle’s Dermatology App Announcement Highlights Promises and Potential Perils of Computer Vision TechnologyBy:Connor ToupsJune 28, 2021Last week, Google announced a new dermatology app that will use Computer Vision to automatically classify skin conditions from images and provide “dermatologist-reviewed information and answers to commonly asked questions, along with similar matching images from the web” for the matching skin condition. Why it’s important: Google first published their model in Nature in 2020. Moving to embed that technology into a public-facing app highlights the continued impact of computer vision in the healthcare industry.‍Concerns around bias: Google’s announcement prompted concerns that the model would be less accurate for darker-skinned people. According to the Nature paper Google published, Fitzpatrick Type V and VI skin, which broadly correlates to brown and dark brown skin, accounted for under 3.5% of the train set. That class imbalance could cause the model to underperform on darker-skinned people -- raising concerns about disparate impact bias. Things we noticed in Google’s validation set numbers‍As others have noted, just 2.7% of the validation set examples from the Nature publication used Type V or VI skin, so it’s difficult to assess how well the model generalizes to dark skin.We also noticed that the topline accuracy numbers Google reported for ethnicity-level accuracy might obscure racial disparities in the validation set. In slides shared with Motherboard (Vice’s tech publication), Google reported that its model had an 87.9 percent accuracy rate for Black patients -- the highest of any ethnicity group. The Nature publication also reported a relatively high accuracy rate for Black patients (though we couldn’t find the 87.9 percent figure) that were on par with accuracy rates for White patients.Just looking at the accuracy rate, however, can obscure potential racial disparities. In the supplementary figures, Google’s Nature publication reported its average sensitivity, which they calculate by averaging sensitivity across the 26 conditions the model considers. This has the advantage of weighting each condition’s performance equally, so common conditions don't skew the accuracy rates as much.Looking at average sensitivity, Google’s model performed worst on Black patients. The difference between accuracy and sensitivity rates for Black patients could suggest that the Black patients in their validation set were more likely to have a common condition (eg. Acne) that the model can perform very well on, but when it comes time to detect a more serious but less common condition like skin cancer, the model underperforms for Black patients. Non skin-cancer conditions could be overrepresented in the Black sample because the prevalence of skin cancer in non-White people is lower than the prevalence for White people (however, skin cancer diagnoses in non-White people often come at a more advanced stage -- increasing the lethality rate). Imagine we randomly sample a group of Black patients with some type of skin condition (not exclusively skin cancer) and an equally sized group of White patients with some type of skin condition: if the prevalence of skin cancer is higher in the White population than the Black population, a random sampling process will result in the White sample having more patients with skin cancer and the Black sample having more patients with some non-cancerous skin condition. The code below demonstrates this phenomenon. Whatever the underlying cause for the disparities, low sensitivity rates for Black patients could exacerbate existing racial inequalities in healthcare. A Brief History of Bias in Computer Vision ModelsAs computer vision research has exploded, there’s been an increased interest in auditing these models for potential racial and gender bias.  In 2018, Gebru and Buolamwini found three major commercial facial recognition systems performed significantly worse on darker-skinned people and women than lighter-skinned people and men. That landmark study inspired further research into auditing other computer vision models that might exhibit these pernicious biases.Studies have also examined CV models in a healthcare context: last year, a study looked at chest X-ray diagnosis computer vision models and found that they exhibited TPR (true positive rate) disparities with respect to gender, race, and age. TPR disparities with respect to race were especially prominent with the models performing best on White patients and worst on Hispanic patients. Difficulties in auditing dermatology CV modelsAfter Google’s announcement, our team looked into conducting an audit of a dermatology computer vision model using an external test set to look for possible racial or gender bias. However, as we explored the feasibility of such a study, it became clear that there were no open source dermatology datasets that included metadata on each patient’s race or skin type. External audits are important methods of ensuring that models in production are equitable and inclusive: balanced datasets that include patient skin type and race metadata are critical to enabling these audits, and we at Arthur hope that the healthcare and tech industries work together to create these datasets and conduct these audits.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/googles-dermatology-app-announcement-highlights-promises-and-potential-perils-of-computer-vision-technology
217,"Arthur named a 2021 Gartner Cool Vendor in AI Governance and Responsible AI
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur named a 2021 Gartner Cool Vendor in AI Governance and Responsible AIBy:Arthur TeamJune 29, 2021NEW YORK, NY—Arthur, the AI monitoring and governance company, is announcing today that it has been named a “Cool Vendor” as featured in the report titled “Cool Vendors in AI Governance and Responsible AI” by expert analysts at Gartner, Inc.The report “is designed to highlight interesting, new and innovative vendors, products and services”, and highlights the “adoption of artificial intelligence is pushing organizations from declarations of their responsible and ethical AI principles to actions.”According to Gartner, “Responsible artificial intelligence (AI) is in the spotlight because of the increased organizational and societal exposure to AI bias, distrust and lack of fairness that stand in the way of AI adoption. The more ubiquitous AI becomes, the more AI practitioners recognize in their everyday work that data and algorithms amplify and perpetuate human biases.”According to the same report, Gartner predicts that “Through 2025, 80% of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance.”Through 2025, 80% of organizations seeking to scale digital business will fail because they do not take a modern approach to data and analytics governance. (Gartner)Since its inception in 2018, Arthur has cemented its position as the leading model monitoring and governance solution in the market with many industry-exclusive features such as comprehensive support for natural language processing (NLP) and computer vision models. In April, it was named one of CB Insight’s AI 100 list, which recognizes promising private AI companies globally, for the second consecutive year.“We consider it a great honor to be named a Gartner ‘Cool Vendor,’” says Adam Wenchel, CEO of Arthur. “We strongly believe in the importance of AI governance and responsibility and are glad to see Gartner highlighting trends in this industry. We believe this designation recognizes our commitment to creating the most comprehensive set of model monitoring tools possible to enable any company to operationalize AI governance seamlessly.”If you’re a Gartner client, you can access the full report here: Cool Vendors in AI Governance and Responsible AILearn more about Arthur at arthur.ai/product. To schedule a demo of Arthur, please contact info@arthur.ai.Gartner DisclaimerGartner does not endorse any vendor, product or service depicted in our research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner's research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose.About ArthurArthur is a proactive machine learning model monitoring platform that gives you the confidence that your AI deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. With performance monitoring, bias detection, and customizable alerts, Arthur makes sure that you never miss an issue, and Arthur’s explainability engine makes runtime debugging effortless.Arthur’s customers, including Fortune 100 companies like Humana and AI-driven startups like Expel and Truebill, are using the platform to ensure that they can catch and fix any issues with their production AI systems before they become billion-dollar problems.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/2021-gartner-cool-vendor
218,"Introducing Monitoring for Computer Vision Models
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringIntroducing Monitoring for Computer Vision ModelsBy:Arthur TeamJune 17, 2021As computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. The adoption of computer vision into industry applications promises enormous potential upside; however, computer vision models, like any ML model, must be carefully monitored. A promising model that has gone off the rails can quickly become a dangerous liability. Today, Arthur is excited to provide the first model monitoring support for computer vision (CV) models.  With Arthur, you can launch CV models into production, and rest assured that you’ll be immediately notified when something warrants your attention. Arthur supports both image classification and object detection, providing monitoring for performance and data drift, as well as in-depth explainability. In this post, we describe the key components for successful monitoring of CV models and how Arthur helps ensure your models are performing as expected – maximizing success for your organization while mitigating risk.Understanding Data Drift in Computer Vision ApplicationsA critical aspect of monitoring any ML model is to ensure that the data coming into the model continues to look as expected. In computer vision models, this means ensuring that the images we see today are similar to those used to train the model. In technical terminology, we refer to this as out-of-distribution detection or anomaly detection, and it is a field of burgeoning research in the ML scientific community. Using a reference dataset of images, we can perform ongoing monitoring of all new images to understand which ones are similar to the training data and which ones seem like anomalies.It is essential to know exactly when your model won’t generalize to new settings. For example, if your object detection model was trained primarily on images from outdoor locations in good weather and good lighting, it will likely underperform in rainy and dark conditions. Arthur’s data drift detection tool can automatically detect if these dissimilar images start coming in so that your data scientists can get ahead of any model issues.In another example, let’s say you’ve trained a computer vision model that examines chest x-rays to diagnose tuberculosis. Despite your best efforts, you’re only able to collect training examples from a small number of x-ray machines, each of which tends to have its own set of artifacts and nuances. Once you deploy your model into the wider world, you’re nervous that these artifacts will prevent your model from generalizing well to image data drawn from a much larger set of machines.In this case, it would be helpful to quantify how much each production image adheres to the attributes of the training dataset. Luckily, Arthur’s data drift detection tool can identify if your model fails to generalize and prompts you to take action before your model derails and causes adverse impacts.The diagram above demonstrates how anomaly detection works. Here, we trained our computer vision model using images that consist of typical street scenes. Most of the images involve transportation, outdoor lighting, people, buildings, and so on. At production time, some of the incoming data is visually (and semantically) similar to the training data, such as the top image of a train platform. However, the bottom image of the toy robot and cat is quite different from anything in the training set. Therefore, our tool would flag this image as an anomaly. This kind of drift detection for image data will ensure that your model’s output predictions remain trustworthy and reliable. Furthermore, we allow for sorting and searching through images based on their anomaly score: how dissimilar are they to the training data. This classification model was trained on aerial imagery primarily of green landscapes. However, the model occasionally sees dense urban images and we are able to quickly identify that these images are likely going to lead to misclassifications. This dynamic filtering gives data scientists the tool to quickly find representative examples of anomalous images.Whatever your computer vision model does, it’s easy to imagine how data drift could cause issues. It’s critical to monitor data drift to prevent unwanted surprises and wasted time debugging your model.Explainability in CV modelsOne concern with complex ML models, especially computer vision models, is that they can be “right for the wrong reasons.” When models are in production, we want to ensure that they’re looking at the ‘right things’ to make decisions. For example, if we have a model for identifying cancerous cells in micrographs, we would want to ensure that the model picks up on medically important aspects of cells instead of some artifacts that happened to be present in the training data. Using Arthur’s local explainability techniques, you can visualize saliency maps over images to reveal which image components were particularly important for the model’s decision. The importance scores are associated with a class, so a positive score indicates that a region strongly contributed to a positive class prediction. A negative score indicates that a region was negatively associated with a target class.The Arthur platform makes image explanations easy to use - it shades regions of the original image with green or red to indicate the importance of that region. Green shading indicates a region that positively contributed to the selected class, while red shading indicates a region that negatively contributed to the selected class. A user can drag a slider bar interactively, indicating how many regions are shown, sorted by overall importance. In this example, data scientists, researchers, and business analysts alike can utilize the Arthur platform to guarantee peak model performance. Arthur’s computer vision explainability tool is simple, easy to understand, and provides cross-functional teams with key insights. Monitoring your Computer Vision Model for Algorithmic BiasAs with any machine learning model, it’s necessary to ensure that algorithmic bias hasn’t seeped into your computer vision model. Unfortunately, a growing body of research demonstrates that some of the most popular computer vision models are biased. In 2018, researchers Gebru and Buolamwini found three major commercial facial recognition systems performed significantly worse on darker-skinned people and women than lighter-skinned people and men. A year later, a National Institute of Standards and Technology study that evaluated 189 facial recognition models found pervasive bias across facial recognition models.That same NIST study, however, found that some models performed equitably across all demographic groups. While the study didn’t evaluate causal reasons for this outcome, it suggests that the model you use—and the data you train on—affects the degree of bias in computer vision models. This study further emphasizes the importance of continuously monitoring and evaluating each of your models to see where it falls on the fairness spectrum.As computer vision continues to offer new opportunities for innovation and growth, we must ensure that its applications are equitable and inclusive to avoid encoding dangerous systemic biases. Arthur has built-in bias monitoring so you can easily compare equity across various groups, and maintain high standards of fairness.Monitor your CV Models with ArthurThe Arthur platform has recently released extensive monitoring support for CV models, including performance monitoring, data drift and bias detection, and explainability features. Unlike other model monitoring solutions, Arthur has you covered for Tabular data, NLP data, and Image data, so you can be assured that your monitoring platform can grow easily with your ambitious AI agenda.If you’re deploying CV models into production and are looking for a solution for monitoring those models over time, we’d love to connect and show you how Arthur can help. Request a demo today.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/introducing-monitoring-for-computer-vision-models
219,"Arthur releases the first computer vision model monitoring solution for enterprise
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesArthur releases the first computer vision model monitoring solution for enterpriseBy:Arthur TeamJune 17, 2021‍NEW YORK, NY—Today, Arthur, a machine learning model monitoring company, is releasing a suite of new tools and features that enable organizations to monitor the performance of their computer vision (CV) models, including image classification and object detection models. Arthur’s new CV feature set allows companies to:Monitor CV model pipelines for data anomaliesReview potential biases in image modelsUnderstand model predictions and decisionsExplore, investigate, and triage model predictions and errorsView model predictions and important regions for determining a class output in a particular imagePerformance monitoring: Arthur now offers performance monitoring of CV models, including tracking data drift and anomaly detection (out-of-distribution detection for images). This helps users identify any images being sent to the model where the model is likely to be underperforming. In addition to supporting all tabular and NLP models, Arthur’s CV functionality allows companies to monitor all of their machine learning models on one centralized dashboard, regardless of model types.Bias detection: The new CV feature set enables companies to identify biases in their CV models by evaluating image classification outputs for particular classes that users are interested in evaluating. By examining outputs for a specific class, users can better understand where their model misclassifies and perpetuates biases.Explainability: Arthur’s CV feature set offers insights into how CV models are making decisions. This capability gives users a clear visualization of which regions of an image are impactful for a model’s decision, making it easy to identify when models perform unexpectedly. Additionally, users can explore the results of their object detection models with an interactive interface that displays the model outputs for pipeline images.The release of the CV feature set comes just a few weeks after Arthur released the first natural language processing (NLP) model monitoring solution, furthering Arthur’s position as the most comprehensive machine learning model monitoring solution in the market.View object detection model outputs on Arthur's interactive interfaceThe growing need for CV model monitoringAs computer vision technology has grown more sophisticated and computational power has become more available, companies have increasingly adopted computer vision models to augment and automate critical processes. According to Stanford HAI’s 2021 AI Index Report, companies are investing a larger amount of computational resources in training computer vision algorithms than ever before.The adoption of computer vision into industry applications promises enormous potential upside; however, computer vision models, like any ML model, must be carefully monitored. A promising model that has gone awry can quickly become a dangerous liability. In 2018, researchers Gebru and Buolamwini found three major commercial facial recognition systems performed significantly worse on darker-skinned people and women than lighter-skinned people and men. A year later, a National Institute of Standards and Technology study that evaluated 189 facial recognition models found pervasive bias across facial recognition models. Without the proper tools to audit CV models, it’s difficult to pinpoint the issues they may cause.Armed with Arthur’s CV model monitoring functionality, companies can maximize the performance of their CV models and minimize any risks associated with them.---To schedule a demo of Arthur, visit arthur.ai/demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthur-releases-the-first-computer-vision-model-monitoring-solution-for-enterprise
220,"Reinforcement Learning for Counterfactual Explanations
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedExplainable AIReinforcement Learning for Counterfactual ExplanationsBy:Sahil VermaJune 10, 2021Authors: Sahil Verma, Keegan Hines, John DickersonIn the field of Explainable AI, a recent area of exciting and rapid development has been counterfactual explanations. In this approach, we aim to understand the decisions of a black-box machine learning model by quantifying what would have needed to have been different in order to get a different decision. A common example is that of credit lending: if I am denied for a loan (by an algorithm), then a counterfactual explanation of that decision could inform me that if my income were $10K higher and my credit score were 30 points higher, then I would have been approved. This basic premise is intriguing, but it also comes along with several subtle constraints and desirable add-ons. These might include things like, causal constraints, realism constraints, actionability, sparsity, computational efficiency, and so on. If you’re interested in learning more about these overall areas of research, feel free to read our recent review paper from the NeurIPS workshop on ML Retrospectives, which won a Best Paper Award.We’ve recently posted a new approach to these problems which allows us to solve for many of these constraints. By framing the problem of generating counterfactual explanations as a Markov Decision Problem, we can associate many of the desiderata to various components of an agent and environment (such as the transition function or reward function). Then, using common techniques from reinforcement learning, we can train an agent to calculate counterfactuals for a given model. Once this agent is trained, we have amortized the computation of new counterfactuals, since any new counterfactual is calculated by simply evaluating the agent’s policy for any starting point. This makes the method extremely computationally efficient for calculating new counterfactuals. As you can see in the table below, our approach, which we termed FastCFE, is able to accommodate all of the major desiderata/constraints that have been brought forth recently in the counterfactual explainability community. This is an exciting development in operationalizing counterfactual explainability for real-world and high-volume use cases. We hope you enjoy the paper. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/reinforcement-learning-for-counterfactual-explanations
221,"Serving, hosting and monitoring of an xgboost model: UbiOps and Arthur
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesServing, hosting and monitoring of an xgboost model: UbiOps and Arthur By:Arthur TeamMay 25, 2021Making ML Ops observable and explainable: serving, hosting and monitoring combined. Maximizing performance and minimizing risk are at the heart of model monitoring. In mission-critical AI models, real-time visibility is crucial. Respond quickly by retraining your model, replacing the current version or tweaking the pipelines. Additionally, setting up your serving and hosting infrastructure in a scalable way can be time consuming and complex. For these reasons, Arthur and UbiOps are pleased to announce an exciting integration that brings best-in-class tooling for model serving and model monitoring.UbiOps is the easy-to-use serving and hosting layer for data science code. UbiOps makes it easier than ever to use a top-notch deployment, serving, and management layer on top of your preferred infrastructure. Accessible via the UI, client library, or CLI, it’s suitable for every type of data scientist, without the need for in-depth engineering knowledge.Arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their AI deployments are performing at peak. Arthur provides a layer of performance monitoring, algorithmic bias detection, and explainability, even for black box models, so data science teams can detect, diagnose, and fix any issues in production.Together, Arthur and UbiOps provide a powerful set of tools that give you complete control over your production AI. This blog post will demonstrate just how easy it is to get started with the Arthur and Ubiops integration, so you can complete your AI stack and stay in control.1. Use case descriptionWith the recent increases in house prices across the world, a model to predict what the price of a specific house would be could come in handy. But how can we keep track of the model’s performance, and how can we explain why one house may be priced higher than another?Using a publicly available dataset from King County, USA, we trained a xgboost model to predict the house prices in that region. Using the UbiOps CLI, the model is easily containerised, deployed on a Kubernetes pod, and served behind an automatic API endpoint. Via the API, Arthur picks up the model’s inputs and predictions and calculates data drift, model stability, and explanations.2. Notebook walkthrough and instructionsThis Notebook is based on the Xgboost recipe from the UbiOps cookbook. Please check out the full notebook for everything you need to set this up. Compared to the Xgboost recipe there are 2 big differences.The first adjustment is the following: we need to register the (pre-trained) model to the Arthur platform. We do that using the following code snippets:
from arthurai import ArthurAI
from arthurai.common.constants import Stage, InputType, OutputType, ValueType
ARTHUR_URL = ""https://app.arthur.ai""
ARTHUR_ACCESS_KEY = """" # Fill this in
connection = ArthurAI(url=ARTHUR_URL, access_key=ARTHUR_ACCESS_KEY)
# Define the model schema
arthur_model = connection.model(partner_model_id=""UbiOps House Prices"",
input_type=InputType.Tabular,
output_type=OutputType.Regression,
is_batch=True)
arthur_model.from_dataframe(train_data[input_columns], Stage.ModelPipelineInput)
arthur_model.add_regression_output_attributes({""price"": ""price_gt""}, value_type=ValueType.Float)
arthur_model.review()
The dataframe above represents how the model will look to Arthur, and the format of the data it will expect. Notice how it detected some columns as categorical (such as Waterfront and View).Now we can save the model to Arthur, and store the Arthur Model ID to be used by our deployment
arthur_model_id = arthur_model.save()
with open(""xgboost-deployment/arthur-model-id.txt"", 'w') as f:
f.write(arthur_model_id)
Then, we’ll upload the data we used to train the model as a reference set. Future data sent to the model will be compared to this reference set, to measure how much it has drifted from the types of inputs the model was built from.
ref_df = train_data[['price'] + input_columns].rename(columns={'price': 'price_gt'})
ref_df['price'] = xgb.predict(ref_df[input_columns].to_numpy())
ref_df
arthur_model.set_reference_data(data=ref_df)
The second addition is sending the inference data from within a deployment to the Arthur platform. You can see it in a snippet from the actual deployment file below (again look at our full notebook for the complete code). It is as simple as using the `send_inference` method of the Arthur client library.
def request(self, data):
""""""
Method for deployment requests, called separately for each individual request.
:param dict/str data: request input data. In case of deployments with structured data, a Python dictionary
with as keys the input fields as defined upon deployment creation via the platform. In case of a deployment
with plain input, it is a string.
:return dict/str: request output. In case of deployments with structured output data, a Python dictionary
with as keys the output fields as defined upon deployment creation via the platform. In case of a deployment
with plain output, it is a string. In this example, a dictionary with the key: output.
""""""
print('Loading data')
batch = pd.read_csv(data['data']).astype({'id': str})
batch_id = str(uuid4()).split('-')[-1]
print(""Predictions being made"")
batch['price'] = self.model.predict(batch.drop(columns=['id']).to_numpy())
print(""Sending batch to Arthur"")
inference_data = [{'inference_timestamp': datetime.datetime.now(pytz.utc),
'partner_inference_id': row['id'],
'batch_id': batch_id,
'inference_data': {k: row[k] for k in row.keys() if k != 'id'}}
for row in batch.to_dict(orient='records')]
self.arthur_model.send_inferences(inference_data)
Now the Arthur platform can compare training data to actual inferences. You can explore the data in Data Explorer and view drift metrics for the input and output variables, as well as accuracy metrics such as Root Mean Square Error (RMSE).3. Wrap upNow that you’ve seen how UbiOps and Arthur.ai integrate to make serving, hosting and monitoring easily accessible, we hope that this helps you in your day-to-day projects. For any questions or suggestions and the latest product updates please join the UbiOps community slack channel and the Arthur slack community.Download the full notebook here. PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/serving-hosting-and-monitoring-of-an-xgboost-model-ubiops-and-arthur
222,"CB Insights recognizes Arthur as one of the most innovative AI startups in the world
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesCB Insights recognizes Arthur as one of the most innovative AI startups in the worldBy:Arthur TeamApril 7, 2021Today, CB Insights released their “AI 100” ranking for 2021, highlighting the 100 most promising and innovative private AI companies from around the world—and we are thrilled to be included in the list for the second year in a row.Arthur is one of just two companies that CB Insights selected in the model monitoring space, and we are the only model monitoring company to make the list two years in a row. Last year’s AI 100 companies went on to raise a total of $5.2B in additional funding (including us!), and many companies on the list last year went on to exit via M&A, SPAC, or IPO. We’re honored that CB Insights has recognized Arthur alongside some of our favorite companies in the space, including our partners at Algorithmia.With so many notable startups lined up next to us on this year’s AI 100 list, the future looks bright. We believe in AI’s amazing potential to transform the world and are excited to be doing our part to ensure this transformation is safe, reliable, and fair. CB Insights' AI 100 is a ranking of the 100 most innovative and promising AI startups around the world—and we're thrilled to be included for the second year in a row.‍About ArthurArthur is a proactive machine learning model monitoring platform that gives you the confidence that your AI deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. With performance monitoring, bias detection, and customizable alerts, Arthur makes sure that you never miss an issue, and Arthur’s explainability engine makes runtime debugging effortless.Arthur’s customers, including Fortune 100 companies like Humana and AI-driven startups like Expel and Truebill, are using the platform to ensure that they can catch and fix any issues with their production AI systems before they become billion-dollar problems.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/cb-insights-ai-100-2021
223,"Interactive analysis with petabytes of model data
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringInteractive analysis with petabytes of model dataBy: Keegan HinesApril 6, 2021Machine learning models in production can degrade in lots of unexpected ways. There are certain questions, concepts, and metrics that we might know ahead of time that we’ll want to check in on week over week. And then, of course, there are unknown unknowns - problems, questions, and calculations that we are unable to anticipate but will become critical in 6-12 months from now. For this reason, the Arthur platform has been built to be as configurable as possible, to not be prescriptive, and to allow our users full and flexible interfaces for interrogating and understanding model performance. In this post, we’ll describe some of the architectural choices that underlie this design philosophy, and share a couple of the powerful modes of interaction that are made possible by these choices.Optimized StorageAs your model data is ingested into the Arthur platform, it is indexed into a distributed columnar datastore. Not only is this highly scalable (can easily handle petabytes of model data), but it allows for rapid aggregations and queries, thus allowing exploratory and ad-hoc access to all the information about historical model performance. You don’t have to make the difficult choice to subsample or summarize your data, and you don’t need to decide your favorite metrics ahead of time. You’ll have at your fingertips all the historical data about your model’s inputs, predictions, performance, explanations, and other insights. This enables a powerful ability to slice-and-dice model performance across any facets or subpopulations that are relevant to you. For Arthur users, this capability results in two particularly powerful ways of monitoring data and models.Data ExplorerThe Arthur platform provides an interactive data visualization suite that allows you to explore and understand the data pertaining to your model. Our backend architecture allows us to compute the necessary aggregations, groupings, and filters in milliseconds, even over hundreds of millions of data points. In the examples below, we can visualize the distributions and correlations amongst model inputs, outputs, and even explanations. We can quickly navigate through different time slices, facet the data by groupings of different variables, and understand your model’s predictions and data landscape.Query EngineIn addition to the rich set of visualizations and metrics available in the Arthur UI, you can also fetch any and all of this underlying data (and computation) through our API. Our API-first approach means that data scientists can quickly check-in on model performance using a familiar tool, such as a Jupyter notebook. Our Query Engine exposes a SQL-like language that will be quickly familiar to data scientists, so that they can compute and visualize ad-hoc summaries and aggregations on large sets of data. As an example, one day we might be curious to know how our model is performing for Males versus Females, and if that has been changing over time. We construct a query with familiar group-by’s and filters and the Arthur backend computes aggregations over millions of inferences in just a few milliseconds. The result of this query is easy to drop into a pandas DataFrame for quick visualization.You can use Arthur's Query Engine to explore and visualize your monitoring data—for example, plotting average prediction value by sex over time.In addition to model evaluation metrics, it is a snap to get a quick view of the distribution(s) of a model’s input or outputs, to understand how they may be shifting over time.Arthur's Query Engine makes it easy to visualize how your model input and output distributions are shifting over time.From a notebook, we can quickly and easily dive into any subpopulations and assess model performance and data stability. Once we are alerted to issues with model performance or data drift, having this data at our fingertips empowers us to conduct an investigation and drill down to its root cause.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/interactive-analysis-with-petabytes-of-model-data
224,"Everything you need to know about model monitoring for natural language processing
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringEverything you need to know about model monitoring for natural language processingBy: Keegan HinesMarch 29, 2021From simple chatbots to document classifiers to generative models like GPT-3, natural language processing models are seemingly everywhere these days. NLP models are powerful tools for processing unstructured text data—but with great power comes great responsibility. If you’re not monitoring your NLP models just as you would your tabular models, you can overlook a number of sticky issues that could quickly become billion-dollar problems.Here are a few things that any organization deploying NLP models into production should be doing to ensure that those models continue to perform as expected.Monitor for data drift to prevent drops in performance.An NLP model, just like any other machine learning model, is trained on a training dataset that represents the world at a point in time—and as the world changes and the input data coming into your language model starts to change, your model will start to run into performance issues. Over time, your NLP model performance will degrade if you’re not proactively monitoring for and correcting data drift as it occurs.Monitoring NLP models for data drift involves comparing the statistical similarity of new input documents to your training set documents.  As input documents shift in their typical word-use, you might need to update your model to account for the linguistic patterns. Understanding when and where drift is occurring is essential to maintaining the integrity of your NLP models over time.The Arthur platform monitors NLP models for data drift and automatically alerts you when your input documents start drifting beyond acceptable bounds. Monitor for bias to protect against discrimination.Algorithmic bias is a critical issue in any machine learning system, but issues of bias can be especially pernicious and difficult to solve in language models.There are several methods of detecting and mitigating bias, which we cover extensively in blog posts, Making Models More Fair and A Crash Course in Fair NLP for Practioners. In addition to the techniques laid out in those blog posts, there are a few additional ways to track potential bias in your machine learning models.First, you can track the performance of your NLP models on input documents partitioned by sensitive attributes that you may know. For example, you can track whether your medical document classifier is more or less accurate for documents from medical visits of men versus women, or Black patients versus white patients. Uncovering differences in accuracy and other performance measures across different subgroups can help you identify—and fix—unfair model bias. Check out our Data Drift Detection Part II: Unstructured Data in NLP and CV blog post. The Arthur platform offers this type of performance-bias analysis for natural language and tabular models, as well as the ability to partition by multiple attributes at a time to provide you more granular insights into potential biasesAnother step is to take proactive steps to debias language outputs from your models. We at Arthur are working on building out more features to do this in our platform—stay tuned for more!Use token-level explanations to understand black box NLP model behavior.Whether your NLP model is a “bag of words” model (the position of the words doesn’t matter) or a sequence-based model (the context and position of words do matter), getting a token-level explanation of your model output is incredibly useful and important for understanding why your model might be getting something wrong, and understanding how to take the right steps to fix it.For example, if we are using a medical document classifier to predict different document types in a given set of medical records, we can use this technique to understand the specific words that resulted in a given document being classified as a document about cardiovascular and pulmonary issues.On the far right-hand side, we see the top words contributing to the predicted class in green and words contributing against it in red. We see this illustrated in the document itself. This is useful for a domain expert to understand in human context the “real world” value of the highlighted words. For example, we see the word “procedure” appears at the top of the list of explanations, meaning it most significantly impacted the (correctly) predicted classification of Cardiovascular/Pulmonary.If we click through, on the left-hand side, some of the other incorrect predictions, we begin to notice this word is also a top influencer in those predictions as well. See Urology, below:Arthur's NLP explainability tools provide token-level explanations for model outputs.Above, we see that “procedure” was the second most influential word in this classification prediction.This is where we see the importance of domain expertise and human involvement. Given the context of a healthcare system, it is easy for us to imagine how the word “procedure” might appear frequently across medical specialties. Perhaps it’s worth exploring a different model that accounts for the positioning of particular words, or perhaps we simply need to retrain how it treats certain words based on what we know is most influencing predictions. In any case, we see how providing this analysis for each prediction and word is crucial information.Monitor your NLP Models with Arthur.The Arthur platform has recently released extensive monitoring support for NLP models, including NLP data drift detection, token-level explainability that provides insight into the key drivers in NLP classification model predictions, and bias detection for NLP models.If you’re deploying NLP models into production and are looking for a solution for monitoring those models over time, we’d love to connect and show you how Arthur can help. Request a demo today.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/nlp-model-monitoring-with-arthur
225,"Making models more fair: everything you need to know about algorithmic bias mitigation
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessMaking models more fair: everything you need to know about algorithmic bias mitigationBy:Jessica DaiMarch 23, 2021As ML models affect our lives more and more, machine learning practitioners need to ensure that our models are not creating harm for end-users. It is especially important to make sure that our models are not unfairly harming any subgroups of our population. The first step is identifying and quantifying any potential bias in a model, and many different definitions of group fairness have been proposed. The Arthur platform provides proactive bias monitoring and alerting, so you know exactly when, where, and how algorithmic bias is occurring in your models. At Arthur, we’re also interested in the question of what we can do to help make your models more fair. In this post, we briefly describe families of techniques for bias mitigation: ways to improve an unfair model. While this is an active area of research, current mitigation techniques target specific parts of the model development lifecycle: preprocessing, or adjustments on the training data; in-processing, or algorithms specifically intended to be fair; and post-processing, or adjusting the outputs of the model. The right solution might depend on the use case, the industry, and the model deployment; in this post, we outline several families of approaches to fair ML. Why do we observe unfair classifiers?Intuitively, training classifiers on historical datasets “bakes in” bias into the model: if your hiring algorithm uses data from the 1960s, it’s likely to think that women are great at being secretaries, while men should be CEOs. However, actually characterizing how “biased data” might be related to a “biased model” is a more complicated task.One commonly discussed cause of bias in a learned model is an imbalance between subgroups in the data. By definition, if we have a subgroup that is a minority represented in our training data, then that means we have fewer observations of them compared to majority groups. When the classifier is training, it is optimizing a loss function over the whole dataset. If the major class is truly dominant, then it is possible that the best way to achieve high overall accuracy on the training data is to be as accurate as possible on the majority group while incurring errors on the minority group. Therefore, if the majority group and minority group have any differences in their properties and relationship to the target variable, the model is likely adhering primarily to the patterns of the major data and potentially ignoring the contributions of the minority. This could mean the model is fairly accurate for the majority groups, but much less accurate for the smaller subgroups. Of course, it’s worth mentioning that not all data biases result from undersampling. In many cases where the data in question represents human behaviors, as in the cases of loan performance, hiring, or crime data, historical biases from human personalities can also show up in the information. Hiring data from the 1960s made predictively, for instance, might suggest that women are best suited as secretaries rather than executives. No matter what technique you choose to ameliorate bias among these options, close attention should be paid to the historical context through which the data collection practices and societal influence can be better understood.Pre-Processing Bias MitigationPre-processing techniques for bias mitigation tend to be all about the data. As described in the previous section, particular characteristics of the training data may directly cause the problematic performance of a learned model. For this reason, many techniques for pre-processing focus on modifying the training set to overcome versions of dataset imbalance. This could be achieved in many ways including resampling rows of the data, reweighting rows of the data, flipping the class labels across groups, and omitting sensitive variables or proxies. Other techniques consider learning direct modifications and transformation functions that achieve desired fairness constraints.  In all cases, the strategy is to change the underlying training data, and then proceed with training using any classification algorithm desired. By modifying the training data in these specific ways, the outputs of the learned classifier will be less biased.In-Processing Bias MitigationWith in-processing techniques, we want to create a classifier that is explicitly aware of our fairness goals. That is, in training the classifier, it is not enough to simply optimize for accuracy on the training data. Instead, we modify the loss function to account simultaneously for our two goals: our model should be both accurate and fair. This modification can be achieved in many ways such as using adversarial techniques, ensuring underlying representations are fair, or by framing constraints and regularization. In each case, the goal is that the underlying classifier is directly taking fairness into consideration. As a result, the outcomes of that trained classifier will be less biased as compared to a classifier that knew nothing about fairness. Post-Processing Bias MitigationFinally, there is a family of techniques that aim to only adjust the outputs of a model and leave the underlying classifier and data untouched. The benefit here is appealing in its simplicity - in using post-processing methods, we allow the model development team to use any modeling algorithms they wish, and they don’t need to modify their algorithm or retrain a new model to make it more fair. Instead, post-processing methods center on the idea of adjusting the outputs of an unfair model such that the final outputs become fair. As an example, early works in this area have focused on modifying outcomes and thresholds in a group-specific manner. Suppose we build a classification model to assist in credit risk decisions. After much hyperparameter tuning, we arrive at a model that is accurate and generalizes well, but we notice that it tends to favor older loan applicants over younger applicants. With post-processing techniques, we would keep the classifier as is, but adjust the outcomes so that the overall acceptance rates are more equitable. We would pick a definition of fairness (say, Demographic Parity), and adjust the treatments across the groups such that the final outcomes are as desired. This means we might have group-specific thresholds instead of a single threshold for the classifier. Detecting unintended bias is the first step to mitigating it. Arthur gives you the power to detect and analyze bias in your ML models.It’s important to note that in this scenario, there remains a lot of legal ambiguity around bias mitigation. With so much unknown about how courts will handle algorithmic discrimination, many organizations are leaning heavily on their legal teams for how to navigate this complexity!Many post-processing techniques have this basic structure in common: they leave the classifier and the data alone, and only adjust the outcomes in a group-dependent way. And while binary classification has been a focus in the past, recent work has sought to extend these ideas to regression models as well. The overall framework can be effective for achieving fairness in an ML system, though in some use cases, treating groups differently could be an uncomfortable proposition, or even an illegal one. Accuracy/Fairness TradeoffWhen we embark on deploying ML models that are more fair, we have to acknowledge that this fairness does not come for free; in fact, in many cases, it may conflict with model accuracy. Consider one extreme: a model that is as accurate as possible (relative to available ground truth) is potentially quite unfair and discriminates against at least one subpopulation. Consider the other extreme: a model which is perfectly fair and is equitable across all populations, this model must be less accurate than a model that did not consider fairness as a constraint. (Though some recent work suggests the tradeoff may not necessarily always occur, the behavior of ""fair algorithms"" when deployed in the real world will not always match the results demonstrated in theory; as a result, understanding the relationship between fairness and accuracy is critical to being confident in the models we choose to use. )Between these two extremes live a broad family of possible models that balance the concerns of accuracy and fairness. This set of models forms a Pareto frontier (efficient frontier) in the space of Accuracy vs Fairness. The figure below, from a 2019 survey paper characterizing the performance of many popular fair ML algorithms, illustrates this tradeoff empirically: on the x-axis is Disparate Impact, a measure of fairness, while the y-axis is accuracy. (The entire paper is worth reading; it is an excellent introduction to many common considerations for the performance of fair ML models.) As practitioners and stakeholders, we must confront questions about this tradeoff: for each use case, we must weigh the costs of potential harm through unfairness against costs of potential harm through reduced accuracy.The following charts illustrate the trade-off between fairness and accuracy among some of the more popular fair ML algorithms.These are challenging questions that have no single right answer. Instead, ML practitioners must work together with stakeholders such as business leaders, humanities experts, compliance, and legal teams and formulate a program for how to best treat your population. The Arthur platform brings together performance monitoring and algorithmic bias monitoring into a unified view for all of your stakeholders, so you can make informed decisions about how to make your models more fair and effective. If you’d like to learn more about how Arthur can help you combat bias in your AI systems, please reach out to schedule a demo.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/ai-bias-mitigation-101
226,"Deploy, serve, monitor, and maintain AI at scale with Arthur and Algorithmia
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesDeploy, serve, monitor, and maintain AI at scale with Arthur and AlgorithmiaBy:Arthur TeamFebruary 22, 2021At Arthur, we are obsessed with making sure that our customers have complete AI observability for all of their machine learning models, no matter where they are deployed. But we know that production AI systems need more than just monitoring—and that’s why today, we are excited to announce our partnership with Algorithmia, the industry leader in machine learning operations (MLOps) and model management.Algorithmia’s enterprise MLOps platform manages all stages of the machine learning lifecycle within existing operational processes, enabling you to put models into production quickly, securely, and cost-effectively. Unlike inefficient and expensive do-it-yourself MLOps management solutions that lock users into specific technology stacks, Algorithmia automates ML deployment, optimizes collaboration between operations and development, leverages existing SDLC and CI/CD systems, and provides enterprise-grade security and governance across the lifecycle.Arthur is the proactive model monitoring platform that gives organizations the confidence and peace of mind that their AI deployments are performing at peak. Arthur provides a layer of performance monitoring, algorithmic bias detection, and explainability, even for black box models, so data science teams can detect, diagnose, and fix any issues in production.We are working with the Algorithmia team to make it easier than ever to use their top-notch deployment, serving, and management tools and our leading monitoring capabilities. Together, Arthur and Algorithmia provide a powerful set of tools that give you complete control over your production AI. This blog post will demonstrate just how easy it is to get started with the Arthur and Algorithmia integration, so you can complete your AI stack.Arthur and Algorithmia: Helping you reach AI maturity fasterAlgorithmia’s 2021 enterprise trends in machine learning report revealed that 56% of organizations struggle with governance, security, and auditability issues—making it the #1 challenge for model deployment and management. Arthur extends Algorithmia’s monitoring capabilities to give you rich insights into model performance, data integrity, and AI fairness, helping you manage AI governance more completely. If you’re already using Algorithmia for MLOps, it takes just minutes to add Arthur monitoring. With our integration, Algorithmia customers can take full advantage of advanced ML lifecycle management workflows enabled by Arthur’s runtime monitoring capabilities—think automated refits based on data drift, horse racing (A/B testing) different versions of a model, and more.Let’s walk through what it looks like to use Arthur and Algorithmia together to monitor and maintain AI at scale.Deploy your model behind an endpoint with AlgorithmiaWe’ll use Algorithmia to serve our trained model behind an Algorithmia endpoint. Here is an example Algorithmia project where we’re doing just that: We have a saved model (pkl file), and we’re loading this model into memory. Our implementation of Algorithmia’s apply() function is to invoke the predict_proba() method for any new inputs to the endpoint.Deploying your machine learning model behind an endpoint with Algorithmia's apply() function.This model is now deployed behind an endpoint that we can hit with REST, or with any of Algorithmia’s supported SDKs in several languages.Log your model inputs and outputs with ArthurNext, let’s integrate this project with Arthur so we can monitor this model. Let’s assume we’ve already registered a model with Arthur, and will fetch this object by its model ID. We’ll import the Arthur AI Python library and set up a connection with the platform. We will use that connection to fetch our model by its ID.The last step here is that we’ll want to wrap our model’s predict_proba() method using the loginferences decorator provided in the arthurai library. To do this, we’ll define a simple function called **modelpredict()** for this wrapping, and we’ll decorate that method as shown below.Finally, our implementation of Algorithmia’s apply() function is simply to call our model_predict() wrapper for any inputs to the endpoint. Each time the endpoint is invoked, the model’s input and output are logged with Arthur for automated analytics and alerting.Just a few lines of Python, and you're all set to monitor your Algorithmia-deployed model with Arthur's ML monitoring platform.With a couple of extra lines of Python, we can easily add Arthur’s monitoring tools to any models we have deployed with Algorithmia.Unlock model optimization, versioning, and fairness with ArthurAI and AlgorithmiaThe Arthur-Algorithmia integration enables you to deploy, serve, manage, and maintain your AI at scale with advanced monitoring and governance. Ready to get started today?If you’re already using Algorithmia and Arthur, it takes minutes to implement the integration as demonstrated above. If you’re not already using Algorithmia, sign up to watch a demo today and learn how enterprise MLOps can accelerate your time-to-value for ML.And if you’re not already using Arthur, get in touch with us to schedule a demo and learn more about how we can help you get started with proactive model monitoring.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/algorithmia-partnership
227,"Our top takeaways from NeurIPS 2020 on Responsible Machine Learning
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsOur top takeaways from NeurIPS 2020 on Responsible Machine LearningBy:Jessica DaiJanuary 8, 2021No longer the niche sub-field it was several years ago, ""fair"" ML and related topics have now become a core part of the field of machine learning. At NeurIPS 2020, the broader theme of responsibility (even beyond fairness) in ML has a much larger footprint at the conference; on a more granular level, fair ML as a field overall has also developed substantially, asking a broader range of questions critical to operationalizing fairness.From the start of the conference, NeurIPS 2020 emphasized the importance of thinking deeply about the implications of work in machine learning research: the conference kicked off with a keynote from Georgia Tech's Charles Isbell, titled ""You Can't Escape Hyperparameters and Latent Variables: Machine Learning as a Software Engineering Enterprise."" Despite its somewhat dry title, the talk is anything but. Isbell makes the case for machine learning researchers to approach their work in the same way as software engineers must, considering how the usage of software must shape its development. Isbell dispells the myth of bias being ""only due to biased datasets"" and therefore something that individual scientists need not be concerned about; this keynote set the tone for how the rest of the conference unfolded. (You can watch the full video of the keynote; this is nothing like any pre-recorded talk I've ever seen, and I cannot recommend it more strongly.)The growing community around fair/responsible ML is clear not only in the papers accepted to the main conference, but also in the workshops, several of which were dedicated to discussing concerns related to developing fair ML—Resistance AI, Dataset Curation and Security, Fair AI in Finance, Algorithmic Fairness through the Lens of Causality and Interpretability, Consequential Decisions in Dynamic Environments—and several more where Retrospectives, Surveys, and Meta-Analyses, ML for Economic Policy, Broader Impacts of AI Research, Human and Machine-in-the-Loop Evaluation and Training Strategies. In the rest of this piece, I will be highlighting contributions from both workshops and the main conference.At Arthur, we’re really excited about the development of these research directions:Fairness for a broader range of algorithms and applications.Most early work in fair ML focused on binary classifiers with a single protected attribute; today, the vast majority of available open-source implementations are also only available for binary classifiers with a single protected attribute. Of course, there are many other problem settings where fairness may be a concern, and this year's NeurIPS saw the introduction of many new fair ML algorithms for many more scenarios: clustering, streaming, online learning, regression models, overlapping group membership, multiple classes, and scenarios without access to information about demographics.More nuanced ways of measuring, understanding, and communicating fairness.While metrics of fairness based on output distributions and group-conditional error rates have been common for some time now, some exciting lines of work have been developing around more nuanced approaches to measurement. Can I Trust My Fairness Metric? emphasizes the high variance of fairness metrics when batch or dataset sizes are small, and introduces a way to generate more accurate and lower-variance estimates of metric values; meanwhile, Measuring Bias with Wasserstein Distance, from the Dataset Curation and Security workshop, proposes an alternate metric for fairness that captures inequity that may be missed by current standard metrics. Opportunities for a More Interdisciplinary Approach to Perceptions of Fairness in ML, from the ML Retrospectives, Surveys, and Analyses workshop (ML-RSA), draws insight from the field of psychology to discuss how common fairness metrics are interpreted by human end-users.Analyzing the qualities of fair algorithms, such as robustness and downstream implications.Complementary to the development of fair algorithms themselves, a substantial amount of new work discusses how fair algorithms might perform under a variety of conditions and situating the algorithms in the broader context of deployment. For example, How Do Fair Decisions Fare in Long-term Qualification? considers the impact of static fairness constraints on long-term well-being; the Workshop on Consequential Decision Making in Dynamic Environments was dedicated to work in this area. Similarly, Fair Multiple Decisionmaking Through Soft Interventions considers the scenario where there are multiple, potentially-interacting algorithms. In a parallel thread, Ensuring Fairness Beyond the Training Data proposes the first (to my knowledge) algorithm to train a fair classifier that is provably robust to a set of possible distribution shifts. More broadly, the Workshop on Algorithmic Fairness through the Lens of Causality and Interpretability has work contextualizing fair algorithms in many ways, such as Fairness and Robustness in Invariant Learning, which connects topics from causal inference and domain generalization to fairness. The Fair AI in Finance workshop also touches on related issues, particularly with regards to the application.Concerns of fairness and bias complement concerns of model robustness writ large. Indeed, discussion of the “robustness” of a machine-learning-based system necessarily includes discussions of dataset and model security, policy and privacy, dataset and model bias, data ingest via scraping and labeling, amongst numerous other considerations. Toward that end, joint with colleagues at CMU, IBM, Illinois, Maryland, and TTIC, our Chief Scientist John Dickerson hosted the Workshop on Dataset Curation and Security, which brought together researchers from the adversarial ML and fairness in ML communities as well as policy wonks from the Brookings Institute and other “tech-adjacent” bodies for a full day of discussion of what it means to claim, and what it might take to improve, “robustness” of machine learning models. In short, it is hard to make statements about model behavior (vis-à-vis, for example, fairness) without also deeply considering other dimensions such as security and the legal landscape. Our VP of Responsible AI, Liz O’Sullivan, also gave an invited talk at this workshop on some of the dangers of scraping data from the Internet, focusing primarily on how this can take agency from unknowing humans and may result in otherwise unexpected or undefined behavior. Similar concerns and sentiments were raised at other NeurIPS workshops, in papers, and in the aforementioned invited talk by Charles Isbell; we expect this trend to continue in the coming months and years.Beyond fairness: bigger-picture views of algorithmic (in)justice.It's been clear for a while now that it is not enough for algorithms to simply be ""fair"" — there are several related topics of technical interest, as well as broader considerations that must be made when analyzing the impact of algorithms in the world. The ML-RSA workshop has several works in this category, such as Arthur's own Counterfactual Explanations for Machine Learning, as well as A Survey of Algorithmic Recourse. ML-RSA also has some more critical work, such as Data and its (dis)contents: a survey of dataset development and use in ML research. Finally, it behooves everyone working in fair ML to take a look at the work from the Resistance AI workshop, which is full of thought-provoking work, both technical and non-technical, that questions the way power is arranged and rearranged by ML systems.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/neurips-responsible-ml
228,"An Overview of Counterfactual Explainability
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedExplainable AIAn Overview of Counterfactual ExplainabilityBy: Keegan HinesDecember 10, 2020Within the field of explainable AI (XAI), the technique of counterfactual explainability has progressed rapidly, with many exciting developments just in the past couple years. To help crystallize and understand the major development areas, we’ll be presenting a new paper at the NeurIPS Workshop on ML Retrospectives, Surveys, and Meta-Analyses. This post will provide a high-level summary, but if you’re interested in getting started in this area, we encourage you to check out the full paper.What are counterfactual explanations?Counterfactual explanations (CFEs) are an emerging technique for local, example-based post-hoc explanations methods. Given a datapoint A and its prediction P from a model, a counterfactual is a datapoint close to A, such that the model predicts it to be in a different class Q (P ≠ Q). A close datapoint is considered a minimal change that needs to be made to get a different prediction. This can help in scenarios like rejection of a loan or credit card request, where the applicant is willing to know about the smallest change in the feature set that can lead to acceptance of the request.Explicitly, consider a person who applies for a loan and is rejected. In our simple example, say that the person is represented by a length-3 feature vector x: liquid assets of $10K; outstanding debt of $50K; and annual income of $45K. So, x = ($10K, $50K, $45K). Our fictitious loan agency uses a pre-trained binary “loan acceptance” classifier, f, that takes as input length-3 feature representations of applications and returns one of two labels: y = 0 (reject), or y = 1 (accept). Here, then, the applicant is rejected because f( x = ($10K, $50K, $45K) ) = reject. Roughly speaking, a counterfactual explanation for this decision (reject) would describe changes, call them x’, to the applicant’s initial feature vector x such that f( x’ ) = accept. There may be many possible counterfactual explanations: for example, lowering outstanding debt from $50K to $25K to form x’ = ($10K, $25K, $45K); or, increasing liquid assets from $10K to $20K to form x’’ = ($20K, $50K, $45K). Some may be easier or harder to attain for the applicant -- and some may be completely impossible to achieve -- motivating research into the creation of “the best” counterfactual explanations for a particular use case, which we discuss in greater depth below.Fig1: Illustrative diagram counterfactual explanations. The datapoint labeled X (blue) got classified in the negative class. CF1 (red) and CF2 (green) are two counterfactuals for X, which the model classifies in the positive class. Several counterfactuals can be generated for a datapoint, which differ in closeness to the original datapoint and other desirable properties.Themes of research in CFEsMuch of the literature in counterfactual explanations have proposed algorithms to address additional aspects of the problem. We categorize recent research into the following major themes:Actionability: A CFE is only useful if it prescribes changes to features that can actually change. It would be unhelpful if I were told to change my birthplace in order to receive a loan.Sparsity: A useful CFE should modify only a few features in order to be simple and easy to use.Proximity: A useful CFE should be the smallest possible change that achieves the desired outcome.Causality: A useful CFE must be able to adhere to any causal constraints that a domain expert specifies. For example, I should not have to decrease my age in order to get a loan.Data Manifold: A useful CFE should result in a datapoint that is similar to other datapoints seen in the training data. It would be less trustworthy if the resulting datapoint is utterly unlike anything the classifier has ever seen.Speed: CFEs should be generated quickly for new, incoming datapoints.Model Access: Some CFE approaches require detailed knowledge of model internals and gradients. Others can work in a black-box fashion and are model-agnostic.In our survey paper, we collect, review, and categorize 39 recent papers that propose algorithms to solve the counterfactual explanation problem. We design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. This provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.ConclusionsCFEs present a compelling form of XAI, providing users with understandable and actionable feedback. The additional constraints and desiderata explored in recent years seek to ensure that these explanations are always reasonable and useful. Many exciting open questions remain, and we close our paper by proposing research challenges for the community to tackle in the coming years. We firmly believe that CFEs will form a long-lasting part of the ML explainability toolkit.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/counterfactual-explainability-neurips
229,"Recommendation Engines Need Fairness Too!
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessRecommendation Engines Need Fairness Too!By: Keegan HinesJuly 16, 2020As we turn to digital sources for news and entertainment, recommendation engines are increasingly influencing the daily experience of life, especially in a world where folks are encouraged to stay indoors. These systems are not just responsible for suggesting what we read or watch for fun, but also for doling out news and political content, and for surfacing potential connections with other people online. When we talk about bias in AI systems, we often read about unintentional discrimination in ways that apply only to simple binary classifiers (e.g. in the question “Should we let this prisoner out on parole?”, there are only two potential predictions yes, or no). Thinking about mitigating bias in recommendation engines is much more complex. In this post, we’ll briefly describe how these systems work, then surface some examples of how they can go wrong, before offering suggestions on how to detect bias and improve your users’ experience online, in a fair and thoughtful way.Part 1 The Anatomy of a Recommender SystemIf you’re coming to this article as someone who regularly builds or works on recommender systems, feel free to skip this part. For those of you needing a refresher or primer on the topic, read on!Recommender engines help companies predict what they think you’ll like to see. For Netflix, YouTube and other content providers, this might happen in the format of choosing which video cues next in auto-play. For a retailer like Amazon, it could be picking which items to suggest in a promotional email. At their core, recommender systems take as input two “sides” of a problem -- users and items. In the case of Netflix, each user is an account, and each item is a movie. For Amazon, users are shoppers, and items are things you can buy. For YouTube, users are viewers, items are videos, and a third component are the users that create the content. You can imagine analogues with newspapers and other media sources such as the New York Times and the Wall Street Journal, music streaming services such as Spotify and Pandora, as well as social networking services such as Twitter and Facebook.Users rate some items, but not all of them. For example, even if you binge watch shows on Netflix, it’s unlikely that you have rated even a small fraction of Netflix’s vast content catalogue, much less so when it comes to YouTube’s library, where over 300 hours of content are uploaded every minute. A recommender system’s goal is, given a user, find the items or items that will be of greatest interest to that user, under the assumption that most items have not been rated by most users. How is this done? By learning from other, similar items, similar users, and combinations of the two.Recommender systems recommend content based on inductive biases. One common inductive bias is that users who seem similar in the past will continue to seem similar in the present and future. In the context of recommender systems, this means that users who have, for example, rated videos similarly on YouTube in the past will probably rate videos similarly moving forward. Recommendations based on this intuition might try to find similar users to a particular user, and similar pieces of content to a particular piece of content, and then combine learnings from those two neighborhoods into an individual score for that particular pairing of a user and item. By doing this for every user-content pair, the recommender system can “fill in all the blanks”, that is, predict a rating for each combination of user and piece of content. After that, it is simply a matter of picking the most highly-rated pieces of content for that customer, and serving those up as you might see in a sidebar on YouTube or a “view next” carousel on Amazon Shopping.Part 2 What Could Go Wrong?As we’ve discussed above, recommender engines attempt to “fill in the blanks” for a particular user by guessing at their level of interest in other topics when we only know how they feel about things they’ve already seen or read. Most recommender engines are a blend of “nearest neighbor” calculations and active rating elicitation, using a combination of supervised and unsupervised learning alongside deterministic rules that modify the selection process among the content that you could potentially recommend. To discuss some of the issues that often arise in recommender engine bias, we’ll look at a couple of examples from industry that illustrate the nuance and complexity involved.One of the more common issues we see in industry can be illustrated by YouTube’s spectacularly named “Gangnam Style Problem”. The problem is this no matter what content you recommend to your user, when one looks at the potential pathways they could take from one recommendation to the next, they all lead back to whatever happens to be the most popular video that day. While this may be good news for PSY and K-pop stans worldwide, gaining traction within a recommender engine can make or break the experience for someone creating content on these platforms, where they need their content to be seen in order to survive.More so every day, we hear about complaints from within the YouTube creator community, claiming that their channels suffer from this disparity, and that YouTube is biased against emerging artists. Thinking this through from a business perspective, it’s easy to see why this might be the case YouTube wants to keep users on the page, and they’re more likely to do that if they can show you content that they know you’ll actually like. In fact, the less YouTube knows about how users will interact with your particular brand of content, the more risky it becomes to promote it.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/recommender-systems-need-fairness-too
230,"ArthurAI Fintech Innovation Lab: Class of 2020 Recap
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsArthurAI Fintech Innovation Lab: Class of 2020 RecapBy:Victoria VassilevaJuly 1, 2020The LabEvery year since 2010, the Fintech Innovation Lab - a joint program between the Partnership Fund for New York City and Accenture - chooses just 10 emerging enterprise tech startups developing innovative solutions for financial institutions. This year, over 250 early- and growth-stage companies applied. Over 40 participating financial institutions and VC firms across the globe choose each class. Needless to say, we’re proud to have been selected alongside the rest of our 2020 class.The ExperienceThe Lab provides access to top executives at the world’s leading financial institutions who, throughout our experience, routinely extended exceptionally generous and open feedback. The added exposure to Procurement and Security leadership for insight into their requirements and timelines is a priceless check on our enterprise-readiness. Passing these checks is crucial to success and reflects how thoroughly the Lab works to prepare us .Arthur was born out of the pain points that our founders personally experienced within the financial industry. The opportunity to quickly and repeatedly validate our product and market fit directly with key decision makers and core users alike was invaluable. This is table stakes for producing a best-of-breed, turnkey observability platform.Model performance, explainability, and fairness are board-level, company-wide concerns. We built Arthur to serve as a single pane of glass for the entire organization - serving data scientists and engineers as well as operations, risk teams, and business stakeholders. This made the Lab the perfect opportunity to refine our messaging across dozens of high-leverage use cases.The Challenge - 2020 and Beyond2020 has presented no shortage of tough challenges. For this reason we feel particularly privileged for the chance to join forces with leading financial institutions on the joint mission of not only triaging the immediate and ongoing impact of COVID-19, but also investing in proactive production AI monitoring fit for the advanced technologies driving their customers’ financial health.The profound changes to our society immediately showed up in data and financial models. The impact proved to flow both ways. Model performance, explainability, and bias required a renewed urgency, with the pandemic underscoring how they are connected. We have been inspired by the cultures of care and innovation we witnessed in so many of our partners to address these needs.The regulators and governance & compliance leaders were no different. At Arthur some of our relationships with regulators go back decades and we remain active in the policy space. While we regularly participate as advisors in this arena, we were nonetheless very motivated by the enthusiasm from FINRA, the NYS DFS, the OCC, and the CFTC for open dialogue as partners toward industry model fairness. We are excited by their interest to collaborate on solutions.COVID-19 will certainly carry a lasting impact - it has been promising to engage with our peers on the financial and customer challenges at hand not only today, but ensuring they are also prepared for tomorrow.Going VirtualAs a result of the ongoing pandemic, the Lab, like many, pivoted to a first-time-ever, entirely virtual experience - we’re happy to report that this came with its own advantages. Meeting with senior executives is typically a fairly formal event in a corporate office, but now we were entering each other’s homes - admiring background art, scoping home libraries, and grinning as a few kids made surprise appearances. It was a wonderfully leveling experience that enabled genuine relationships when it mattered most. We’ll see you over drinks in 2021.The FiguresWe mutually paired with 6 premier Financial Institutions40+ meetings with executives and counting7 meetings with regulatory bodies such as FINRA, NYS DFS, the OCC, and the CFTCDozens of meetings with Fintech VCs, Insurance leaders, entrepreneurs, and media and marketing advisorsWhat’s NextThough the program has come to a formal close, the opportunities and support have not. Having tailored and proven our enterprise-grade, production model observability platform and its value with some of the most advanced financial technology providers, we are excited about the discussions of pilot programs and upcoming members of our select Customer Advisory Board. Stay tuned for more on that!We know we have made lasting and ongoing partnerships with the institutions and organizers alike - for that alone we’d advise anyone to pursue a spot in the Lab.Special thanks to Maria Gotsch, Sunny Parikh, and Jeff Kalski for their ever-present positivity and support!Your TurnGet in touch to see for yourself.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/arthurai-fintech-innovation-lab-class-of-2020-recap
231,"How to Build a Production-Ready Model Monitoring System for your Enterprise
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringHow to Build a Production-Ready Model Monitoring System for your EnterpriseBy: Keegan HinesJune 15, 2020Whether your team has recently deployed their first or their 100th ML model to production, you likely appreciate the importance of proactively monitoring your models’ inputs and outputs. In an ever-changing world, the model you trained in the lab is almost certainly performing differently out in the wild. In order to ensure that all stakeholders can trust your ML systems, you need to ensure that your models continue to perform well, that they can be understood, and that they don’t discriminate unfairly. These ideas form the major pillars for AI Observability: Performance, Explainability, and Fairness. In this post, we’ll walk through the components of building an enterprise-wide system for model monitoring, so that you can provide an always-on visibility to stakeholders across your company, including data scientists as well as risk officers, legal advisers, and business stakeholders. This guide will help you understand what you’d need to build for your enterprise, or, you can reach out to us at ArthurAI to learn more about our turnkey system for enterprise model monitoring.Platform Agnostic IntegrationAt a large organization, you might have many different data science teams, all of whom use their favorite tools for model development and deployment. Perhaps some corners of your org are big fans of Tensorflow, others love SageMaker, others love H20, and yet some others have in-house tooling built with open source stacks. We see this pattern all the time and we think it’s great: let data scientists use the tools that they love so they can be as effective as possible. When it comes to model development and deployment, this heterogeneity can be a sign of a diverse and creative data science community. However, from the perspective of technology risk management, all these disparate systems might make you (and your legal team) a little uneasy. To answer a simple question like “Are all the models doing ok today?”, you’d have to go check many different systems and hope they all offer comparable information. What you’d really want is a single go-to place for seeing how all your models are performing, and a way to be notified as soon as something goes wrong. This is why it is important that across your ML ecosystem you need to create a centralized and standardized layer for model monitoring and governance. With this goal in mind, your monitoring system needs to be platform agnostic and allow drop-in integration across all those different stacks.Fortunately, much of model monitoring can be achieved without tight coupling to the specific environments for model development and deployment. As we’ll describe below, monitoring the stability of a model’s outputs is a process that can be completely ignorant of the model’s development process or deployment environment. The same is true for monitoring a model’s inputs and for ensuring a model is fair. And with many methods for model explainability, the specifics of the model training procedure are irrelevant once we have a trained model to probe. Therefore, it is possible to build out a platform-agnostic monitoring system that is accessed through REST APIs so that all the models in your ecosystem are consistently monitored in real-time, no matter how they were built or deployed. Even better, it’ll be important to provide data scientists and engineers with easy-to-use client libraries in their favorite languages (mostly Python, but possibly also Java, Scala, or R).Performance and Data MonitoringModel OutputsThe most salient question you want to answer is this: Is my model performing as well today as I expect it to? Notions of “good” performance will vary depending on different business cases, but there are many general themes you can bring to bear. In the ideal setting, your models will have access to a timely ground-truth so that you can effortlessly compute things like accuracy, confusion matrices, ROC curves, and so on. Monitoring a model’s accuracy can make you aware of a performance issue as soon as it happens. In fact, it would be great if you could make any such metrics extensible, because surely someday you’ll find a data scientist who wants something slightly different than the metrics you are calculating in your system. Allowing users to onboard custom metrics calculations will ensure that no data scientists are left out. In a use case such as product recommendation or targeted advertising, you’ll know instantly whether your model’s outputs resulted in a click or not. However, there are many cases where ground-truth is hard to come by, and what can we do then?An example is issuing credit cards - if you approve someone for a credit card, it will likely be months or years before you decide that was a bad decision. In these cases, you’ll need to generate proxy metrics for accuracy by monitoring the stability of a model’s inputs and outputs. The stability of a model’s output predictions is a useful proxy in the absence of ground truth. Since a model, once fitted and deployed, never changes it’s view of the world (its decision surface), any notable changes to its output can be attributed to significant changes in its inputs - something your data scientists will want to know about. You should consider monitoring the distribution of output predictions from a model, whether a regression model, classification model, multilabel model, multimodal model, or anything else. There are many ways to quantify similarity/stability of distributions, depending on the type of model and dimensionality (more on that below). When it comes to monitoring the stability of distributions, you might quantify changes through time, or changes relative to baseline (such as the training set), or both.Data DriftJust as important as monitoring a model’s outputs, it is vital to monitor the stability of a model’s inputs: often referred to as Data Drift or Concept Drift. Your model is trained on a fixed snapshot of the world - the training data. Once your model is deployed, the world will inevitably change and depending on how severe that change is, the learned decision boundary of your model might become irrelevant. By regularly quantifying the similarity of the data coming through the model today to the data the model was trained on, you’ll be able to quickly identify when things start to go off the rails.There are many ways to quantify distributional similarity and to build a system for data drift detection. Your first approach might be to monitor each of the incoming features to a model and quantify similar to the training set. For example, if a model has Age as an input feature, then you’d want to look at the training set and develop a statistical profile of the data in that training set. Then going forward, you can calculate the similarity of the data coming through, as compared to the training data. Measuring the similarity of two distributions can be achieved in many ways including non-parametric hypothesis tests, KL Divergence, Jensen-Shannon Divergence, Population Stability Index, Wasserstein distance, and many more. What these have in common is that they will take in two empirical distributions as input and will result in a similarity score as output. When that similarity score starts to have big changes, it is an indication that the data today is quite different than the training set, and maybe it’s time for a retrain and redeploy.Computing distributional similarity for each input feature independently will generally tackle a large proportion of the problem, but is not without limitations. An early thing to consider is that we’re implicitly looking for drift only in the marginal distributions of each feature and not in the higher-dimensional joint distribution over the data. For high dimensional datasets, and especially for unstructured data such as imagery and text, it will be important to consider multivariate methods for quantifying distributional similarity. While there is generally not an easily-computed high-dimensional analog for KL Divergence, we can take a model-based approach to quantify data drift. In this approach, we will train a model of some kind on the training set. This model isn’t a classification or regression model, but instead is some kind of density model (or similar) which is trying to understand how the data is distributed in the high dimensional space. Once we have fitted such a model to the training set, we have a snapshot of what “normal” data looks like. Going forward, we can collect new inferences and query this model to quantify how well the new inferences adhere to the distribution of the training dataset. For each single inference, this gives us a powerful mechanism for anomaly detection, since we can now identify incoming points that the model has scored but yet don’t really look anything like the training data. Further, as we aggregate over larger groups of inferences, we have an holistic view into multivariate data drift in a high dimensional space.That general framework can be accomplished with many different modeling techniques. One approach might be using an Isolation Forest or KDTree to fit to your dataset. More recent techniques show promise for fitting properly-normalized probability models to high dimensional datasets, including Variational Autoencoders and Normalizing Flows. Additionally, preprocessing methods such as dimensionality reduction have been shown to be helpful for high-scale problems. In all cases, you’d need to also build a system for training, storing, containerizing, and serving each one of these density models so that streaming inferences can be scored as they come in.Finally, it is worth noting that not all data drift is created equal. This idea is sometimes referred to as ‘virtual’ concept drift and denotes instances where the data has drifted but in a direction that doesn’t materially affect the model’s outputs. Thus, it is very helpful to combine data drift monitoring for each feature with a simultaneous quantification of how important each feature is for model decisioning. Combining data drift modeling with model explainability (more on that below) is a powerful way to prioritize your teams’ time and attention.Explainability as a ServiceUnderstanding a model’s decisions is an important part of building trust for the adoption of ML across your organization. With increasingly-complex ML techniques, their flexibility is often accompanied by a difficulty in understanding why they make their predictions. The field of Explainable AI has put forth many valuable techniques for calculating explanations of model predictions. In your model-monitoring system, if you are able to provide these explanations for every prediction your models make, this can go a long way toward building trust and comfort among a broad class of stakeholders.There are many great techniques for calculating local explanations of ML models. These methods could be model agnostic (like LIME), model-based (like DeepLift), or both (like Shap). In all cases, you need to access a model’s predict function in order to probe the relationships between a model’s inputs and outputs. For your monitoring platform, this means you don’t need to be tightly coupled to the model training environment. Instead, you only need access to the final trained model and ability to probe it. You might hook into existing model deployment APIs, or you might build and replicate a model microservice solely for computing explanations on the fly. In this case, you’ll need to think about containerization for replicating the model’s dependencies and environment. And if your model requires high throughput or high dimensional data (or both) you’ll want to think about ways for autoscaling the computation of explanations in order to keep up with the inference load. Additionally, you might put some thought into refactoring some of those favorite explainability methods to make them more performant for your use cases.Once you’ve computed explanations for every inference coming through your models, it opens many exciting possibilities for helping your organization. First, it provides your data scientists with a granular view in a model’s decision surface, allowing them to identify areas where a model might be underperforming and helping to debug models in production. Second, these explanations form a useful audit trail for your modeling system, ensuring that every decision that a model makes is logged and can be understood at a later time. And finally, considering local and global feature importance will help you understand and prioritize data drift and the emergence of new patterns in your data.Algorithmic FairnessIt is important to not only ensure your models are making the “right” decision from a statistical standpoint, but also the “right” decisions from an ethical standpoint. It has become clear across many industries that systems for automated decision making can exacerbate disparate impact and discrimination against specific groups of people. Ensuring that your models are fair is tantamount to ensuring that they result in similar predictions/outputs for all relevant subpopulations of your customer base. Traditionally, fairness analysis is conducted over protected classes as defined by race, age, and sex. But for your business, there could be many other dimensions and factors over which you want to ensure your model is resulting in comparable outcomes. For example, you might want to understand model disparities by geography, income, spending level, or any business-driven segmentation of your customer base.Many researchers have attempted to quantify notions of algorithmic fairness, with many such definitions proposed in recent years. For your business, you should decide which definitions and fairness metrics are most aligned with your company’s goals for Responsible AI. The next step is to build a Fairness Audit framework into all modeling done in your organization. In this pursuit, you would examine all models’ inputs and outputs per any sensitive/protected classes and arrive at a quantification of disparate impact. Naturally, this kind of fairness audit would not be a static process but would be an ongoing analysis that is continually conducted as new inferences go through your models. Ideally, you could provide an easy-to-use dashboard for identifying, exploring, and mitigating disparate impact in your models. This dashboard would make these fairness metrics accessible to all important stakeholders in your organization, and ensure that this information is not just residing with data scientists.APIs, Storage, and ComputeWith inferences stream into your monitoring platform, and real-time metrics being computed for drift, accuracy, and more, you’ll need to put some careful thought into data storage, access, and streaming analytics. Some data science uses cases operate in large batches while some might operate in a streaming fashion, so a combination architecture with Kafka and Spark might prove useful. Many of the metrics and analytics we’ve described can be computed in a streaming context and autoscaled to meet load requirements. Once these metrics, analytics, and explanations have been computed over the inferences, it would nice to make all of this data available to data science teams to explore on their own. You might consider storing the metrics in a datastore appropriate for fast access and large scale, so that your data scientists can quickly slice and dice this data. You could even connect this backend store to an interactive data visualization dashboard, allowing your teams to explore a model’s decision space and better understand areas to improve and debug.Real-Time AlertingOnce you’re calculating all the previously described metrics (and housing them in a scalable data store), you’ve got everything you need for real-time alerting. You can let data scientists know the moment a model’s accuracy starts to drop too much or an important feature seems to have drifted significantly away from the training set. You can build alert integrations directly to where people are spending their time, including email, Slack, or ServiceNow. Apologies in advance when you get a 2am wakeup call about your model’s performance plummeting.User InterfaceThe great thing about instrumenting a platform-agnostic model monitoring tool, is that it suddenly enables disparate stakeholders to have effortless access to a model’s outputs and behaviors. Concepts that might have typically been stuck inside a data scientist’s notebooks (things like model explanations) are now readily available for a broader audience to consume. You now have a few different personas to think about when designing interfaces. The first might be data science practitioners who are hands-on with model development and will want a very specific and technical view into the data surrounding each model. The second might be data science leaders and risk management leaders, who primarily will be concerned with ensuring that everything is healthy and nothing needs escalation. Finally, you’ll want to think about less-technical business stakeholders who are using these models to accomplish their business goals (and accepting these operational risks). You’ll want to make sure they have easy-to-understand access to major insights around model performance and risk.The Other Important StuffLet’s not forget about things like role-based access control (RBAC), single sign on (SSO) integration with enterprise user directories, end-to-end encryption and other policy compliance. You’ve got to get this stuff right the first try, so be sure to move very carefully and thoughtfully through these topics.Putting It All TogetherWith these pieces in place, your data science teams should be able to effortlessly onboard new (and old) models into your monitoring system and begin sending inferences and telemetry. Your monitoring system will aggregate this information and compute real-time metrics for stability, performance, fairness, and anything else important to your organization. Alerting will provide real-time awareness so that data scientists can begin solving problems before it’s too late. The dashboarding you’ve built will provide access to these concepts to a whole new suite of stakeholders across your business - not just for data scientists anymore. Good luck!Monitor your Models with ArthurOf course, you can build! Or you can trust Arthur. Arthur is a proactive machine learning model monitoring platform that gives you the confidence that your AI deployments are performing as expected and the peace of mind that you can catch and fix issues before they impact your business or cause harm. With performance monitoring, bias detection, and customizable alerts, Arthur makes sure that you never miss an issue, and Arthur’s explainability engine makes runtime debugging effortless.If you’re deploying any (i.e., Tabular, NLP, Computer Vision, etc.) models into production and are looking for a solution for monitoring those models over time, we’d love to connect and show you how Arthur can help. Request a demo today.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/how-to-build-a-production-ready-model-monitoring-system-for-your-enterprise
232,"AI During Black Swan Events
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model MonitoringAI During Black Swan EventsBy:Liz O'SullivanJune 8, 2020The world has been turned upside down by COVID-19 and civil demonstrations during the last few months. The effects on our society, the overall economy, and just about every industry are unprecedented. April and May look absolutely nothing like January or February, and with the situation still unfolding, June and July will be completely different as well. One of the less obvious impacts of this period of rapid change is to the behavior of AI models that play critical roles in our society.At Arthur, we’ve seen models across many industries affected. Trading algorithms are one clear example, as recently covered in the Wall Street Journal.“Hedge funds that use artificial intelligence models to suggest trades and stock picks declined when stock markets unraveled in late February... market experts warn the unpredictable nature of the economy these days could trip up some algorithms that continue to rely on data gathered during better times.”-WSJ, AI Funds Decline—Then Recover—During Market Turmoil, May 1, 2020Let’s examine why these models are so vulnerable.The Shortcomings of Machine LearningMachine Learning (ML) is the most prevalent form of AI today, to the point that the two terms are often used synonymously. ML works by learning patterns in existing data (aka the training set) and then using those learned patterns to reason about future data. It performs quite well when that future data is similar to the training set. As that similarity disappears, the lessons the ML model has learned become less relevant and the model becomes less able to “reason” correctly about the new data it is seeing.In a recent MIT Technology Review interview, Gary Marcus, a vocal critic of deep learning-based approaches to AI said “I think that we’re living in a weird moment in history where we are giving a lot of trust to software that doesn’t deserve that trust.” So what are these shortcomings and how do they relate to our current situation?Consider the ancient game of Go. We all watched as AlphaGo proved that algorithms had finally surpassed humans on this extraordinarily complex challenge. But what if we were to replay those games, adding in a little twist. In this new game, every few moves the rules get altered slightly -- additional rows of dots added, the ability to place more than one stone per turn, etc. This time around, the human would best the algorithm because it would never have seen any data like this in the past. The human, while faced with a more difficult task than a routine game of go, would still be able to play relatively effectively.Simply stated, current approaches to AI do not adapt well to changing situations.AI in the age of COVID-19Right now, the automated systems affecting our finances, healthcare, travel, purchasing intents, and many other aspects of our life are making decisions based on patterns they learned prior to COVID-19. So are models that make business decisions in capital markets, high frequency trading, supply chain management, and manufacturing. But those patterns are much less applicable now; the world has shifted.These shifts and models’ inability to adapt will result in companies losing money at a time when they are already being strained. Meanwhile, consumers and end users who interact with these systems will be on the receiving end of less reliable decisions.Is there a solution?There are avenues of research that seek to solve this problem. Reinforcement and online learning focus on models that continue to learn as new data flows in, so they aren’t limited to patterns learned from a “point in time” snapshot training set. But these approaches are often hard or impossible to apply to real world scenarios.Symbolic AI is an alternative to deep learning that seeks to explicitly model truths about the real world and then allow machines to programmatically apply formal logic to these truths to draw conclusions. It has the huge advantage of being able to reason about scenarios it has never seen before. While it has been out of fashion for many years due to reliability concerns, it has had a resurgence in the last couple of years as researchers grapple with the shortcomings of deep learning.These types of scientific advances are a ways off and will require time to mature to the point they can be utilized in the real world.The immediate, pragmatic solution is to make your AI observable. Systematic model monitoring allows you to immediately assess the shifts in the data feeding your models, and understand the degree of impact it is having on model decisioning. With this knowledge you can respond immediately, smartly prioritizing model performance improvements to ensure your business maintains maximum continuity.Going forwardPeople who have worked in data science long enough have battle scars from model performance loss. Sometimes it’s quick, such as when you encounter a black swan event. More frequently, it’s gradual. Often the gradual degradation is worse because it flies under the radar for a long time.A lot will change as a result of COVID-19. People are already questioning social norms that go back centuries -- handshakes, sporting events, working in an office, etc. For those of us in the ML community this is a wake up call to build systems that are resilient. That starts with building AI that is observable. Knowing when your AI is struggling to adapt to rapid change and which models are the most impacted is critical for real world systems. And longer term, continued research into more adaptable approaches to AI such as symbolic AI and reinforcement learning will be paramount. Automated systems are creating extraordinary value -- making them more observable will ensure that continues even when the world shifts.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/ai-during-black-swan-events
233,"How Explainable AI and Bias are Interconnected
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessHow Explainable AI and Bias are InterconnectedBy: Keegan HinesApril 24, 2020In the world of artificial intelligence, explainability has become a contentious topic. One view among machine learning experts is that the less a model can be interpreted, the more accurate it will be. Some fear that adoption of explainable AI will slow down the adoption of machine learning. However, explainability doesn’t seek to slow down advancements in AI - it seeks to make that advancement fairer and safer for both everyday people and businesses implementing the AI. Explainability also goes hand-in-hand with decreasing bias in AI.How important is the interpretability of your taxi driver?Yann Lecun, VP & Chief AI Strategist at Facebook, once asked the question, “How important is the interpretability of your taxi driver?” On the surface, this thought experiment might seem to be an explainability killer. As long as you get from point A to point B safely, who cares how and why the taxi driver got you there? But this is a red herring -- of course you want to understand your taxi driver’s choices! Otherwise, what’s to stop someone from taking a longer route that makes you late or costs your more money? Also, the taxi driver needs to be able to explain their actions to protect themselves against any legal issues.Explainable AI aims to address the lack of information around how AI decisions are made. It is important because when an uninterpreted model fails, those in vulnerable groups are most likely to be negatively impacted, as we saw in the Gender Shades study. In the study, it was revealed that three top AI companies performed better on men than women and especially performed the worst on women with dark skin tones. Without explainability, discriminatory decisions go unchecked.There are also growing legal issues surrounding AI that explainability seeks to address. The European Union, for example, has expanded guidelines around automated decision making rights under GDPR. Meanwhile, in the US, certain decisions like those based on creditworthiness are also subject to right to explanation laws. As regulations catch up with technologies, the need to be able to explain AI decisions will become increasingly important.Plus, when you do get anomalous decisions from a model, that often does reflect something in the world that wasn’t picked up on during model development. It’s important that when this happens data scientists know what data or new features are needed to have a properly trained model.A lack of clarity around AI decisions can lead to frustrations from users who aren’t able to understand why a decision was made and even legal implications if black box decisions further discrimination of protected groups. This underscores the importance of understanding AI predictions better to develop AI that are more fair and helpful for society.Ignorance is not bliss.There’s a real and very valid concern around the legality of implementing black box models in sensitive areas. If a police officer catches someone based on an algorithm, how do they explain that that wasn’t based on prior bias ? If an algorithm tells a doctor a patient is dying, but the AI is wrong, who’s liable for the incorrect diagnosis -- the doctor using the algorithm? The company that sold the software? or the architects of the original algorithm?For example, look at the now-scrapped HR system Amazon built:Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry. In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter. They did not specify the names of the schools. (Reuters)Even though Amazon edited the programs to make them neutral to these particular terms, there was still no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory. This led to Amazon ultimately disbanding the team with recruiters not able to trust the system enough to rely on it solely, leading executives to lose hope for the project. And herein is one of biggest arguments for explainability: it allows for the safe, responsible adoption of AI. Explainability could’ve helped these developers catch this disparity prior to deployment, and consequently, a lot more women would have had opportunities to be screened.Explainability will likely continue to be a push and pull in the AI world. However, we’re already seeing regulations like GDPR push explainable AI forward. Being able to interpret AI remains key to addressing the lack of trust around black box decisions, avoiding vulnerabilities in models, and decreasing the amount of human bias in machine learning.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/how-explainable-ai-and-bias-are-interconnected
234,"3 Reasons Model Monitoring is Vital for Strong AI Performance
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedML Model Monitoring3 Reasons Model Monitoring is Vital for Strong AI PerformanceBy: Keegan HinesApril 16, 2020Model monitoring is key for having continually high-performing artificial intelligence models in place. As we know, AI models make predictions based on historical data. However, as the world changes, that historical data becomes less and less relevant, which can lead to incorrect predictions and business outcomes . It’s important to understand the ways in which models fail, so you can become aware the moment it happens, and update your models to reflect the new conditions in production . To help with understanding how model monitoring can be put into use, we’ve outlined 3 of the top ways data issues can cause AI performance loss below.1. A Changing WorldWith a few exceptions, machine learning models do not learn on their own. Businesses rely on humans and automated systems to deploy updates. And while the world naturally changes and evolves, your model can fall more and more out of tune with reality . This is data drift, and it can happen in a few different ways .First, a models may be inadvertently used in ways that data scientists did not intend. For instance, suppose your business sells skincare products targeted towards a more mature clientele, and you use AI to manage your ad spend. Your data is likely built up using demographic information about your original age target. However, if you launch a new product targeted towards teens and begin using those models without updating the training data, your advertising will likely underperform.Second, the world isn’t static -- it evolves, constantly. This means past data points aren’t always good indicators for future predictions. For example, let’s say you run an app with a built-in messaging feature. You may implement a model to predict and suppress spam in order to improve the user experience. However, over time, spammers will likely figure out what your model looks for and change their behavior to subvert your models.2. Extreme Novel EventsGood and bad novel moments happen all the time - we’re living through one right now with COVID19. And these events can have a large impact on businesses. As a lighter example, there’s the event of sudden, and unexpected popularity .Take, for example, dalgona coffee. Thanks to this viral TikTok coffee trend, searches for instant coffee on Google grew by more than 700% in about a week.Retailers online and offline often have models that use historical data to predict how much stock they should carry of any given product, but that data doesn’t take into account sudden trends - meaning lots of customers found themselves frustrated at the lack of stock available for purchase and tons of missed revenue for the business. Model monitoring, here, could have given these retailers advance notice of the incoming surge in purchases.3. Data Ecosystem ChangesUsually, you don’t just have one model feeding into one outcome. Instead, models feed into more models to make much more complex predictions and decisions. This means if you change even something seemingly innocuous like going from nulls to 0s in one model, it can have a big impact in the performance of your whole system. In many cases, changes may not be immediately catastrophic. However, these quieter inaccuracies are often what keep data scientists up at night because they’re much harder to spot and can cause huge monetary losses.Here, model monitoring is vital to catch those subtle (yet powerful) changes that would be otherwise overlooked. Those small changes and inconsistencies are easy to miss, but having a model monitoring system in place provides protection from data changes leading to model failures and lost revenue.Today, AI is vital for growing and scaling businesses. But we must be cognisant that AI is still quite brittle . Understanding the ways in which AI models can lose accuracy enables your business to monitor and address those issues before they cause real problems for your team. These guardrails ensure you will deploy a safe and reliable AI system within your business and get the most out of the models you’ve spent so much effort to deploy into production.Have questions about model monitoring? Shoot us an email at info@arthur.ai‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/3-reasons-model-monitoring-is-vital-for-strong-ai-performance
235,"Fairness in Machine Learning is Tricky
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedAI Bias & FairnessFairness in Machine Learning is TrickyBy:John DickersonApril 8, 2020Non-experts and experts alike have trouble even understanding popular definitions of fairness in machine learning — let alone agreeing on which definitions, if any, should be used in practice.Human decision-making processes are known to be biased. Look at the promotions process at a typical large company. In the words of Tomas Chamorro-Premuzic, the Chief Talent Scientist at one of the world’s largest staffing firms, “… most companies focus on the wrong traits, hiring on confidence rather than competence, charisma rather than humility, and narcissistic tendencies rather than integrity.”This observation isn’t new, and myriad other examples exist. In Australia, if the name on your resume “sounds” Middle Eastern or Chinese, you are less likely to be hired; in the US, replacing a male-sounding name with a female-sounding one on an otherwise identical CV can result in a lower chance of offer, as well as a lower starting salary. When giving a negative medical evaluation, doctors exhibit similar levels of implicit bias based on race and gender as the general public might — which is to say, quite a bit. And, while open and explicit redlining for banking and insurance in the US is no longer legal, there is evidence that its impacts are still implicitly felt today.Can we just take humans out of the loop and rely on cold, hard data? Initially, some proponents of automated decisioning techniques stemming from the data mining and machine learning (ML) communities pushed for this. But algorithms take data as input, along with whatever biases come along with how those data were — or weren’t — sampled. This includes: which features were stored, which humans made those decisions, and disparities in sample sizes of different subgroups of inputs, among others. Worse still, often any resultant discrimination is an emergent, i.e. learned, property of the system (rather than a hand-coded rule written by an algorithm designer), making identification and any partial mitigation of the issue more difficult.Remember our earlier examples of bias in human decision-making systems for promotions, hiring, healthcare, and banking? Their counterparts have all been found in the automated versions of those systems as well. So, bias in ML-based decisioning processes is ubiquitous, just like bias in human decision-making processes. Can we do anything about it? Yes, and we should — but it’s important to set our goals realistically, and keep both stakeholders and domain experts heavily and continually involved.One approach taken by the machine learning community is explicitly “defining fairness” — that is, proposing different metrics of fairness as well as approaches to encoding that into machine learning pipelines.Some of these definitions are explicitly or implicitly based on existing legal doctrine. For example, channeling Title VII of the Civil Rights Act of 1964, an algorithm is said to result in disparate impact if it adversely affects one group of people of a protected characteristic (aka “sensitive attribute”) over another. Similarly, that algorithm is said to result in disparate treatment if its decisioning is performed in part based on membership in a group. Then, one goal that a fairness in machine learning practitioner might have is to mathematically certify that an algorithm does not suffer from disparate treatment or disparate impact, perhaps given some expected use case or input distribution.Toward that end, let’s dive a bit deeper into disparate treatment — specifically, what does it mean to make a decision based on membership in a group? Well, the algorithm could formally discriminate, that is, take as input explicit membership in a group, and then use that in some way to determine its output. This is often illegal to do, so many systems already do not do this. Yet, given a rich enough dataset, membership in a protected group is almost surely redundantly encoded, at least to some extent, by other features. Indeed, unless the target is completely uncorrelated with membership in a protected group, given enough data a well-trained model will completely recapture the protected group membership’s impact on the target — without ever having explicit access to that particular (protected) feature!A standard example treats “race” as a protected group in a dataset including features such as “zip code.” Here, observing “zip code” alone often provides strong signal about “race” — even without explicit access to the “race” feature. So, do we remove “zip code” from the input as well? Maybe not, because it’s quite likely some other set of features correlate with “zip code,” too, and we’re back to the drawing board. Indeed, and in general, it’s not immediately clear how to write down a formal set of rules to enforce accepted legal definitions of fairness in decisioning systems.It’s not the point of this article to completely overview the state-of-the-art in definitions of fairness in machine learning; many definitions have been proposed,¹ and many people have written² about them. My point so far is that it’s tough or impossible to write down agreed-upon legal rules and definitions using formal mathematics — even for “simple” systems performing binary classification on relatively small, well-defined inputs. For the sake of discussion, though, let’s say we have decided on a definition of fairness,³ and we have been able to write it down using formal mathematics, ready to be put into our ML pipeline. Now what?Given a well-defined definition of fairness implemented in a machine-learning-based system, it is natural to ask what the people impacted by that system (i) understand about the system itself and (ii) think about the rules under which it is operating.Ditto with the operators of the system, as well as other stakeholders (e.g., policymakers, lawyers, domain experts). And, when different classes of stakeholder have different opinions about what “fairness” means, how should we manage that?Let’s start with a simpler setting: asking one class of stakeholder if they comprehend well-known definitions of fairness. In joint work⁴ with researchers at Maryland and Berkeley ICSI, we recently did just this: we created a metric to measure comprehension of three common definitions: demographic parity, equal opportunity, and equalized odds, and then evaluated it using an online survey with the goal of investigating relationships between demographics, comprehension, and sentiment.In our study, fairness definitions were presented in multiple real-world scenarios (e.g., in our vignette on hiring, demographic fairness was described as “[t]he fraction of applicants who receive job offers that are female should equal the fraction of applicants that are female. Similarly, the fraction of applicants who receive job offers that are male should equal the fraction of applicants that are male”). Then, comprehension and sentiment questions were asked. Some takeaways:Education is a strong predictor of comprehension, at least for the accessible explanations of fairness used in our study. The negative impacts of ML-based systems are expected to disproportionately impact some segments of society, for example by displacing employment opportunities for those with the least education. Thus, that already-at-risk group’s ability to effectively advocate for its members may be adversely impacted by lower comprehension.Weaker comprehension correlates with less negative sentiment toward fairness rules. One way to interpret this is that those with the lowest comprehension of fairness concerns in ML systems would also be the least likely to protest against it.One promising direction is to learn stakeholders’ views about fairness via simulation or observation of actions over time. Some research has been done in this space. for example, researchers at ETH Zürich fit functions to users’ preferences over a finite and pre-determined feature space using pairwise comparisons of simulated profiles. They found that the well-known notion of demographic parity aligned reasonably well with their human subjects’ responses. One outstanding issue in this study, and in most preference and moral value judgment aggregation studies in this space, is the lack of consideration of different classes of stakeholder. For example, how can we combine the input of a layperson with that of a (domain expert) doctor in a healthcare setting — especially when people’s judgments often disagree?So what do stakeholders of the same type want, in general? Perhaps unsurprisingly, leading technology firms such as Microsoft and Google have taken steps in this direction with respect to the applied production settings that their engineers encounter. For example:Researchers at Microsoft Research surveyed practitioners from 25 ML product teams in 10 major technology firms, and found (broadly) that the “fair ML” research literature focuses too specifically on methods to assess biases, and would benefit from focusing more broadly on the full machine learning pipeline. A striking example from their paper involved an image labeler in a computer vision application systematically labeling female doctors as “nurses,”⁵ which would then serve as “gold standard” input to any downstream algorithms.Researchers from Google recently published a case study of a fielded classification system where adverse actions are taken against examples predicted to be in the positive class (e.g., “if you are predicted to be a spammer, then I will deactivate your account”). They found that it is difficult to even measure a direct-from-the-literature definition (equality of opportunity), and then give a series of steps they took to build a more applicable tweak to that definition into their models.Research suggests that: (i) laypeople largely do not understand the accepted definitions of fairness in machine learning; (ii) those who do understand those definitions do not like them; (iii) those who do not understand them could be further marginalized; and (iv) practitioners are not being served well by the current focus of the fairness in ML community. It sounds negative, but there are explicit next steps to take to help mitigate these issues. Read on!Earlier, I asked if we could “do anything about it” when it comes to the difficulties — and often, the impossibilities — of deciding on, and enforcing, fairness in decisioning systems. The answer is, still, that we can — but responsibly, with input from all appropriate parties, and with an understanding that there is no panacea.Below, I give a non-exhaustive list of action items for researchers and practitioners in the “fair ML” space.We need to understand what (lay)people perceive to be fair decision making. Given an explicit definition of fairness, is it understood, and is it acceptable to a wide audience? If particular subgroups of the general population do not comprehend parts of automated systems that impact them, then they will be more easily disadvantaged.Dovetailing with the above, we need to understand what specialists perceive to be fair decision making as well — and what tools they would need to help them do their jobs. This could mean developing tools to help audit systems, or to better curate high-quality and well-sampled input datasets, or to permit faster exploratory data analysis (EDA) to help find holes in the input and output of prototype or deployed systems.We need techniques that, given a definition of fairness or of bias, can measure at enterprise-scale whether or not an ML-based system is adhering to that definition or those definitions — and, if not, (i) describe by how much and (ii) alert humans, when appropriate, if the system deviates beyond an acceptable level.Additionally, effective UI/UX will be required to allow stakeholders of all walks to comprehend the state-of-the-art in various fielded automated systems. Fielded systems ingest (incomplete) high-dimensional data and output high-dimensional data, over time. Communicating the state of a system vis à vis particular definitions of fairness and bias in a human-understandable way is paramount.Quoting directly from the Microsoft Research study discussed earlier, “[a]nother rich area for future research is the development of processes and tools for fairness-focused debugging.” Debugging tools with a fairness focus would help practitioners identify, e.g., under-sampled portions of an input dataset, or previously overlooked subgroups being adversely impacted by new decisioning rules.Finally, we need to develop shared languages between all involved parties, but particularly engineers, laypeople, and policymakers. Engineers implement, policymakers make society-wide rules — and laypeople are impacted by the interaction between the two. All three need to understand the wants, incentives, and limitations of the others through open and continuous communication.Throughout, it is important to balance prescriptive and descriptive approaches to understanding, measuring, and implementing fairness in machine-learning-based systems. Prescriptive approaches necessarily assume some consensus around what “should” occur under a (societally decided-upon) definition of fairness, whereas descriptive approaches focus more on uncovering what that consensus might be in the first place.Researchers and practitioners interested in fairness in machine learning — myself included! — have focused too much on the former, largely due to its amenability to mathematical characterization. Yet, the latter is nowhere close to understood, and is an absolutely necessary complement if not precedent to more formal prescriptive approaches. That will, of course, require in-depth discussions with stakeholders of all walks — laypeople, policymakers, politicians, ethicists, lawyers, and domain experts. Yet, these discussions will need to be complemented with accurate and scalable techniques that measure and communicate real-world systems’ adherence to various definitions of bias and fairness in machine learning — so that they can provide human feedback to further improve automated decision systems performance in practice.Thanks to Michelle Mazurek, Liz O’Sullivan, and Monica Watson for comments on earlier versions of this piece.[1] I’d urge you to check out the (free, ever-updating) Fairness and Machine Learning book by field experts Barocas, Hardt, & Narayanan, a formally published overview by to-be-ArthurAI researcher Verma & Rubin, or the proceedings of area-specific conferences such as FAccT and AIES.[2] I’d typically also recommend Wikipedia, but at the time of writing, the Wikipedia page for Fairness (machine learning) is a bit of a mess, with fourteen different binary-classification-centric fairness criteria roughly defined amongst a mess of mathematics and essentially nothing else. This one sentence in the third paragraph of the introduction really sums it up, though: “The algorithms used for assuring fairness are still being improved.” Still a ways to go![3] This is a pretty strong assumption! Indeed, it’s almost always impossible to create a system that ensures three “reasonable” definitions of fairness, even in binary classification: calibration, a form of proportional treatment based on relative group size; balance for the negative class, which roughly states that people in the (true) “zero” class should be scored the same; and, balance for the positive class, which is its complement for the (true) “one” class.[4] Our study is ongoing. The working paper is available as “Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,” with authors Debjani Saha, Candice Schumann, Duncan McElfresh, John Dickerson, Michelle Mazurek, and Michael Tschantz. An initial report on our study, titled “Human Comprehension of Fairness in Machine Learning,” appeared at the 2020 ACM/AAAI Conference on AI, Ethics, & Society (AIES-20).[5] Here, the practitioners used a “failsoft” solution to mitigate this input bias and combined the “nurse” and “doctor” labels in their input dataset. Yet, without explicit monitoring, this systematic labeling error likely would’ve gone undiscovered.‍PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/fairness-in-ml
236,"CB Insights AI 100
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedCompany UpdatesCB Insights AI 100By:Arthur TeamMarch 5, 2020Today we are honored to announce that we've been included in the CB Insights AI 100. We’re thrilled that CB Insights recognizes the value we are bringing to the leading companies in healthcare, finance, technology, and other industries who trust us with their AI monitoring. CB Insights has long been a signal of the best companies to follow in every category, with incredible companies filling in the top 100 list each year. We believe in the amazing potential of AI to transform the world, and are excited to be doing our part to ensure this transformation is done in a way that is safe, reliable, and fair. With so many incredible companies lined up next to us on the top 100 list, the future of AI looks bright. We're excited to be a part of it.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/cbinsights
237,"Team Arthur at NeurIPS-19: A Retrospective
Solutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressSolutions
EvaluationFirewallObservabilityProducts
The Most Robust Way to Evaluate LLMs
The First Firewall for LLMs
The Complete AI Performance Solution
Fast, Safe, Custom AI for BusinessModels
LLMNLPCVTabularResources
BlogGAPDocsCompany
R&DTeamCareersNews / PressRequest a demoSign InSign InGet StartedEventsTeam Arthur at NeurIPS-19: A RetrospectiveBy:John DickersonDecember 17, 2019Arthur is fresh off the plane returning from NeurIPS, AI’s largest — and somewhat infamous — research conference. While there, Arthur announced its seed round and hosted a 50-person model monitoring meetup next to the convention center. Beyond that, the full NeurIPS was seven packed days of new advances and directional changes in the machine learning community. Here’s what the experience was like for us, written by Arthur Chief Scientist, John Dickerson.I’ll start with a broad takeaway: NeurIPS this year was welcoming.In previous years, the conference was known for being somewhat rowdy, coming to a head in 2017/18 with controversies that led, in part, to the conference being renamed. With 13,000 attendees this year, my worry was that things would feel even more chaotic. But, a largely inclusive and constructive tone was set via community-wide norm changes led by leaders in the field, community-building events such as Black in AI and Queer in AI, and workshops such as the ever-growing Women in Machine Learning. This progress is great to see, as it is increasingly evident that AI algorithms are influenced not just by the raw data fed into them, but also the traits and experiences of those who build them.Along those lines, bias, fairness, and explainability were front and center in many talks, posters, and workshops. One of my favorite talks, given by Aaron Roth from Penn, blended ideas from individual and statistical (aka “group”) notions of fairness. The former constrains treatment across pairs of inputs (e.g., “similar individuals are treated similarly” due to Dwork et al.), while the latter partitions inputs into groups (e.g., based on age, race, or sex) and enforces equalization of various metrics across those groups. Both of these approaches have well-known pros and cons. Group-level fairness is relatively easy to define and enforce on general input distributions, but may result in unexpected behavior as one looks at sub-groups (e.g., inputs with a specific race and sex). Individual notions of fairness, unsurprisingly, come with attractive guarantees at the individual input level, but may be hard to define and incorporate into models. Aaron and colleagues’ work provides a natural middle ground, where explicit group selection is no longer needed and some level of guarantee (in expectation) is given to individual inputs; check out the paper!Explainability was also center stage across the board, ranging from workshops such as Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness, and Privacy with panel participants affirming the financial services industry’s need for trustworthy and explainable systems, to industry expo days like Fairness and Explainability: From Ideation to Implementation to domain-specific papers and demos focusing on computer vision and natural language processing tasks. Understanding why models make particular inferences is important for everything from debugging systems to adhering to regulatory requirements, and it was great to see the research community stepping up to provide new tools that will, hopefully, find their way into industry.This was a big year for NLP and, specifically, all things BERT (Bidirectional Encoder Representations from Transformers, from Google) — and all kinds of visualizations of and attacks on BERT and friends.¹Over the last couple of years especially, NLP has seen explosive progress, and that was showcased at this year’s NeurIPS as well.Panel at the NeurIPS-19 “Minding the Gap: Between Fairness and Ethics” workshop.Probably the biggest NLP paper presented XLNet (also from Google), which dominates BERT on a number of tasks. It’s exciting to see NLP move forward so quickly, of course, but many — including big names in the field — are starting to get grouchy² about the field’s seeming obsession with making already big models even bigger. This Twitter thread by Yoav Goldberg is a nice place to start for under-loved holes in NLP research, ranging from theory to explainability to generalization to incorporating linguistic theory back into deep-learned-based techniques.The generalization capabilities of deep networks are poorly understood — they famously don’t align with the traditional statistical view that, after a point, bigger models are worse. Rather, they tend to exhibit a “double descent”, where as a model grows in size test error decreases, then increases, then decreases again. At least two nice papers came out in this “understanding deep learning generalization” space: the winner of the Outstanding New Directions Paper Award from researchers at CMU argues against a particular class of theoretical bounding technique, and another nice paper from Princeton and CMU advanced the state of understanding what larger (namely, infinitely larger) networks can represent.At the NeurIPS Retrospectives Workshop on Friday, leading researchers asked about their past work, “What should readers of this paper know now, that is not in the original publication?”I love this idea and hope it takes off — researchers, especially younger researchers (when many do some of their “most famous” work!), often write papers from a single perspective. As their research gains prominence, folks from other fields inevitably chime in with references to earlier related work or methods for better framing. This feels personal to me: before heading off to do my PhD, I submitted my first research paper and was informed a few weeks later that a nearly-identical problem had been solved by researchers in the USSR and at RAND in the US in the 1970s, available online in poorly-scanned PDFs — in both Russian and English!It turns out Michael Littman, a reinforcement learning luminary, had a similar experience. In the 1990s, he published seminal and extremely highly-cited work (pdf) introducing “Markov games” to the CS community, which helped spur thinking in multi-agent reinforcement learning. In his retrospective on that paper, he mentioned wishing he’d known about “stochastic games” — effectively the same idea, but introduced in the 1950s (!!), and with more theoretical rigor. Still, Mike states he is happy he wrote the paper — as is the community, because this pushed forward interest in multi-agent reinforcement learning, a topic of increasing importance today in application areas ranging from simple game playing to energy production and disaster relief operations. AI is such an interdisciplinary field that this type of cross-pollination is more a blessing than a curse; indeed, we’d all benefit from looking — and talking! — beyond traditional technology fields.Also on Friday, our co-founder Liz spoke on a panel at the Minding the Gap: Between Fairness and Ethics workshop, alongside a variety of research scientists and engineers from Google focusing on ethics, fairness, and AI. The workshop featured practitioners and, in general, participants from outside the traditional AI/ML communities, which led to in-depth conversations about the gulf between what “standard” ML-based definitions of fairness offer and what practitioners might want or at least find practical to use in their day-to-day jobs.While the AI/ML community (including myself) has started to address this divide, there’s clearly a ton to be done when it comes to properly incorporating the wants and needs of stakeholders into modern AI systems.I spent Saturday at the CausalML [b1] aka Do the Right Thing: Machine Learning and Causal Inference for Improved Decision Making workshop. Invited speakers such as Susan Athey and Susan Murphy connected techniques for uncertainty management from the AI/ML world with application areas in business, including labor markets and advertising, and healthcare. I found the poster sessions in this workshop particularly enlightening: causal and counterfactual reasoning are two intertwined topics that are forming the basis for both fair and robust (e.g., to adversarial manipulations, or to simple noise in data) automated systems, and I look forward to these concerns and ideas melting more into the greater ML community.Finally, what would a large AI/ML conference be without a little drama?Bengio and Schmidhuber, two deep learning visionaries from different continents, stay at loggerheads about who should give whom credit about what. I won’t dignify linking to the /r/machinelearning and Blind threads discussing this, but suffice it to say that the “who did it first in deep learning” rabbit hole is still getting, ah, deeper.All in all, NeurIPS this year captured the largely positive zeitgeist of the machine learning and AI community. After a period of extreme growth, the conference — and, by proxy, the community — feels like it’s growing, if not grown, up. Yet, NeurIPS’ workshops, events, and keynotes captured another, more subtle thread: as the impact of AI continues to spread — and the community continues to learn and understand how that spread impacts society — researchers and practitioners alike are desperate for tools and guidance about how to integrate models safely and responsibly. There’s so much left to be done regarding not just the scalability and generalizability of modern ML methods, but also the ability to define and incorporate notions of fairness, bias, trustworthiness, and accountability into models and pipelines in ways that are interpretable at both train/test time as well as in deployment. I’d love to see more research and development time spent on&$58;Crossing boundaries and discussing exactly what industry practitioners and other stakeholders want and need. The human-computer interaction (HCI) and AI/ML communities are starting to build this knowledge out, but what we need is a feedback loop between stakeholders and researchers formalizing the communication pipelines between both parties. This is the only way we will settle on meaningful definitions of what it means to approach “fairness” in different domainsTracking and analyzing the impact of implementing various objectives or constraints (e.g, promoting combinations of fairness, diversity, and economic efficiency) on truly dynamic systems, that is, systems where the input data distribution drifts over time, metrics change, and so on.Simply put, measuring things. A discussion we have constantly at Arthur revolves around which metrics we should (i) show by default, (ii) pre-compute and allow to be toggled on or off, (iii) don’t pre-compute but allow to be computed, and (iv) leave to the user to define and pass to our system as a custom metric. We need to help industry understand what it needs, then develop scalable methods to measure that, and then integrate those metrics and measurements in live systems to track model performance. That means not just the so-called efficiency metrics such as accuracy, precision, and f-score, but also measures for fairness, bias, diversity, explainability, and anything else stakeholders might want.That last point is a multidisciplinary one, and — frankly — there’s no way to escape that. As AI practitioners, we aim to build systems for users in myriad fields, and we build them so they (i) work and (ii) work in a way that users understand. The more progress the field makes toward understanding and then building scalable and general methods, the better. Those methods need to consider the competing and sometimes contradictory wants of stakeholders: efficiency, fairness, robustness, explainability, and even justice. It’s not a simple problem, and it’s not one that can be solved by the AI/ML community alone. I look forward to the work and discussion that will come from future AI/ML conferences — such as AIES and AAAI in NYC, Arthur’s home city, in early 2020![1] Or, in this case, attacks on BERT by friends. I’m a fan of bad jokes, and approve of the team from AI2 and UW building Grover, a fake news generator.[2] There’s an Oscar the Grouch joke in here somewhere.PREVIOUS POSTSHAREnext post
We make AI better for everyone.Sign up for our newsletter to get the latest Arthur news!SubscribeProductShieldBenchScopeChatLLMNLPCVTabularR&DResourcesBlogGAPDocumentationcompanyTeamCareersNewsPress InquiriesARTHUR 2023 © ALL RIGHTS RESERVEDTerms of ServicePrivacy",arthur_blog,https://www.arthur.ai/blog/team-arthur-at-neurips-19-a-retrospective
